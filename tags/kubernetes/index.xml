<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on From 0.985mhz... to several Ghz</title>
    <link>https://blog.andreasm.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on From 0.985mhz... to several Ghz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Myself</copyright>
    <lastBuildDate>Tue, 16 Apr 2024 09:25:28 +0200</lastBuildDate><atom:link href="https://blog.andreasm.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring some Antrea Features</title>
      <link>https://blog.andreasm.io/2024/04/16/exploring-some-antrea-features/</link>
      <pubDate>Tue, 16 Apr 2024 09:25:28 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2024/04/16/exploring-some-antrea-features/</guid>
      <description>
        
          
            In this post I will go through a couple of Antrea features I find interesting, some new features and some older features
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere with Tanzu - Avi and Multiple Supervisors</title>
      <link>https://blog.andreasm.io/2024/04/08/vsphere-with-tanzu-avi-and-multiple-supervisors/</link>
      <pubDate>Mon, 08 Apr 2024 14:56:56 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2024/04/08/vsphere-with-tanzu-avi-and-multiple-supervisors/</guid>
      <description>
        
          
            In this post I will try to describe how to use the new NSX and Avi together integration in vSphere with Tanzu in combination with several Supervisor clusters using only 1 NSX manager cluster and 1 Avi controller cluster
          
          
        
      </description>
    </item>
    
    <item>
      <title>Argo CD &amp; vSphere with Tanzu</title>
      <link>https://blog.andreasm.io/2024/02/04/argo-cd-vsphere-with-tanzu/</link>
      <pubDate>Sun, 04 Feb 2024 11:28:21 +0100</pubDate>
      
      <guid>https://blog.andreasm.io/2024/02/04/argo-cd-vsphere-with-tanzu/</guid>
      <description>
        
          
            In this post I am using ArgoCD bootstrapping Tanzu Kubernetes Clusters with AKO and Antrea configs
          
          
        
      </description>
    </item>
    
    <item>
      <title>Proxmox with OpenTofu Kubespray and Kubernetes</title>
      <link>https://blog.andreasm.io/2024/01/15/proxmox-with-opentofu-kubespray-and-kubernetes/</link>
      <pubDate>Mon, 15 Jan 2024 14:15:19 +0100</pubDate>
      
      <guid>https://blog.andreasm.io/2024/01/15/proxmox-with-opentofu-kubespray-and-kubernetes/</guid>
      <description>
        
          
            In this post I will quickly go through how I made use of OpenTofu to provision VMs on my Proxmox cluster and Ansible using Kubespray to deploy my Kubernetes clusters on demand.
          
          
        
      </description>
    </item>
    
    <item>
      <title>A Quick Glance at Cilium CNI</title>
      <link>https://blog.andreasm.io/2023/12/18/a-quick-glance-at-cilium-cni/</link>
      <pubDate>Mon, 18 Dec 2023 11:49:56 +0100</pubDate>
      
      <guid>https://blog.andreasm.io/2023/12/18/a-quick-glance-at-cilium-cni/</guid>
      <description>
        
          
            In this post I will deploy Cilium in my k8s cluster and test some features and how it is to manage
          
          
        
      </description>
    </item>
    
    <item>
      <title>Antrea Multi-cluster - in TKG and vSphere with Tanzu</title>
      <link>https://blog.andreasm.io/2023/09/25/antrea-multi-cluster-in-tkg-and-vsphere-with-tanzu/</link>
      <pubDate>Mon, 25 Sep 2023 10:59:30 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2023/09/25/antrea-multi-cluster-in-tkg-and-vsphere-with-tanzu/</guid>
      <description>
        
          
            In this post I will go through how to configure and use the Antrea feature gate Multi-cluster in both TKGs and TKG
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere with Tanzu 8 U2 using NSX AND NSX Advanced Loadbalancer</title>
      <link>https://blog.andreasm.io/2023/09/23/vsphere-with-tanzu-8-u2-using-nsx-and-nsx-advanced-loadbalancer/</link>
      <pubDate>Sat, 23 Sep 2023 21:35:17 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2023/09/23/vsphere-with-tanzu-8-u2-using-nsx-and-nsx-advanced-loadbalancer/</guid>
      <description>
        
          
            A quick test on the new feature in vSphere 8 U2 with Tanzu using NSX and NSX Advanced Loadbalancer for Kubernetes API endpoint
          
          
        
      </description>
    </item>
    
    <item>
      <title>TKG Autoscaler</title>
      <link>https://blog.andreasm.io/2023/09/07/tkg-autoscaler/</link>
      <pubDate>Thu, 07 Sep 2023 07:46:13 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2023/09/07/tkg-autoscaler/</guid>
      <description>
        
          
            In this post I will go through the TKG Autoscaler, how to configure it and how it works.
          
          
        
      </description>
    </item>
    
    <item>
      <title>TMC Self-Managed Upgrade to 1.0.1</title>
      <link>https://blog.andreasm.io/2023/09/06/tmc-self-managed-upgrade-to-1.0.1/</link>
      <pubDate>Wed, 06 Sep 2023 09:34:04 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2023/09/06/tmc-self-managed-upgrade-to-1.0.1/</guid>
      <description>
        
          
            In this post I will go through how to upgrade TMC Self-Managed from version 1.0.0 to version 1.0.1
          
          
        
      </description>
    </item>
    
    <item>
      <title>Securing Kubernetes clusters with Antrea Network Policies</title>
      <link>https://blog.andreasm.io/2023/06/21/securing-kubernetes-clusters-with-antrea-network-policies/</link>
      <pubDate>Wed, 21 Jun 2023 10:01:46 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2023/06/21/securing-kubernetes-clusters-with-antrea-network-policies/</guid>
      <description>
        
          
            In this post I will go through how to utilize Antrea Network policies with Tanzu Mission Control and a little bit NSX. So jump in and hopefully get some ideas how what we can do with Antrea Network Policies and how to use them.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tanzu Kubernetes Grid 2.1</title>
      <link>https://blog.andreasm.io/2023/03/22/tanzu-kubernetes-grid-2.1/</link>
      <pubDate>Wed, 22 Mar 2023 09:24:01 +0100</pubDate>
      
      <guid>https://blog.andreasm.io/2023/03/22/tanzu-kubernetes-grid-2.1/</guid>
      <description>
        
          
            Tanzu Kubernetes Grid This post will go through how to deploy TKG 2.1, the management cluster, a workload cluster (or two), and the necessary preparations to be done on the underlaying infrastructure to support TKG 2.1. In this post I will use vSphere 8 with vSAN, Avi LoadBalancer, and NSX. So what we want to end up with it something like this:
Preparations before deployment This post will assume the following:
          
          
        
      </description>
    </item>
    
    <item>
      <title>Antrea Egress</title>
      <link>https://blog.andreasm.io/2023/02/20/antrea-egress/</link>
      <pubDate>Mon, 20 Feb 2023 15:42:54 +0100</pubDate>
      
      <guid>https://blog.andreasm.io/2023/02/20/antrea-egress/</guid>
      <description>
        
          
            Antrea Egress: What is Egress when we talk about Kubernetes? Well if a pod wants to communicate to the outside world, outside the Kubernetes cluster it runs in, out of the worker node the pod resides on, this is egress traffic (definition &amp;quot;the action of going out of or leaving a place&amp;quot; and in network terminology means the direction is outward from itself).
Why does egress matter? Well, usually when the pods communicate out, they will use the IP address of the worker node they currently is deployed on.
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere 8 with Tanzu using VDS and Avi Loadbalancer</title>
      <link>https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-vds-and-avi-loadbalancer/</link>
      <pubDate>Wed, 26 Oct 2022 12:06:08 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-vds-and-avi-loadbalancer/</guid>
      <description>
        
          
            Deploy Tanzu in vSphere 8 with VDS and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using vSphere VDS networking and Avi as loadbalancer. The goal is to deploy Tanzu by using vSphere Distributed Switch (no NSX this time) and utilize Avi as loadbalancer for Supervisor and workload cluster L4 endpoint (kubernetes API). When that is done I will go through how we also can extend this into L7 (Ingress) by using AKO in our workload clusters.
          
          
        
      </description>
    </item>
    
    <item>
      <title>vSphere 8 with Tanzu using NSX-T &amp; Avi LoadBalancer</title>
      <link>https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-nsx-t-avi-loadbalancer/</link>
      <pubDate>Wed, 26 Oct 2022 12:03:35 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-nsx-t-avi-loadbalancer/</guid>
      <description>
        
          
            Deploy Tanzu in vSphere 8 with NSX and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using NSX networking (including built in L4 loadbalancer) and Avi as L7 loadbalancer. The goal is to deploy Tanzu by using NSX for all networking needs, including the Kubernetes Api endpoint (L4) and utilize Avi as loadbalancer for all L7 (Ingress). The deployment of Tanzu with NSX is an automated process, but it does not include L7 loadbalancing.
          
          
        
      </description>
    </item>
    
    <item>
      <title>AKO Explained</title>
      <link>https://blog.andreasm.io/2022/10/26/ako-explained/</link>
      <pubDate>Wed, 26 Oct 2022 12:02:39 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/26/ako-explained/</guid>
      <description>
        
          
            What is AKO? AKO is an operator which works as an ingress controller and performs Avi-specific functions in an OpenShift/Kubernetes environment with the Avi Controller. It runs as a pod in the cluster and translates the required OpenShift/Kubernetes objects to Avi objects and automates the implementation of ingresses/routes/services on the Service Engines (SE) via the Avi Controller. ref: link
How to install AKO AKO is very easy installed with Helm. Four basic steps needs to be done.
          
          
        
      </description>
    </item>
    
    <item>
      <title>We Take a Look at the AKO Crds</title>
      <link>https://blog.andreasm.io/2022/10/23/we-take-a-look-at-the-ako-crds/</link>
      <pubDate>Sun, 23 Oct 2022 08:21:52 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/23/we-take-a-look-at-the-ako-crds/</guid>
      <description>
        
          
            AKO settings: What happens if we need to to this
What happens if I need passthrough
How does AKO work
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hugo in Kubernetes</title>
      <link>https://blog.andreasm.io/2022/10/12/hugo-in-kubernetes/</link>
      <pubDate>Wed, 12 Oct 2022 08:28:23 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/12/hugo-in-kubernetes/</guid>
      <description>
        
          
            This blog post will cover how I wanted to deploy Hugo to host my blog-page.
Preparations To achieve what I wanted, deploy an highly available Hugo hosted blog page, I decided to run Hugo in Kubernetes. For that I needed
Kubernetes cluster, obviously, consisting of several workers for the the &amp;quot;hugo&amp;quot; pods to run on (already covered here. Persistent storage (NFS in my case, already covered here) An Ingress controller (already covered here) A docker image with Hugo, nginx and go (will be covered here) Docker installed so you can build the image A place to host the docker image (Docker hub or Harbor registry will be covered here) Create the Docker image Before I can deploy Hugo I need to create an Docker image that contains the necessary bits.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Pinniped Authentication Service</title>
      <link>https://blog.andreasm.io/2022/10/11/pinniped-authentication-service/</link>
      <pubDate>Tue, 11 Oct 2022 22:39:07 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/11/pinniped-authentication-service/</guid>
      <description>
        
          
            How to use Pinniped as the authentication service in Kubernets with OpenLDAP
Goal: Deploy an authentication service to handle RBAC in Kubernetes Purpose: User/access management in Kubernetes
Pinniped introduction 
          
          
        
      </description>
    </item>
    
    <item>
      <title>Cert Manager and Letsencrypt</title>
      <link>https://blog.andreasm.io/2022/10/11/cert-manager-and-letsencrypt/</link>
      <pubDate>Tue, 11 Oct 2022 22:33:41 +0200</pubDate>
      
      <guid>https://blog.andreasm.io/2022/10/11/cert-manager-and-letsencrypt/</guid>
      <description>
        
          
            This article will quickly go through how to create wildcard certificates and automatically renew them with Lets Encrypt and Cert-Manager
Cert-Manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.
It can issue certificates from a variety of supported sources, including Let&#39;s Encrypt, HashiCorp Vault, and Venafi as well as private PKI.
It will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry.
          
          
        
      </description>
    </item>
    
    <item>
      <title>Kubernetes Persistent Volumes with NFS</title>
      <link>https://blog.andreasm.io/2021/07/12/kubernetes-persistent-volumes-with-nfs/</link>
      <pubDate>Mon, 12 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.andreasm.io/2021/07/12/kubernetes-persistent-volumes-with-nfs/</guid>
      <description>
        
          
            Use NFS for your PVC needs If you are running vShere with Tanzu, TKG on vSphere or are using vSphere as your hypervisor for your worker-nodes you have the option to use the vSphere CSI plugin here. In Tanzu this is automatically configured and enabled. But if you are not so privileged to have vSphere as your foundation for your environment one have to look at other options. Thats where NFS comes in.
          
          
        
      </description>
    </item>
    
    <item>
      <title>K8s Ingress with NSX Advanced Load Balancer</title>
      <link>https://blog.andreasm.io/2021/07/11/k8s-ingress-with-nsx-advanced-load-balancer/</link>
      <pubDate>Sun, 11 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.andreasm.io/2021/07/11/k8s-ingress-with-nsx-advanced-load-balancer/</guid>
      <description>
        
          
            Abbreviations used in this article:
NSX Advanced Load Balancer = NSX-ALB K8s = Kubernetes (8 letters between the K and s in Kubernetes) SSL = Secure Sockets Layer AKO = Avi Kubernetes Operator (AVI now a VMware product called NSX Advanced Load Balancer) In one of my previous posts I wrote about how to install and configure AKO (Avi Kubernetes Operator) to use as Service type LoadBalancer.
This post will try to cover the basics of how to use NSX Advanced LoadBalancer by using AKO to handle our Ingress requests (ingress-controller).
          
          
        
      </description>
    </item>
    
    <item>
      <title>Antrea - Kubernetes CNI</title>
      <link>https://blog.andreasm.io/2021/07/10/antrea-kubernetes-cni/</link>
      <pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.andreasm.io/2021/07/10/antrea-kubernetes-cni/</guid>
      <description>
        
          
            This is an introduction post to Antrea, what it is and which features it has.
For more details head over to:
https://antrea.io/ and https://github.com/antrea-io/antrea
First of, Antrea is a CNI. CNI stands for Container Network Interface. As the world moves into Kubernetes more and more, we need a good CNI to support everything from network to security within Kubernetes. Thats where Antrea comes into play.
Antrea has a rich set of features such as:
          
          
        
      </description>
    </item>
    
    <item>
      <title>NSX Advanced LoadBalancer with Antrea on Native K8s</title>
      <link>https://blog.andreasm.io/2020/10/08/nsx-advanced-loadbalancer-with-antrea-on-native-k8s/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blog.andreasm.io/2020/10/08/nsx-advanced-loadbalancer-with-antrea-on-native-k8s/</guid>
      <description>
        
          
            This post will cover the steps to bring up a Kubernetes cluster in Ubuntu, then how to install and configure Antrea as CNI and how to install and configure NSX Advanced Load Balancer to use as a servicetype Load Balancer in the k8s environment with the use of Avi Kubernetes Operator.
Abbreviations used in this post:
NSX Advanced Load Balancer = NSX ALB Avi Kubernetes Operator = AKO Kubernetes = k8s Container Network Interface = CNI Load Balancer = lb Introduction to this post When working with pods in a k8s cluster there is often the use of nodePort, clusterIP and LoadBalancer.
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
