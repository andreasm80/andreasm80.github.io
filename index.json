
[{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/","section":"blog.andreasm.io","summary":"","title":"blog.andreasm.io","type":"page"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/categories/encapsulation/","section":"Categories","summary":"","title":"Encapsulation","type":"categories"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/tags/evpn/","section":"Tags","summary":"","title":"Evpn","type":"tags"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/categories/evpn/","section":"Categories","summary":"","title":"EVPN","type":"categories"},{"content":" Ethernet Virtual Private Network # This post marks the begining of a series I am covering on EVPN. The first informational RFC(RFC7209) for EVPN was posted May 2014. It is just a coincidence that EVPN is now on its 10th year and I am writing this blog series. As I have the pleasure of working together with some of the best in the industry, which is motivating on its own, I highly encourage everyone to have a look at this video from my colleague Johan where he takes us brilliantly through EVPN\u0026rsquo;s 10 year history.\nAs this blog series will evolve I will cover EVPN and its different Route Types and EVPN gateways to get a better understanding of what they are, and why they are needed. But first, a quick introduction to EVPN in general. What is EVPN, what is EVPN\u0026rsquo;s responsibilities in the datacenter, and why EVPN. Lets go ahead and try to cast some light on this.\nLayer 2 over layer 3 # As the datacenter networking (and campus to a certain degree) has evolved from the traditional core, distribution and access topology to the much more scalable layer 3 spine-leaf topology, the need for layer 2 did not disappear. MAC mobility/vMotion of virtual machines, application requirements like cluster services, iot devices etc are examples that rely on layer 2 reachability. The move away from large broadcast domains in the networks to IP fabric solved a lot of challenges in terms of scalability, performance and resilience. In its own it was a case closed for any loops, broadcast storms scenarios and poor utilization. Though many services in your network still relies on layer 2 though, so how to solve that in a pure layer 3 IP fabric like spine-leaf?\nExisting services as VPLS and Ethernet L2VPN existed, but had its limitations.\nTo quote the RFC7209:\nThe widespread adoption of Ethernet L2VPN services and the advent of new applications for the technology (e.g., data center interconnect) have culminated in a new set of requirements that are not readily addressable by the current Virtual Private LAN Service (VPLS) solution. In particular, multihoming with all-active forwarding is not supported, and there\u0026rsquo;s no existing solution to leverage Multipoint-to-Multipoint (MP2MP) Label Switched Paths (LSPs) for optimizing the delivery of multi-destination frames. Furthermore, the provisioning of VPLS, even in the context of BGP-based auto- discovery, requires network operators to specify various network parameters on top of the access configuration. This document specifies the requirements for an Ethernet VPN (EVPN) solution, which addresses the above issues.\nEVPN, BGP and Network Virtualization Overlay - control plane vs dataplane # What is EVPN? The short answer is that EVPN is an extension to BGP. But that doesn\u0026rsquo;t help much does it? Lets see if I can go a bit deeper.\nIn a spine-leaf fabric all switches are connected using layer 3 links. If all the services connected to the fabric can do fine with layer 3 and be in their own IP subnet, I am fine. Traffic is routed as needed and full reachability is taken care of.\nIf I need to extend layer 2 beyond any leaf I am not so fine.\nWithout any form of mechanisms like overlay control-/and dataplane I am not able to extend my layer 2 network beyond a single leaf. So if I need layer 2 reachability across leafs in the same fabric I need mechanisms that can handle layer 2 over layer 3. The first thing we need is a protocol to create overlay networks that can tunnel, or simulate, layer 2 subnets over layer 3, this can be VXLAN as an example.\nBy having protocols like VXLAN to create overlay networks in place it provides the ability to extend my layer 2 subnets across my layer 3 fabric. Managing this at scale though can be hard if not the right mechanisms is in place to accommodate a very dynamic environment such as vm sprawl, multiple tenants etc, we need something that can manage the reachability information in a clever, controlled and scalable way and something that understands network virtualization overlay.\nI can use static routes then? Well, in theory, I could, but that would defeat the purpose of scalability and simpler management, leaving no reason to build a spine-leaf fabric to begin with.\nBGP? Yes, it is a very good start. But I cant use regular BGP.\nRegular BGP (RFC4271):\nThe only three pieces of information carried by BGP-4 [BGP-4] that are IPv4 specific are (a) the NEXT_HOP attribute (expressed as an IPv4 address), (b) AGGREGATOR (contains an IPv4 address), and (c) NLRI (expressed as IPv4 address prefixes).\nSource: https://datatracker.ietf.org/doc/html/rfc4760\nAs regular BGP can only handle IP addresses and IP prefixes it is not sufficient. We need something that can handle a richer set of reachability information, to be able to handle network reachability information when using overlay protocols like VXLAN.\nIn a spine-leaf fabric these layer 2 networks can be referred to as tenants, virtualized network segments or VPN instances. The need for multiple tenants could be to allow several layer 2 networks sharing the same physical underlay fabric, multiple customers and services sharing the same fabric and even overlapping layer 2 subnets. EVPN allows for these tenants to be configured and isloated in the shared fabric as it enables BGP to have MAC addresses and MAC plus IP addresses as routing entries. In a regular layer 2 switch network, the switches learn how to forward traffic by looking at the Ethernet Frame\u0026rsquo;s MAC headers. An Ethernet Frame consists of a source MAC header and destination MAC header. With an overlay protocol like VXLAN, the standard Ethernet frame is encapsulated adding some additional headers including the source and destination mac address per hop in the underlay, see short explanation below:\n# Ping between two clients attached to two separate leaf switches leaf1a and leaf2a - mac src and mac dst 1st hop (leaf1a): mac src leaf1a -\u0026gt; mac dst spine2 2nd hop (spine2): mac src spine2 -\u0026gt; mac dst leaf2a And the source and destination IP in the vxlan header will be the source and destination ip of the VTEPS:\n# Ping between two clients attached to two separate leaf switches leaf1a and leaf2a - src IP and dst IP 1st hop (leaf1a): src IP vxlan loopback interface leaf1a -\u0026gt; dst IP vxlan loopback interface leaf2a 2nd hop (spine2): src IP vxlan loopback interface leaf1a -\u0026gt; dst IP vxlan loopback interface leaf2a For more explanation: see my previous post on overlay\nWith EVPN we have a way to capture this additional information and add it to the routing table dynamically allowing the VXLAN tunnels to be established to relevant peers (VTEPS). The peers in VXLAN are called VTEPs, VXLAN Tunnel End Points.\nAfter the first informational RFC7209 that specified the intention and requirements with EVPN, just under a year later, the initial EVPN standard RFC7432 was posted. The initial EVPN standard (RFC7432) was written with MPLS in mind, however some years later (2018) the RFC8365 was posted describing how EVPN can be used as a network virtualization overlay in combinaton with various encapsulation options like VXLAN, NVGRE, MPLS over GRE and even Geneve. Now why do I mention these RFCs in addtion to the initial RFC7209 from 2014? Well stay tuned to get some clarity on this point.\nFirst, EVPN stands for Ethernet VPN. EVPN is capable of managing both layer 2 (Ethernet MAC) and IP (layer 3) reachability information. That means EVPN is not the carrier of the actual ethernet frames or IP packets, it just informs the interested parties whats the source and destination. And to do that, it uses BGP or rather MP-BGP (Multi-Protocol BGP RFC4760 an extension to the regular BGP). One of the benefits of BGP is that it does not rely on flood-and-learn in the dataplane, but uses instead a control-plane based learning allowing to restrict who learns what and the ability to apply policies. This reduces unnecessary flooding and is much more scalable. Wasn\u0026rsquo;t this a post about EVPN? Well EVPN uses BGP as its control plane, and the extension to BGP, MP-BGP, makes BGP capable of handle the advertising of additional EVPN routes. Suddenly there is MP-BGP into the mix?\nMP-BGP # Yes, it is. MP-BGP extends BGP to carry multiple address families, which is needed to handle overlay encapsulation that comes with the use of e.g VXLAN.\nTo provide backward compatibility, as well as to simplify introduction of the multiprotocol capabilities into BGP-4, this document uses two new attributes, Multiprotocol Reachable NLRI (MP_REACH_NLRI) and Multiprotocol Unreachable NLRI (MP_UNREACH_NLRI). The first one (MP_REACH_NLRI) is used to carry the set of reachable destinations together with the next hop information to be used for forwarding to these destinations. The second one (MP_UNREACH_NLRI) is used to carry the set of unreachable destinations. Both of these attributes are optional and non-transitive. This way, a BGP speaker that doesn\u0026rsquo;t support the multiprotocol capabilities will just ignore the information carried in these attributes and will not pass it to other BGP speakers.\nMultiprotocol Reachable NLRI - MP_REACH_NLRI (Type Code 14):\nThis is an optional non-transitive attribute that can be used for the following purposes:\n(a) to advertise a feasible route to a peer\n(b) to permit a router to advertise the Network Layer address of the router that should be used as the next hop to the destinations listed in the Network Layer Reachability Information field of the MP_NLRI attribute.\nThe attribute is encoded as shown below:\nSource: https://datatracker.ietf.org/doc/html/rfc4760\nMP-BGP Reachable NLRI +---------------------------------------------------------+ | Address Family Identifier (2 octets) | +---------------------------------------------------------+ | Subsequent Address Family Identifier (1 octet) | +---------------------------------------------------------+ | Length of Next Hop Network Address (1 octet) | +---------------------------------------------------------+ | Network Address of Next Hop (variable) | +---------------------------------------------------------+ | Reserved (1 octet) | +---------------------------------------------------------+ | Network Layer Reachability Information (variable) | +---------------------------------------------------------+ EVPN will use MP-BGP to add new BGP Network Layer Reachability Information (NLRI), the EVPN NLRI. This is where we will get a set of different EVPN route types, which I will cover in separate posts later. The EVPN NLRI is carried in BGP using MP-BGP with an Address Family Identifier of 25 (L2VPN) and a Subsequent Address Family Identifier of 70 (EVPN).\nThe different route types of EVPN will be covered in detail in their own as part of this EVPN blog post series.\nI will just quickly use EVPN route type 2, MAC/IP advertisement, as an example for clarification of how this fits in.\nThis is how a EVPN MAC/IP, route type 2, advertisement specific NLRI consists of (scroll down inside code viewer to see all):\n+---------------------------------------+ | RD (8 octets) | +---------------------------------------+ |Ethernet Segment Identifier (10 octets)| +---------------------------------------+ | Ethernet Tag ID (4 octets) | +---------------------------------------+ | MAC Address Length (1 octet) | +---------------------------------------+ | MAC Address (6 octets) | +---------------------------------------+ | IP Address Length (1 octet) | +---------------------------------------+ | IP Address (0, 4, or 16 octets) | +---------------------------------------+ | MPLS Label1 (3 octets) | +---------------------------------------+ | MPLS Label2 (0 or 3 octets) | +---------------------------------------+ All fields above are necessary to describe the complete EVPN route, though the fields Ethernet Segment Identifier, MPLS Label 1 and MPLS Label 2 are all in the NLRI, but not part of the route prefix used for route uniqueness. If reading the RFC7432:\nFor the purpose of BGP route key processing, only the Ethernet Tag ID, MAC Address Length, MAC Address, IP Address Length, and IP Address fields are considered to be part of the prefix in the NLRI. The Ethernet Segment Identifier, MPLS Label1, and MPLS Label2 fields are to be treated as route attributes as opposed to being part of the \u0026ldquo;route\u0026rdquo;. Both the IP and MAC address lengths are in bits.\nThis can be a bit confusing. In terms of BGP they are not to be seen as attributes in a traditional \u0026ldquo;BGP sense\u0026rdquo; like AS_PATH or COMMUNITY etc. Note: Even though there are two fields called MPLS Label 1 and 2 does not mean EVPN is not multi-protocol aware. Overlay related information like VNI is carried through EVPN extended communities defined here:\nIn order to indicate which type of data-plane encapsulation (i.e., VXLAN, NVGRE, MPLS, or MPLS in GRE) is to be used, the BGP Encapsulation Extended Community defined in [RFC5512] is included with all EVPN routes (i.e., MAC Advertisement, Ethernet A-D per EVI, Ethernet A-D per ESI, IMET, and Ethernet Segment) advertised by an egress PE. Five new values have been assigned by IANA to extend the list of encapsulation types defined in [RFC5512]; they are listed in Section 11.\nNow the RFC5512 referred to above has been updated with RFC9012:\nThis document defines a BGP path attribute known as the \u0026ldquo;Tunnel Encapsulation attribute\u0026rdquo;, which can be used with BGP UPDATEs of various Subsequent Address Family Identifiers (SAFIs) to provide information needed to create tunnels and their corresponding encapsulation headers. It provides encodings for a number of tunnel types, along with procedures for choosing between alternate tunnels and routing packets into tunnels.\nSo EVPN just solved extending layer 2 networks its own? Well no. I mentioned the RFC7432 and RFC8365 earlier. EVPN is not the \u0026ldquo;carrier\u0026rdquo; of the layer 2 networks, EVPN is the control-plane, the single control-plane for multiple network virtualization overlays (VXLAN, MPLS over GRE, Geneve). We also need something that can transport these networks. We need something that creates a tunnel for our layer 2 ethernet frames, to be able to actually transport these layer 2 networks over layer 3. This can be VXLAN (RFC8365) or MPLS over GRE (RFC4023). EVPN may also be referred to as EVPN-VXLAN, EVPN-MPLS or BGP-EVPN. I have already covered VXLAN to some extent in this post. But a quick section on VXLAN will probably make sense. I will mainly focus on EVPN-VXLAN in this blog series.\nVXLAN - the dataplane # Now we understand what EVPN does right? Not quite. TL;DR: Well it has the ability to segment and isolate tenants (layer 2) effectively using a single control plane, BGP, in a shared layer 3 spine-leaf fabric and VXLAN (or Geneve, NVGRE) as the dataplane creating the tunnels between the involved VTEPS (leafs). This allows for an effective, scalable and controllable way to transport layer 2 over layer 3.\nHow is EVPN configured with Arista EOS # To configure EVPN with VXLAN in Arista switches in a spine leaf there are two distinctions that needs to be made. First we have the underlay, that is where all the peer to peer links (routed interfaces) between the spines and leafs are configured. These are the carriers for whatever communication goes between any leaf in the fabric. The second is the overlay, this is where we will have our overlay networks configured, carrying all our layer 2 networks over the layer 3 peer to peer links configured in the underlay. Common for both the underlay and the overlay is BGP, as BGP is configured in both underlay and overlay. This could also be OSPF but why have two different routing protocols when one can just one to handle both the underlay and overlay for simpler management. For this section I have 2 spines and 4 leafs that I will configure EVPN and VXLAN on.\nLets start by configuring the underlay in a fabric consisting of 2 spines and 4 leafs.\nThe underlay # In the underlay I need to configure both the ptp links, but also the underlay BGP configuration, starting with the peer to peer links for my BGP in the underlay to have something to peer with.\nPeer to Peer links # As I have 6 switches in total, I need to allocate 16 ipv4 addresses for the ptp links. There will be one uplink from every leaf to every spine. To save on IP addresses I will define the submask for these links as small as possible and it should only make room for 2 ip addresses per link (1 for the leaf uplink and 1 for the spine downlink to respective leaf (spine leaf \u0026ldquo;pair\u0026rdquo;)), and that should be a /31 netmask. It may make sense to reserve a range for all the potential future expansion in the same range for several reasons. As in my example below I will be using 10.255.255.x/31 for each ptp interface, this may again be carved out of a bigger /24 subnet giving me a total of 256 addresses. 256 addresses? 0 and 255 are broadcast aren\u0026rsquo;t it? Arista EOS supports the RFC3021 which states the following:\nWith ever-increasing pressure to conserve IP address space on the Internet, it makes sense to consider where relatively minor changes can be made to fielded practice to improve numbering efficiency. One such change, proposed by this document, is to halve the amount of address space assigned to point-to-point links (common throughout the Internet infrastructure) by allowing the use of 31-bit subnet masks in a very limited way.\nThis means I can perfectly well use .0 as a useable address as long as it is a /31 submask wich gives me exactly two useable addresses, perfect fit for a peer to peer link.\nEach interface on every switch used for these peer to peer links should look like this, from my spine-1:\ninterface Ethernet1 description P2P_LINK_TO_VEOS-DC1-LEAF1A_Ethernet1 mtu 9214 no switchport ip address 10.255.255.0/31 ! interface Ethernet2 description P2P_LINK_TO_VEOS-DC1-LEAF1B_Ethernet1 mtu 9214 no switchport ip address 10.255.255.4/31 ! interface Ethernet3 description P2P_LINK_TO_VEOS-DC1-LEAF2A_Ethernet1 mtu 9214 no switchport ip address 10.255.255.8/31 ! interface Ethernet4 description P2P_LINK_TO_VEOS-DC1-LEAF2B_Ethernet1 mtu 9214 no switchport ip address 10.255.255.12/31 ! Notice the .0 in Ethernet1?\nThe same config is done on my spine 2 using other ip addresses obviously. For the leafs the peer to peer links looks like this:\ninterface Ethernet1 description P2P_LINK_TO_VEOS-DC1-SPINE1_Ethernet1 mtu 9214 no switchport ip address 10.255.255.1/31 ! interface Ethernet2 description P2P_LINK_TO_VEOS-DC1-SPINE2_Ethernet1 mtu 9214 no switchport ip address 10.255.255.3/31 ! The same is done on the remaing leafs accordingly.\nHere is the ip table for all the PTP links I am using:\nType Node Node Interface Leaf IP Address Peer Type Peer Node Peer Interface Peer IP Address l3leaf veos-dc1-leaf1a Ethernet1 10.255.255.1/31 spine veos-dc1-spine1 Ethernet1 10.255.255.0/31 l3leaf veos-dc1-leaf1a Ethernet2 10.255.255.3/31 spine veos-dc1-spine2 Ethernet1 10.255.255.2/31 l3leaf veos-dc1-leaf1b Ethernet1 10.255.255.5/31 spine veos-dc1-spine1 Ethernet2 10.255.255.4/31 l3leaf veos-dc1-leaf1b Ethernet2 10.255.255.7/31 spine veos-dc1-spine2 Ethernet2 10.255.255.6/31 l3leaf veos-dc1-leaf2a Ethernet1 10.255.255.9/31 spine veos-dc1-spine1 Ethernet3 10.255.255.8/31 l3leaf veos-dc1-leaf2a Ethernet2 10.255.255.11/31 spine veos-dc1-spine2 Ethernet3 10.255.255.10/31 l3leaf veos-dc1-leaf2b Ethernet1 10.255.255.13/31 spine veos-dc1-spine1 Ethernet4 10.255.255.12/31 l3leaf veos-dc1-leaf2b Ethernet2 10.255.255.15/31 spine veos-dc1-spine2 Ethernet4 10.255.255.14/31 BGP in the underlay # Next up is the BGP part that is responsible for exchanging routes in the underlay. This BGP configuration is quite straightforward and minimal.\nOn the spine my underlay BGP configuration looks like this:\n! interface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.1/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bgp 65110 router-id 10.255.0.1 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.255.1 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.1 remote-as 65111 neighbor 10.255.255.1 description veos-dc1-leaf1a_Ethernet1 neighbor 10.255.255.5 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.5 remote-as 65112 neighbor 10.255.255.5 description veos-dc1-leaf1b_Ethernet1 neighbor 10.255.255.9 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.9 remote-as 65113 neighbor 10.255.255.9 description veos-dc1-leaf2a_Ethernet1 neighbor 10.255.255.13 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.13 remote-as 65114 neighbor 10.255.255.13 description veos-dc1-leaf2b_Ethernet1 redistribute connected route-map RM-CONN-2-BGP ! address-family ipv4 neighbor IPv4-UNDERLAY-PEERS activate And similarly on my leaf1a:\ninterface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.3/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bgp 65111 router-id 10.255.0.3 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.255.0 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.0 remote-as 65110 neighbor 10.255.255.0 description veos-dc1-spine1_Ethernet1 neighbor 10.255.255.2 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.2 remote-as 65110 neighbor 10.255.255.2 description veos-dc1-spine2_Ethernet1 redistribute connected route-map RM-CONN-2-BGP ! address-family ipv4 neighbor IPv4-UNDERLAY-PEERS activate The added Loopback0 interface is used for BGP EVPN sessions as we will see a bit later in the BGP summary and routes. Then the IP prefix is created and mapped to the route map telling BGP only redistribute the EVPN overlay loopback interfaces subnet. There is no need to redistribute the underlay peer to peer links as they are directly connected. The sole reason for using BGP also in the underlay (plus all the benefits of using BGP, scalability, robustness, less management overhead) is to achieve reachability of Loopback0 as a stable endpoint for EVPN BGP sessions.\nLater on I will add another Loopback interface (Loopback1) for VXLAN and a corresponding prefix-list on my leaf switches, which is also part of the underlay configuration. I have not added it yet as I want to add it in the VXLAN section. This additional Loopback interface is part of the BGP in the underlay and being redistributed there.\nAS table:\nDevice AS Spine 1 AS 65110 Spine 2 AS 65110 Leaf1a AS 65111 Leaf1b AS 65112 Leaf2a AS 65113 Leaf2b AS 65114 When configured in all my switches lets have a look at the BGP status from my spine 1:\nveos-dc1-spine1#show bgp summary BGP summary information for VRF default Router identifier 10.255.0.1, local AS number 65110 Neighbor AS Session State AFI/SAFI AFI/SAFI State NLRI Rcd NLRI Acc ------------- ----------- ------------- ----------------------- -------------- ---------- ---------- 10.255.255.1 65111 Established IPv4 Unicast Negotiated 2 2 10.255.255.5 65112 Established IPv4 Unicast Negotiated 2 2 10.255.255.9 65113 Established IPv4 Unicast Negotiated 2 2 10.255.255.13 65114 Established IPv4 Unicast Negotiated 2 2 Thats it for the underlay configurations. Next up is the overlay parts.\nThe overlay # Now the more interesting parts are up, BGP in the overlay, loopback interfaces and VXLAN configurations.\nBGP in the overlay # Configuring BGP in the overlay will just build on top of already existing BGP config, I do not need to create any additional BGP router AS\u0026rsquo;es. Its all about adding the necessary overlay config to support EVPN.\nOn my spine 1 I have added these configurations, in addition to the already configured BGP settings:\n! interface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.1/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bgp 65110 neighbor EVPN-OVERLAY-PEERS peer group #Added neighbor EVPN-OVERLAY-PEERS next-hop-unchanged #Added neighbor EVPN-OVERLAY-PEERS update-source Loopback0 #Added neighbor EVPN-OVERLAY-PEERS bfd #Added neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 #Added neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== #Added neighbor EVPN-OVERLAY-PEERS send-community #Added neighbor EVPN-OVERLAY-PEERS maximum-routes 0 #Added neighbor 10.255.0.3 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.3 remote-as 65111 neighbor 10.255.0.3 description veos-dc1-leaf1a neighbor 10.255.0.4 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.4 remote-as 65112 neighbor 10.255.0.4 description veos-dc1-leaf1b neighbor 10.255.0.5 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.5 remote-as 65113 neighbor 10.255.0.5 description veos-dc1-leaf2a neighbor 10.255.0.6 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.6 remote-as 65114 neighbor 10.255.0.6 description veos-dc1-leaf2b redistribute connected route-map RM-CONN-2-BGP ! address-family evpn #Added neighbor EVPN-OVERLAY-PEERS activate #Added ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate ! A short explanation of what is added:\nAll the EVPN-OVERLAY-PEERS are added to BGP using their respective Loopback interfaces added in the \u0026ldquo;BGP underlay section\u0026rdquo; and AS numbers accordingly. Then the EVPN address family is added and the EVPN-OVERLAY-PEERS group is added. The same group is negated under the address family ipv4.\nThe complete BGP related config on spine 1 looks like this now:\ninterface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.1/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bgp 65110 router-id 10.255.0.1 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS next-hop-unchanged neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.0.3 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.3 remote-as 65111 neighbor 10.255.0.3 description veos-dc1-leaf1a neighbor 10.255.0.4 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.4 remote-as 65112 neighbor 10.255.0.4 description veos-dc1-leaf1b neighbor 10.255.0.5 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.5 remote-as 65113 neighbor 10.255.0.5 description veos-dc1-leaf2a neighbor 10.255.0.6 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.6 remote-as 65114 neighbor 10.255.0.6 description veos-dc1-leaf2b neighbor 10.255.255.1 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.1 remote-as 65111 neighbor 10.255.255.1 description veos-dc1-leaf1a_Ethernet1 neighbor 10.255.255.5 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.5 remote-as 65112 neighbor 10.255.255.5 description veos-dc1-leaf1b_Ethernet1 neighbor 10.255.255.9 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.9 remote-as 65113 neighbor 10.255.255.9 description veos-dc1-leaf2a_Ethernet1 neighbor 10.255.255.13 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.13 remote-as 65114 neighbor 10.255.255.13 description veos-dc1-leaf2b_Ethernet1 redistribute connected route-map RM-CONN-2-BGP ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate Similarly on my leaf1a the additional BGP config:\ninterface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.3/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bgp 65111 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor 10.255.0.1 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.1 remote-as 65110 neighbor 10.255.0.1 description veos-dc1-spine1 neighbor 10.255.0.2 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.2 remote-as 65110 neighbor 10.255.0.2 description veos-dc1-spine2 redistribute connected route-map RM-CONN-2-BGP ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate Then the full BGP related config on my leaf1a:\ninterface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.3/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bgp 65111 router-id 10.255.0.3 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.0.1 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.1 remote-as 65110 neighbor 10.255.0.1 description veos-dc1-spine1 neighbor 10.255.0.2 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.2 remote-as 65110 neighbor 10.255.0.2 description veos-dc1-spine2 neighbor 10.255.255.0 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.0 remote-as 65110 neighbor 10.255.255.0 description veos-dc1-spine1_Ethernet1 neighbor 10.255.255.2 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.2 remote-as 65110 neighbor 10.255.255.2 description veos-dc1-spine2_Ethernet1 redistribute connected route-map RM-CONN-2-BGP ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate The BGP EVPN loopback0 interfaces on all my devices for context and reference:\n### Loopback Interfaces (BGP EVPN Peering) | Loopback Pool | Available Addresses | Assigned addresses | Assigned Address % | | ------------- | ------------------- | ------------------ | ------------------ | | 10.255.0.0/27 | 32 | 6 | 18.75 % | ### Loopback0 Interfaces Node Allocation | POD | Node | Loopback0 | | --- | ---- | --------- | | VEOS_FABRIC | veos-dc1-leaf1a | 10.255.0.3/32 | | VEOS_FABRIC | veos-dc1-leaf1b | 10.255.0.4/32 | | VEOS_FABRIC | veos-dc1-leaf2a | 10.255.0.5/32 | | VEOS_FABRIC | veos-dc1-leaf2b | 10.255.0.6/32 | | VEOS_FABRIC | veos-dc1-spine1 | 10.255.0.1/32 | | VEOS_FABRIC | veos-dc1-spine2 | 10.255.0.2/32 | When showing ip bgp summary on spine 1 after all switches are configured:\nveos-dc1-spine1#show ip bgp summary BGP summary information for VRF default Router identifier 10.255.0.1, local AS number 65110 Neighbor Status Codes: m - Under maintenance Description Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc veos-dc1-leaf1a_Ethernet 10.255.255.1 4 65111 45087 45068 0 0 26d15h Estab 2 2 veos-dc1-leaf1b_Ethernet 10.255.255.5 4 65112 45139 45097 0 0 16d23h Estab 2 2 veos-dc1-leaf2a_Ethernet 10.255.255.9 4 65113 45046 45081 0 0 26d15h Estab 2 2 veos-dc1-leaf2b_Ethernet 10.255.255.13 4 65114 45096 45138 0 0 26d14h Estab 2 2 And on leaf1a:\nveos-dc1-leaf1a#show ip bgp summary BGP summary information for VRF default Router identifier 10.255.0.3, local AS number 65111 Neighbor Status Codes: m - Under maintenance Description Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc veos-dc1-spine1_Ethernet 10.255.255.0 4 65110 45056 45078 0 0 26d15h Estab 7 7 veos-dc1-spine2_Ethernet 10.255.255.2 4 65110 45056 45080 0 0 26d15h Estab 7 7 Now the BGP is configured in both the underlay and the overlay and the EVPN address family is activated. Next up is VXLAN and some networks in the overlay.\nVXLAN configurations # Now it is time to configure the dataplane, where my layer 2 networks reside. For that I need to configure VXLAN. The VXLAN configurations will only involve the leaf switches as they are the only VTEPs being configured in my spine/leaf fabric.\nOn my leaf1a switch I have added the following VXLAN configuration:\n! interface Loopback1 description VTEP_VXLAN_Tunnel_Source ip address 10.255.1.3/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 20 permit 10.255.1.0/27 eq 32 ! interface Vxlan1 description veos-dc1-leaf1a_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 I have added another loopback interface (Loopback1) which will serve as the VTEP IP address, then the general VXLAN configuration, description, source-interface and udp-port. Note also that I have added another prefix to my route map PL-LOOPBACKS-EVPN-OVERLAY to distribute the VTEP IP addresses in the underlay. As mentioned above these Loopback interfaces are considered part of the underlay configuration.\nI could have achieved the same with just one loopback interface for both EVPN and VXLAN but best practice is to have a dedicated loopback for EVPN and a dedicated loopback for VXLAN.\nThe VXLAN loopback1 interface for all my leaf switches for reference:\n### VTEP Loopback VXLAN Tunnel Source Interfaces (VTEPs Only) | VTEP Loopback Pool | Available Addresses | Assigned addresses | Assigned Address % | | --------------------- | ------------------- | ------------------ | ------------------ | | 10.255.1.0/27 | 32 | 4 | 12.5 % | ### VTEP Loopback Node allocation | POD | Node | Loopback1 | | --- | ---- | --------- | | VEOS_FABRIC | veos-dc1-leaf1a | 10.255.1.3/32 | | VEOS_FABRIC | veos-dc1-leaf1b | 10.255.1.4/32 | | VEOS_FABRIC | veos-dc1-leaf2a | 10.255.1.5/32 | | VEOS_FABRIC | veos-dc1-leaf2b | 10.255.1.6/32 | The same configuration is applied to the rest of the leaf switches accordingly (except for loopback1 ip address, which is unique).\nThe VXLAN configuration is done, but I have not added any networks. Next up is adding networks, or overlay networks and informing EVPN about them\nAdding layer 2 networks using VXLAN # Now that VXLAN is configured, I need to create some networks. When creating layer 2 subnets using VXLAN they are often referred to as virtualized layer 2 or simply overlay subnets. In VXLAN these layer 2 subnets are placed in segments or VXLAN segments where VNI (VXLAN Network Identifier) is used as the identifier for the respective segment, not VLAN. VXLAN provides a 24-bit VNI meaning we can have a total of 16 million segments in a VXLAN fabric. This does not mean 16 million per VTEP, but across different VTEPS in the same fabric. The reason for this is because there is a 1:1 mapping between VLAN and VNI (more info on this later). So to create a layer 2 network I will have to map a VLAN to a VNI. Lets have a look at my configuration below:\n! vlan 11 name VRF10_VLAN11 ! interface Vxlan1 description veos-dc1-leaf1b_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 vxlan vlan 11 vni 10011 To keep it very simple I have in my config above added one vlan, VLAN 11. Then in my VXLAN configuration I am doing a VLAN to VNI mapping:\nvxlan vlan 11 vni 10011 The VNI number can be anything from 1 to 16777215. The VLAN to VNI mappings are called L2VNI. There is no SVI (Switch Virtual Interface) configured, no gateway so a pure layer 2 and reachability is isolated inside this broadcast domain.\nIf I take a look at my VLAN 11 on leaf1a after the VLAN to VNI mapping:\nveos-dc1-leaf1a#show vlan 11 VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 11 VRF10_VLAN11 active Cpu, Et5, Et6, Vx1 # notice the Vx1 The vxlan1 interface has been added.\nWhen a server is attached to leaf1a in VLAN 11 in subnet 10.20.11.0/24 and this VLAN is mapped to VNI 10011 and it wants to do an ICMP request to another server attached to leaf2b using the same VLAN , same subnet and same VNI mapping they are in the same layer 2 broadcast domain and there will be no routing in the overlay. In the underlay there will ofcourse be some routing done as explained earlier. The responsibility of VXLAN in this case is encapsulating the ethernet frame on leaf1a (source or ingress vtep) and decapsulate it on leaf2b (destination or egress vtep) before it is delivered to server 2, also called bridging.\nBut will this work? How does the VTEP leaf1a know that I do have a server 2 on the same subnet on leaf2b? Well here is where the control plane comes into play, BGP. BGP advertises the Loopback0 and Loopback1 (EVPN and VXLAN) in the underlay for reachability.\nveos-dc1-leaf1a#show vxlan vtep detail Remote VTEPS for Vxlan1: VTEP Learned Via MAC Address Learning Tunnel Type(s) ---------------- ------------------- -------------------------- -------------- 10.255.1.4 control plane control plane flood, unicast 10.255.1.5 control plane control plane flood, unicast 10.255.1.6 control plane control plane flood, unicast Total number of remote VTEPS: 3 That is why I dont configure any static peering in my VXLAN configuration, BGP is handling this in the control plane. Right now though, my VLAN 11 is not being advertised anywhere so it will not be possible for Server 1 and Server 2 to reach each other. I need to configure BGP to advertise the VLAN 11 to VNI 10011 mapping in my overlay so the VTEPs can form a tunnel and span this layer 2 subnet across my layer 3 spine leaf fabric.\nrouter bgp 65111 router-id 10.255.0.3 \u0026lt;----redacted---\u0026gt; ! vlan 11 rd 10.255.0.3:10011 route-target both 10011:10011 redistribute learned ! The tunnel between leaf1a and leaf2b is established dynamically meaning its not only until any VTEP is advertising the same VNI in BGP and there is an ARP request from a source to another destination in the same VNI (L2VNI) the VXLAN tunnel is established. No flood and learn, but MAC addresses learnt locally are advertised to EVPN peers.\nI mentioned that VNI uses 24-bit and can create 16777215 segments. The maximum number of VLANs on a single switch is 4094, even less if factoring in VLANS that are system/internally reserved. So to be able to achieve 4094\u0026lt; (max 16777215) VNIs or VXLAN segments it is fully possible to map different same VLAN to a different VNI, but this cant be done on the same switch or VTEP. In a spine leaf fabric with several leafs and every leaf is a VTEP they can have a different range of VNI mapped to same VLAN individually, creating their own layer 2 broadcast domains. In the illustration below I have 4 leafs where each \u0026ldquo;pair\u0026rdquo; of leafs have been configured using a set of VLAN to VNI mappings. Leaf pair leaf1a and leaf1b maps VLAN 10,20 and 30 to VNIs 10010, 10020 and 10030 while the other leaf pair leaf2a and leaf2b maps VLAN 10,20 and 30 to VNIs 20010, 20020 and 20030:\nThats it for L2VNI, but what if I want to route between L2VNIs?\nIntegrated routing and bridging (IRB) # In a traditional network with a lot of north south traffic it made sense to have a \u0026ldquo;nortbound\u0026rdquo; centralized gateway to both maintain isolation and apply policies. In todays modern datacenter that traffic pattern has changed to become dominantly east-west due to the distributed nature of the applications like virtual machines and containers (e.g Kubernetes clusters). There is no longer one big server handling many roles at the same time, but instead many virtual machines and often a huge amount of containers must work together to provide the \u0026ldquo;full\u0026rdquo; application. This change is a natural evolution in the datacenter where scalability, performance and management is key. For these VMs and containers to \u0026ldquo;interact\u0026rdquo; network is critical in both performance and reliability. Having a centralized router/gateway such a scenario may not be so optimal. Therefore we need to have a solution that can provide \u0026ldquo;de-centralized\u0026rdquo; routing or distributed routing.\nDistributed routing means in this case that all my switches with VTEPS can become a default gateway for my VXLAN segments, where routing is done between L2VNIs (inter-subnet routing) without exiting the VXLAN fabric at line-rate and in the first hop, close to my application.\nWith VXLAN it is possible to configure exactly this behavior, instead of just doing regular L2VNIs it is also possible to do L3VNI using an extension in VXLAN called Integrated Routing and Bridging (IETF draft). Integrated Routing and Bridging can be configured two ways, asymmetric IRB or symmetric IRB.\nAsymmetric IRB # In asymmetric IRB, the ingress VTEP performs both the routing and bridging tasks. When a packet moves between subnets, the ingress VTEP routes the packet to the destination subnet and then encapsulates it with the corresponding VXLAN VNI. The egress VTEP simply decapsulates the packet and forwards it based on layer 2 information. Each VTEP must maintain information about all tenant hosts, including their MAC and IP bindings. This necessitates that every VTEP be a member of all tenant subnets/VNIs and have associated SVIs with anycast IPs for each subnet. That is the only way for VXLAN to route L2VNIs withouth a L3VNI.\nDue to the need for each VTEP to handle all tenant subnets, asymmetric IRB can face scalability challenges, especially in large multi-tenant environments.\nSymmetric IRB # In symmetric IRB both ingress and egress VTEPs participate in routing. When a packet traverses subnets, the ingress VTEP routes the packet from the source subnet to an intermediate IP-VRF. The packet is then VXLAN encapsulated and sent to the egress VTEP, which routes it from the IP-VRF to the destination subnet. With an L3VNI configured, I no longer need to have all the other VTEPs VNIs configured across all VTEPS, it can route from eg, VNI10010 to VNI 10020 located on a remote tep using the L3VNI (VRF).\nVTEPs need to be configured with an IP-VRF that interconnects all subnets. This model allows for better scalability as each VTEP doesn\u0026rsquo;t need to maintain complete information about all tenant hosts.\nSymmetric IRB offers improved scalability and flexibility, making it more suitable for large-scale deployments with multiple tenants.\nInfo: Arista supports both asymmetric and symmetric IRB! For more information on how this works in Arista EOS see here\nIn my lab I am using symmetrical IRB. I have two VRFs mapped to their own unique VNI (L3VNIs). VRF20 is mapped to VNI:10 and VRF21 is mapped to VNI:11. Almost all VLANs and VNIs (L2VNIs) are configured on all leafs, except VLAN14 which is only configured on leaf2b (in VRF20). I am not prone to scaling issues in my lab when it comes to the amount VNIs and mac address tables, and even though most or all VNIs are represented on my leafs I still use symmetrical IRB due to the nature of having L3VNIs (VRF) configured. Without a L3VNI it has to do asymmetric IRB (ingress vtep does all the routing and bridging). Anycast gateway is configured and VLAN to VNI mapping is done and added to my BGP configuration.\nSee the config below for leaf1a and leaf2b.\nLeaf1a:\n! vlan 11 name VRF10_VLAN11 ! vlan 12-13 name VRF10_VLAN12 ! vlan 21 name VRF11_VLAN21 ! vlan 22 name VRF11_VLAN22 ! vlan 23 name VRF11_VLAN23 ! vlan 1079 name VRF10_VLAN1079_WAN ! vlan 3401 name L2_VLAN3401 ! vlan 3402 name L2_VLAN3402 ! vrf instance VRF20 ! vrf instance VRF21 ! interface Vlan11 description VRF10_VLAN11 vrf VRF20 ip address virtual 10.20.11.1/24 ! interface Vlan12 description VRF10_VLAN12 vrf VRF20 ip address virtual 10.20.12.1/24 ! interface Vlan13 description VRF10_VLAN12 vrf VRF20 ip address virtual 10.20.13.1/24 ! interface Vlan21 description VRF11_VLAN21 mtu 9194 vrf VRF21 ip address virtual 10.21.21.1/24 ! interface Vlan22 description VRF11_VLAN22 mtu 9194 vrf VRF21 ip address virtual 10.21.22.1/24 ! interface Vlan23 description VRF11_VLAN23 mtu 9194 vrf VRF21 ip address virtual 10.21.23.1/24 ! interface Vlan1079 description VRF10_VLAN1079_WAN vrf VRF20 ! interface Vxlan1 description veos-dc1-leaf1b_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 vxlan vlan 11 vni 10011 vxlan vlan 12 vni 10012 vxlan vlan 13 vni 10013 vxlan vlan 21 vni 10021 vxlan vlan 22 vni 10022 vxlan vlan 23 vni 10023 vxlan vlan 1079 vni 11079 vxlan vlan 3401 vni 13401 vxlan vlan 3402 vni 13402 vxlan vrf VRF20 vni 10 vxlan vrf VRF21 vni 11 ! ip virtual-router mac-address 00:1c:73:00:00:99 ip address virtual source-nat vrf VRF20 address 10.255.10.4 ip address virtual source-nat vrf VRF21 address 10.255.11.4 ! ip routing no ip routing vrf MGMT ip routing vrf VRF20 ip routing vrf VRF21 Leaf2b:\nvlan 11 name VRF10_VLAN11 ! vlan 12-13 name VRF10_VLAN12 ! vlan 14 name VRF10_VLAN14 ! vlan 21 name VRF11_VLAN21 ! vlan 22 name VRF11_VLAN22 ! vlan 23 name VRF11_VLAN23 ! vlan 1079 name VRF10_VLAN1079_WAN ! vlan 3401 name L2_VLAN3401 ! vlan 3402 name L2_VLAN3402 ! vrf instance VRF20 ! vrf instance VRF21 ! interface Ethernet1 description P2P_LINK_TO_VEOS-DC1-SPINE1_Ethernet4 mtu 9194 no switchport ip address 10.255.255.13/31 ! interface Ethernet2 description P2P_LINK_TO_VEOS-DC1-SPINE2_Ethernet4 mtu 9194 no switchport ip address 10.255.255.15/31 ! interface Ethernet5 description dc1-leaf2b-client4_CLIENT4 switchport trunk native vlan 14 switchport trunk allowed vlan 11-14,21-22 switchport mode trunk ! interface Ethernet6 description leaf2b-wan-u1_u1-vrf10 mtu 1500 switchport trunk allowed vlan 1079 switchport mode trunk ! interface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.6/32 ! interface Loopback1 description VTEP_VXLAN_Tunnel_Source ip address 10.255.1.6/32 ! interface Loopback10 description VRF20_VTEP_DIAGNOSTICS vrf VRF20 ip address 10.255.10.6/32 ! interface Loopback11 description VRF21_VTEP_DIAGNOSTICS vrf VRF21 ip address 10.255.11.6/32 ! interface Vlan11 description VRF10_VLAN11 vrf VRF20 ip address virtual 10.20.11.1/24 ! interface Vlan12 description VRF10_VLAN12 vrf VRF20 ip address virtual 10.20.12.1/24 ! interface Vlan13 description VRF10_VLAN12 vrf VRF20 ip address virtual 10.20.13.1/24 ! interface Vlan14 description VRF10_VLAN14 vrf VRF20 ip address virtual 10.20.14.1/24 ! interface Vlan21 description VRF11_VLAN21 mtu 9194 vrf VRF21 ip address virtual 10.21.21.1/24 ! interface Vlan22 description VRF11_VLAN22 mtu 9194 vrf VRF21 ip address virtual 10.21.22.1/24 ! interface Vlan23 description VRF11_VLAN23 mtu 9194 vrf VRF21 ip address virtual 10.21.23.1/24 ! interface Vlan1079 description VRF10_VLAN1079_WAN vrf VRF20 ip address 10.179.0.2/29 ! interface Vxlan1 description veos-dc1-leaf2b_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 vxlan vlan 11 vni 10011 vxlan vlan 12 vni 10012 vxlan vlan 13 vni 10013 vxlan vlan 14 vni 10014 vxlan vlan 21 vni 10021 vxlan vlan 22 vni 10022 vxlan vlan 23 vni 10023 vxlan vlan 1079 vni 11079 vxlan vlan 3401 vni 13401 vxlan vlan 3402 vni 13402 vxlan vrf VRF20 vni 10 vxlan vrf VRF21 vni 11 ! ip virtual-router mac-address 00:1c:73:00:00:99 ip address virtual source-nat vrf VRF20 address 10.255.10.6 ip address virtual source-nat vrf VRF21 address 10.255.11.6 ! ip routing ip routing vrf VRF20 ip routing vrf VRF21 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 seq 20 permit 10.255.1.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bfd multihop interval 300 min-rx 300 multiplier 3 ! router bgp 65114 router-id 10.255.0.6 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.0.1 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.1 remote-as 65110 neighbor 10.255.0.1 description veos-dc1-spine1 neighbor 10.255.0.2 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.2 remote-as 65110 neighbor 10.255.0.2 description veos-dc1-spine2 neighbor 10.255.255.12 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.12 remote-as 65110 neighbor 10.255.255.12 description veos-dc1-spine1_Ethernet4 neighbor 10.255.255.14 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.14 remote-as 65110 neighbor 10.255.255.14 description veos-dc1-spine2_Ethernet4 redistribute connected route-map RM-CONN-2-BGP ! vlan 11 rd 10.255.0.6:10011 route-target both 10011:10011 redistribute learned ! vlan 12 rd 10.255.0.6:10012 route-target both 10012:10012 redistribute learned ! vlan 13 rd 10.255.0.6:10013 route-target both 10013:10013 redistribute learned ! vlan 14 rd 10.255.0.6:10014 route-target both 10014:10014 redistribute learned ! vlan 21 rd 10.255.0.6:10021 route-target both 10021:10021 redistribute learned ! vlan 22 rd 10.255.0.6:10022 route-target both 10022:10022 redistribute learned ! vlan 23 rd 10.255.0.6:10023 route-target both 10023:10023 redistribute learned ! vlan 1079 rd 10.255.0.6:11079 route-target both 11079:11079 redistribute learned ! vlan 3401 rd 10.255.0.6:13401 route-target both 13401:13401 redistribute learned ! vlan 3402 rd 10.255.0.6:13402 route-target both 13402:13402 redistribute learned ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate neighbor pfsense01 activate ! vrf VRF20 rd 10.255.0.6:10 route-target import evpn 10:10 route-target export evpn 10:10 router-id 10.255.0.6 redistribute connected ! vrf VRF21 rd 10.255.0.6:11 route-target import evpn 11:11 route-target export evpn 11:11 router-id 10.255.0.6 redistribute connected ! Complete configurations # Below I will provide the full relevant configuration for spine-1 and leaf1a for easier reference:\nspine-1:\ninterface Ethernet1 description P2P_LINK_TO_VEOS-DC1-LEAF1A_Ethernet1 mtu 9194 no switchport ip address 10.255.255.0/31 ! interface Ethernet2 description P2P_LINK_TO_VEOS-DC1-LEAF1B_Ethernet1 mtu 9194 no switchport ip address 10.255.255.4/31 ! interface Ethernet3 description P2P_LINK_TO_VEOS-DC1-LEAF2A_Ethernet1 mtu 9194 no switchport ip address 10.255.255.8/31 ! interface Ethernet4 description P2P_LINK_TO_VEOS-DC1-LEAF2B_Ethernet1 mtu 9194 no switchport ip address 10.255.255.12/31 ! interface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.1/32 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bfd multihop interval 300 min-rx 300 multiplier 3 ! router bgp 65110 router-id 10.255.0.1 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS next-hop-unchanged neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.0.3 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.3 remote-as 65111 neighbor 10.255.0.3 description veos-dc1-leaf1a neighbor 10.255.0.4 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.4 remote-as 65112 neighbor 10.255.0.4 description veos-dc1-leaf1b neighbor 10.255.0.5 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.5 remote-as 65113 neighbor 10.255.0.5 description veos-dc1-leaf2a neighbor 10.255.0.6 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.6 remote-as 65114 neighbor 10.255.0.6 description veos-dc1-leaf2b neighbor 10.255.255.1 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.1 remote-as 65111 neighbor 10.255.255.1 description veos-dc1-leaf1a_Ethernet1 neighbor 10.255.255.5 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.5 remote-as 65112 neighbor 10.255.255.5 description veos-dc1-leaf1b_Ethernet1 neighbor 10.255.255.9 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.9 remote-as 65113 neighbor 10.255.255.9 description veos-dc1-leaf2a_Ethernet1 neighbor 10.255.255.13 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.13 remote-as 65114 neighbor 10.255.255.13 description veos-dc1-leaf2b_Ethernet1 redistribute connected route-map RM-CONN-2-BGP ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate Leaf1a configuration:\nvlan 11 name VRF10_VLAN11 ! vlan 12-13 name VRF10_VLAN12 ! vlan 21 name VRF11_VLAN21 ! vlan 22 name VRF11_VLAN22 ! vlan 23 name VRF11_VLAN23 ! vlan 1079 name VRF10_VLAN1079_WAN ! vlan 3401 name L2_VLAN3401 ! vlan 3402 name L2_VLAN3402 ! vrf instance VRF20 ! vrf instance VRF21 ! interface Ethernet1 description P2P_LINK_TO_VEOS-DC1-SPINE1_Ethernet1 mtu 9194 no switchport ip address 10.255.255.1/31 ! interface Ethernet2 description P2P_LINK_TO_VEOS-DC1-SPINE2_Ethernet1 mtu 9194 no switchport ip address 10.255.255.3/31 ! interface Ethernet5 description dc1-leaf1a-client1_CLIENT1 switchport trunk native vlan 11 switchport trunk allowed vlan 11-13,21-22 switchport mode trunk ! interface Ethernet6 description dc1-leaf1a-client1-vxlan_CLIENT1-VXLAN switchport trunk native vlan 21 switchport trunk allowed vlan 11-13,21-23 switchport mode trunk ! interface Loopback0 description EVPN_Overlay_Peering ip address 10.255.0.3/32 ! interface Loopback1 description VTEP_VXLAN_Tunnel_Source ip address 10.255.1.3/32 ! interface Loopback10 description VRF20_VTEP_DIAGNOSTICS vrf VRF20 ip address 10.255.10.3/32 ! interface Loopback11 description VRF21_VTEP_DIAGNOSTICS vrf VRF21 ip address 10.255.11.3/32 ! interface Vlan11 description VRF10_VLAN11 vrf VRF20 ip address virtual 10.20.11.1/24 ! interface Vlan12 description VRF10_VLAN12 vrf VRF20 ip address virtual 10.20.12.1/24 ! interface Vlan13 description VRF10_VLAN12 vrf VRF20 ip address virtual 10.20.13.1/24 ! interface Vlan21 description VRF11_VLAN21 mtu 9194 vrf VRF21 ip address virtual 10.21.21.1/24 ! interface Vlan22 description VRF11_VLAN22 mtu 9194 vrf VRF21 ip address virtual 10.21.22.1/24 ! interface Vlan23 description VRF11_VLAN23 mtu 9194 vrf VRF21 ip address virtual 10.21.23.1/24 ! interface Vlan1079 description VRF10_VLAN1079_WAN vrf VRF20 ! interface Vxlan1 description veos-dc1-leaf1a_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 vxlan vlan 11 vni 10011 vxlan vlan 12 vni 10012 vxlan vlan 13 vni 10013 vxlan vlan 21 vni 10021 vxlan vlan 22 vni 10022 vxlan vlan 23 vni 10023 vxlan vlan 1079 vni 11079 vxlan vlan 3401 vni 13401 vxlan vlan 3402 vni 13402 vxlan vrf VRF20 vni 10 vxlan vrf VRF21 vni 11 ! ip virtual-router mac-address 00:1c:73:00:00:99 ip address virtual source-nat vrf VRF20 address 10.255.10.3 ip address virtual source-nat vrf VRF21 address 10.255.11.3 ! ip routing ip routing vrf VRF20 ip routing vrf VRF21 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.255.0.0/27 eq 32 seq 20 permit 10.255.1.0/27 eq 32 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bfd multihop interval 300 min-rx 300 multiplier 3 ! router bgp 65111 router-id 10.255.0.3 no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.255.0.1 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.1 remote-as 65110 neighbor 10.255.0.1 description veos-dc1-spine1 neighbor 10.255.0.2 peer group EVPN-OVERLAY-PEERS neighbor 10.255.0.2 remote-as 65110 neighbor 10.255.0.2 description veos-dc1-spine2 neighbor 10.255.255.0 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.0 remote-as 65110 neighbor 10.255.255.0 description veos-dc1-spine1_Ethernet1 neighbor 10.255.255.2 peer group IPv4-UNDERLAY-PEERS neighbor 10.255.255.2 remote-as 65110 neighbor 10.255.255.2 description veos-dc1-spine2_Ethernet1 redistribute connected route-map RM-CONN-2-BGP ! vlan 11 rd 10.255.0.3:10011 route-target both 10011:10011 redistribute learned ! vlan 12 rd 10.255.0.3:10012 route-target both 10012:10012 redistribute learned ! vlan 13 rd 10.255.0.3:10013 route-target both 10013:10013 redistribute learned ! vlan 21 rd 10.255.0.3:10021 route-target both 10021:10021 redistribute learned ! vlan 22 rd 10.255.0.3:10022 route-target both 10022:10022 redistribute learned ! vlan 23 rd 10.255.0.3:10023 route-target both 10023:10023 redistribute learned ! vlan 1079 rd 10.255.0.3:11079 route-target both 11079:11079 redistribute learned ! vlan 3401 rd 10.255.0.3:13401 route-target both 13401:13401 redistribute learned ! vlan 3402 rd 10.255.0.3:13402 route-target both 13402:13402 redistribute learned ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate ! vrf VRF20 rd 10.255.0.3:10 route-target import evpn 10:10 route-target export evpn 10:10 router-id 10.255.0.3 redistribute connected ! vrf VRF21 rd 10.255.0.3:11 route-target import evpn 11:11 route-target export evpn 11:11 router-id 10.255.0.3 redistribute connected ! Verify configurations and a quick look at the routes # Below I will provide some commands to verify BGP-EVPN and VXLAN configurations. Then have a quick look at the routes being advertised.\nShow vxlan interface:\nveos-dc1-leaf1a#show interfaces vxlan 1 Vxlan1 is up, line protocol is up (connected) Hardware is Vxlan Description: veos-dc1-leaf1a_VTEP Source interface is Loopback1 and is active with 10.255.1.3 Listening on UDP port 4789 Replication/Flood Mode is headend with Flood List Source: EVPN Remote MAC learning via EVPN VNI mapping to VLANs Static VLAN to VNI mapping is [11, 10011] [12, 10012] [13, 10013] [21, 10021] [22, 10022] [23, 10023] [1079, 11079] [3401, 13401] [3402, 13402] Dynamic VLAN to VNI mapping for \u0026#39;evpn\u0026#39; is [4097, 10] [4098, 11] Note: All Dynamic VLANs used by VCS are internal VLANs. Use \u0026#39;show vxlan vni\u0026#39; for details. Static VRF to VNI mapping is [VRF20, 10] [VRF21, 11] Headend replication flood vtep list is: 11 10.255.1.6 10.255.1.5 10.255.1.4 12 10.255.1.6 10.255.1.5 10.255.1.4 13 10.255.1.6 10.255.1.5 10.255.1.4 21 10.255.1.6 10.255.1.5 10.255.1.4 22 10.255.1.6 10.255.1.5 10.255.1.4 23 10.255.1.6 10.255.1.5 10.255.1.4 1079 10.255.1.6 10.255.1.5 10.255.1.4 3401 10.255.1.6 10.255.1.5 10.255.1.4 3402 10.255.1.6 10.255.1.5 10.255.1.4 Shared Router MAC is 0000.0000.0000 VNI to VLAN mapping leaf1a:\nveos-dc1-leaf1a#show vxlan vni VNI to VLAN Mapping for Vxlan1 VNI VLAN Source Interface 802.1Q Tag ----------- ---------- ------------ --------------- ---------- 10011 11 static Ethernet5 11 Ethernet6 11 Vxlan1 11 10012 12 static Ethernet5 12 Ethernet6 12 Vxlan1 12 10013 13 static Ethernet5 13 Ethernet6 13 Vxlan1 13 10021 21 static Ethernet5 21 Ethernet6 21 Vxlan1 21 10022 22 static Ethernet5 22 Ethernet6 22 Vxlan1 22 10023 23 static Ethernet6 23 Vxlan1 23 11079 1079 static Vxlan1 1079 13401 3401 static Vxlan1 3401 13402 3402 static Vxlan1 3402 VNI to dynamic VLAN Mapping for Vxlan1 VNI VLAN VRF Source --------- ---------- ----------- ------------ 10 4097 VRF20 evpn 11 4098 VRF21 evpn Show vxlan flood vtep:\nveos-dc1-leaf1a#show vxlan flood vtep VXLAN Flood VTEP Table -------------------------------------------------------------------------------- VLANS Ip Address ----------------------------- ------------------------------------------------ 11-13,21-23,1079,3401-3402 10.255.1.4 10.255.1.5 10.255.1.6 Show vxlan address-table:\nveos-dc1-leaf1a#show vxlan address-table Vxlan Mac Address Table ---------------------------------------------------------------------- VLAN Mac Address Type Prt VTEP Moves Last Move ---- ----------- ---- --- ---- ----- --------- 12 22f5.d0e1.1e17 EVPN Vx1 10.255.1.4 1 0:03:57 ago 12 bc24.11ca.c703 EVPN Vx1 10.255.1.4 1 37 days, 19:47:32 ago 13 bc24.116d.964b EVPN Vx1 10.255.1.5 1 45 days, 18:31:55 ago 22 bc24.11b8.1363 EVPN Vx1 10.255.1.4 1 37 days, 19:57:39 ago 23 bc24.115f.7497 EVPN Vx1 10.255.1.5 1 45 days, 14:03:13 ago 1079 bc24.110b.c61f EVPN Vx1 10.255.1.6 1 46 days, 22:22:27 ago Total Remote Mac Addresses for this criterion: 6 Show arp for vxlan interface:\nveos-dc1-leaf1a#show arp interface vxlan 1 Address Age (sec) Hardware Addr Interface veos-dc1-leaf1a#show arp vrf VRF20 interface vxlan 1 Address Age (sec) Hardware Addr Interface 10.20.12.10 - bc24.11ca.c703 Vlan12, Vxlan1 10.20.13.10 - bc24.116d.964b Vlan13, Vxlan1 veos-dc1-leaf1a#show arp vrf VRF21 interface vxlan 1 Address Age (sec) Hardware Addr Interface 10.21.22.10 - bc24.11b8.1363 Vlan22, Vxlan1 10.21.23.10 - bc24.115f.7497 Vlan23, Vxlan1 Show ip route to a remote destination IP:\nveos-dc1-leaf1a#show ip route vrf VRF20 10.20.14.11 VRF: VRF20 Source Codes: C - connected, S - static, K - kernel, O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1, E2 - OSPF external type 2, N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type2, B - Other BGP Routes, B I - iBGP, B E - eBGP, R - RIP, I L1 - IS-IS level 1, I L2 - IS-IS level 2, O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary, NG - Nexthop Group Static Route, V - VXLAN Control Service, M - Martian, DH - DHCP client installed default route, DP - Dynamic Policy Route, L - VRF Leaked, G - gRIBI, RC - Route Cache Route, CL - CBF Leaked Route B E 10.20.14.11/32 [200/0] via VTEP 10.255.1.6 VNI 10 router-mac bc:24:11:5e:4a:59 local-interface Vxlan1 Show bgp evpn detail:\nveos-dc1-leaf1a#show bgp evpn detail BGP routing table information for VRF default Router identifier 10.255.0.3, local AS number 65111 BGP routing table entry for ip-prefix 10.20.14.0/24, Route Distinguisher: 10.255.0.6:10 Paths: 2 available 65110 65114 10.255.1.6 from 10.255.0.2 (10.255.0.2) Origin IGP, metric -, localpref 100, weight 0, tag 0, valid, external, ECMP head, ECMP, best, ECMP contributor Extended Community: Route-Target-AS:10:10 TunnelEncap:tunnelTypeVxlan EvpnRouterMac:bc:24:11:5e:4a:59 VNI: 10 65110 65114 10.255.1.6 from 10.255.0.1 (10.255.0.1) Origin IGP, metric -, localpref 100, weight 0, tag 0, valid, external, ECMP, ECMP contributor Extended Community: Route-Target-AS:10:10 TunnelEncap:tunnelTypeVxlan EvpnRouterMac:bc:24:11:5e:4a:59 VNI: 10 In the next posts I will cover the different EVPN route types.\nveos-dc1-leaf1a#show bgp evpn route-type ? auto-discovery Filter by Ethernet auto-discovery (A-D) route (type 1) count Route-type based path count ethernet-segment Filter by Ethernet segment route (type 4) imet Filter by inclusive multicast Ethernet tag route (type 3) ip-prefix Filter by IP prefix route (type 5) join-sync Filter by multicast join sync route (type 7) leave-sync Filter by multicast leave sync route (type 8) mac-ip Filter by MAC/IP advertisement route (type 2) smet Filter by selective multicast Ethernet tag route (type 6) spmsi Filter by selective PMSI auto discovery route (type 10) Sources # IETF RFC7209 https://datatracker.ietf.org/doc/rfc7209/ IETF RFC7432 https://datatracker.ietf.org/doc/rfc7432/ IETF RFC8365 https://datatracker.ietf.org/doc/rfc8365/ IETF RFC9012 https://datatracker.ietf.org/doc/rfc9012/ IETF RFC4760 https://datatracker.ietf.org/doc/html/rfc4760 IETF RFC9014 https://datatracker.ietf.org/doc/rfc9014/ IETF draft-ietf-bess-evpn-geneve-08 https://datatracker.ietf.org/doc/html/draft-ietf-bess-evpn-geneve-08 IETF RFC2858 https://datatracker.ietf.org/doc/html/rfc2858 IETF RFC4271 https://datatracker.ietf.org/doc/html/rfc4271 IETF RFC4023 https://datatracker.ietf.org/doc/html/rfc4023 IETF RFC7348 https://datatracker.ietf.org/doc/html/rfc7348 IETF RFC3021 https://datatracker.ietf.org/doc/rfc3021/ Arista Networks https://www.arista.com/en/um-eos/eos-evpn-overview Arista Networks https://www.arista.com/en/um-eos/eos-integrated-routing-and-bridging ","date":"9 December 2024","externalUrl":null,"permalink":"/2024/12/09/evpn-introduction/","section":"Posts","summary":"First post in a series of posts covering EVPN and getting to know EVPN with Arista EOS","title":"EVPN introduction","type":"posts"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/tags/multi-tenancy/","section":"Tags","summary":"","title":"Multi-Tenancy","type":"tags"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/tags/networking/","section":"Tags","summary":"","title":"Networking","type":"tags"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/categories/networking/","section":"Categories","summary":"","title":"Networking","type":"categories"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/tags/overlay/","section":"Tags","summary":"","title":"Overlay","type":"tags"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/categories/overlay/","section":"Categories","summary":"","title":"Overlay","type":"categories"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"9 December 2024","externalUrl":null,"permalink":"/tags/vxlan/","section":"Tags","summary":"","title":"Vxlan","type":"tags"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/tags/arista/","section":"Tags","summary":"","title":"Arista","type":"tags"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/categories/arista/","section":"Categories","summary":"","title":"Arista","type":"categories"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/tags/encapsulation/","section":"Tags","summary":"","title":"Encapsulation","type":"tags"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/tags/eos/","section":"Tags","summary":"","title":"EOS","type":"tags"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/categories/eos/","section":"Categories","summary":"","title":"EOS","type":"categories"},{"content":" Spine/Leaf with EVPN and VXLAN - underlay and overlay # In a typical datacenter today a common architecture is the Spine/Leaf design (Clos). This architecture comes with many benefits such as:\nPerformance Scalability Redundancy High Availability Simplified Network Management Supports East-West Underlay # There are two ways to design a spine leaf fabric. We can do layer 2 designs and layer 3 designs. I will be focusing on the layer 3 design in this post as this is the most common design. Layer 3 scales and performs better, we dont need to consider STP (spanning tree) and we get Equal Cost Multi Path as a bonus. In a layer 3 design all switches in the fabric are connected using routed ports, which also means there is no layer 2 possibilities between the switches, unless we introduce some kind of layer on top that can carry this layer 2 over the layer 3 links. This layer is what this post will cover and is often referrred to as the overlay layer. Before going all in on overlay I need to also cover the underlay. The underlay in a spine leaf fabric is the physical ports configured as routed ports connecting the switches together. All switches in a layer 3 fabric is being connected to each other using routed ports. All leafs exchange routes/peers with the spines.\nAs everything is routed we need to add some routing information in the underlay to let all switches know where to go to reach certain destinations. This can in theory be any kind of routing protocols (even static routes) supported by the IETF such as OSFP, ISIS and BGP.\nIn Arista the preferred and recommended routing protocol in both the underlay and overlay is BGP. Why? BGP is the most commonly used routing protocol, it is very robust, feature rich and customisable. It supports EVPN, and if you know you are going to use EVPN why consider a different routing protocol in the underlay. That just adds more complexity and management overhead. Using BGP in both the underlay and the overlay gives a much more consistent config and benefits like BGP does not need multicast.\nSo if you read any Arista deployment recommendations, use Arista Validated Design, Arista\u0026rsquo;s protocol of choice will always be BGP. It just makes everything so much easier.\nIn a spine leaf fabric, all leafs peers with the spines. The leafs do not peer with the other leafs in the underlay, unless there is an mlag config between two leaf pairs. All leafs have their own BGP AS, the spines usually share the same BGP AS.\nA diagram of the fabric underlay:\nIn the above diagram I have confgured my underlay with layer 3 routing, BGP exhanges route information between the leafs. So far in my configuration I can not have two servers connected to different leafs on the same layer 2 subnet. Only if they are connected to the same leaf I can have layer 2 adjacency. So when server 1, connected to leaf1a, wants to talk to server 2, connected on leaf2a, it has to be over layer 3 and both servers needs to be in their own subnet. Leaf1a and leaf2a advertises its subnets to both spine1 and spine2, server 1 and 2 has been configured to use their connected leaf switches as gateway respectively.\nIf I do a ping in the \u0026ldquo;underlay\u0026rdquo; how will it look like when I ping from leaf1a to leaf2a using loopback interface 0 on my leaf1a and leaf2a to just simulate layer 3 connectivity without any overlay protocol involved (I dont have any servers connected to a routed interfaces in my lab on any of my leafs at the moment).\nTcpdump on leaf1a on both its uplinks (the source of the icmp request):\n[ansible@veos-dc1-leaf1a ~]$ tcpdump -i vmnicet1 -nnvvv -s 0 icmp -c2 tcpdump: listening on vmnicet1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:41:41.626368 bc:24:11:4a:9a:d0 \u0026gt; bc:24:11:1d:5f:a1, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 33961, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 53, seq 1, length 80 13:41:41.628659 bc:24:11:4a:9a:d0 \u0026gt; bc:24:11:1d:5f:a1, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 33962, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 53, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-leaf1a ~]$ tcpdump -i vmnicet2 -nnvvv -s 0 icmp -c2 tcpdump: listening on vmnicet2, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:41:51.177083 bc:24:11:1d:5f:a1 \u0026gt; bc:24:11:f5:c0:d2, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 21419, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 54, seq 1, length 80 13:41:51.179460 bc:24:11:1d:5f:a1 \u0026gt; bc:24:11:f5:c0:d2, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 21420, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 54, seq 2, length 80 2 packets captured 2 packets received by filter 0 packets dropped by kernel Notice the change of direction on the MAC addresses.\nNow if I do a tcpdump on both spine 1 and spine 2 - depending on which path my request takes:\nSpine1 on downlinks to leaf1a and leaf2a :\n[ansible@veos-dc1-spine1 ~]$ tcpdump -i vmnicet1 -nnvvv -s 0 icmp -c2 tcpdump: listening on vmnicet1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:44:38.739492 bc:24:11:4a:9a:d0 \u0026gt; bc:24:11:1d:5f:a1, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 39153, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 55, seq 1, length 80 13:44:38.741578 bc:24:11:4a:9a:d0 \u0026gt; bc:24:11:1d:5f:a1, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 39154, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 55, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-spine1 ~]$ tcpdump -i vmnicet3 -nnvvv -s 0 icmp -c2 tcpdump: listening on vmnicet3, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:44:57.720811 bc:24:11:31:35:db \u0026gt; bc:24:11:4a:9a:d0, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 41546, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 57, seq 1, length 80 13:44:57.723117 bc:24:11:31:35:db \u0026gt; bc:24:11:4a:9a:d0, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 41547, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 57, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-spine1 ~]$ Spine2 on downlinks to leaf1a and leaf2a:\n[ansible@veos-dc1-spine2 ~]$ tcpdump -i vmnicet1 icmp -nnvvv -s 0 -c 2 tcpdump: listening on vmnicet1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:46:42.934823 bc:24:11:1d:5f:a1 \u0026gt; bc:24:11:f5:c0:d2, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 64040, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 58, seq 1, length 80 13:46:42.937184 bc:24:11:1d:5f:a1 \u0026gt; bc:24:11:f5:c0:d2, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 64041, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 58, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-spine2 ~]$ tcpdump -i vmnicet3 icmp -nnvvv -s 0 -c 2 tcpdump: listening on vmnicet3, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:46:51.617532 bc:24:11:f5:c0:d2 \u0026gt; bc:24:11:31:35:db, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 497, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 59, seq 1, length 80 13:46:51.620311 bc:24:11:f5:c0:d2 \u0026gt; bc:24:11:31:35:db, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 498, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 59, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-spine2 ~]$ And finally on leaf2a on both its uplinks:\n[ansible@veos-dc1-leaf2a ~]$ tcpdump -i vmnicet1 -nnvvv -s 0 icmp -c2 tcpdump: listening on vmnicet1, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:50:01.166378 bc:24:11:31:35:db \u0026gt; bc:24:11:4a:9a:d0, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 21154, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 60, seq 1, length 80 13:50:01.168794 bc:24:11:31:35:db \u0026gt; bc:24:11:4a:9a:d0, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 64, id 21155, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.5 \u0026gt; 10.255.0.3: ICMP echo reply, id 60, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-leaf2a ~]$ tcpdump -i vmnicet2 -nnvvv -s 0 icmp -c2 tcpdump: listening on vmnicet2, link-type EN10MB (Ethernet), snapshot length 262144 bytes 13:50:07.251988 bc:24:11:f5:c0:d2 \u0026gt; bc:24:11:31:35:db, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 31490, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 61, seq 1, length 80 13:50:07.254459 bc:24:11:f5:c0:d2 \u0026gt; bc:24:11:31:35:db, ethertype IPv4 (0x0800), length 114: (tos 0x0, ttl 63, id 31491, offset 0, flags [none], proto ICMP (1), length 100) 10.255.0.3 \u0026gt; 10.255.0.5: ICMP echo request, id 61, seq 2, length 80 2 packets captured 5 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-leaf2a ~]$ If I take a tcpdump and open it in Wireshark, I can have a look at the protocols in the frame.\nSomething to have in the back of the mind to later in this post\u0026hellip;\nDepending on which path my traffic takes, via spine 1 or spine 2, (remember one of the bonuses with L3 is ECMP), I see the mac address of my leaf1a bc:24:11:1d:5f:a1 as source and the destination spine2 mac address bc:24:11:f5:c0:d2. Then the source IP and destination IP of their respective loopback interfaces. Nothing special here. Why do I see the spine2\u0026rsquo;s mac address as destination mac address? Remember that this is a pure layer 3 fabric which means leaf1a and leaf2a are not seeing each other directly, everything is routed over spine1 and spine2. So the destination mac will be either spine1 and spine2 as those are both intermediary. The packets IP header (source and destination IP address) on the other hand corresponds to the right source and destination ip address. Its only the layer 2 ethernet header thats is being updated along the hops: leaf1a -\u0026gt; spine 1 or spine 2 -\u0026gt; leaf2a.\nFor reference I have my leaf1a, spine1, spine2 and leaf2a\u0026rsquo;s mac and ip address below:\n[ansible@veos-dc1-leaf1a ~]$ ip link show et1 29: et1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9194 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:1d:5f:a1 brd ff:ff:ff:ff:ff:ff [ansible@veos-dc1-leaf1a ~]$ veos-dc1-leaf1a#show interfaces loopback 0 Loopback0 is up, line protocol is up (connected) Hardware is Loopback Description: EVPN_Overlay_Peering Internet address is 10.255.0.3/32 Broadcast address is 255.255.255.255 [ansible@veos-dc1-spine1 ~]$ ip link show et1 16: et1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9194 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:4a:9a:d0 brd ff:ff:ff:ff:ff:ff [ansible@veos-dc1-spine1 ~]$ [ansible@veos-dc1-spine2 ~]$ ip link show et1 22: et1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9194 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:f5:c0:d2 brd ff:ff:ff:ff:ff:ff [ansible@veos-dc1-spine2 ~]$ [ansible@veos-dc1-leaf2a ~]$ ip link show et1 26: et1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9194 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:31:35:db brd ff:ff:ff:ff:ff:ff [ansible@veos-dc1-leaf2a ~]$ veos-dc1-leaf2a#show interfaces loopback 0 Loopback0 is up, line protocol is up (connected) Hardware is Loopback Description: EVPN_Overlay_Peering Internet address is 10.255.0.5/32 Broadcast address is 255.255.255.255 This is fine, I can go home and rest, no one needs to use layer 2. Or\u0026hellip; Well it depends.\nHaving solved a well performing layer 3 fabric, the routing in the underlay does not help me if I want layer 2 mobility between servers, virtual machines etc connected to different layer 3 leafs. This is where the overlay part comes in to the rescue. And to make it a bit more interesting, I will add several overlay layers into the mix.\nOverlay in the \u0026ldquo;underlay\u0026rdquo; # In a Spine/Leaf architecture the most used overlay protocol is VXLAN as the transport plane and BGP EVPN as the control plane. Why use overlay in the physical network you say? Well its the best way of moving L2 over L3, and in a \u0026ldquo;distributed world\u0026rdquo; as the services in the datacenter today mostly is we need to make sure this can be done in a controlled and effective way in our network. Doing pure L2 will not scale, L3 is the way. EVPN for multi-tenancy.\nAdding VXLAN as the overlay protocol in my spine leaf fabric I can stretch my layer 2 networks across all my layer 3 leafs without worries. This means I can now suddenly have multiple servers physical as virtual in the same layer 2 subnet. To make that happen as effectively and with as low admin overhead as possible VXLAN will be the transport plane (overlay protocol) and again BGP will be used as the control plane. This means we now need to configure BGP in the overlay, on top of our BGP in the underlay. BGP EVPN will be used to create isolation and multi tenancy on top of the underlay. To quickly summarize, VXLAN is the transport protocol responsible of carrying the layer 2 subnets over any layer 3 link in the fabric. BGP and EVPN will be the control plane that always knows where things are located and can effectively inform where the traffic should go. This is stil all in the physical network fabric. As we will see a bit later, we can also introduce network overlay protocols in other parts of the infrastructure.\nIn the diagram above all my leaf switches has become VTEPs, VXLAN Tunnel Endpoints. Meaning they are responsible of taking my ethernet packet coming from server 1, encapsulate it, send it over the layer 3 links in my fabric, to the destination leaf where server 2 is located. The BGP configured in the overlay here is using an EVPN address family to advertise mac addresses. If one have a look at the ethernet packet coming from server 1 and destined to server 2 we will see some differerent mac addresses, depending on where you capture the traffic of course. Lets quickly illustrate that.\nThe two switches in the diagram above that will be the best place to look at all packages using tcpdump will be the spine 1 and spine 2 as they are connecting everything together and there is no direct communication between any leaf, they have to go through spine 1 and spine 2.\nIf I do a ping from server 1 to server 2 going over my VXLAN tunnel, how will the ethernet packet look like when captured at spine1s ethernet interface 1 (the one connected to leaf1a)?\nLooking at the tcpdump below from spine 1 I can clearly see some additional information added to the packet. First I see two mac addresses where source bc:24:11:1d:5f:a1 is my leaf1a and the destination mac address, bc:24:11:4a:9a:d0 is my spine1 switch. The first two ip addresses 10.255.1.3 and 10.255.1.4 is leaf1a and leaf1b VXLAN loopback interfaces respectively with the corresponding mac addresses bc:24:11:1d:5f:a1and bc:24:11:9b:8f:08. Then I get to the actual payload itself, the ICMP, between my server 1 and server 2 where I can see the source and destination ip of the actual servers doing the ping.\n15:44:33.125961 bc:24:11:1d:5f:a1 \u0026gt; bc:24:11:4a:9a:d0, ethertype IPv4 (0x0800), length 148: (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto UDP (17), length 134) 10.255.1.3.53892 \u0026gt; 10.255.1.4.4789: VXLAN, flags [I] (0x08), vni 10 bc:24:11:1d:5f:a1 \u0026gt; bc:24:11:9b:8f:08, ethertype IPv4 (0x0800), length 98: (tos 0x0, ttl 63, id 6669, offset 0, flags [DF], proto ICMP (1), length 84) 10.20.11.10 \u0026gt; 10.20.12.10: ICMP echo request, id 10, seq 108, length 64 A quick explanation on what we are looking at. The ICMP request comes in from server 1 to leaf1a and before it sends it to its destination leaf1b (vtep) it encapsulates the packet by removing some headers and adding some headers. As seen above it is adding an outer header including the source and destination VTEP mac addresses (leaf1a and leaf1b). Spine1 knows of the mac and IP address for both leaf1a and leaf1b. More info on VXLAN encapsulation further down.\nHaving a further look at a tcpdump in Wireshark captured at Spine1:\nI now notice another protocol in the header, VXLAN. The \u0026ldquo;outer\u0026rdquo; source IP and destination IP addresses are now my leaf1a and leaf1b\u0026rsquo;s VXLAN loopback interfaces. The inner source and destination IP addresses will still be my original frame, namely server 1 and server 2\u0026rsquo;s ip addresses:\nTCPdump on Arista switches (EOS) # Doing tcpdump on Arista switches is straight forward. Its just one of the benefits Arista has as it uses its EOS operating system which is a pure Linux operating system. I can do tcpdump directly from bash or remote. Here I will show a couple of methods of doing tcpumps in EOS.\nInfo:\rAs I am using vEOS (the virtual machine based EOS) I can capture dataplane traffic directly with tcpdump by using the virtual machine nics (eg vmnicX). On an actual Arista switch using the etX interfaces only captures controlplane traffic. To capture dataplane traffic on an Arista switch you can use mirror to CPU or monitoring ports. See the two links below for more info -\u003e here and here\nLocally on the switch in bash\nI will log in to the switch and be in my cli context, enter privilege mode (enable) then enter bash:\nveos-dc1-leaf1a\u0026gt;enable veos-dc1-leaf1a#bash Arista Networks EOS shell [ansible@veos-dc1-leaf1a ~]$ From here I have full access to the Linux operating system and can more or less use the exact same tools as I am used to in a regular Linux operating system.\nTo list all available interfaces, type ip link as usual.\nNow I want to do a tcpdump in the interface where my Client 1 is connected and just have a quick look of whats going on:\n[ansible@veos-dc1-leaf1a ~]$ tcpdump -i vmnicet5 -s 0 -c 2 tcpdump: verbose output suppressed, use -v[v]... for full protocol decode listening on vmnicet5, link-type EN10MB (Ethernet), snapshot length 262144 bytes 15:14:18.343604 bc:24:11:44:85:95 (oui Unknown) \u0026gt; 00:1c:73:00:00:99 (oui Arista Networks), ethertype IPv4 (0x0800), length 98: 10.20.11.10 \u0026gt; 10.20.12.10: ICMP echo request, id 11, seq 1302, length 64 15:14:18.347329 bc:24:11:1d:5f:a1 (oui Unknown) \u0026gt; bc:24:11:44:85:95 (oui Unknown), ethertype IPv4 (0x0800), length 98: 10.20.12.10 \u0026gt; 10.20.11.10: ICMP echo reply, id 11, seq 1302, length 64 2 packets captured 3 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-leaf1a ~]$ If I want to save a dump to a pcap file for Wireshark I can do the following:\n[ansible@veos-dc1-leaf1a ~]$ tcpdump -i vmnicet5 -s 0 -c 2 -w wireshark.pcap tcpdump: listening on vmnicet5, link-type EN10MB (Ethernet), snapshot length 262144 bytes 2 packets captured 39 packets received by filter 0 packets dropped by kernel [ansible@veos-dc1-leaf1a ~]$ ll total 4 -rw-r--r-- 1 ansible eosadmin 361 Nov 28 15:20 wireshark.pcap [ansible@veos-dc1-leaf1a ~]$ pwd /home/ansible [ansible@veos-dc1-leaf1a ~]$ Now I need to copy this one out of my switch to open it in Wireshark on my laptop. To do that I can use SCP. I will initate the scp command from my laptop and initiate the scp session from my laptop to my Arista switch and grab the file.\nI have configured my out of band interface in a MGMT vrf, so for me to be able to access the switch via that interface I need to put the bash session on the switch in the MGMT vrf context. So before going into bash I need to enter the VRF and then enter bash.\n[ansible@veos-dc1-leaf1a ~]$ exit logout veos-dc1-leaf1a#cli vrf MGMT veos-dc1-leaf1a(vrf:MGMT)#bash Arista Networks EOS shell [ansible@veos-dc1-leaf1a ~]$ pwd /home/ansible [ansible@veos-dc1-leaf1a ~]$ ls wireshark.pcap [ansible@veos-dc1-leaf1a ~]$ Now I have placed bash in the correct context and grab the file using SCP from my client:\nInfo:\rThe user that logs into the switch needs to be allowed to log directly to Enable mode. This is done by adding the following:\rAristaSwitch#aaa authorization exec default local ** In configure mode ** From my client where I have Wireshark installed I will execute scp to copy the pcap file from the switch (I can also run scp from the switch and copy to any destination):\n➜ vxlan_dumps scp ansible@172.18.100.103:/home/ansible/wireshark.pcap . (ansible@172.18.100.103) Password: wireshark.pcap 100% 361 46.1KB/s 00:00 ➜ vxlan_dumps Now I can open it in Wireshark\nWhen done, I can go back to the Arista switch log out of bash and into default vrf again like this:\n[ansible@veos-dc1-leaf1a ~]$ exit logout veos-dc1-leaf1a(vrf:MGMT)#cli vrf default veos-dc1-leaf1a# TCPdump remotely on the switch\nThe first approach introduced many steps to get hold of some tcpdumps.\nOne can also trigger tcpdumps remotely and either show them in your own terminal session or dump to a pcap file. To trigger the tcpdump and show live in your session I will now from my own client execute the following command:\n➜ vxlan_dumps ssh ansible@172.18.100.103 \u0026#34;bash tcpdump -s 0 -v -vv -w - -i vmnicet5 -c 4 icmp\u0026#34; | tshark -i - (ansible@172.18.100.103) Password: Capturing on \u0026#39;Standard input\u0026#39; tcpdump: listening on vmnicet5, link-type EN10MB (Ethernet), snapshot length 262144 bytes 4 packets captured 4 packets received by filter 0 packets dropped by kernel 1 0.000000 10.20.11.10 → 10.20.12.10 ICMP 98 Echo (ping) request id=0x000b, seq=3558/58893, ttl=64 2 0.003400 10.20.12.10 → 10.20.11.10 ICMP 98 Echo (ping) reply id=0x000b, seq=3558/58893, ttl=62 (request in 1) 3 1.001586 10.20.11.10 → 10.20.12.10 ICMP 98 Echo (ping) request id=0x000b, seq=3559/59149, ttl=64 4 1.005124 10.20.12.10 → 10.20.11.10 ICMP 98 Echo (ping) reply id=0x000b, seq=3559/59149, ttl=62 (request in 3) 4 packets captured If I want to write to a pcap file and save the content directly on my laptop:\n➜ vxlan_dumps ssh ansible@172.18.100.103 \u0026#34;bash tcpdump -s 0 -v -vv -w - -i vmnicet5 icmp -c 2\u0026#34; \u0026gt; wireshark.pcap (ansible@172.18.100.103) Password: tcpdump: listening on vmnicet5, link-type EN10MB (Ethernet), snapshot length 262144 bytes 2 packets captured 2 packets received by filter 0 packets dropped by kernel ➜ vxlan_dumps Now I can open it in Wireshark. All the different tcpdump options and variables is ofcourse options to play around with. It is the exact tcpdump tool you use in your everyday Linux operating system. Its not a Arista specific tcpdump tool.\nFor more information using tcpdump and scp with Arista switches head over here and here.\nOverlay in the compute stack # Up until now I have covered the underlay config briefly in a spine leaf, then the overlay in the spine leaf to overcome the layer 3 boundaries. But in the software or compute stack that is connecting to our fabric we can stumble upon other overlay protocols too.\nEven though VXLAN is the most common overlay protocol, there are other overlay protocols being used in the datacenter, like Geneve, and even NVGre and STT. This blog will discuss some typical scenarios where we deal with \u0026ldquo;layers\u0026rdquo; of overlay protocols.\nYes, the services connecting to my network also happen to use some kind of overlay protocol, can I have encapsulation on top of encapsulation? Yes, why not? In some environments we may even end up with overlay on top of overlay on top of another overlay (three \u0026ldquo;layers\u0026rdquo; of overlay). That\u0026rsquo;s not possible, you are pulling a joke here right? No I am not, yes that is fully possible and occurs very frequent. But I will loose all visibility!? Why? Using VXLAN or Geneve does not mean the traffic is being encrypted. For an network admin its not always obvious that such scenarios exist, but they do and should be something to be aware of in case something goes wrong or they suddenly discovers a new overlay protocol in their network.\nIn such scenarios though, like the movie Inception, it will be important to know how things fits together, where to monitor, how to monitor, what to think of making sure there is nothing in the way of the tunnels to be established. This is something I will try to go through in this post. How these thing fits together.\nOne example of running overlay on top of overlay is in environments where VMware (by Broadcom) NSX is involved. VMware NSX is using Geneve1 to encapsulate their Layer2 NSX Segments over any fabric between their VTEPS (usually the ESXi host transport nodes, the NSX Edges is also considered TEPS in an NSX environment). If this is connected to a spine/leaf fabric we will have VXLAN and Geneve overlay protocols moving L2 segments between their own respective VTEPS (VTEP for VXLAN, NSX just calls it TEP nowadays).\nBelow we have a spine leaf fabric using VXLAN and NSX in the compute layer using Geneve. All network segments created in NSX leaving and entering a ESXi host will be encapsulated using Geneve, and that also means all services ingressing the leafs from these ESXi hosts will get another round of encapsulation using VXLAN. I will describe this a bit better later, below is a very high level illustration.\nBut what if we are also running Kubernetes in our VMware NSX environment, which also happens to use some kind of overlay protocol (common CNIs supports VXLAN/Geneve/NVGre/STT) between the control plane and worker nodes. That will be hard to do right? Should we disable encapsulation in our Kubernetes clusters then? No, well it depends of course, but if you dont have any specific requirements to NOT use overlay (like no-snat) between your Kubernetes nodes then it makes your Kubernetes network connectivity (like pod to pod across nodes) so much easier.\nHow will this look like then?\nDoesn\u0026rsquo;t look that bad? Now if I were Leonardo DiCaprio in Inception it would be a walk in the park.\nWhere is encapsulation and decapsulation done # It is important to know where the ethernet frames are being encapsulated and decapsulated so one can understand where things may go wrong. Lets start with the Spine/Leaf fabric.\nWhen traffic or the network services enters or ingresses on any of the leafs in the fabric its the leafs role to encapsulate and decapsulate the traffic. The leafs in a VXLAN spine/leaf fabric are considered our VTEP\u0026rsquo;s (VXLAN Tunnel Endpoints). I have two servers connected to two different leafs, and also happens to be on two different VNIs (VXLAN Network Identifier), where server A needs to send a packet to server B the receiving leaf will encapsulate the ingress, then send it via any of the spines to the leaf where server B is connected and this leaf will then decapsulate the packet before its delivered to server B. See below:\nHow will this look if I add NSX into the mix? The below traffic flow will only be true if source and destination is not leaving the NSX \u0026ldquo;fabric\u0026rdquo;, meaning it is traffic between two vms in same NSX segment or two different NSX segments. If traffic is destined to go outside the NSX fabric it will need to leave via the NSX edges which will \u0026ldquo;normalise\u0026rdquo; the traffic and decapsulate the traffic to regular MTU size 1500 if not adjusted in the Edge uplinks to use bigger mtu size than default 1500.\nWhat about Kubernetes as the third overlay layer?\nWhen traffic egresses a Kubernetes nodes configured to use VXLAN or Geneve it will encapsulate the original header, if the Kubernetes nodes are running in a NSX segment the ESXi hosts currently holding the Kubernetes nodes will then add its encapsulation before egressing the packet, and finally this traffic will hit the physical Arista VTEPS/Leaf switches in the physical fabric which will also add its encapsulation before egressing to the spines. Then from the leaf to the esxi hosts to the pods the traffic will slowly but surely be decapsulated to finally arrive at the destination as the original ethernet frame stripped from any additional encapsulation headers.\nWill it fit then? # A default IP frame size is 1500MTU (Maximum Transmission Unit). When encapsulating a standard ethernet frame using VXLAN some headers are removed from the original frame, and additional headers are being added. See illustration:\nStandard Ethernet frame to a VXLAN encapsulated ethernet frame:\nSee more explanation on ethernet mtu and ip mtu further down.\nThese additional headers requires some more room which makes the default size of 1500mtu too small. So in a very simple setup using VXLAN encapsulation we need to accomodate for this increase in size. If this is not considered you will end up with nothing working. A general rule of thumb, VXLAN and Geneve will not handle fragmentation and will drop if it does not fit. A new buzzword will then become its always MTU.\nWhy is that? Imagine you have a rather big car and the road you are driving has a tunnel, this tunnel is not big enough to actually fit your car (One of the few times in your life, but assume its a monster truck you are driving). If you dont pay attention and just make a run for it, entering the tunnel will come to a hard stop.\nAnd if something should happen to come out on the other side of the tunnel it will most likely be fragments of the car. Which we dont like. We do like our cars whole, as we like our network packets whole.\nIf the tunnel is big enough to accommodate the biggest monster truck, then it should be fine.\nThe monster truck both enters and exits the tunnel safe and sound and in the exact same shape it entered. No pieces torn off or ripped off to fit the tunnel.\nIf the tunnel will bearly fit the car, you can bet some pieces would be torn off, like side mirrors etc. This can be translated into dropped packets, which we dont want.\nIf the monster truck diagram is not depictive enough, lets try another illustration. Imagine you have two steel pipes of equal diameter and width and you want one pipe to be inserted into the other pipe to maybe extend the total length by combining the two. Pretty hard to do, welding is propably your best option here. But if one pipe had a smaller diameter so it could actually fit inside the other pipe, then it would slide in without any issue.\nImagine the last pipe on the right is the Arista spine/leaf fabric. This should be the part of the infrastructure that must accommodate the biggest allowed MTU size. Then the green pipe will be the NSX environment which needs to tune their MTU setting on the TEP side (host profile) to fit the MTU size set in the Arista Fabric. Arista is default configured to use 9214 MTU. Then the Kubernetes cluster running in NSX segments also needs to adapt to the MTU size in the NSX environment.\nThis should be fairly easy to calculate as long as one know which overlay protocol being used, how much the additional headers will impose on the ethernet frame. See VXLAN above. Be aware that VXLAN has fixed headers so you will always know the exact MTU size, Geneve on the other hand (call it an VXLAN extension) is using dynamic headers and needs to be taken into consideration, it may need more room than VXLAN.\nFor VLXLAN we need at least 50 additional bytes: 14b (outer ethernet header) + 20b (outer IP header) + 8b (outer udp header) + 8b (VXLAN header) = 50b\nSo, to summarize. As long as one are aware of these MTU requirements it should be fine to run multiple stacked overlay protocols.\nSimulating an environment with triple encapsulation # I have configured in my lab a spine leaf fabric using Arista vEOS switches. It is a full spine leaf fabric using EVPN and VXLAN. The fabric has been configured with 3 VRFS. A dedicated oob mgmt vrf called MGMT. Then VRF 10 for vlan 11-13 and VRF 11 for vlan 21-23. Then I have attached three virtual machines on each of their leaf switches. Server 1 is attached to leaf1a, server 2 is attached to leaf1b and server 3 is attached to leaf2a. These three servers have been configured with two network cards each.\nNIC1 (ens18) on all three have been configured and placed in VRF10 but in their own vlan, vlan 11-13 respectively for their \u0026ldquo;management\u0026rdquo; interface. VRF 10 is the only VRF that is configured to reach internet. NIC2 (ens19) on all three servers have been configured and placed in an another VRF, VRF11, also placed in their separate vlan (vlan 21-23) respectively. NIC2 (ens19) have been configured to use VXLAN via a bridge called br-vxlan on all three servers to span a common layer 2 subnet across these 3 servers. Kubernetes is installed and configured on these servers. The pod network is using the second nic interfaces over VXLAN and the CNI inside Kubernetes has been configured to provide its own overlay protocol which happens to be Geneve. So in this scenario I have 3 overlay protocols in motion. VXLAN configured in my Arista spine leaf, VXLAN in the Ubuntu operating system layer, then Geneve in the pod network stack. The only reason I have chosen to use a dedicated management interface is just so I can have a network that is only encapsulated in my Arista fabric. Kubernetes can work perfectly well with just on network card, also most common config.\nI have tried to illustrate my setup below, to get some more context.\nServer 1 Server 2 Server 3 ens18 - 10.20.11.10/24 ens18 - 10.20.12.10/24 ens18 - 10.20.13.10/24 ens19 - 10.21.21.10/24 ens19 - 10.21.22.10/24 ens19 - 10.21.23.10/24 br-vxlan (vxlan via ens19) - 192.168.100.11/24 br-vxlan (vxlan via ens19) - 192.168.100.12/24 br-vxlan (vxlan via ens19) - 192.168.100.13/24 antrea-gw0 (interface mtu -50) antrea-gw0 (interface mtu -50) antrea-gw0 (interface mtu -50) pod-cidr (Antrea geneve tun0 via ens19) 10.40.0.0/24 pod-cidr (Antrea geneve tun0 via ens19) 10.40.1.0/24 pod-cidr (Antrea geneve tun0 via ens19) 10.40.2.0/24 K8s cluster pod cidr is 10.40.0.0/16, each node carves out pr default a /24.\nWhen the 3 nodes communicate with each other using ens18 they will only be encapsulated once, but when using the br-vxlan interface it will be double encapsulated, first vxlan in the server, then in the Arista fabric. When the pods communicate between nodes I will end up with triple encapsulation, Geneve, VXLAN in the server then VXLAN in the Arista fabric. Now it starts to be interesting. The Antrea CNI has been configured to use br-vxlan as pod transport interface:\n# traffic across Nodes. transportInterface: \u0026#34;br-vxlan\u0026#34; A note on the Antrea CNI geneve tunnel. When Antrea is deployed in the cluster it will automatically adjust the MTU based on the interfaces it is selected to use a transport interface. It does that by reading the current MTU, if it is 1500 on the transport interface it will create a Antrea GW interface -50 MTU. If br-vxlan as above is 1500MTU Antrea GW will then be 1450MTU. If I happen to adjust this MTU at a later stage to either a higher or lower MTU I just need to restart the Antrea Agents and the respective Antrea GW interfaces should automatically adjust to the new MTU again. So in theory the Antrea GW should not be making any headaches in regards to MTU issues, but one never know and it should be something to be aware of.\nLet see some triple encapsulation in action # I have two pods deployed called ubuntu-1 and ubuntu-2 with one Ubuntu container instance in each pod, these two are running on ther own Kubernetes node 1 and 2. So they have to egress the nodes to communicate. How will this look like if I do a TCP dump on leaf1b (source), where node1 is connected, if I initiate a ping from pod ubuntu-1 and ubuntu-2?\nProtocols in frame: vxlan, vxlan and geneve - look at that. Lets break it down further, layer by layer.\nAs there is some encapsulation going on for sure, at different layers, it can be a good excercise to a layer by layer breakdown:\n[Protocols in frame: eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:udp:geneve:eth:ethertype:ip:icmp:data] In total 4 pairs of source \u0026gt; destination ip addresses, including the actual payload ICMP packet between the two pods.\nFull dump for reference below.\nFrame 7: 248 bytes on wire (1984 bits), 248 bytes captured (1984 bits) Encapsulation type: Ethernet (1) Arrival Time: Dec 2, 2024 10:01:56.510773000 CET UTC Arrival Time: Dec 2, 2024 09:01:56.510773000 UTC Epoch Arrival Time: 1733130116.510773000 [Time shift for this packet: 0.000000000 seconds] [Time delta from previous captured frame: 0.034183000 seconds] [Time delta from previous displayed frame: 0.034183000 seconds] [Time since reference or first frame: 0.229242000 seconds] Frame Number: 7 Frame Length: 248 bytes (1984 bits) Capture Length: 248 bytes (1984 bits) [Frame is marked: False] [Frame is ignored: False] [Protocols in frame: eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:udp:vxlan:eth:ethertype:ip:udp:geneve:eth:ethertype:ip:icmp:data] [Coloring Rule Name: ICMP] [Coloring Rule String: icmp || icmpv6] Ethernet II, Src: ProxmoxServe_9b:8f:08 (bc:24:11:9b:8f:08), Dst: ProxmoxServe_4a:9a:d0 (bc:24:11:4a:9a:d0) Destination: ProxmoxServe_4a:9a:d0 (bc:24:11:4a:9a:d0) Source: ProxmoxServe_9b:8f:08 (bc:24:11:9b:8f:08) Type: IPv4 (0x0800) [Stream index: 0] Internet Protocol Version 4, Src: 10.255.1.4, Dst: 10.255.1.5 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 234 Identification: 0x0000 (0) 010. .... = Flags: 0x2, Don\u0026#39;t fragment ...0 0000 0000 0000 = Fragment Offset: 0 Time to Live: 64 Protocol: UDP (17) Header Checksum: 0x21fd [validation disabled] [Header checksum status: Unverified] Source Address: 10.255.1.4 Destination Address: 10.255.1.5 [Stream index: 4] User Datagram Protocol, Src Port: 64509, Dst Port: 4789 Source Port: 64509 Destination Port: 4789 Length: 214 Checksum: 0x0000 [zero-value ignored] [Stream index: 7] [Stream Packet Number: 1] [Timestamps] UDP payload (206 bytes) Virtual eXtensible Local Area Network Flags: 0x0800, VXLAN Network ID (VNI) Group Policy ID: 0 VXLAN Network Identifier (VNI): 11 Reserved: 0 Ethernet II, Src: ProxmoxServe_9b:8f:08 (bc:24:11:9b:8f:08), Dst: ProxmoxServe_31:35:db (bc:24:11:31:35:db) Destination: ProxmoxServe_31:35:db (bc:24:11:31:35:db) Source: ProxmoxServe_9b:8f:08 (bc:24:11:9b:8f:08) Type: IPv4 (0x0800) [Stream index: 3] Internet Protocol Version 4, Src: 10.21.22.10, Dst: 10.21.23.10 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 184 Identification: 0x0de9 (3561) 000. .... = Flags: 0x0 ...0 0000 0000 0000 = Fragment Offset: 0 Time to Live: 63 Protocol: UDP (17) Header Checksum: 0x2c0f [validation disabled] [Header checksum status: Unverified] Source Address: 10.21.22.10 Destination Address: 10.21.23.10 [Stream index: 5] User Datagram Protocol, Src Port: 56889, Dst Port: 4789 Source Port: 56889 Destination Port: 4789 Length: 164 Checksum: 0x91c6 [unverified] [Checksum Status: Unverified] [Stream index: 8] [Stream Packet Number: 1] [Timestamps] UDP payload (156 bytes) Virtual eXtensible Local Area Network Flags: 0x0800, VXLAN Network ID (VNI) Group Policy ID: 0 VXLAN Network Identifier (VNI): 666 Reserved: 0 Ethernet II, Src: 0e:a9:a0:1b:df:4b (0e:a9:a0:1b:df:4b), Dst: a6:58:bc:c5:47:62 (a6:58:bc:c5:47:62) Destination: a6:58:bc:c5:47:62 (a6:58:bc:c5:47:62) Source: 0e:a9:a0:1b:df:4b (0e:a9:a0:1b:df:4b) Type: IPv4 (0x0800) [Stream index: 2] Internet Protocol Version 4, Src: 192.168.100.12, Dst: 192.168.100.13 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 134 Identification: 0x8e54 (36436) 010. .... = Flags: 0x2, Don\u0026#39;t fragment ...0 0000 0000 0000 = Fragment Offset: 0 Time to Live: 64 Protocol: UDP (17) Header Checksum: 0x62a8 [validation disabled] [Header checksum status: Unverified] Source Address: 192.168.100.12 Destination Address: 192.168.100.13 [Stream index: 3] User Datagram Protocol, Src Port: 19380, Dst Port: 6081 Source Port: 19380 Destination Port: 6081 Length: 114 Checksum: 0x0000 [zero-value ignored] [Stream index: 9] [Stream Packet Number: 1] [Timestamps] UDP payload (106 bytes) Generic Network Virtualization Encapsulation, VNI: 0x000000 Ethernet II, Src: ce:7f:43:8a:0e:3c (ce:7f:43:8a:0e:3c), Dst: aa:bb:cc:dd:ee:ff (aa:bb:cc:dd:ee:ff) Destination: aa:bb:cc:dd:ee:ff (aa:bb:cc:dd:ee:ff) Source: ce:7f:43:8a:0e:3c (ce:7f:43:8a:0e:3c) Type: IPv4 (0x0800) [Stream index: 4] Internet Protocol Version 4, Src: 10.40.1.3, Dst: 10.40.2.4 0100 .... = Version: 4 .... 0101 = Header Length: 20 bytes (5) Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT) 0000 00.. = Differentiated Services Codepoint: Default (0) .... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0) Total Length: 84 Identification: 0x54ad (21677) 010. .... = Flags: 0x2, Don\u0026#39;t fragment ...0 0000 0000 0000 = Fragment Offset: 0 Time to Live: 63 Protocol: ICMP (1) Header Checksum: 0xcfa5 [validation disabled] [Header checksum status: Unverified] Source Address: 10.40.1.3 Destination Address: 10.40.2.4 [Stream index: 6] Internet Control Message Protocol Type: 8 (Echo (ping) request) Code: 0 Checksum: 0xf1a5 [correct] [Checksum Status: Good] Identifier (BE): 690 (0x02b2) Identifier (LE): 45570 (0xb202) Sequence Number (BE): 52 (0x0034) Sequence Number (LE): 13312 (0x3400) [No response seen] Timestamp from icmp data: Dec 2, 2024 10:01:56.508523000 CET [Timestamp from icmp data (relative): 0.002250000 seconds] Data (40 bytes) Monitor and troubleshoot # Everything has been configured but nothing works. Could it be MTU? There is a high probability it is MTU as both VXLAN and Geneve will not tolerate fragmentation. Lets quickly go through how to check for MTU issues and if it is related to any overlay protocols being dropped due to defragmentation.\nEthernet MTU and IP MTU # A quick note on MTU. MTU can be referred to as Ethernet MTU and IP MTU. This can be important to be aware of as these numbers are quite different and operate at two different levels: the Data Link Layer and Network Layer.\nThe Ethernet MTU is the maximum payload in bytes an Ethernet frame can carry. This refers to the Layer 2 Data Link Layer size limit. What does that mean then? Well an ethernet MTU only considers the the size of the actual packet, it does not include the ethernet frame headers MAC addresses, ether type, and fcs. Remember the ethernet frame explanation above:\nThe IP MTU on the other hand is the maximum size in bytes of the IP packet than can be transmitted. This operates at the Layer 3 Network layer and includes the entire IP packet, ip header, and transport payload (tcp/udp headers and application data). The IP MTU must fit within the Ethernet MTU. The IP packet is encapsulated in the Ethernet payload.\n+--------------------+--------------------+ | Ethernet Frame (1518 bytes) | | 14B Header + 1500B Payload + 4B Trailer| +----------------------------------------+ | IP Packet (1500 bytes max) | | 20B IP Header + 1480B Payload | +----------------------------------------+ Verifying everything in the Arista fabric # Underly links To do this as \u0026ldquo;methodically\u0026rdquo; as possible I will first start by checking mtu in the Arista fabric. This will be the ptp links between the spines and leafs. They should be configured with the highest supported MTU in EOS.\nInfo:\rThis is all done in my lab using vEOS (virtual machines running on my hypervisor). The max MTU I can use in my lab is 9194 in vEOS. In a real physical Arista switch it is 9214MTU. So the numbers I am operating with is not representative to the real products As soon as I have verified the mtu size there, I also need to verify that there is no issues with the VXLAN tunnel.\nFrom leaf1b uplink 1 and 2 to spine 1 and 2:\nveos-dc1-leaf1b#ping 10.255.255.4 source 10.255.255.5 df-bit size 9194 PING 10.255.255.4 (10.255.255.4) from 10.255.255.5 : 9166(9194) bytes of data. 9174 bytes from 10.255.255.4: icmp_seq=1 ttl=64 time=1.50 ms 9174 bytes from 10.255.255.4: icmp_seq=2 ttl=64 time=0.836 ms 9174 bytes from 10.255.255.4: icmp_seq=3 ttl=64 time=0.549 ms 9174 bytes from 10.255.255.4: icmp_seq=4 ttl=64 time=0.558 ms 9174 bytes from 10.255.255.4: icmp_seq=5 ttl=64 time=0.556 ms --- 10.255.255.4 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 5ms rtt min/avg/max/mdev = 0.549/0.800/1.503/0.367 ms, ipg/ewma 1.255/1.134 ms If I try one byte too high it should tell me:\nveos-dc1-leaf1b#ping 10.255.255.4 source 10.255.255.5 df-bit size 9195 PING 10.255.255.4 (10.255.255.4) from 10.255.255.5 : 9167(9195) bytes of data. ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 --- 10.255.255.4 ping statistics --- 5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 41ms Ok, MTU in the ptp links are good. How is the status of the VXLAN tunnel:\nveos-dc1-leaf1b#show int vx1 Vxlan1 is up, line protocol is up (connected) Hardware is Vxlan Description: veos-dc1-leaf1b_VTEP Source interface is Loopback1 and is active with 10.255.1.4 Listening on UDP port 4789 Replication/Flood Mode is headend with Flood List Source: EVPN Remote MAC learning via EVPN VNI mapping to VLANs Static VLAN to VNI mapping is [11, 10011] [12, 10012] [13, 10013] [21, 10021] [22, 10022] [23, 10023] [1079, 11079] [3401, 13401] [3402, 13402] Dynamic VLAN to VNI mapping for \u0026#39;evpn\u0026#39; is [4097, 11] [4098, 10] Note: All Dynamic VLANs used by VCS are internal VLANs. Use \u0026#39;show vxlan vni\u0026#39; for details. Static VRF to VNI mapping is [VRF20, 10] [VRF21, 11] Headend replication flood vtep list is: 11 10.255.1.6 10.255.1.3 10.255.1.5 12 10.255.1.6 10.255.1.3 10.255.1.5 13 10.255.1.6 10.255.1.3 10.255.1.5 21 10.255.1.6 10.255.1.3 10.255.1.5 22 10.255.1.6 10.255.1.3 10.255.1.5 23 10.255.1.6 10.255.1.3 10.255.1.5 1079 10.255.1.6 10.255.1.3 10.255.1.5 3401 10.255.1.6 10.255.1.3 10.255.1.5 3402 10.255.1.6 10.255.1.3 10.255.1.5 Shared Router MAC is 0000.0000.0000 veos-dc1-leaf1b#show vxlan config-sanity detail Category Result ---------------------------------- -------- Local VTEP Configuration Check OK Loopback IP Address OK VLAN-VNI Map OK Flood List OK Routing OK VNI VRF ACL OK Decap VRF-VNI Map OK VRF-VNI Dynamic VLAN OK Remote VTEP Configuration Check OK Remote VTEP OK Platform Dependent Check OK VXLAN Bridging OK VXLAN Routing OK CVX Configuration Check OK CVX Server OK MLAG Configuration Check OK Peer VTEP IP OK MLAG VTEP IP OK Peer VLAN-VNI OK Virtual VTEP IP OK MLAG Inactive State OK Detail -------------------------------------------------- Not in controller client mode Run \u0026#39;show mlag config-sanity\u0026#39; to verify MLAG config MLAG peer is not connected Downlinks to connected endpoints and VLAN interfaces\nSo my underlay links are in good shape, but I also need to have a look at the downlinks to my servers including the respective vlan interfaces whether they have been configured to also support jumbo frames. This is needed as they will be configured with jumbo frames too to accommodate their overlay tunnels. To verify that the MTU is correct there I can use one of my leafs and do some ping tests using higher mtu payload. From my leaf1b I start by verifying the default MTU on the ethernet interface 6 which is connected to server 2:\nveos-dc1-leaf1b(config)#interface ethernet 6 veos-dc1-leaf1b(config-if-Et6)#show active interface Ethernet6 description dc1-leaf1b-client2-vxlan_CLIENT2-VXLAN switchport trunk native vlan 22 switchport trunk allowed vlan 11-13,21-23 switchport mode trunk veos-dc1-leaf1b(config-if-Et6)#show interfaces ethernet 6 Ethernet6 is up, line protocol is up (connected) Hardware is Ethernet, address is bc24.117e.e982 (bia bc24.117e.e982) Description: dc1-leaf1b-client2-vxlan_CLIENT2-VXLAN Ethernet MTU 9194 bytes, BW 1000000 kbit Full-duplex, 1Gb/s, auto negotiation: off, uni-link: n/a Up 9 days, 10 hours, 40 minutes, 10 seconds Loopback Mode : None 2 link status changes since last clear Last clearing of \u0026#34;show interface\u0026#34; counters never 5 minutes input rate 6.34 kbps (0.0% with framing overhead), 5 packets/sec 5 minutes output rate 7.01 kbps (0.0% with framing overhead), 6 packets/sec 3517540 packets input, 614301075 bytes Received 35 broadcasts, 459 multicast 0 runts, 0 giants 0 input errors, 0 CRC, 0 alignment, 0 symbol, 0 input discards 0 PAUSE input 3913885 packets output, 625341082 bytes Sent 526 broadcasts, 427958 multicast 0 output errors, 0 collisions 0 late collision, 0 deferred, 0 output discards 0 PAUSE output Info:\rA note on MTU in Arista EOS. By default all Layer 2 ports are default 9214, while layer 3/routed ports are default 1500. Then I will need to verify the MTU size on the VLAN interface I am using for VXLAN in my servers, vlan interfaces 21,22 and 23.\nveos-dc1-leaf1b(config-if-Et6)#interface vlan 22 veos-dc1-leaf1b(config-if-Vl22)#show active interface Vlan22 description VRF11_VLAN22 mtu 9194 vrf VRF21 ip address virtual 10.21.22.1/24 veos-dc1-leaf1b(config-if-Vl22)#show interfaces vlan 22 Vlan22 is up, line protocol is up (connected) Hardware is Vlan, address is bc24.119b.8f08 (bia bc24.119b.8f08) Description: VRF11_VLAN22 Internet address is virtual 10.21.22.1/24 Broadcast address is 255.255.255.255 IP MTU 9194 bytes Up 9 days, 10 hours, 42 minutes, 23 seconds Now I would like to see it with my own eyes that I can actually use this MTU to the server (server2) connected. I will use ping as the tool to set a payload size of 9194 which is the switch interface mtu size that is connected to server 2. Server 2 nic has been configured with a MTU of 9000. The last ping is just 1 byte more than allowed.\nveos-dc1-leaf1b#ping vrf VRF21 10.21.22.10 source 10.21.21.1 size 9194 df-bit PING 10.21.22.10 (10.21.22.10) from 10.21.21.1 : 9166(9194) bytes of data. 9174 bytes from 10.21.22.10: icmp_seq=1 ttl=64 time=1.71 ms 9174 bytes from 10.21.22.10: icmp_seq=2 ttl=64 time=1.55 ms 9174 bytes from 10.21.22.10: icmp_seq=3 ttl=64 time=1.29 ms 9174 bytes from 10.21.22.10: icmp_seq=4 ttl=64 time=1.41 ms 9174 bytes from 10.21.22.10: icmp_seq=5 ttl=64 time=1.15 ms --- 10.21.22.10 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 8ms rtt min/avg/max/mdev = 1.149/1.422/1.707/0.194 ms, ipg/ewma 2.001/1.552 ms veos-dc1-leaf1b#ping vrf VRF21 10.21.22.10 source 10.21.21.1 size 9195 df-bit PING 10.21.22.10 (10.21.22.10) from 10.21.21.1 : 9167(9195) bytes of data. ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ping: local error: message too long, mtu=9194 ^C --- 10.21.22.10 ping statistics --- 5 packets transmitted, 0 received, +5 errors, 100% packet loss, time 41ms There is also some other nifty tools in EOS that can be used to check if fragmentation is going on, show kernel ip counters:\nveos-dc1-leaf2a#show kernel ip counters vrf VRF21 | grep -A1 \u0026#34;ICMP output\u0026#34; ICMP output histogram: destination unreachable: 285 And netstat -s from EOS bash:\nArista Networks EOS shell\r[ansible@veos-dc1-leaf2a ~]$ netstat -s\rIp:\rForwarding: 2\r372 total packets received\r367 forwarded\r0 incoming packets discarded\r5 incoming packets delivered\r657 requests sent out\rIcmp:\r5 ICMP messages received\r0 input ICMP message failed\rICMP input histogram:\recho replies: 5\r290 ICMP messages sent\r0 ICMP messages failed\rICMP output histogram:\rdestination unreachable: 285\recho requests: 5\rIcmpMsg:\rInType0: 5\rOutType3: 285\rOutType8: 5\rTcp:\r0 active connection openings\r0 passive connection openings\r0 failed connection attempts\r0 connection resets received\r0 connections established\r0 segments received\r0 segments sent out\r0 segments retransmitted\r0 bad segments received\r0 resets sent\rUdp:\r0 packets received\r0 packets to unknown port received\r0 packet receive errors\r0 packets sent\r0 receive buffer errors\r0 send buffer errors\rUdpLite:\rTcpExt:\r0 packet headers predicted\rIpExt:\rInOctets: 47155\rOutOctets: 139570\rInNoECTPkts: 372\rArista:\r[ansible@veos-dc1-leaf2a ~]$ netstat -s will also be used in the compute stack later on.\nVerifying MTU in the compute stack # To make it a bit easier to follow I will refer to the table below again with relevant information on the differents servers in my compute stack:\nServer 1 Server 2 Server 3 ens18 - 10.20.11.10/24 MTU 1500 ens18 - 10.20.12.10/24 - MTU 1500 ens18 - 10.20.13.10/24 - MTU 1500 ens19 - 10.21.21.10/24 - MTU 9000 ens19 - 10.21.22.10/24 - MTU 9000 ens19 - 10.21.23.10/24 - MTU 9000 vxlan10: MTU 1500 VNI: 666 vxlan10: MTU 1500 VNI: 666 vxlan10: MTU 1500 VNI: 666 br-vxlan (vxlan via ens19) - 192.168.100.11/24 - MTU 1500 br-vxlan (vxlan via ens19) - 192.168.100.12/24 - MTU 1500 br-vxlan (vxlan via ens19) - 192.168.100.13/24 - MTU 1500 antrea-gw0 (interface mtu -50) antrea-gw0 (interface mtu -50) antrea-gw0 (interface mtu -50) pod-cidr (Antrea geneve tun0 via ens19) 10.40.0.0/24 - MTU 1450 (auto adapts to br-vxlan) pod-cidr (Antrea geneve tun0 via ens19) 10.40.1.0/24 - MTU 1450 (auto adapts to br-vxlan) pod-cidr (Antrea geneve tun0 via ens19) 10.40.2.0/24 - MTU 1450 (auto adapts to br-vxlan) ens18 \u0026gt; leaf1a ethernet 5 - mtu1500 \u0026gt; mtu9194 ens18 \u0026gt; leaf1b ethernet 5 - mtu1500 \u0026gt; mtu9194 ens18 \u0026gt; leaf2a ethernet 5 - mtu1500 \u0026gt; mtu9194 ens19 \u0026gt; leaf1a ethernet 6 - mtu9000 \u0026gt; mtu9194 ens19 \u0026gt; leaf1b ethernet 6 - mtu9000 \u0026gt; mtu9194 ens19 \u0026gt; leaf2a ethernet 6 - mtu9000 \u0026gt; mtu9194 My first test will be to verify that I can do a max IP MTU of 8972 using the ens19 interface between server 2 and server 3. The reason for that is because interface ens19 will be the uplink that vxlan in my compute stack will use, so I need to make sure it can handle bigger than 1500 mtu.\nandreasm@eos-client-2:~$ ping -I ens19 10.21.23.10 -M do -s 8972 -c 4 PING 10.21.23.10 (10.21.23.10) 8972(9000) bytes of data. 8980 bytes from 10.21.23.10: icmp_seq=1 ttl=62 time=3.89 ms 8980 bytes from 10.21.23.10: icmp_seq=2 ttl=62 time=4.36 ms 8980 bytes from 10.21.23.10: icmp_seq=3 ttl=62 time=4.70 ms 8980 bytes from 10.21.23.10: icmp_seq=4 ttl=62 time=4.17 ms --- 10.21.23.10 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3006ms rtt min/avg/max/mdev = 3.889/4.278/4.697/0.293 ms That went fine. Now I have verified the mtu between my servers running on top of my Arista fabrics that they can actually handle the additional overhead by using VXLAN between my hosts.\nNext test is trying to ping using the overlay subnet configured on a bridge called br-vxlan:\nandreasm@eos-client-2:~$ ping -I br-vxlan 192.168.100.13 -M do -s 1472 -c 4 PING 192.168.100.13 (192.168.100.13) from 192.168.100.12 br-vxlan: 1472(1500) bytes of data. 1480 bytes from 192.168.100.13: icmp_seq=1 ttl=64 time=3.52 ms 1480 bytes from 192.168.100.13: icmp_seq=2 ttl=64 time=4.24 ms 1480 bytes from 192.168.100.13: icmp_seq=3 ttl=64 time=4.04 ms 1480 bytes from 192.168.100.13: icmp_seq=4 ttl=64 time=3.97 ms --- 192.168.100.13 ping statistics --- 4 packets transmitted, 4 received, 0% packet loss, time 3004ms rtt min/avg/max/mdev = 3.515/3.939/4.242/0.265 ms This is for now only confgured to use an mtu of 1500.\nTo verify the vxlan tunnel status:\nandreasm@eos-client-2:~$ bridge fdb show 33:33:00:00:00:01 dev ens18 self permanent 01:00:5e:00:00:01 dev ens18 self permanent 33:33:ff:ca:c7:03 dev ens18 self permanent 01:80:c2:00:00:00 dev ens18 self permanent 01:80:c2:00:00:03 dev ens18 self permanent 01:80:c2:00:00:0e dev ens18 self permanent 01:00:5e:00:00:01 dev ens19 self permanent 33:33:00:00:00:01 dev ens19 self permanent 33:33:ff:b8:13:63 dev ens19 self permanent 01:80:c2:00:00:00 dev ens19 self permanent 01:80:c2:00:00:03 dev ens19 self permanent 01:80:c2:00:00:0e dev ens19 self permanent 33:33:00:00:00:01 dev ovs-system self permanent 33:33:00:00:00:01 dev genev_sys_6081 self permanent 01:00:5e:00:00:01 dev genev_sys_6081 self permanent 33:33:ff:b9:ce:70 dev genev_sys_6081 self permanent 01:00:5e:00:00:01 dev antrea-gw0 self permanent 33:33:00:00:00:01 dev antrea-gw0 self permanent 33:33:ff:4f:1c:f0 dev antrea-gw0 self permanent 33:33:00:00:00:01 dev antrea-egress0 self permanent a6:58:bc:c5:47:62 dev vxlan10 master br-vxlan 42:87:fd:bd:57:63 dev vxlan10 master br-vxlan 1a:f1:cc:65:97:37 dev vxlan10 vlan 1 master br-vxlan permanent 1a:f1:cc:65:97:37 dev vxlan10 master br-vxlan permanent 00:00:00:00:00:00 dev vxlan10 dst 10.21.21.10 self permanent 00:00:00:00:00:00 dev vxlan10 dst 10.21.23.10 self permanent 33:33:00:00:00:01 dev br-vxlan self permanent 01:00:5e:00:00:6a dev br-vxlan self permanent 33:33:00:00:00:6a dev br-vxlan self permanent 01:00:5e:00:00:01 dev br-vxlan self permanent 33:33:ff:1b:df:4b dev br-vxlan self permanent 0e:a9:a0:1b:df:4b dev br-vxlan vlan 1 master br-vxlan permanent 0e:a9:a0:1b:df:4b dev br-vxlan master br-vxlan permanent 33:33:00:00:00:01 dev ubuntu-2-ce6914 self permanent 01:00:5e:00:00:01 dev ubuntu-2-ce6914 self permanent 33:33:ff:8f:cf:b7 dev ubuntu-2-ce6914 self permanent Its also possible to see some increasing statistics here:\nandreasm@eos-client-2:~$ cat /sys/class/net/vxlan10/statistics/tx_packets 82088 andreasm@eos-client-2:~$ cat /sys/class/net/vxlan10/statistics/tx_packets 82100 andreasm@eos-client-2:~$ cat /sys/class/net/vxlan10/statistics/tx_packets 82102 andreasm@eos-client-2:~$ cat /sys/class/net/vxlan10/statistics/tx_packets 82104 andreasm@eos-client-2:~$ cat /sys/class/net/vxlan10/statistics/rx_packets 81016 andreasm@eos-client-2:~$ cat /sys/class/net/vxlan10/statistics/rx_packets 81035 If I want to allow for higher mtu I will need to adjust both the vxlan interface and br-vxlan bridge interface. Lets try by just raising the MTU on the br-vxlan interface:\nandreasm@eos-client-2:~$ sudo ip link set mtu 8000 dev br-vxlan [sudo] password for andreasm: andreasm@eos-client-2:~$ ping -I br-vxlan 192.168.100.13 -M do -s 1600 -c 4 PING 192.168.100.13 (192.168.100.13) from 192.168.100.12 br-vxlan: 1600(1628) bytes of data. --- 192.168.100.13 ping statistics --- 4 packets transmitted, 0 received, 100% packet loss, time 3062ms Thats not promising.. Remember the monster truck? Thats just what I did now. I came with a car too big to fit the tunnel. The br-vxlan interface had a higher MTU than the VXLAN interface which is still at 1500\nReverting br-vxlan back to MTU1500 again. I will now set the mtu to 100 on the vlan interfaces where my server 2 ens19 interface is connected, I will hit the same scenario as above, but can I see it somehow in my Arista fabric, and the server itself?\nveos-dc1-leaf1b(config-if-Vl22)#show active interface Vlan22 description VRF11_VLAN22 mtu 100 vrf VRF21 ip address virtual 10.21.22.1/24 Using netstat -s on my affected server 2 I can notice an increase in destination unreachable:\nandreasm@eos-client-2:~/vxlan$ netstat -s | grep -A 10 -E \u0026#34;^Icmp:\u0026#34; Icmp: 5420 ICMP messages received 216 input ICMP message failed ICMP input histogram: destination unreachable: 3452 echo requests: 619 echo replies: 1349 7111 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 3152 andreasm@eos-client-2:~/vxlan$ andreasm@eos-client-2:~/vxlan$ netstat -s | grep -A 10 -E \u0026#34;^Icmp:\u0026#34; Icmp: 5455 ICMP messages received 216 input ICMP message failed ICMP input histogram: destination unreachable: 3453 echo requests: 619 echo replies: 1383 7179 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 3152 Doing tcpdump on interface ethernet 6 on my leaf1b switch I can see a whole bunch of Fragmented IP protocol:\nThe same is seen on my server 2 ens19 interface:\nDoing tcpdump directly on server 2 itself I see a lots of bytes missing!:\nandreasm@eos-client-2:~/tcpdump/mtu_issues$ sudo tcpdump -i ens19 -c 100 -vvv tcpdump: listening on ens19, link-type EN10MB (Ethernet), snapshot length 262144 bytes 10:47:43.473406 IP (tos 0x0, ttl 61, id 48283, offset 0, flags [+], proto UDP (17), length 100) 10.21.23.10.43221 \u0026gt; eos-client-2.4789: VXLAN, flags [I] (0x08), vni 666 IP truncated-ip - 63 bytes missing! (tos 0x0, ttl 64, id 43823, offset 0, flags [DF], proto UDP (17), length 113) 192.168.100.13.10351 \u0026gt; 192.168.100.11.10351: UDP, length 85 10:47:43.473407 IP (tos 0x0, ttl 61, id 48283, offset 80, flags [none], proto UDP (17), length 83) 10.21.23.10 \u0026gt; eos-client-2: udp 10:47:43.476149 IP (tos 0x0, ttl 62, id 11890, offset 0, flags [none], proto UDP (17), length 100) 10.21.21.10.55208 \u0026gt; eos-client-2.4789: [udp sum ok] VXLAN, flags [I] (0x08), vni 666 IP (tos 0x0, ttl 64, id 33812, offset 0, flags [DF], proto UDP (17), length 50) 192.168.100.11.10351 \u0026gt; 192.168.100.13.10351: [udp sum ok] UDP, length 22 10:47:43.709133 IP (tos 0x0, ttl 61, id 48311, offset 0, flags [+], proto UDP (17), length 100) 10.21.23.10.35009 \u0026gt; eos-client-2.4789: VXLAN, flags [I] (0x08), vni 666 IP truncated-ip - 978 bytes missing! (tos 0x0, ttl 64, id 0, offset 0, flags [DF], proto ICMP (1), length 1028) Reverting back to the correct mtu size on the vlan interface on leaf1b again, I should no longer see any fragments or missing bytes:\nandreasm@eos-client-2:~/tcpdump/mtu_issues$ sudo tcpdump -i ens19 -c 100 -vvv tcpdump: listening on ens19, link-type EN10MB (Ethernet), snapshot length 262144 bytes 09:29:38.728403 IP (tos 0x0, ttl 64, id 36841, offset 0, flags [none], proto UDP (17), length 1128) eos-client-2.53002 \u0026gt; 10.21.23.10.4789: [udp sum ok] VXLAN, flags [I] (0x08), vni 666 IP (tos 0x0, ttl 64, id 39910, offset 0, flags [DF], proto UDP (17), length 1078) eos-client-2.34860 \u0026gt; 192.168.100.13.6081: [no cksum] Geneve, Flags [none], vni 0x0 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto ICMP (1), length 1028) 10.40.1.4 \u0026gt; 10.40.2.4: ICMP echo request, id 350, seq 1369, length 1008 09:29:38.728444 IP (tos 0x0, ttl 64, id 21433, offset 0, flags [none], proto UDP (17), length 1128) eos-client-2.53002 \u0026gt; 10.21.21.10.4789: [udp sum ok] VXLAN, flags [I] (0x08), vni 666 IP (tos 0x0, ttl 64, id 39910, offset 0, flags [DF], proto UDP (17), length 1078) eos-client-2.34860 \u0026gt; 192.168.100.13.6081: [no cksum] Geneve, Flags [none], vni 0x0 IP (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto ICMP (1), length 1028) 10.40.1.4 \u0026gt; 10.40.2.4: ICMP echo request, id 350, seq 1369, length 1008 Summary # It is not unusual to end up with double or even triple encapsulation. Its just a matter of having control of the MTU at the different levels. They physical fabric needs to accommodate the highest MTU as thats where all traffic needs to traverse. Then the next encapsulation layer also needs to adjust according to the physical MTU configuration. If a third encapsulation layer is deployed this needs to be adjusted to the second encapsulation layers MTU configuration. The third encapsulation layer in my environment above comes from the Kubernetes CNI Antrea, which handles this automatically, but should still be something to be aware of in case of performance or any issues thats needs to be looked into.\nIf one follow the MTU sizing accordingly it shouldn\u0026rsquo;t be any issues, but if it does it is good to know where to look?\nIf using tcpdump and manual cli commands is cumbersome, there may be another more elegant way to monitor such things. I will create a follow up post on this at a later stage.\nI will end this post with a simple diagram.\n(Earlier version of VMware NSX called NSX-V used VXLAN too, for a period VMware had two NSX versions NSX-V and NSX-T where the latter used Geneve and is the current NSX product, NSX-V is obsolete and NSX-T now is the only product and is just called NSX.)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"8 November 2024","externalUrl":null,"permalink":"/2024/11/08/overlay-on-overlay-arista-in-the-underlay/","section":"Posts","summary":"Describing typical deployments where we end up with double encapsulation, how to monitor, troubleshoot","title":"Overlay on overlay - Arista in the underlay","type":"posts"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/tags/topologies/","section":"Tags","summary":"","title":"Topologies","type":"tags"},{"content":"","date":"8 November 2024","externalUrl":null,"permalink":"/categories/topologies/","section":"Categories","summary":"","title":"Topologies","type":"categories"},{"content":" Arista CloudVision, AVD and Containerlab # This post will continue upon my previous post about automated provisioning of EOS using Arista Validated Design. I will be using Arista Validated Design in combination with Arista CloudVision for provisioning and configuration instead of sending the config directly to the switches. There are some differences (and many benefits) of using CloudVision, which I will quickly touch upon in the context of this post a bit further down. I will probably at a later stage create a dedicated post on Arista CloudVision as it is a very comprehensive management tool.\nAnother difference in this post is that I will be using a containerized EOS (cEOS) instead of a virtual machine based EOS (vEOS). The benefit here is that I can make use of Containerlab which takes care of the orchestrating of all the cEOS containers I need, and that in a very rapid way too. An absolute incredible tool to quickly facilitate a full blown lab with support for a vast set of scenarios and topologies.\nA summary of this post will be Containerlab providing the platform to deploy and run the cEOS switches, Arista Validated Design providing the automated config for my selected topology (spine-leaf, using same as in previous post) and Arista CloudVision as the tool that handles and manages all my devices, pushing the config to the cEOS\u0026rsquo;es.\nArista CloudVision # CloudVision® is Arista’s modern, multi-domain management platform that leverages cloud networking principles to deliver a simplified NetOps experience. Unlike traditional domain-specific management solutions, CloudVision enables zero-touch network operations with consistent operations enterprise-wide, helping to break down the complexity of siloed management approaches.\nAs Arista’s platform for Network as-a-Service, CloudVision is designed to bring OpEx efficiency through automation across the entire network lifecycle - from design, to operations, to support and ongoing maintenance.\nSource: Arista\nAs this post is not meant to focus on CloudVision (will be referred to as CVP throughout this post) alone I will just concentrate on the parts in CloudVision that are relevant and differs from my previous post.\nCloudVision configlets, containers, tasks and change control # When using CVP in combination with Arista Validated Design (will be referred to as AVD throughout this post) I have much more control on how and when configurations are being sent to my Arista EOS switches. Just to name one, change control and the ability to review and compare configs before approving and pushing the config to the devices.\nEOS in CVP inventory or not # AVD in itself does not require any actual devices to send the configs to, as it can also be used to solely create configurations and documentation for a planned topology. This is also possible in combination with CVP as this will then create the necessary containers, configlets and tasks in CVP regardless of the devices being in CVPs inventory or not. For more reference on this using AVD see here.\nIf I want to also push the config to the EOS switches as a full hands-off automated configuration using CVP and AVD I need to have the EOS switches already in CVPs inventory, otherwise there will be no devices for CVP to send the configs to (kind of obvious). What\u0026rsquo;s not so obvious maybe is that one need to inform AVD whether the devices are already in CVPs inventory or not or else the AVD playbook deploy-cvp will fail.\nAdding the EOS switches to CVPs inventory can be done either through the Zero Touch Provisioning (ZTP from here on) of the EOS switches or manually add them to CVP after they have been initially configured. In this post I will be adding the EOS swtiches to the CVP inventory as part of the ZTP.\nChange Control and tasks # The Change Control module selects and executes a group of tasks that you want to process simultaneously. Selecting tasks and creating Change Controls function similarly in Change Control and Task Management modules.\nChange Controls provides the following benefits:\nSequencing tasks Adding unlimited snapshots to every device impacted by the Change Control execution Adding custom actions Pushing images via Multi-Chassis Link Aggregation (MLAG) In-Service Software Upgrade (ISSU) or Border Gateway Protocol (BGP) maintenance mode Reviewing the entire set of changes to approve Change Controls Even with the devices added to CVPs inventory I have a choice whether I want the config to be automatically approved and pushed to the devices when I run playbook-ansible deploy-cvp.yml or if I just want the task to be configured and wait for a change manager to review and approve before the config is pushed. This is a very useful and powerful feature in a production environment.\nTo control how AVD is handling this is described here, the key execute= false or true in the deploy-cvp.yaml. If the latter is configured with false, AVD will instruct CVP to only create the configlets, containers and tasks. The tasks will be in a pending state until the change manager creates a task and approve or rejects it.\nContainers and configlets # In CVP the switches can be put into respective containers. In combination with AVD these containers are being created automatically and the switches are moved into their container based on their group membership in the inventory.yml\nFABRIC: children: DC1: children: DC1_SPINES: hosts: dc1-spine1: ansible_host: 192.168.20.2 dc1-spine2: ansible_host: 192.168.20.3 DC1_L3_LEAVES: hosts: dc1-leaf1: ansible_host: 192.168.20.4 dc1-leaf2: ansible_host: 192.168.20.5 dc1-borderleaf1: ansible_host: 192.168.20.6 If the EOS switches are being added manually or as part of the ZTP they will be automatically placed in the Undefined container and when AVD creates the task and it is approved they will be moved from there to their container accordingly.\nAVD will create the intended configuration under Configlets:\nThese contains the configs CVP will use to push to the respective devices, which can be easily inspected by just clicking on one of them:\nContainerlab # Now onto the next gem in this post, Containerlab\nWith the growing number of containerized Network Operating Systems grows the demand to easily run them in the user-defined, versatile lab topologies.\nUnfortunately, container orchestration tools like docker-compose are not a good fit for that purpose, as they do not allow a user to easily create connections between the containers which define a topology.\nContainerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them to create lab topologies of users choice and manages labs lifecycle.\nSource: Containerlab\nTo get started with Containerlab couldn\u0026rsquo;t be more simple. Probably one of the easiest project out there to get started with.\nIn my lab I have prepared a Ubuntu virtual machine with some disk, 8 CPU and 16GB of ram (this could probably be reduces to a much smaller spec, but I have the resources available for it).\nOn my clean ubuntu machine, I just had to run this script provided by Containerlab to prepare and install everything needed (including Docker).\ncurl -sL https://containerlab.dev/setup | sudo -E bash -s \u0026#34;all\u0026#34; Thats it, after a couple of minutes it is ready. All I need now is to grab my cEOS image and upload it to my local Docker image registry (same host as Containerlab is installed).\nContainerlab supports a bunch of network operating system containers, like cEOS, and there is ofcourse a lot of options, configuration possibilites and customization that can be done. I recommend Containerlab highly and the documentation provided on the page is very good. All the info I needed to get started was provided in the Containerlab documentation.\nGetting the lab up and running # My intention is to deploy 5 cEOS containers to form my spine-leaf topology, like this:\nPrepare Containerlab to deploy my cEOS containers and desired topology # To get started with cEOS in Containerlab I have to download the cEOS image to my VM running Containerlab. Then I need to upload it to the local Docker image registry.\nandreasm@containerlab:~$ docker import cEOS-lab-4.32.2F.tar.xz ceos:4.32.2F andreasm@containerlab:~$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ceos 4.32.2F deaae9fc39b3 3 days ago 2.09GB When the image is available locally I can start creating my Containerlab Topology file:\nname: spine-leaf-borderleaf mgmt: network: custom_mgmt # management network name ipv4-subnet: 192.168.20.0/24 # ipv4 range topology: nodes: node-1: kind: arista_ceos image: ceos:4.32.2F startup-config: node1-startup-config.cfg mgmt-ipv4: 192.168.20.2 node-2: kind: arista_ceos image: ceos:4.32.2F startup-config: node2-startup-config.cfg mgmt-ipv4: 192.168.20.3 node-3: kind: arista_ceos image: ceos:4.32.2F startup-config: node3-startup-config.cfg mgmt-ipv4: 192.168.20.4 node-4: kind: arista_ceos image: ceos:4.32.2F startup-config: node4-startup-config.cfg mgmt-ipv4: 192.168.20.5 node-5: kind: arista_ceos image: ceos:4.32.2F startup-config: node5-startup-config.cfg mgmt-ipv4: 192.168.20.6 br-node-3: kind: bridge br-node-4: kind: bridge br-node-5: kind: bridge links: - endpoints: [\u0026#34;node-3:eth1\u0026#34;, \u0026#34;node-1:eth1\u0026#34;] - endpoints: [\u0026#34;node-3:eth2\u0026#34;, \u0026#34;node-2:eth1\u0026#34;] - endpoints: [\u0026#34;node-4:eth1\u0026#34;, \u0026#34;node-1:eth2\u0026#34;] - endpoints: [\u0026#34;node-4:eth2\u0026#34;, \u0026#34;node-2:eth2\u0026#34;] - endpoints: [\u0026#34;node-5:eth1\u0026#34;, \u0026#34;node-1:eth3\u0026#34;] - endpoints: [\u0026#34;node-5:eth2\u0026#34;, \u0026#34;node-2:eth3\u0026#34;] - endpoints: [\u0026#34;node-3:eth3\u0026#34;, \u0026#34;br-node-3:n3-eth3\u0026#34;] - endpoints: [\u0026#34;node-4:eth3\u0026#34;, \u0026#34;br-node-4:n4-eth3\u0026#34;] - endpoints: [\u0026#34;node-5:eth3\u0026#34;, \u0026#34;br-node-5:n5-eth3\u0026#34;] A short explanation to the above yaml. I define a custom management network for the cEOS oob Management0 interface, then I add all my nodes, defining a static management ip pr node. I also point to a startup config that provides the necessary minimum config for my cEOS\u0026rsquo;es. I will provide the output of the configs further down. Then I have added 3 bridges, these are being used for the downlinks on the leaves for client/server/vm connectivity later on. The links section defines how the cEOS \u0026ldquo;interconnect\u0026rdquo; with each other. This linking is taken care of by Containerlab. I only need to define which interfaces from which nodes that links to which interface on the other nodes. These links are my \u0026ldquo;peer-links\u0026rdquo;. Then I have the br-node links that just links to these br-node-x bridges created in my os like this:\nandreasm@containerlab:~$ sudo ip link add name br-node-3 type bridge andreasm@containerlab:~$ sudo ip link set br-node-3 up Below is my startup-config for each cEOS:\n! daemon TerminAttr exec /usr/bin/TerminAttr -cvaddr=172.18.100.99:9910 -cvauth=token,/tmp/token -cvvrf=MGMT -disableaaa -smashexcludes=ale,flexCounter,hardware,kni,pulse,strata -ingestexclude=/Sysdb/cell/1/agent,/Sysdb/cell/2/agent -taillogs no shutdown ! hostname dc1-spine1 ! ! Configures username and password for the ansible user username ansible privilege 15 role network-admin secret sha512 $4$redacted/ ! ! Defines the VRF for MGMT vrf instance MGMT ! ! Defines the settings for the Management1 interface through which Ansible reaches the device interface Management0 # Note the Management0 here.. not 1 description oob_management no shutdown vrf MGMT ! IP address - must be set uniquely per device ip address 192.168.20.2/24 ! ! Static default route for VRF MGMT ip route vrf MGMT 0.0.0.0/0 192.168.20.1 ! ! Enables API access in VRF MGMT management api http-commands protocol https no shutdown ! vrf MGMT no shutdown ! end ! ! Save configuration to flash copy running-config startup-config The startup config above includes the config for adding them to my CVP inventory, daemon TerminAttr.\nNow it is time to deploy my lab. Let see how this goes \u0026#x1f604;\nandreasm@containerlab:~/containerlab/lab-spine-leaf-cvp$ sudo containerlab deploy -t spine-leaf-border.yaml INFO[0000] Containerlab v0.56.0 started INFO[0000] Parsing \u0026amp; checking topology file: spine-leaf-border.yaml INFO[0000] Creating docker network: Name=\u0026#34;custom_mgmt\u0026#34;, IPv4Subnet=\u0026#34;192.168.20.0/24\u0026#34;, IPv6Subnet=\u0026#34;\u0026#34;, MTU=0 INFO[0000] Creating lab directory: /home/andreasm/containerlab/lab-spine-leaf-cvp/clab-spine-leaf-borderleaf INFO[0000] Creating container: \u0026#34;node-1\u0026#34; INFO[0000] Creating container: \u0026#34;node-5\u0026#34; INFO[0000] Creating container: \u0026#34;node-3\u0026#34; INFO[0000] Creating container: \u0026#34;node-4\u0026#34; INFO[0000] Creating container: \u0026#34;node-2\u0026#34; INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-5\u0026#39; node INFO[0001] Created link: node-5:eth3 \u0026lt;--\u0026gt; br-node-5:n5-eth3 INFO[0001] Created link: node-4:eth1 \u0026lt;--\u0026gt; node-1:eth2 INFO[0001] Created link: node-3:eth1 \u0026lt;--\u0026gt; node-1:eth1 INFO[0001] Created link: node-4:eth2 \u0026lt;--\u0026gt; node-2:eth2 INFO[0001] Created link: node-3:eth2 \u0026lt;--\u0026gt; node-2:eth1 INFO[0001] Created link: node-5:eth1 \u0026lt;--\u0026gt; node-1:eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-1\u0026#39; node INFO[0001] Created link: node-4:eth3 \u0026lt;--\u0026gt; br-node-4:n4-eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-4\u0026#39; node INFO[0001] Created link: node-3:eth3 \u0026lt;--\u0026gt; br-node-3:n3-eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-3\u0026#39; node INFO[0001] Created link: node-5:eth2 \u0026lt;--\u0026gt; node-2:eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-2\u0026#39; node INFO[0050] Adding containerlab host entries to /etc/hosts file INFO[0050] Adding ssh config for containerlab nodes INFO[0050] 🎉 New containerlab version 0.57.0 is available! Release notes: https://containerlab.dev/rn/0.57/ Run \u0026#39;containerlab version upgrade\u0026#39; to upgrade or go check other installation options at https://containerlab.dev/install/ +---+-----------------------------------+--------------+--------------+-------------+---------+-----------------+--------------+ | # | Name | Container ID | Image | Kind | State | IPv4 Address | IPv6 Address | +---+-----------------------------------+--------------+--------------+-------------+---------+-----------------+--------------+ | 1 | clab-spine-leaf-borderleaf-node-1 | 9947cd235370 | ceos:4.32.2F | arista_ceos | running | 192.168.20.2/24 | N/A | | 2 | clab-spine-leaf-borderleaf-node-2 | 2051bdcc81e6 | ceos:4.32.2F | arista_ceos | running | 192.168.20.3/24 | N/A | | 3 | clab-spine-leaf-borderleaf-node-3 | 0b6ef17f29e8 | ceos:4.32.2F | arista_ceos | running | 192.168.20.4/24 | N/A | | 4 | clab-spine-leaf-borderleaf-node-4 | f88bfe335603 | ceos:4.32.2F | arista_ceos | running | 192.168.20.5/24 | N/A | | 5 | clab-spine-leaf-borderleaf-node-5 | a1f6eff1bd18 | ceos:4.32.2F | arista_ceos | running | 192.168.20.6/24 | N/A | +---+-----------------------------------+--------------+--------------+-------------+---------+-----------------+--------------+ andreasm@containerlab:~/containerlab/lab-spine-leaf-cvp$ Wow, a new version of Containerlab is out\u0026hellip;\nAfter a minute or two The cEOS containers are up and running it seems. Lets see if I can log into one of them.\nandreasm@containerlab:~/containerlab/lab-spine-leaf-cvp$ ssh ansible@192.168.20.2 (ansible@192.168.20.2) Password: dc1-spine1\u0026gt; dc1-spine1\u0026gt;en dc1-spine1#configure dc1-spine1(config)#show running-config ! Command: show running-config ! device: dc1-spine1 (cEOSLab, EOS-4.32.2F-38195967.4322F (engineering build)) ! no aaa root ! username ansible privilege 15 role network-admin secret sha512 $6redactedxMEEocchsdf/ ! management api http-commands no shutdown ! vrf MGMT no shutdown ! daemon TerminAttr exec /usr/bin/TerminAttr -cvaddr=172.18.100.99:9910 -cvauth=token,/tmp/token -cvvrf=MGMT -disableaaa -smashexcludes=ale,flexCounter,hardware,kni,pulse,strata -ingestexclude=/Sysdb/cell/1/agent,/Sysdb/cell/2/agent -taillogs no shutdown ! no service interface inactive port-id allocation disabled ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname dc1-spine1 ! spanning-tree mode mstp ! system l1 unsupported speed action error unsupported error-correction action error ! vrf instance MGMT ! interface Ethernet1 ! interface Ethernet2 ! interface Ethernet3 ! interface Management0 description oob_management vrf MGMT ip address 192.168.20.2/24 ! no ip routing no ip routing vrf MGMT ! ip route vrf MGMT 0.0.0.0/0 192.168.20.1 ! router multicast ipv4 software-forwarding kernel ! ipv6 software-forwarding kernel ! end dc1-spine1(config)# This is really nice, now did something happen in my CVP?\nBefore I started my lab, this was the view in my CVP:\nNow, lets go in and check again:\nAlright, my lab is up. But I am missing the full config ofcourse. They have just been deployed, but no connections, peerings etc have been made.\nBack to Containerlab. There is one command I would like to test out, containerlab graph -t topology.yml. Lets see what this does:\nandreasm@containerlab:~/containerlab/lab-spine-leaf-cvp$ sudo containerlab graph -t spine-leaf-border.yaml INFO[0000] Parsing \u0026amp; checking topology file: spine-leaf-border.yaml INFO[0000] Serving static files from directory: /etc/containerlab/templates/graph/nextui/static INFO[0000] Serving topology graph on http://0.0.0.0:50080 Lets open my browser:\nA full topology layout, including the bridge interfaces!!!! NICE\nPreparing Arista Validated Design to use CVP # I will not go through all the files I have edited in my AVD project folder as most of them are identical to my previous post but will reflect upon the changes for using CVP instead of directly to the EOS switches in the relevant file. I have commented under which files needs to be updated, and provide my examples. The rest is not shown or done any changes on.\nBelow is the files I need to edit in general for AVD to deploy my desired single DC L3LS spine-leaf topology.\n├── ansible.cfg ├── deploy-cvp.yml # execute false or true ├── group_vars/ │ ├── CONNECTED_ENDPOINTS.yml # untouched │ ├── DC1_L2_LEAVES.yml # untouched │ ├── DC1_L3_LEAVES.yml # untouched │ ├── DC1_SPINES.yml # untouched │ ├── DC1.yml # added \u0026#34;mgmt_interface: Management0\u0026#34; and updated dict-of-dicts to list-of-dicts │ ├── FABRIC.yml # This needs to reflect on my CVP endpoint │ └── NETWORK_SERVICES.yml # untouched ├── inventory.yml # This needs to reflect my CVP configuration In the deploy-cvp.yml I need to edit execute_tasks If I want to execute the tasks directly from AVD in CVP or not. I have disabled execution of the task (default) as I want to show how it looks like in CVP.\n--- - name: Deploy Configurations to Devices Using CloudVision Portal # (1)! hosts: CLOUDVISION gather_facts: false connection: local tasks: - name: Deploy Configurations to CloudVision # (2)! ansible.builtin.import_role: name: arista.avd.eos_config_deploy_cvp vars: cv_collection: v3 # (3)! fabric_name: FABRIC # (4)! execute_tasks: false In the inventory.yml the CVP relevant section is added. (it is in there by default, but I removed it in previous post as I did not use it)\n--- all: children: CLOUDVISION: hosts: cvp: # Ansible variables used by the ansible_avd and ansible_cvp roles to push configuration to devices via CVP ansible_host: cvp-01.domain.net ansible_httpapi_host: cvp-01.domain.net ansible_user: ansible ansible_password: password ansible_connection: httpapi ansible_httpapi_use_ssl: true ansible_httpapi_validate_certs: false ansible_network_os: eos ansible_httpapi_port: 443 ansible_python_interpreter: $(which python3) FABRIC: children: DC1: children: DC1_SPINES: hosts: dc1-spine1: ansible_host: 192.168.20.2 dc1-spine2: ansible_host: 192.168.20.3 DC1_L3_LEAVES: hosts: dc1-leaf1: ansible_host: 192.168.20.4 dc1-leaf2: ansible_host: 192.168.20.5 dc1-borderleaf1: ansible_host: 192.168.20.6 NETWORK_SERVICES: children: DC1_L3_LEAVES: CONNECTED_ENDPOINTS: children: DC1_L3_LEAVES: DC1.yml is updated to reflect the coming deprecation of dict-of-dicts to list-of-dicts and added the mgmt_interface: Management0.\n--- # Default gateway used for the management interface mgmt_gateway: 192.168.0.1 mgmt_interface: Management0 # Spine switch group spine: # Definition of default values that will be configured to all nodes defined in this group defaults: # Set the relevant platform as each platform has different default values in Ansible AVD platform: cEOS-lab # Pool of IPv4 addresses to configure interface Loopback0 used for BGP EVPN sessions loopback_ipv4_pool: 192.168.0.0/27 # ASN to be used by BGP bgp_as: 65100 # Definition of nodes contained in this group. # Specific configuration of device must take place under the node definition. Each node inherits all values defined under \u0026#39;defaults\u0026#39; nodes: # Name of the node to be defined (must be consistent with definition in inventory) - name: dc1-spine1 # Device ID definition. An integer number used for internal calculations (ie. IPv4 address of the loopback_ipv4_pool among others) id: 1 # Management IP to be assigned to the management interface mgmt_ip: 192.168.20.2/24 - name: dc1-spine2 id: 2 mgmt_ip: 192.168.20.3/24 # L3 Leaf switch group l3leaf: defaults: # Set the relevant platform as each platform has different default values in Ansible AVD platform: cEOS-lab # Pool of IPv4 addresses to configure interface Loopback0 used for BGP EVPN sessions loopback_ipv4_pool: 192.168.0.0/27 # Offset all assigned loopback IP addresses. # Required when the \u0026lt; loopback_ipv4_pool \u0026gt; is same for 2 different node_types (like spine and l3leaf) to avoid over-lapping IPs. # For example, set the minimum offset l3leaf.defaults.loopback_ipv4_offset: \u0026lt; total # spine switches \u0026gt; or vice versa. loopback_ipv4_offset: 2 # Definition of pool of IPs to be used as Virtual Tunnel EndPoint (VXLAN origin and destination IPs) vtep_loopback_ipv4_pool: 192.168.1.0/27 # Ansible hostname of the devices used to establish neighborship (IP assignments and BGP peering) uplink_switches: [\u0026#39;dc1-spine1\u0026#39;, \u0026#39;dc1-spine2\u0026#39;] # Definition of pool of IPs to be used in P2P links uplink_ipv4_pool: 192.168.100.0/26 # Definition of pool of IPs to be used for MLAG peer-link connectivity #mlag_peer_ipv4_pool: 10.255.1.64/27 # iBGP Peering between MLAG peers #mlag_peer_l3_ipv4_pool: 10.255.1.96/27 # Virtual router mac for VNIs assigned to Leaf switches in format xx:xx:xx:xx:xx:xx virtual_router_mac_address: 00:1c:73:00:00:99 spanning_tree_priority: 4096 spanning_tree_mode: mstp # If two nodes (and only two) are in the same node_group, they will automatically form an MLAG pair node_groups: # Definition of a node group that will include two devices in MLAG. # Definitions under the group will be inherited by both nodes in the group - group: DC1_L3_LEAF1 # ASN to be used by BGP for the group. Both devices in the MLAG pair will use the same BGP ASN bgp_as: 65101 nodes: # Definition of hostnames under the node_group - name: dc1-leaf1 id: 1 mgmt_ip: 192.168.20.4/24 # Definition of the port to be used in the uplink device facing this device. # Note that the number of elements in this list must match the length of \u0026#39;uplink_switches\u0026#39; as well as \u0026#39;uplink_interfaces\u0026#39; uplink_switch_interfaces: - Ethernet1 - Ethernet1 # Definition of a node group that will include two devices in MLAG. # Definitions under the group will be inherited by both nodes in the group - group: DC1_L3_LEAF2 # ASN to be used by BGP for the group. Both devices in the MLAG pair will use the same BGP ASN bgp_as: 65102 nodes: # Definition of hostnames under the node_group - name: dc1-leaf2 id: 2 mgmt_ip: 192.168.20.5/24 uplink_switch_interfaces: - Ethernet2 - Ethernet2 # Definition of a node group that will include two devices in MLAG. # Definitions under the group will be inherited by both nodes in the group - group: DC1_L3_BORDERLEAF1 # ASN to be used by BGP for the group. Both devices in the MLAG pair will use the same BGP ASN bgp_as: 65102 nodes: # Definition of hostnames under the node_group - name: dc1-borderleaf1 id: 3 mgmt_ip: 192.168.20.6/24 uplink_switch_interfaces: - Ethernet3 - Ethernet3 That should be it. Now it is time to run two playbooks: build.yml to create the documentation and intended configs and any errors. Then I will execute the deploy-cvp.yml playbook to push the config and tasks to CVP. Lets see whats going to happen.\nInfo:\rFor AVD and CVP to reach my cEOS containers I have created a static route in my physical router like this: *ip route 192.168.0.0/24 10.100.5.40* (ip of my Containerlab host/vm) Info:\rA note on the virtual network adapters for the cEOS appliances.The Management interface is Management0 not Management1, this needs to be reflected in the AVD configs by adding \"mgmt_interface: Management0\" in the DC1.yaml\rSince my previous post, AVD has been upgraded to version 4.10, latest version in time of writing this post. I have also updated my yamls to accomodate this coming deprecation:\n[DEPRECATION WARNING]: [dc1-spine1]: The input data model \u0026#39;dict-of-dicts to list-of-dicts automatic conversion\u0026#39; is deprecated. See \u0026#39;https://avd.arista.com/stable/docs/porting- guides/4.x.x.html#data-model-changes-from-dict-of-dicts-to-list-of-dicts\u0026#39; for details. This feature will be removed from arista.avd.eos_designs in version 5.0.0. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. build.yml\n(clab01) andreasm@linuxmgmt10:~/containerlab/clab01/single-dc-l3ls$ ansible-playbook build.yml PLAY [Build Configurations and Documentation] ***************************************************** TASK [arista.avd.eos_designs : Verify Requirements] *********************************************** AVD version 4.10.0 Use -v for details. ok: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Create required output directories if not present] ***************** ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/containerlab/clab01/single-dc-l3ls/intended/structured_configs) ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/containerlab/clab01/single-dc-l3ls/documentation/fabric) TASK [arista.avd.eos_designs : Set eos_designs facts] ********************************************* ok: [dc1-spine1] TASK [arista.avd.eos_designs : Generate device configuration in structured format] **************** changed: [dc1-leaf2 -\u0026gt; localhost] changed: [dc1-borderleaf1 -\u0026gt; localhost] changed: [dc1-spine1 -\u0026gt; localhost] changed: [dc1-spine2 -\u0026gt; localhost] changed: [dc1-leaf1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Generate fabric documentation] ************************************* changed: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Generate fabric point-to-point links summary in csv format.] ******* changed: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Generate fabric topology in csv format.] *************************** changed: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Remove avd_switch_facts] ******************************************* ok: [dc1-spine1] TASK [arista.avd.eos_cli_config_gen : Verify Requirements] **************************************** skipping: [dc1-spine1] TASK [arista.avd.eos_cli_config_gen : Generate eos intended configuration and device documentation] *** changed: [dc1-spine2 -\u0026gt; localhost] changed: [dc1-spine1 -\u0026gt; localhost] changed: [dc1-leaf2 -\u0026gt; localhost] changed: [dc1-leaf1 -\u0026gt; localhost] changed: [dc1-borderleaf1 -\u0026gt; localhost] PLAY RECAP **************************************************************************************** dc1-borderleaf1 : ok=2 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-leaf1 : ok=2 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-leaf2 : ok=2 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-spine1 : ok=9 changed=5 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 dc1-spine2 : ok=2 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 This went well. Nothing has happened in CVP yet. Thats next\u0026hellip;\nNow it is time to run deploy-cvp.yml\ndeploy-cvp.yml\n(clab01) andreasm@linuxmgmt10:~/containerlab/clab01/single-dc-l3ls$ ansible-playbook deploy-cvp.yml PLAY [Deploy Configurations to Devices Using CloudVision Portal] ********************************** TASK [arista.avd.eos_config_deploy_cvp : Create required output directories if not present] ******* ok: [cvp -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/containerlab/clab01/single-dc-l3ls/intended/structured_configs/cvp) TASK [arista.avd.eos_config_deploy_cvp : Verify Requirements] ************************************* AVD version 4.10.0 Use -v for details. ok: [cvp -\u0026gt; localhost] TASK [arista.avd.eos_config_deploy_cvp : Start creation/update process.] ************************** included: /home/andreasm/.ansible/collections/ansible_collections/arista/avd/roles/eos_config_deploy_cvp/tasks/v3/main.yml for cvp TASK [arista.avd.eos_config_deploy_cvp : Generate intended variables] ***************************** ok: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Build DEVICES and CONTAINER definition for cvp] ********** changed: [cvp -\u0026gt; localhost] TASK [arista.avd.eos_config_deploy_cvp : Start creation/update process.] ************************** included: /home/andreasm/.ansible/collections/ansible_collections/arista/avd/roles/eos_config_deploy_cvp/tasks/v3/present.yml for cvp TASK [arista.avd.eos_config_deploy_cvp : Load CVP device information for cvp] ********************* ok: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Create configlets on CVP cvp.] *************************** changed: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Execute any configlet generated tasks to update configuration on cvp] *** skipping: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Building Containers topology on cvp] ********************* changed: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Execute pending tasks on cvp] **************************** skipping: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Configure devices on cvp] ******************************** changed: [cvp] TASK [arista.avd.eos_config_deploy_cvp : Execute pending tasks on cvp] **************************** skipping: [cvp] PLAY RECAP **************************************************************************************** cvp : ok=10 changed=4 unreachable=0 failed=0 skipped=3 rescued=0 ignored=0 That went without any issues.\nWhats happening in CVP # Before I ran my playbook above, this was the content in the below sections:\nNow after I have ran my playbook:\nI can see some new containers, and 5 tasks\nNew configlets have been added.\nLets have a look at the tasks.\nI have indeed 5 tasks pending. Let me inspect one of them before I decide to approve them or not.\nThe Designed Configuration certainly looks more interesting than the Running Configuration. So I think I will approve these tasks indeed.\nI need to then create a Change Control, and to do it as simple as possible I will select all 5 tasks and create a single Change Control including all 5 tasks in same.\nLet me review and hopefully approve\nThere is a warning there, but I think I will take my chances on those.\nApprove and Execute\nNow my cEOS switches should be getting their configuration. Lets check one of them when the task has been completed.\nAll tasks completed, the switches has been placed in their respective containers:\nLets check a switch for the config:\ndc1-borderleaf1#show running-config ! Command: show running-config ! device: dc1-borderleaf1 (cEOSLab, EOS-4.32.2F-38195967.4322F (engineering build)) ! no aaa root ! username admin privilege 15 role network-admin nopassword username ansible privilege 15 role network-admin secret sha512 $4$redactedxMEEoccYHS/ ! management api http-commands no shutdown ! vrf MGMT no shutdown ! daemon TerminAttr exec /usr/bin/TerminAttr -cvaddr=172.18.100.99:9910 -cvauth=token,/tmp/token -cvvrf=MGMT -disableaaa -smashexcludes=ale,flexCounter,hardware,kni,pulse,strata -ingestexclude=/Sysdb/cell/1/agent,/Sysdb/cell/2/agent -taillogs no shutdown ! vlan internal order ascending range 1100 1300 ! no service interface inactive port-id allocation disabled ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname dc1-borderleaf1 ip name-server vrf MGMT 10.100.1.7 ! spanning-tree mode mstp spanning-tree mst 0 priority 4096 ! system l1 unsupported speed action error unsupported error-correction action error ! vlan 1070 name VRF11_VLAN1070 ! vlan 1071 name VRF11_VLAN1071 ! vlan 1074 name L2_VLAN1074 ! vlan 1075 name L2_VLAN1075 ! vrf instance MGMT ! vrf instance VRF11 ! interface Ethernet1 description P2P_LINK_TO_DC1-SPINE1_Ethernet3 mtu 1500 no switchport ip address 192.168.100.9/31 ! interface Ethernet2 description P2P_LINK_TO_DC1-SPINE2_Ethernet3 mtu 1500 no switchport ip address 192.168.100.11/31 ! interface Ethernet3 description dc1-borderleaf1-wan1_WAN1 ! interface Loopback0 description EVPN_Overlay_Peering ip address 192.168.0.5/32 ! interface Loopback1 description VTEP_VXLAN_Tunnel_Source ip address 192.168.1.5/32 ! interface Loopback11 description VRF11_VTEP_DIAGNOSTICS vrf VRF11 ip address 192.168.11.5/32 ! interface Management0 description oob_management vrf MGMT ip address 192.168.20.6/24 ! interface Vlan1070 description VRF11_VLAN1070 vrf VRF11 ip address virtual 10.70.0.1/24 ! interface Vlan1071 description VRF11_VLAN1071 vrf VRF11 ip address virtual 10.71.0.1/24 ! interface Vxlan1 description dc1-borderleaf1_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 vxlan vlan 1070 vni 11070 vxlan vlan 1071 vni 11071 vxlan vlan 1074 vni 11074 vxlan vlan 1075 vni 11075 vxlan vrf VRF11 vni 11 ! ip virtual-router mac-address 00:1c:73:00:00:99 ip address virtual source-nat vrf VRF11 address 192.168.11.5 ! ip routing no ip routing vrf MGMT ip routing vrf VRF11 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 192.168.0.0/27 eq 32 seq 20 permit 192.168.1.0/27 eq 32 ! ip route vrf MGMT 0.0.0.0/0 192.168.20.1 ! ntp local-interface vrf MGMT Management0 ntp server vrf MGMT 10.100.1.7 prefer ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bfd multihop interval 300 min-rx 300 multiplier 3 ! router bgp 65102 router-id 192.168.0.5 update wait-install no bgp default ipv4-unicast maximum-paths 4 ecmp 4 neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 192.168.0.1 peer group EVPN-OVERLAY-PEERS neighbor 192.168.0.1 remote-as 65100 neighbor 192.168.0.1 description dc1-spine1 neighbor 192.168.0.2 peer group EVPN-OVERLAY-PEERS neighbor 192.168.0.2 remote-as 65100 neighbor 192.168.0.2 description dc1-spine2 neighbor 192.168.100.8 peer group IPv4-UNDERLAY-PEERS neighbor 192.168.100.8 remote-as 65100 neighbor 192.168.100.8 description dc1-spine1_Ethernet3 neighbor 192.168.100.10 peer group IPv4-UNDERLAY-PEERS neighbor 192.168.100.10 remote-as 65100 neighbor 192.168.100.10 description dc1-spine2_Ethernet3 redistribute connected route-map RM-CONN-2-BGP ! vlan 1070 rd 192.168.0.5:11070 route-target both 11070:11070 redistribute learned ! vlan 1071 rd 192.168.0.5:11071 route-target both 11071:11071 redistribute learned ! vlan 1074 rd 192.168.0.5:11074 route-target both 11074:11074 redistribute learned ! vlan 1075 rd 192.168.0.5:11075 route-target both 11075:11075 redistribute learned ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate ! vrf VRF11 rd 192.168.0.5:11 route-target import evpn 11:11 route-target export evpn 11:11 router-id 192.168.0.5 redistribute connected ! router multicast ipv4 software-forwarding kernel ! ipv6 software-forwarding kernel ! end dc1-borderleaf1# dc1-borderleaf1# show bgp summary BGP summary information for VRF default Router identifier 192.168.0.5, local AS number 65102 Neighbor AS Session State AFI/SAFI AFI/SAFI State NLRI Rcd NLRI Acc -------------- ----------- ------------- ----------------------- -------------- ---------- ---------- 192.168.0.1 65100 Established L2VPN EVPN Negotiated 7 7 192.168.0.2 65100 Established L2VPN EVPN Negotiated 7 7 192.168.100.8 65100 Established IPv4 Unicast Negotiated 3 3 192.168.100.10 65100 Established IPv4 Unicast Negotiated 3 3 dc1-borderleaf1#show ip bgp BGP routing table information for VRF default Router identifier 192.168.0.5, local AS number 65102 Route status codes: s - suppressed contributor, * - valid, \u0026gt; - active, E - ECMP head, e - ECMP S - Stale, c - Contributing to ECMP, b - backup, L - labeled-unicast % - Pending best path selection Origin codes: i - IGP, e - EGP, ? - incomplete RPKI Origin Validation codes: V - valid, I - invalid, U - unknown AS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop Network Next Hop Metric AIGP LocPref Weight Path * \u0026gt; 192.168.0.1/32 192.168.100.8 0 - 100 0 65100 i * \u0026gt; 192.168.0.2/32 192.168.100.10 0 - 100 0 65100 i * \u0026gt;Ec 192.168.0.3/32 192.168.100.8 0 - 100 0 65100 65101 i * ec 192.168.0.3/32 192.168.100.10 0 - 100 0 65100 65101 i * \u0026gt; 192.168.0.5/32 - - - - 0 i * \u0026gt;Ec 192.168.1.3/32 192.168.100.8 0 - 100 0 65100 65101 i * ec 192.168.1.3/32 192.168.100.10 0 - 100 0 65100 65101 i * \u0026gt; 192.168.1.5/32 - - - - 0 i dc1-borderleaf1#show vxlan address-table Vxlan Mac Address Table ---------------------------------------------------------------------- VLAN Mac Address Type Prt VTEP Moves Last Move ---- ----------- ---- --- ---- ----- --------- 1300 001c.7357.10f4 EVPN Vx1 192.168.1.3 1 0:03:22 ago Total Remote Mac Addresses for this criterion: 1 dc1-borderleaf1# All BGP neighbors are peering and connected, VXLAN is up and running.\nJob well done. When it comes to post-updates to the config they can continue to be performed (even should/must) as the changes will then come in to CVP, someone needs to review and approve before they are applied. And by using a declarative approach like AVD there is minimal risk of someone overwriting or overriding the config manually taking the human error out of the picture.\nAgain, a note on CVP. There will be another post coming only focusing on CVP. So stay tuned for that one.\nConnecting generic containers to the cEOS switches # Now that I have my full fabric up and running, I would like to test connectivity between two generic containers running Ubuntu connected to each of their Leaf L3 switch, each on their different VLAN. Containerlabs supports several ways to interact with the network nodes. I decided to go with an easy approach, spin a couple of generic docker containers. When I deployed my cEOS lab earlier I created a couple of Linux bridges that my Leaf switches connect their Ethernet/3 interfaces to. Each leaf have their own dedicated bridge for their Ethernet/3 interfaces. That means I just need to deploy my container clients and connecting their interface to these bridges as well.\nTo make it a bit more interesting test I want to attach Client-1 to br-node-3 where dc1-leaf1 Ethernet/3 is attached and Client-2 to br-node4 where dc1-leaf2 Ethernet/3 is attached.\nTo deploy a generic Docker container, like a Ubuntu container, using Containerlab I need to either add additional nodes using the kind: linux to my existing lab topology yaml, or create a separate topology yaml. Both ways are explained below.\nCreate new topology and connect to existing bridges # If I quickly want to add generic container nodes and add them to my already running cEOS topology I need to define an additional topology yaml where I define my \u0026ldquo;clients\u0026rdquo; and where they should be linked. Its not possible to update or apply updates to an existing running topology in Containerlab using Docker. Here is my \u0026ldquo;client\u0026rdquo; yaml:\nname: clients-attached topology: nodes: client-1: kind: linux image: ubuntu:latest client-2: kind: linux image: ubuntu:latest br-node-4: kind: bridge br-node-3: kind: bridge links: - endpoints: [\u0026#34;client-1:eth1\u0026#34;,\u0026#34;br-node-3:eth4\u0026#34;] - endpoints: [\u0026#34;client-2:eth1\u0026#34;,\u0026#34;br-node-4:eth12\u0026#34;] Client 1 eth1 is attached to br-node-3 bridge eth4, that is the same bridge my *Leaf-1 Ethernet/3 is connected to. Client 2 eth1 is attached to br-node-4 bridge eth12, that is the same bridge my *Leaf-1 Ethernet/3 is connected to. The ethx in the bridge is just another free number.\nAdding generic containers to existing topology # As it is very easy to just bring down my lab and re-provision everything back up again using Containerlab, AVD and CVP I can also modify my existing topology to include my test clients (generic linux containers).\nname: spine-leaf-borderleaf mgmt: network: custom_mgmt # management network name ipv4-subnet: 192.168.20.0/24 # ipv4 range topology: nodes: node-1: kind: arista_ceos image: ceos:4.32.2F startup-config: node1-startup-config.cfg mgmt-ipv4: 192.168.20.2 node-2: kind: arista_ceos image: ceos:4.32.2F startup-config: node2-startup-config.cfg mgmt-ipv4: 192.168.20.3 node-3: kind: arista_ceos image: ceos:4.32.2F startup-config: node3-startup-config.cfg mgmt-ipv4: 192.168.20.4 node-4: kind: arista_ceos image: ceos:4.32.2F startup-config: node4-startup-config.cfg mgmt-ipv4: 192.168.20.5 node-5: kind: arista_ceos image: ceos:4.32.2F startup-config: node5-startup-config.cfg mgmt-ipv4: 192.168.20.6 # Clients attached to specific EOS Interfaces client-1: kind: linux image: ubuntu:latest client-2: kind: linux image: ubuntu:latest # Bridges for specific downlinks br-node-3: kind: bridge br-node-4: kind: bridge br-node-5: kind: bridge links: - endpoints: [\u0026#34;node-3:eth1\u0026#34;, \u0026#34;node-1:eth1\u0026#34;] - endpoints: [\u0026#34;node-3:eth2\u0026#34;, \u0026#34;node-2:eth1\u0026#34;] - endpoints: [\u0026#34;node-4:eth1\u0026#34;, \u0026#34;node-1:eth2\u0026#34;] - endpoints: [\u0026#34;node-4:eth2\u0026#34;, \u0026#34;node-2:eth2\u0026#34;] - endpoints: [\u0026#34;node-5:eth1\u0026#34;, \u0026#34;node-1:eth3\u0026#34;] - endpoints: [\u0026#34;node-5:eth2\u0026#34;, \u0026#34;node-2:eth3\u0026#34;] - endpoints: [\u0026#34;node-3:eth3\u0026#34;, \u0026#34;br-node-3:n3-eth3\u0026#34;] - endpoints: [\u0026#34;node-4:eth3\u0026#34;, \u0026#34;br-node-4:n4-eth3\u0026#34;] - endpoints: [\u0026#34;node-5:eth3\u0026#34;, \u0026#34;br-node-5:n5-eth3\u0026#34;] - endpoints: [\u0026#34;client-1:eth1\u0026#34;,\u0026#34;br-node-3:eth4\u0026#34;] - endpoints: [\u0026#34;client-2:eth1\u0026#34;,\u0026#34;br-node-4:eth12\u0026#34;] Then I re-deployed my topology containing the additional two above linux nodes.\nINFO[0000] Containerlab v0.56.0 started INFO[0000] Parsing \u0026amp; checking topology file: spine-leaf-border.yaml INFO[0000] Creating docker network: Name=\u0026#34;custom_mgmt\u0026#34;, IPv4Subnet=\u0026#34;192.168.20.0/24\u0026#34;, IPv6Subnet=\u0026#34;\u0026#34;, MTU=0 INFO[0000] Creating lab directory: /home/andreasm/containerlab/lab-spine-leaf-cvp/clab-spine-leaf-borderleaf INFO[0000] Creating container: \u0026#34;node-1\u0026#34; INFO[0000] Creating container: \u0026#34;node-4\u0026#34; INFO[0000] Creating container: \u0026#34;node-3\u0026#34; INFO[0000] Creating container: \u0026#34;node-5\u0026#34; INFO[0000] Creating container: \u0026#34;node-2\u0026#34; INFO[0000] Creating container: \u0026#34;client-2\u0026#34; INFO[0000] Created link: node-5:eth1 \u0026lt;--\u0026gt; node-1:eth3 INFO[0000] Running postdeploy actions for Arista cEOS \u0026#39;node-1\u0026#39; node INFO[0001] Created link: node-3:eth1 \u0026lt;--\u0026gt; node-1:eth1 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-5\u0026#39; node INFO[0001] Created link: node-4:eth1 \u0026lt;--\u0026gt; node-1:eth2 INFO[0001] Creating container: \u0026#34;client-1\u0026#34; INFO[0001] Created link: node-5:eth3 \u0026lt;--\u0026gt; br-node-5:n5-eth3 INFO[0001] Created link: node-4:eth3 \u0026lt;--\u0026gt; br-node-4:n4-eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-4\u0026#39; node INFO[0001] Created link: node-3:eth2 \u0026lt;--\u0026gt; node-2:eth1 INFO[0001] Created link: node-4:eth2 \u0026lt;--\u0026gt; node-2:eth2 INFO[0001] Created link: node-3:eth3 \u0026lt;--\u0026gt; br-node-3:n3-eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-3\u0026#39; node INFO[0001] Created link: node-5:eth2 \u0026lt;--\u0026gt; node-2:eth3 INFO[0001] Running postdeploy actions for Arista cEOS \u0026#39;node-2\u0026#39; node INFO[0001] Created link: client-2:eth1 \u0026lt;--\u0026gt; br-node-4:eth12 INFO[0001] Created link: client-1:eth1 \u0026lt;--\u0026gt; br-node-3:eth4 INFO[0046] Adding containerlab host entries to /etc/hosts file INFO[0046] Adding ssh config for containerlab nodes INFO[0046] 🎉 New containerlab version 0.57.0 is available! Release notes: https://containerlab.dev/rn/0.57/ Run \u0026#39;containerlab version upgrade\u0026#39; to upgrade or go check other installation options at https://containerlab.dev/install/ +---+-------------------------------------+--------------+---------------+-------------+---------+-----------------+--------------+ | # | Name | Container ID | Image | Kind | State | IPv4 Address | IPv6 Address | +---+-------------------------------------+--------------+---------------+-------------+---------+-----------------+--------------+ | 1 | clab-spine-leaf-borderleaf-client-1 | b88f71f8461f | ubuntu:latest | linux | running | 192.168.20.8/24 | N/A | | 2 | clab-spine-leaf-borderleaf-client-2 | 030de9ac2c1a | ubuntu:latest | linux | running | 192.168.20.7/24 | N/A | | 3 | clab-spine-leaf-borderleaf-node-1 | 5aaa12968276 | ceos:4.32.2F | arista_ceos | running | 192.168.20.2/24 | N/A | | 4 | clab-spine-leaf-borderleaf-node-2 | 0ef9bfc0c093 | ceos:4.32.2F | arista_ceos | running | 192.168.20.3/24 | N/A | | 5 | clab-spine-leaf-borderleaf-node-3 | 85d18564c3fc | ceos:4.32.2F | arista_ceos | running | 192.168.20.4/24 | N/A | | 6 | clab-spine-leaf-borderleaf-node-4 | 11a1ecccb2aa | ceos:4.32.2F | arista_ceos | running | 192.168.20.5/24 | N/A | | 7 | clab-spine-leaf-borderleaf-node-5 | 34ecdecd10db | ceos:4.32.2F | arista_ceos | running | 192.168.20.6/24 | N/A | +---+-------------------------------------+--------------+---------------+-------------+---------+-----------------+--------------+ The benefit of adding the generic containers to my existing topology, I can view the connection diagram using containerlab graph.\nAs soon as they were up and running I exec into each and one of them and configured static IP addresses on both their eth1 interfaces, adding a route on client-1 pointing to client-2s subnet using the eth1 as gateway. And vice versa on client-2.\nroot@client-1:/# ip addr add 10.71.0.11/24 dev eth1 root@client-1:/# ip route add 10.70.0.0/24 via 10.71.0.1 root@client-1:/# ping 10.70.0.12 PING 10.70.0.12 (10.70.0.12) 56(84) bytes of data. 64 bytes from 10.70.0.12: icmp_seq=1 ttl=63 time=16.3 ms 64 bytes from 10.70.0.12: icmp_seq=2 ttl=62 time=3.65 ms ^C --- 10.70.0.12 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 3.653/9.955/16.257/6.302 ms root@client-1:/# root@client-2:/# ip addr add 10.70.0.12/24 dev eth1 root@client-2:/# ip route add 10.71.0.0/24 via 10.70.0.1 root@client-2:/# ping 10.71.0.1 PING 10.71.0.1 (10.71.0.1) 56(84) bytes of data. 64 bytes from 10.71.0.1: icmp_seq=1 ttl=64 time=3.67 ms 64 bytes from 10.71.0.1: icmp_seq=2 ttl=64 time=0.965 ms Then I could ping from client-1 to client-2 and vice versa.\nClient-1 is connected directly to Leaf-1 Ethernet/3 via Bridge br-node-3 and client-2 is connected directly to Leaf-2 Ethernet/3 via Bridge br-node-4. Client-1 is configured with a static ip of 10.71.0.11/24 and client-2 is configured with a static of 10.70.0.12/24. For these two clients to reach each other it has to go over VXLAN where both VLANs is encapsulated. A quick ping test from each client shows that this works:\n## client 1 pinging client 2 root@client-1:/# ip addr 567: eth1@if568: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9500 qdisc noqueue state UP group default link/ether aa:c1:ab:18:8e:bb brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.71.0.11/24 scope global eth1 valid_lft forever preferred_lft forever 569: eth0@if570: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever root@client-1:/# ping 10.70.0.12 PING 10.70.0.12 (10.70.0.12) 56(84) bytes of data. 64 bytes from 10.70.0.12: icmp_seq=1 ttl=62 time=2.96 ms 64 bytes from 10.70.0.12: icmp_seq=2 ttl=62 time=2.66 ms 64 bytes from 10.70.0.12: icmp_seq=3 ttl=62 time=2.61 ms 64 bytes from 10.70.0.12: icmp_seq=4 ttl=62 time=3.05 ms 64 bytes from 10.70.0.12: icmp_seq=5 ttl=62 time=3.08 ms 64 bytes from 10.70.0.12: icmp_seq=6 ttl=62 time=3.03 ms ^C --- 10.70.0.12 ping statistics --- 6 packets transmitted, 6 received, 0% packet loss, time 5007ms rtt min/avg/max/mdev = 2.605/2.896/3.080/0.191 ms # client 2 pinging client-1 root@client-2:~# ip add 571: eth1@if572: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 9500 qdisc noqueue state UP group default link/ether aa:c1:ab:f7:d8:60 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.70.0.12/24 scope global eth1 valid_lft forever preferred_lft forever 573: eth0@if574: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0 valid_lft forever preferred_lft forever root@client-2:~# ping 10.71.0.11 PING 10.71.0.11 (10.71.0.11) 56(84) bytes of data. 64 bytes from 10.71.0.11: icmp_seq=3 ttl=62 time=2.81 ms 64 bytes from 10.71.0.11: icmp_seq=4 ttl=62 time=2.49 ms 64 bytes from 10.71.0.11: icmp_seq=8 ttl=62 time=2.97 ms 64 bytes from 10.71.0.11: icmp_seq=9 ttl=62 time=2.56 ms 64 bytes from 10.71.0.11: icmp_seq=10 ttl=62 time=3.14 ms 64 bytes from 10.71.0.11: icmp_seq=11 ttl=62 time=2.82 ms ^C --- 10.71.0.11 ping statistics --- 6 packets transmitted, 6 received, 0% packet loss, time 10013ms rtt min/avg/max/mdev = 2.418/2.897/4.224/0.472 ms root@client-2:~# And on both Leaf-1 and Leaf-2 I can see the arp correctly and VXLAN address-table:\n## DC Leaf-1 dc1-leaf1(config)#show vxlan address-table Vxlan Mac Address Table ---------------------------------------------------------------------- VLAN Mac Address Type Prt VTEP Moves Last Move ---- ----------- ---- --- ---- ----- --------- 1070 aac1.abf7.d860 EVPN Vx1 192.168.1.4 1 0:03:52 ago 1300 001c.7356.0016 EVPN Vx1 192.168.1.4 1 7:44:07 ago 1300 001c.73c4.4a1d EVPN Vx1 192.168.1.5 1 7:44:07 ago Total Remote Mac Addresses for this criterion: 3 dc1-leaf1(config)#show arp vrf VRF11 Address Age (sec) Hardware Addr Interface 10.70.0.12 - aac1.abf7.d860 Vlan1070, Vxlan1 10.71.0.11 0:04:00 aac1.ab18.8ebb Vlan1071, Ethernet3 dc1-leaf1(config)# ## DC Leaf-2 dc1-leaf2(config)#show vxlan address-table Vxlan Mac Address Table ---------------------------------------------------------------------- VLAN Mac Address Type Prt VTEP Moves Last Move ---- ----------- ---- --- ---- ----- --------- 1071 aac1.ab18.8ebb EVPN Vx1 192.168.1.3 1 0:04:36 ago 1300 001c.7357.10f4 EVPN Vx1 192.168.1.3 1 7:44:50 ago Total Remote Mac Addresses for this criterion: 2 dc1-leaf2(config)# dc1-leaf2(config)#show arp vrf VRF11 Address Age (sec) Hardware Addr Interface 10.70.0.12 0:04:36 aac1.abf7.d860 Vlan1070, Ethernet3 10.71.0.11 - aac1.ab18.8ebb Vlan1071, Vxlan1 dc1-leaf2(config)# Outro # The combination of Arista CloudVision, Arista Validated Design and Containerlab made this post such a joy to do. Spinning up a rather complex topology using Container suddenly became just so fun to do, and in seconds. If something breaks or fails, just do a destroy and deploy again. Minutes later up and running again. Using AVD to define and create the configuration for the topologies also turns a complex task to a easy and understandable thing to do at the same time as it eliminates the chance of doing human errors. Arista CloudVision is the cherry on the top with all its features, the vast set of information readily available from the same UI/dashboard and control mechanism like the Change Control.\nThis concludes this post.\nHappy networking\n","date":"22 August 2024","externalUrl":null,"permalink":"/2024/08/22/arista-cloudvision-and-avd-using-containerlab/","section":"Posts","summary":"Using Containerlab to provision cEOS containers for AVD and CloudVision","title":"Arista Cloudvision and AVD using Containerlab","type":"posts"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/automation/","section":"Tags","summary":"","title":"Automation","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/categories/automation/","section":"Categories","summary":"","title":"Automation","type":"categories"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/ceos/","section":"Tags","summary":"","title":"Ceos","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/categories/ceos/","section":"Categories","summary":"","title":"CEOS","type":"categories"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/containerlab/","section":"Tags","summary":"","title":"Containerlab","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/provisioning/","section":"Tags","summary":"","title":"Provisioning","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/categories/provisioning/","section":"Categories","summary":"","title":"Provisioning","type":"categories"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/tags/veos/","section":"Tags","summary":"","title":"Veos","type":"tags"},{"content":"","date":"22 August 2024","externalUrl":null,"permalink":"/categories/veos/","section":"Categories","summary":"","title":"VEOS","type":"categories"},{"content":" Arista Networks # Arista Networks is an industry leader in data-driven, client to cloud networking for large data center/AI, campus and routing environments. Arista’s award-winning platforms deliver availability, agility, automation, analytics and security through an advanced network operating stack\nSource: Arista\u0026rsquo;s homepage.\nArista has some really great products and solutions, and in this post I will have a deeper look at using Arista Validated Design automating the whole bringup process of a Spine-Leaf topology. I will also see how this can be used to make the network underlay more agile by adding changes \u0026ldquo;on-demand\u0026rdquo;.\nI have been working with VMware NSX for many years and one of NSX benefits is how easy it is to automate. I am very keen on testing out how I can achieve the same level of automation with the physical underlay network too.\nWhat I have never been working with is automating the physical network. Automation is not only for easier deployment, handling dynamics in the datacenter more efficient, but also reducing/eliminate configuration issues. In this post I will go through how I make use of Arista vEOS and Arista\u0026rsquo;s Ansible playbooks to deploy a full spine-leaf topology from zero to hero.\nArista Extensible Operating System - EOS # Arista Extensible Operating System (EOS®) is the core of Arista cloud networking solutions for next-generation data centers and cloud networks. Cloud architectures built with Arista EOS scale to hundreds of thousands of compute and storage nodes with management and provisioning capabilities that work at scale. Through its programmability, EOS enables a set of software applications that deliver workflow automation, high availability, unprecedented network visibility and analytics and rapid integration with a wide range of third-party applications for virtualization, management, automation and orchestration services.\nSource: Arista\nvEOS # vEOS is a virtual appliance making it possible to run EOS as a virtual machine in vSphere, KVM, Proxmox, VMware Workstation, Fusion, and VirtualBox just to name a few. For more information head over here.\nWith this it is absolutely possible to deploy/test Arista EOS with all kinds of functionality in the comfort of my own lab. So without further ado, lets jump in to it.\nvEOS on Proxmox # The vEOS appliance consists of two files, the aboot-veos-x.iso and the veos-lab-4-x.disk. The aboot-veos-x.iso is mounted as a CD/DVD ISO and the disk file is the harddisk of your VM. I am running Proxmox in my lab that supports importing both VMDK and qcow2 disk files, but I will be using qcow2 as vEOS also includes this file. So here what I did to create a working vEOS VM in Proxmox:\nUpload the file aboot-veos.iso to a datastore on my Proxmox hosts I can store ISO files. Upload the qcow2 image/disk file to a temp folder on my Proxmox host. (e.g /tmp) root@proxmox-02:/tmp# ls vEOS64-lab-4.32.1F.qcow2 Create a new VM like this: Add a Serial Port, a USB device and mount the aboot-iso on the CD/DVD drive, and select no hardisk in the wizard (delete the proposed harddisk). Operating system type is Linux 6.x. I chose to use x86-64-v2-AES CPU emulation.\nAdd the vEOS disk by utilizing the qm import command like this, where 7011 is the ID of my VM, raid-10-node02 is the datastore on my host where I want the qcow2 image to be imported/placed. root@proxmox-02:/tmp# qm importdisk 7011 vEOS64-lab-4.32.1F.qcow2 raid-10-node02 -format raw importing disk \u0026#39;vEOS64-lab-4.32.1F.qcow2\u0026#39; to VM 7011 ... transferred 0.0 B of 4.0 GiB (0.00%) transferred 50.9 MiB of 4.0 GiB (1.24%) ... transferred 4.0 GiB of 4.0 GiB (100.00%) Successfully imported disk as \u0026#39;unused0:raid-10-node02:vm-7011-disk-0\u0026#39; When this is done it will turn up as an unused disk in my VM.\nTo add the unused disk I select it, click edit and choose SATA and bus 0. This was the only way for the vEOS to successfully boot. This is contrary to what is stated in the official documentation here The Aboot-veos iso must be set as a CD-ROM image on the IDE bus, and the EOS vmdk must be a hard drive image on the same IDE bus. The simulated hardware cannot contain a SATA controller or vEOS will fail to fully boot.\nNow the disk has been added. One final note, I have added the network interfaces I need in my lab as seen above. The net0 will be used for dedicated oob management.\nInfo:\rA note on the virtual network adapters for the vEOS appliances. I struggled to get any stateful sessions between my two test VMs connected to the Leaf switches and realized soon it had to be MTU issues. It is also mentioned in the official documentation to use the VIRTIO network adapter, it seemingly works (ping succeeded, traceroute not) but trying to SSH between my test VMs just timed out. Using tcpdump I could see some of the traffic, but only fragments. After changing the NICs to Intel E1000 no MTU issue and everything works as expected.\rI also changed the CPU emulation to use HOST. With this setup I could raise the MTU for the underlay BGP uplinks to 9000.\nThats it, I can now power on my vEOS.\nWhen its done booting, can take a couple of seconds, it will present you with the following screen:\nI can decide to log in and configure it manually by logging in with admin, disable Zero Touch Provisioning. But thats not what this post is about, it is about automating the whole process as much as possible. So this takes me to the next chapter. Zero Touch Provisioning.\nI can now power it off, clone this instance to the amount of vEOS appliances I need. I have created 5 instances to be used In the following parts of this post.\nZTP - Zero Touch Provisioning # Now that I have created all my needed vEOS VMs I need some way to set the basic config like Management Interface IP and username password so I can hand them over to Ansible to automate the whole configuration.\nEOS starts by default in ZTP mode, meaning it will do a DHCP request and acquire an IP address if there is a DHCP server that reply, this also means I can configure my DHCP server with a option to run a script from a TFTP server to do these initial configurations.\nFor ZTP to work I must have a DHCP server with some specific settings, then a TFTP server. I decided to create a dedicated DHCP server for this purpose, and I also run the TFTPD instance on the same server as where I run the DHCPD server. The Linux distribution I am using is Ubuntu Server.\nFollowing the official documentation here I have configured my DHCP server with the following setting:\n# ####GLOBAL Server config### default-lease-time 7200; max-lease-time 7200; authoritative; log-facility local7; ddns-update-style none; one-lease-per-client true; deny duplicates; option option-252 code 252 = text; option domain-name \u0026#34;int.guzware.net\u0026#34;; option domain-name-servers 10.100.1.7,10.100.1.6; option netbios-name-servers 10.100.1.7,10.100.1.6; option ntp-servers 10.100.1.7; # ###### Arista MGMT#### subnet 172.18.100.0 netmask 255.255.255.0 { pool { range 172.18.100.101 172.18.100.150; option domain-name \u0026#34;int.guzware.net\u0026#34;; option domain-name-servers 10.100.1.7,10.100.1.6; option broadcast-address 10.100.1.255; option ntp-servers 10.100.1.7; option routers 172.18.100.2; get-lease-hostnames true; option subnet-mask 255.255.255.0; } } host s_lan_0 { hardware ethernet bc:24:11:7b:5d:e6; fixed-address 172.18.100.101; option bootfile-name \u0026#34;tftp://172.18.100.10/ztp-spine1-script\u0026#34;; } host s_lan_1 { hardware ethernet bc:24:11:04:f8:f8; fixed-address 172.18.100.102; option bootfile-name \u0026#34;tftp://172.18.100.10/ztp-spine2-script\u0026#34;; } host s_lan_2 { hardware ethernet bc:24:11:ee:53:83; fixed-address 172.18.100.103; option bootfile-name \u0026#34;tftp://172.18.100.10/ztp-leaf1-script\u0026#34;; } host s_lan_3 { hardware ethernet bc:24:11:b3:2f:74; fixed-address 172.18.100.104; option bootfile-name \u0026#34;tftp://172.18.100.10/ztp-leaf2-script\u0026#34;; } host s_lan_4 { hardware ethernet bc:24:11:f8:da:7f; fixed-address 172.18.100.105; option bootfile-name \u0026#34;tftp://172.18.100.10/ztp-borderleaf1-script\u0026#34;; } The 5 host entries corresponds with my 5 vEOS appliances mac addresses respectively and the option bootfile-name refers to a unique file for every vEOS appliance.\nThe TFTP server has this configuration:\n# /etc/default/tftpd-hpa TFTP_USERNAME=\u0026#34;tftp\u0026#34; TFTP_DIRECTORY=\u0026#34;/home/andreasm/arista/tftpboot\u0026#34; TFTP_ADDRESS=\u0026#34;:69\u0026#34; TFTP_OPTIONS=\u0026#34;--secure\u0026#34; Then in the tftp_directory I have the following files:\nandreasm@arista-dhcp:~/arista/tftpboot$ ll total 48 drwxrwxr-x 2 777 nogroup 4096 Jun 10 08:59 ./ drwxrwxr-x 3 andreasm andreasm 4096 Jun 10 08:15 ../ -rw-r--r-- 1 root root 838 Jun 10 08:55 borderleaf-1-startup-config -rw-r--r-- 1 root root 832 Jun 10 08:52 leaf-1-startup-config -rw-r--r-- 1 root root 832 Jun 10 08:53 leaf-2-startup-config -rw-r--r-- 1 root root 832 Jun 10 08:45 spine-1-startup-config -rw-r--r-- 1 root root 832 Jun 10 08:51 spine-2-startup-config -rw-r--r-- 1 root root 103 Jun 10 08:55 ztp-borderleaf1-script -rw-r--r-- 1 root root 97 Jun 10 08:53 ztp-leaf1-script -rw-r--r-- 1 root root 97 Jun 10 08:54 ztp-leaf2-script -rw-r--r-- 1 root root 98 Jun 10 08:39 ztp-spine1-script -rw-r--r-- 1 root root 98 Jun 10 08:51 ztp-spine2-script The content of the ztp-leaf1-script file:\nandreasm@arista-dhcp:~/arista/tftpboot$ cat ztp-leaf1-script #!/usr/bin/Cli -p2 enable copy tftp://172.18.100.10/leaf-1-startup-config flash:startup-config The content of the leaf-1-startup-config file (taken from the Arista AVD repository here):\nandreasm@arista-dhcp:~/arista/tftpboot$ cat leaf-1-startup-config hostname leaf-1 ! ! Configures username and password for the ansible user username ansible privilege 15 role network-admin secret sha512 $hash/ ! ! Defines the VRF for MGMT vrf instance MGMT ! ! Defines the settings for the Management1 interface through which Ansible reaches the device interface Management1 description oob_management no shutdown vrf MGMT ! IP address - must be set uniquely per device ip address 172.18.100.103/24 ! ! Static default route for VRF MGMT ip route vrf MGMT 0.0.0.0/0 172.18.100.2 ! ! Enables API access in VRF MGMT management api http-commands protocol https no shutdown ! vrf MGMT no shutdown ! end ! ! Save configuration to flash copy running-config startup-config Now I just need to make sure both my DHCP service and TFTP service is running:\n# DHCP Server andreasm@arista-dhcp:~/arista/tftpboot$ systemctl status isc-dhcp-server ● isc-dhcp-server.service - ISC DHCP IPv4 server Loaded: loaded (/lib/systemd/system/isc-dhcp-server.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2024-06-10 09:02:08 CEST; 6h ago Docs: man:dhcpd(8) Main PID: 3725 (dhcpd) Tasks: 4 (limit: 4557) Memory: 4.9M CPU: 15ms CGroup: /system.slice/isc-dhcp-server.service └─3725 dhcpd -user dhcpd -group dhcpd -f -4 -pf /run/dhcp-server/dhcpd.pid -cf /etc/dhcp/dhcpd.co\u0026gt; # TFTPD server andreasm@arista-dhcp:~/arista/tftpboot$ systemctl status tftpd-hpa.service ● tftpd-hpa.service - LSB: HPA\u0026#39;s tftp server Loaded: loaded (/etc/init.d/tftpd-hpa; generated) Active: active (running) since Mon 2024-06-10 08:17:55 CEST; 7h ago Docs: man:systemd-sysv-generator(8) Process: 2414 ExecStart=/etc/init.d/tftpd-hpa start (code=exited, status=0/SUCCESS) Tasks: 1 (limit: 4557) Memory: 408.0K CPU: 39ms CGroup: /system.slice/tftpd-hpa.service └─2422 /usr/sbin/in.tftpd --listen --user tftp --address :69 --secure /home/andreasm/arista/tftpb\u0026gt; Thats it. If I have already powered on my vEOS appliance they will very soon get their new config and reboot with the desired config. If not, just reset or power them on and off again. Every time I deploy a new vEOS appliance I just have to update my DHCP server config to add the additional hosts mac addresses and corresponding config files.\nSpine-Leaf - Desired Topology # A spine-leaf topology is a two-layer network architecture commonly used in data centers. It is designed to provide high-speed, low-latency, and highly available network connectivity. This topology is favored for its scalability and performance, especially in environments requiring large amounts of east-west traffic (server-to-server).\nIn virtual environments, from regular virtual machines to containers running in Kubernetes, its common with a large amount of east-west traffic. In this post I will be using a Spine Leaf architecture.\nBefore I did any automated provisioning using Arista Validated Design and Ansible I deployed my vEOS appliances configured them with the amount of network interfaces needed to support my intended use (below), then configured them all manually using CLI so I was sure I had a working configuration, and no issues in my lab. I wanted to make sure I could deploy a spine-leaf topology, create some vlans and attached some VMs to them and checked connectivity. Below was my desired topology:\nAnd here was the config I used on all switches respectively:\nSpine1\nno aaa root ! username admin role network-admin secret sha512 $hash/ ! switchport default mode routed ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname spine-1 ! spanning-tree mode mstp ! system l1 unsupported speed action error unsupported error-correction action error ! interface Ethernet1 description spine-leaf-1-downlink-1 mtu 9214 no switchport ip address 192.168.0.0/31 ! interface Ethernet2 description spine-leaf-1-downlink-2 mtu 9214 no switchport ip address 192.168.0.2/31 ! interface Ethernet3 description spine-leaf-2-downlink-3 mtu 9214 no switchport ip address 192.168.0.4/31 ! interface Ethernet4 description spine-leaf-2-downlink-4 mtu 9214 no switchport ip address 192.168.0.6/31 ! interface Ethernet5 description spine-leaf-3-downlink-5 mtu 9214 no switchport ip address 192.168.0.8/31 ! interface Ethernet6 description spine-leaf-3-downlink-6 mtu 9214 no switchport ip address 192.168.0.10/31 ! interface Loopback0 description spine-1-evpn-lo ip address 10.0.0.1/32 ! interface Management1 ip address 172.18.5.71/24 ! ip routing ! ip prefix-list PL-LOOPBACKS seq 10 permit 10.0.0.0/24 eq 32 ! ip route 0.0.0.0/0 172.18.5.2 ! route-map RM-LOOPBACKS permit 10 match ip address prefix-list PL-LOOPBACKS ! router bgp 65000 router-id 10.0.0.1 maximum-paths 4 ecmp 4 neighbor UNDERLAY peer group neighbor UNDERLAY allowas-in 1 neighbor UNDERLAY ebgp-multihop 4 neighbor UNDERLAY send-community extended neighbor UNDERLAY maximum-routes 12000 neighbor 192.168.0.1 peer group UNDERLAY neighbor 192.168.0.1 remote-as 65001 neighbor 192.168.0.1 description leaf-1-u1 neighbor 192.168.0.3 peer group UNDERLAY neighbor 192.168.0.3 remote-as 65001 neighbor 192.168.0.3 description leaf-1-u2 neighbor 192.168.0.5 peer group UNDERLAY neighbor 192.168.0.5 remote-as 65002 neighbor 192.168.0.5 description leaf-2-u1 neighbor 192.168.0.7 peer group UNDERLAY neighbor 192.168.0.7 remote-as 65002 neighbor 192.168.0.7 description leaf-2-u2 neighbor 192.168.0.9 peer group UNDERLAY neighbor 192.168.0.9 remote-as 65003 neighbor 192.168.0.9 description borderleaf-1-u1 neighbor 192.168.0.11 peer group UNDERLAY neighbor 192.168.0.11 remote-as 65003 neighbor 192.168.0.11 description borderleaf-1-u2 ! address-family ipv4 neighbor UNDERLAY activate ! end Spine2\nno aaa root ! username admin role network-admin secret sha512 $hash/ ! switchport default mode routed ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname spine-2 ! spanning-tree mode mstp ! system l1 unsupported speed action error unsupported error-correction action error ! interface Ethernet1 description spine-leaf-1-downlink-1 mtu 9214 no switchport ip address 192.168.1.0/31 ! interface Ethernet2 description spine-leaf-1-downlink-2 mtu 9214 no switchport ip address 192.168.1.2/31 ! interface Ethernet3 description spine-leaf-2-downlink-3 mtu 9214 no switchport ip address 192.168.1.4/31 ! interface Ethernet4 description spine-leaf-2-downlink-4 mtu 9214 no switchport ip address 192.168.1.6/31 ! interface Ethernet5 description spine-leaf-3-downlink-5 mtu 9214 no switchport ip address 192.168.1.8/31 ! interface Ethernet6 description spine-leaf-3-downlink-6 mtu 9214 no switchport ip address 192.168.1.10/31 ! interface Loopback0 ip address 10.0.0.2/32 ! interface Management1 ip address 172.18.5.72/24 description spine-leaf-2-downlink-4 mtu 9214 no switchport ip address 192.168.1.6/31 ! interface Ethernet5 description spine-leaf-3-downlink-5 mtu 9214 no switchport ip address 192.168.1.8/31 ! interface Ethernet6 description spine-leaf-3-downlink-6 mtu 9214 no switchport ip address 192.168.1.10/31 ! interface Loopback0 ip address 10.0.0.2/32 ! interface Management1 ip address 172.18.5.72/24 ! ip routing ! ip prefix-list PL-LOOPBACKS seq 10 permit 10.0.0.0/24 eq 32 ! ip route 0.0.0.0/0 172.18.5.2 ! route-map RM-LOOPBACKS permit 10 match ip address prefix-list PL-LOOPBACKS ! router bgp 65000 router-id 10.0.0.2 maximum-paths 4 ecmp 4 neighbor UNDERLAY peer group neighbor UNDERLAY allowas-in 1 neighbor UNDERLAY ebgp-multihop 4 neighbor UNDERLAY send-community extended neighbor UNDERLAY maximum-routes 12000 neighbor 192.168.1.1 peer group UNDERLAY neighbor 192.168.1.1 remote-as 65001 neighbor 192.168.1.1 description leaf-1-u3 neighbor 192.168.1.3 peer group UNDERLAY neighbor 192.168.1.3 remote-as 65001 neighbor 192.168.1.3 description leaf-1-u4 neighbor 192.168.1.5 peer group UNDERLAY neighbor 192.168.1.5 remote-as 65002 neighbor 192.168.1.5 description leaf-2-u3 neighbor 192.168.1.7 peer group UNDERLAY neighbor 192.168.1.7 remote-as 65002 neighbor 192.168.1.7 description leaf-2-u4 neighbor 192.168.1.9 peer group UNDERLAY neighbor 192.168.1.9 remote-as 65003 neighbor 192.168.1.9 description borderleaf-1-u3 neighbor 192.168.1.11 peer group UNDERLAY neighbor 192.168.1.11 remote-as 65003 neighbor 192.168.1.11 description borderleaf-1-u4 redistribute connected route-map RM-LOOPBACKS ! address-family ipv4 neighbor UNDERLAY activate ! end Leaf-1\nno aaa root ! username admin role network-admin secret sha512 $hash ! switchport default mode routed ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname leaf-1 ! spanning-tree mode mstp no spanning-tree vlan-id 4094 ! spanning-tree mst configuration instance 1 vlan 1-4094 ! system l1 unsupported speed action error unsupported error-correction action error ! vlan 1070 name subnet-70 ! vlan 1071 name subnet-71 ! aaa authorization exec default local ! interface Ethernet1 description leaf-spine-1-uplink-1 mtu 9000 no switchport ip address 192.168.0.1/31 ! interface Ethernet2 description leaf-spine-1-uplink-2 mtu 9000 no switchport ip address 192.168.0.3/31 ! interface Ethernet3 description leaf-spine-2-uplink-3 mtu 9000 no switchport ip address 192.168.1.1/31 ! interface Ethernet4 description leaf-spine-2-uplink-4 mtu 9000 no switchport ip address 192.168.1.3/31 ! interface Ethernet5 mtu 1500 switchport access vlan 1071 switchport spanning-tree portfast ! interface Ethernet6 no switchport ! interface Loopback0 description leaf-1-lo ip address 10.0.0.3/32 ! interface Management1 ip address 172.18.5.73/24 ! interface Vlan1070 description subnet-70 ip address virtual 10.70.0.1/24 ! interface Vlan1071 description subnet-71 ip address virtual 10.71.0.1/24 ! interface Vxlan1 vxlan source-interface Loopback0 vxlan udp-port 4789 vxlan vlan 1070-1071 vni 10070-10071 ! ip virtual-router mac-address 00:1c:73:ab:cd:ef ! ip routing ! ip prefix-list PL-LOOPBACKS seq 10 permit 10.0.0.0/24 eq 32 ! ip route 0.0.0.0/0 172.18.5.2 ! route-map RM-LOOPBACKS permit 10 match ip address prefix-list PL-LOOPBACKS ! router bgp 65001 router-id 10.0.0.3 maximum-paths 4 ecmp 4 neighbor OVERLAY peer group neighbor OVERLAY ebgp-multihop 5 neighbor OVERLAY send-community extended neighbor UNDERLAY peer group neighbor UNDERLAY allowas-in 1 neighbor UNDERLAY ebgp-multihop 4 neighbor UNDERLAY send-community extended neighbor UNDERLAY maximum-routes 12000 neighbor 10.0.0.4 peer group OVERLAY neighbor 10.0.0.4 remote-as 65002 neighbor 10.0.0.4 update-source Loopback0 neighbor 10.0.0.5 peer group OVERLAY neighbor 10.0.0.5 remote-as 65003 neighbor 192.168.0.0 peer group UNDERLAY neighbor 192.168.0.0 remote-as 65000 neighbor 192.168.0.0 description spine-1-int-1 neighbor 192.168.0.2 peer group UNDERLAY neighbor 192.168.0.2 remote-as 65000 neighbor 192.168.0.2 description spine-1-int-2 neighbor 192.168.1.0 peer group UNDERLAY neighbor 192.168.1.0 remote-as 65000 neighbor 192.168.1.0 description spine-2-int-1 neighbor 192.168.1.2 peer group UNDERLAY neighbor 192.168.1.2 remote-as 65000 neighbor 192.168.1.2 description spine-2-int-2 ! vlan-aware-bundle V1070-1079 rd 10.0.0.3:1070 route-target both 10010:1 redistribute learned vlan 1070-1079 ! address-family evpn neighbor OVERLAY activate ! address-family ipv4 neighbor UNDERLAY activate redistribute connected route-map RM-LOOPBACKS ! end Leaf-2\nno aaa root ! username admin role network-admin secret sha512 $hash ! switchport default mode routed ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname leaf-2 ! spanning-tree mode mstp no spanning-tree vlan-id 4094 ! spanning-tree mst configuration instance 1 vlan 1-4094 ! system l1 unsupported speed action error unsupported error-correction action error ! vlan 1070 name subnet-70 ! vlan 1071 name subnet-71 ! vlan 1072 ! aaa authorization exec default local ! interface Ethernet1 description leaf-spine-1-uplink-1 mtu 9000 no switchport ip address 192.168.0.5/31 ! interface Ethernet2 description leaf-spine-1-uplink-2 mtu 9000 no switchport ip address 192.168.0.7/31 ! interface Ethernet3 description leaf-spine-2-uplink-3 mtu 9000 no switchport ip address 192.168.1.5/31 ! interface Ethernet4 description leaf-spine-2-uplink-4 mtu 9000 no switchport ip address 192.168.1.7/31 ! interface Ethernet5 mtu 1500 switchport access vlan 1070 switchport spanning-tree portfast ! interface Ethernet6 no switchport ! interface Loopback0 ip address 10.0.0.4/32 ! interface Management1 ip address 172.18.5.74/24 ! interface Vlan1070 description subnet-70 ip address virtual 10.70.0.1/24 ! interface Vlan1071 description subnet-71 ip address virtual 10.71.0.1/24 ! interface Vxlan1 vxlan source-interface Loopback0 vxlan udp-port 4789 vxlan vlan 1070-1071 vni 10070-10071 ! ip virtual-router mac-address 00:1c:73:ab:cd:ef ! ip routing ! ip prefix-list PL-LOOPBACKS seq 10 permit 10.0.0.0/24 eq 32 ! ip route 0.0.0.0/0 172.18.5.2 ! route-map RM-LOOPBACKS permit 10 match ip address prefix-list PL-LOOPBACKS ! router bgp 65002 router-id 10.0.0.4 maximum-paths 4 ecmp 4 neighbor OVERLAY peer group neighbor OVERLAY ebgp-multihop 5 neighbor OVERLAY send-community extended neighbor UNDERLAY peer group neighbor UNDERLAY allowas-in 1 neighbor UNDERLAY ebgp-multihop 4 neighbor UNDERLAY send-community extended neighbor UNDERLAY maximum-routes 12000 neighbor 10.0.0.3 peer group OVERLAY neighbor 10.0.0.3 remote-as 65001 neighbor 10.0.0.3 update-source Loopback0 neighbor 10.0.0.5 peer group OVERLAY neighbor 10.0.0.5 remote-as 65003 neighbor 10.0.0.5 update-source Loopback0 neighbor 192.168.0.4 peer group UNDERLAY neighbor 192.168.0.4 remote-as 65000 neighbor 192.168.0.4 description spine-1-int-3 neighbor 192.168.0.6 peer group UNDERLAY neighbor 192.168.0.6 remote-as 65000 neighbor 192.168.0.6 description spine-1-int-4 neighbor 192.168.1.4 peer group UNDERLAY neighbor 192.168.1.4 remote-as 65000 neighbor 192.168.1.4 description spine-2-int-3 neighbor 192.168.1.6 peer group UNDERLAY neighbor 192.168.1.6 remote-as 65000 neighbor 192.168.1.6 description spine-2-int-4 ! vlan-aware-bundle V1070-1079 rd 10.0.0.4:1070 route-target both 10010:1 redistribute learned vlan 1070-1079 ! address-family evpn neighbor OVERLAY activate ! address-family ipv4 neighbor UNDERLAY activate redistribute connected route-map RM-LOOPBACKS ! end Borderleaf-1\nno aaa root ! username admin role network-admin secret sha512 $hash ! switchport default mode routed ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname borderleaf-1 ! spanning-tree mode mstp no spanning-tree vlan-id 4094 ! spanning-tree mst configuration instance 1 vlan 1-4094 ! system l1 unsupported speed action error unsupported error-correction action error ! vlan 1079 name subnet-wan ! aaa authorization exec default local ! interface Ethernet1 description leaf-spine-1-uplink-1 mtu 9214 no switchport ip address 192.168.0.9/31 ! interface Ethernet2 description leaf-spine-1-uplink-2 mtu 9214 no switchport ip address 192.168.0.11/31 ! interface Ethernet3 description leaf-spine-2-uplink-3 mtu 9214 no switchport ip address 192.168.1.9/31 ! interface Ethernet4 description leaf-spine-2-uplink-4 mtu 9214 no switchport ip address 192.168.1.11/31 ! interface Ethernet5 switchport trunk allowed vlan 1070-1079 switchport mode trunk switchport ! interface Ethernet6 no switchport ! interface Loopback0 ip address 10.0.0.5/32 ! interface Management1 ip address 172.18.5.75/24 ! interface Vlan1079 ip address virtual 10.79.0.1/24 ! interface Vxlan1 vxlan source-interface Loopback0 vxlan udp-port 4689 vxlan vlan 1079 vni 10079 ! ip routing ! ip prefix-list PL-LOOPBACKS seq 10 permit 10.0.0.0/24 eq 32 ! ip route 0.0.0.0/0 172.18.5.2 ! route-map RM-LOOPBACKS permit 10 match ip address prefix-list PL-LOOPBACKS ! router bgp 65003 router-id 10.0.0.4 maximum-paths 2 ecmp 2 neighbor OVERLAY peer group neighbor OVERLAY ebgp-multihop 5 neighbor OVERLAY send-community extended neighbor UNDERLAY peer group neighbor UNDERLAY allowas-in 1 neighbor UNDERLAY ebgp-multihop 4 neighbor UNDERLAY send-community extended neighbor UNDERLAY maximum-routes 12000 neighbor 10.0.0.3 peer group OVERLAY neighbor 10.0.0.3 remote-as 65001 neighbor 10.0.0.3 update-source Loopback0 neighbor 10.0.0.4 peer group OVERLAY neighbor 10.0.0.4 remote-as 65002 neighbor 10.0.0.4 update-source Loopback0 neighbor 192.168.0.8 peer group UNDERLAY neighbor 192.168.0.8 remote-as 65000 neighbor 192.168.0.8 description spine-1-int-5 neighbor 192.168.0.10 peer group UNDERLAY neighbor 192.168.0.10 remote-as 65000 neighbor 192.168.0.10 description spine-1-int-6 neighbor 192.168.1.8 peer group UNDERLAY neighbor 192.168.1.8 remote-as 65000 neighbor 192.168.1.8 description spine-2-int-5 neighbor 192.168.1.10 peer group UNDERLAY neighbor 192.168.1.10 remote-as 65000 neighbor 192.168.1.10 description spine-2-int-6 ! vlan-aware-bundle V1079 rd 10.0.0.5:1079 route-target both 10070:1 redistribute learned vlan 1079 ! address-family evpn neighbor OVERLAY activate ! address-family ipv4 neighbor UNDERLAY activate redistribute connected route-map RM-LOOPBACKS ! end With the configurations above manually created, I had a working Spine-Leaf topology in my lab. These configs will also be very interesting to compare later on. A quick note on my config is that I am using two distinct point to point from every leaf to each spine.\nMy physical lab topology # I think it also make sense to quickly go over how my lab is configured. The diagram below illustrates my two Proxmox hosts 1 and 2 both connected to my physical switch on port 49, 51 and 50, 52 respectively. The reason I bring this up is because in certain scenarios I dont want certain vlans to be available on the trunk for both hosts. Like the downlinks from the the vEOS appliances to the attached test VMs, this will not be the case in real world either. This just confuses things.\nBelow is how I interconnect all my vEOS appliances, separating all point to point connections on their own dedicated vlan. This is ofcourse not necessary in \u0026ldquo;real world\u0026rdquo; scnearios, but again this is all virtual environment (including the vEOS switches). All the VLANs are configured on the above illustrated physical switch and made available on the trunks to the respective Proxmox hosts.\nI have also divided the spines and leaf1 and leaf2 to run on each host, as I have only two hosts the borderleaf-1 is placed on same host as leaf-2.\nBelow is the VLAN tag for each vEOS appliance:\nSpine-1\nSpine-2\nLeaf-1\nFor the leafs the last two network cards net5 and net6 are used as host downlinks and not involved in forming the spine/leaf.\nLeaf-2\nBorderleaf-1\nAfter verififying the above configs worked after manually configuring it, I reset all the switches back to factory settings. They will get the initial config set by Zero-Touch-Provisioning and ready to be configured again.\nNow the next chapters is about automating the configuring of the vEOS switches to form a spine/leaf topology using Ansible. To get started I used Arista\u0026rsquo;s very well documented Arista Validated Design here. More on this in the coming chapters\nArista Validated Designs (AVD) # Arista Validated Designs (AVD) is an extensible data model that defines Arista’s Unified Cloud Network architecture as “code”.\nArista.avd is an Ansible collection for Arista Validated Designs. It’s maintained by Arista and accepts third-party contributions on GitHub at aristanetworks/avd.\nWhile Ansible is the core automation engine, AVD is an Ansible Collection described above. It provides roles, modules, and plugins that allow the user to generate and deploy best-practice configurations to Arista based networks of various design types: Data Center, Campus and Wide Area Networks.\nSource: Arista https://avd.arista.com/\nArista Validated Design is a very well maintained project, and by having a quick look at their GitHub repo updates are done very frequently and latest release was 3 weeks ago (at the time of writing this post).\nThe Arista Validated Designs webpage avd.arista.com is very well structured and the documentation to get started using Arista Validated design is brilliant. It also includes some example designs like Single DC L3LS, Dual DC L3LS, L2LS Fabric, Campus Fabric and ISIS-LDP IPVPN to further simplify getting started.\nI will base my deployment on the Single DC L3LS example, with some modifications to achieve a similiar design as illustrated earlier. The major modifications I am doing is removing some of the leafs and no MLAG, keeping it as close to my initial design as possible.\nPreparing my environment for AVD and AVD collection requirements # To get started using Ansible I find it best to create a dedicated Python Environment to keep all the different requirements isolated from other projects. This means I can run different versions and packages within their own dedicated virtual environments without them interfering with other environments.\nSo before I install any of AVDs requirements I will start by creating a folder for my AVD project:\nandreasm@linuxmgmt01:~$ mkdir arista_validated_design andreasm@linuxmgmt01:~$ cd arista_validated_design/ andreasm@linuxmgmt01:~/arista_validated_design$ Then I will create my Python Virtual Environment.\nandreasm@linuxmgmt01:~/arista_validated_design$ python3 -m venv avd-environment andreasm@linuxmgmt01:~/arista_validated_design$ ls avd-environment andreasm@linuxmgmt01:~/arista_validated_design$ cd avd-environment/ andreasm@linuxmgmt01:~/arista_validated_design/avd-environment$ ls bin include lib lib64 pyvenv.cfg This will create a subfolder with the name of the environment. Now all I need to to is to activate the environment so I can deploy the necessary requirements for AVD.\nandreasm@linuxmgmt01:~/arista_validated_design$ source avd-environment/bin/activate (avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ Notice the (avd-environment) indicating I am now in my virtual environment called avd-environment. Now that I have a dedicated Python environment for this I am ready to install all the dependencies for the AVD Collection without running the risk of any conflict with other environments. Below is the AVD Collection Requirements:\nPython 3.9 or later ansible-core from 2.15.0 to 2.17.x arista.avd collection additional Python requirements: # PyAVD must follow the exact same version as the Ansible collection. # For development this should be installed as an editable install as specified in requirement-dev.txt pyavd==4.9.0-dev0 netaddr\u0026gt;=0.7.19 Jinja2\u0026gt;=3.0.0 treelib\u0026gt;=1.5.5 cvprac\u0026gt;=1.3.1 jsonschema\u0026gt;=4.10.3 referencing\u0026gt;=0.35.0 requests\u0026gt;=2.27.0 PyYAML\u0026gt;=6.0.0 deepmerge\u0026gt;=1.1.0 cryptography\u0026gt;=38.0.4 # No anta requirement until the eos_validate_state integration is out of preview. # anta\u0026gt;=1.0.0 aristaproto\u0026gt;=0.1.1 Modify ansible.cfg to support jinja2 extensions. Install AVD Collection and requirements # From my newly created Python Environment I will install the necessary components to get started with AVD.\nThe first requirement the Python version:\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ python --version Python 3.12.1 Next requirement is installing ansible-core:\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ python3 -m pip install ansible Collecting ansible Obtaining dependency information for ansible from https://files.pythonhosted.org/packages/28/7c/a5f708b7b033f068a8ef40db5c993bee4cfafadd985d48dfe44db8566fc6/ansible-10.0.1-py3-none-any.whl.metadata Using cached ansible-10.0.1-py3-none-any.whl.metadata (8.2 kB) Collecting ansible-core~=2.17.0 (from ansible) Obtaining dependency information for ansible-core~=2.17.0 from https://files.pythonhosted.org/packages/2f/77/97fb1880abb485f1df31b36822c537330db86bea4105fdea6e1946084c16/ansible_core-2.17.0-py3-none-any.whl.metadata Using cached ansible_core-2.17.0-py3-none-any.whl.metadata (6.9 kB) ... Installing collected packages: resolvelib, PyYAML, pycparser, packaging, MarkupSafe, jinja2, cffi, cryptography, ansible-core, ansible Successfully installed MarkupSafe-2.1.5 PyYAML-6.0.1 ansible-10.0.1 ansible-core-2.17.0 cffi-1.16.0 cryptography-42.0.8 jinja2-3.1.4 packaging-24.1 pycparser-2.22 resolvelib-1.0.1 (avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ ansible --version ansible [core 2.17.0] The third requirement is to install the arista.avd collection:\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ ansible-galaxy collection install arista.avd Starting galaxy collection install process [WARNING]: Collection arista.cvp does not support Ansible version 2.17.0 Process install dependency map Starting collection install process Downloading https://galaxy.ansible.com/api/v3/plugin/ansible/content/published/collections/artifacts/arista-avd-4.8.0.tar.gz to /home/andreasm/.ansible/tmp/ansible-local-66927_qwu6ou1/tmp33cqptq7/arista-avd-4.8.0-p_88prjp Installing \u0026#39;arista.avd:4.8.0\u0026#39; to \u0026#39;/home/andreasm/.ansible/collections/ansible_collections/arista/avd\u0026#39; arista.avd:4.8.0 was installed successfully Then I need the fourth requirement installing the additional Python requirments:\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ export ARISTA_AVD_DIR=$(ansible-galaxy collection list arista.avd --format yaml | head -1 | cut -d: -f1) (avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ pip3 install -r ${ARISTA_AVD_DIR}/arista/avd/requirements.txt Collecting netaddr\u0026gt;=0.7.19 (from -r /home/andreasm/.ansible/collections/ansible_collections/arista/avd/requirements.txt (line 1)) By pointing to the requirements.txt it will grab all necessary requirements.\nAnd the last requirement, modifying the ansible.cfg to support jinja2 extensions. I will get back to this in a second. I will copy the AVD examples to my current folder first, the reason for this is that they already contain a ansible.cfg file in every example.\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ ansible-playbook arista.avd.install_examples [WARNING]: No inventory was parsed, only implicit localhost is available [WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match \u0026#39;all\u0026#39; PLAY [Install Examples] ***************************************************************************************************************************************************************************************** TASK [Copy all examples to /home/andreasm/arista_validated_design] ********************************************************************************************************************************************** changed: [localhost] PLAY RECAP ****************************************************************************************************************************************************************************************************** localhost : ok=1 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Now lets have a look at the contents in my folder:\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design$ ll total 32 drwxrwxr-x 8 andreasm andreasm 4096 Jun 12 06:08 ./ drwxr-xr-x 43 andreasm andreasm 4096 Jun 11 05:24 ../ drwxrwxr-x 5 andreasm andreasm 4096 Jun 11 06:02 avd-environment/ drwxrwxr-x 7 andreasm andreasm 4096 Jun 12 06:07 campus-fabric/ drwxrwxr-x 7 andreasm andreasm 4096 Jun 12 06:08 dual-dc-l3ls/ drwxrwxr-x 7 andreasm andreasm 4096 Jun 12 06:07 isis-ldp-ipvpn/ drwxrwxr-x 7 andreasm andreasm 4096 Jun 12 06:08 l2ls-fabric/ drwxrwxr-x 8 andreasm andreasm 4096 Jun 12 06:09 single-dc-l3ls/ And by taking a look inside the single-dc-l3ls folder (which I am basing my configuration on) I will see that there is already a ansible.cfg file:\n(avd-environment) andreasm@linuxmgmt01:~/arista_validated_design/single-dc-l3ls$ ll total 92 drwxrwxr-x 8 andreasm andreasm 4096 Jun 12 06:09 ./ drwxrwxr-x 8 andreasm andreasm 4096 Jun 12 06:08 ../ -rw-rw-r-- 1 andreasm andreasm 109 Jun 12 06:08 ansible.cfg # here -rw-rw-r-- 1 andreasm andreasm 422 Jun 12 06:08 build.yml drwxrwxr-x 2 andreasm andreasm 4096 Jun 12 06:09 config_backup/ -rw-rw-r-- 1 andreasm andreasm 368 Jun 12 06:08 deploy-cvp.yml -rw-rw-r-- 1 andreasm andreasm 260 Jun 12 06:08 deploy.yml drwxrwxr-x 4 andreasm andreasm 4096 Jun 12 06:09 documentation/ drwxrwxr-x 2 andreasm andreasm 4096 Jun 12 06:09 group_vars/ drwxrwxr-x 2 andreasm andreasm 4096 Jun 12 06:08 images/ drwxrwxr-x 4 andreasm andreasm 4096 Jun 12 06:09 intended/ -rw-rw-r-- 1 andreasm andreasm 1403 Jun 12 06:08 inventory.yml -rw-rw-r-- 1 andreasm andreasm 36936 Jun 12 06:08 README.md drwxrwxr-x 2 andreasm andreasm 4096 Jun 12 06:09 switch-basic-configurations/ Now to the last requirement, when I open the ansible.cfg file using vim I will notice this content:\n[defaults] inventory=inventory.yml jinja2_extensions = jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n It already contains the requirements. What I can add is the following, as recommended by AVD:\n[defaults] inventory=inventory.yml jinja2_extensions = jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n duplicate_dict_key=error # added this Thats it for the preparations. Now it is time to do some networking automation.\nFor more details and instructions, head over to the avd.arista.com webpage as it is very well documented there.\nPreparing AVD example files # To get started with Arista Validated Design is quite easy as the necessary files are very well structured and easy to understand with already example information populated making it very easy to follow. In my single-dc-l3ls folder there is a couple of files inside the group_cvars folder I need to edit to match my environment. When these have been edited its time to apply the task. But before getting there I will go through the files how I have edited them.\nAs I mentioned above, I will base my deployment on the example *single-dc-l3ls\u0026quot; with some minor modifications by removing some leaf-switches, addings some uplinks etc. So by entering the folder single-dc-l3ls folder, which was created when I copied the collection over to my environment earlier, I will find the content related such a deployment/topology.\nBelow is the files I need to do some edits in, numbered in the order they are configured:\n├── ansible.cfg # added the optional duplicate_dict_key ├── group_vars/ │ ├── CONNECTED_ENDPOINTS.yml #### 7 #### │ ├── DC1_L2_LEAVES.yml ### N/A #### │ ├── DC1_L3_LEAVES.yml #### 3 #### │ ├── DC1_SPINES.yml #### 2 #### │ ├── DC1.yml #### 5 #### │ ├── FABRIC.yml #### 4 #### │ └── NETWORK_SERVICES.yml #### 6 #### ├── inventory.yml #### 1 #### First out is the inventory.yml\nThis file contains a list over which hosts/switches that should be included in the configuration, in my environment it looks like this:\n--- all: children: FABRIC: children: DC1: children: DC1_SPINES: hosts: dc1-spine1: ansible_host: 172.18.100.101 dc1-spine2: ansible_host: 172.18.100.102 DC1_L3_LEAVES: hosts: dc1-leaf1: ansible_host: 172.18.100.103 dc1-leaf2: ansible_host: 172.18.100.104 dc1-borderleaf1: ansible_host: 172.18.100.105 NETWORK_SERVICES: children: DC1_L3_LEAVES: CONNECTED_ENDPOINTS: children: DC1_L3_LEAVES: I have removed the L2 Leaves and the corresponding group, as my plan is to deploy this design:\nWhen done editing the inventory.yml I will cd into the group_cvars folder for the next files to be edited.\nThe first two files in this folder is the device type files DC1_SPINES and DC1_L3_LEAVES.yml which defines which \u0026ldquo;role\u0026rdquo; each device will have in the above topology (spine, l2 or l3 leaf). I will leave these with the default content.\nNext up is the FABRIC.yml which configures \u0026ldquo;global\u0026rdquo; settings on all devices:\n--- # Ansible connectivity definitions # eAPI connectivity via HTTPS is specified (as opposed to CLI via SSH) ansible_connection: ansible.netcommon.httpapi # Specifies that we are indeed using Arista EOS ansible_network_os: arista.eos.eos # This user/password must exist on the switches to enable Ansible access ansible_user: ansible ansible_password: password # User escalation (to enter enable mode) ansible_become: true ansible_become_method: enable # Use SSL (HTTPS) ansible_httpapi_use_ssl: true # Do not try to validate certs ansible_httpapi_validate_certs: false # Common AVD group variables fabric_name: FABRIC # Define underlay and overlay routing protocol to be used underlay_routing_protocol: ebgp overlay_routing_protocol: ebgp # Local users local_users: # Define a new user, which is called \u0026#34;ansible\u0026#34; - name: ansible privilege: 15 role: network-admin # Password set to \u0026#34;ansible\u0026#34;. Same string as the device generates when configuring a username. sha512_password: $hash/ - name: admin privilege: 15 role: network-admin no_password: true # BGP peer groups passwords bgp_peer_groups: # all passwords set to \u0026#34;arista\u0026#34; evpn_overlay_peers: password: Q4fqtbqcZ7oQuKfuWtNGRQ== ipv4_underlay_peers: password: 7x4B4rnJhZB438m9+BrBfQ== # P2P interfaces MTU, includes VLANs 4093 and 4094 that are over peer-link # If you\u0026#39;re running vEOS-lab or cEOS, you should use MTU of 1500 instead as shown in the following line # p2p_uplinks_mtu: 9214 p2p_uplinks_mtu: 1500 # Set default uplink, downlink, and MLAG interfaces based on node type default_interfaces: - types: [ spine ] platforms: [ default ] #uplink_interfaces: [ Ethernet1-2 ] downlink_interfaces: [ Ethernet1-6 ] - types: [ l3leaf ] platforms: [ default ] uplink_interfaces: [ Ethernet1-4 ] downlink_interfaces: [ Ethernet5-6 ] # internal vlan reservation internal_vlan_order: allocation: ascending range: beginning: 1100 ending: 1300 # DNS Server name_servers: - 10.100.1.7 # NTP Servers IP or DNS name, first NTP server will be preferred, and sourced from Management VRF ntp_settings: server_vrf: use_mgmt_interface_vrf servers: - name: dns-bind-01.int.guzware.net In the FABRIC.yml I have added this section:\n# internal vlan reservation internal_vlan_order: allocation: ascending range: beginning: 1100 ending: 1300 After editing the FABRIC.yml its time to edit the DC1.yml. This file will configure the unique BGP settings and map the specific uplinks/downlinks in the spine-leaf. As I have decided to use two distinct uplinks pr Leaf to the Spines I need to edit the DC1.yml accordingly:\n--- # Default gateway used for the management interface mgmt_gateway: 172.18.100.2 # Spine switch group spine: # Definition of default values that will be configured to all nodes defined in this group defaults: # Set the relevant platform as each platform has different default values in Ansible AVD platform: vEOS-lab # Pool of IPv4 addresses to configure interface Loopback0 used for BGP EVPN sessions loopback_ipv4_pool: 10.0.0.0/27 # ASN to be used by BGP bgp_as: 65000 # Definition of nodes contained in this group. # Specific configuration of device must take place under the node definition. Each node inherits all values defined under \u0026#39;defaults\u0026#39; nodes: # Name of the node to be defined (must be consistent with definition in inventory) - name: dc1-spine1 # Device ID definition. An integer number used for internal calculations (ie. IPv4 address of the loopback_ipv4_pool among others) id: 1 # Management IP to be assigned to the management interface mgmt_ip: 172.18.100.101/24 - name: dc1-spine2 id: 2 mgmt_ip: 172.18.100.102/24 # L3 Leaf switch group l3leaf: defaults: # Set the relevant platform as each platform has different default values in Ansible AVD platform: vEOS-lab # Pool of IPv4 addresses to configure interface Loopback0 used for BGP EVPN sessions loopback_ipv4_pool: 10.0.0.0/27 # Offset all assigned loopback IP addresses. # Required when the \u0026lt; loopback_ipv4_pool \u0026gt; is same for 2 different node_types (like spine and l3leaf) to avoid over-lapping IPs. # For example, set the minimum offset l3leaf.defaults.loopback_ipv4_offset: \u0026lt; total # spine switches \u0026gt; or vice versa. loopback_ipv4_offset: 2 # Definition of pool of IPs to be used as Virtual Tunnel EndPoint (VXLAN origin and destination IPs) vtep_loopback_ipv4_pool: 10.255.1.0/27 # Ansible hostname of the devices used to establish neighborship (IP assignments and BGP peering) uplink_interfaces: [\u0026#39;Ethernet1\u0026#39;, \u0026#39;Ethernet2\u0026#39;, \u0026#39;Ethernet3\u0026#39;, \u0026#39;Ethernet4\u0026#39;, \u0026#39;Ethernet5\u0026#39;, \u0026#39;Ethernet6\u0026#39;, \u0026#39;Ethernet1\u0026#39;, \u0026#39;Ethernet2\u0026#39;, \u0026#39;Ethernet3\u0026#39;, \u0026#39;Ethernet4\u0026#39;, \u0026#39;Ethernet5\u0026#39;, \u0026#39;Ethernet6\u0026#39;] uplink_switches: [\u0026#39;dc1-spine1\u0026#39;, \u0026#39;dc1-spine1\u0026#39;, \u0026#39;dc1-spine2\u0026#39;, \u0026#39;dc1-spine2\u0026#39;] # Definition of pool of IPs to be used in P2P links uplink_ipv4_pool: 192.168.0.0/26 # Definition of pool of IPs to be used for MLAG peer-link connectivity #mlag_peer_ipv4_pool: 10.255.1.64/27 # iBGP Peering between MLAG peers #mlag_peer_l3_ipv4_pool: 10.255.1.96/27 # Virtual router mac for VNIs assigned to Leaf switches in format xx:xx:xx:xx:xx:xx virtual_router_mac_address: 00:1c:73:00:00:99 spanning_tree_priority: 4096 spanning_tree_mode: mstp # If two nodes (and only two) are in the same node_group, they will automatically form an MLAG pair node_groups: # Definition of a node group that will include two devices in MLAG. # Definitions under the group will be inherited by both nodes in the group - group: DC1_L3_LEAF1 # ASN to be used by BGP for the group. Both devices in the MLAG pair will use the same BGP ASN bgp_as: 65001 nodes: # Definition of hostnames under the node_group - name: dc1-leaf1 id: 1 mgmt_ip: 172.18.100.103/24 # Definition of the port to be used in the uplink device facing this device. # Note that the number of elements in this list must match the length of \u0026#39;uplink_switches\u0026#39; as well as \u0026#39;uplink_interfaces\u0026#39; uplink_switch_interfaces: - Ethernet1 - Ethernet2 - Ethernet1 - Ethernet2 - group: DC1_L3_LEAF2 bgp_as: 65002 nodes: - name: dc1-leaf2 id: 2 mgmt_ip: 172.18.100.104/24 uplink_switch_interfaces: - Ethernet3 - Ethernet4 - Ethernet3 - Ethernet4 - group: DC1_L3_BORDERLEAF1 bgp_as: 65003 nodes: - name: dc1-borderleaf1 id: 3 mgmt_ip: 172.18.100.105/24 uplink_switch_interfaces: - Ethernet5 - Ethernet6 - Ethernet5 - Ethernet6 When I am satisfied with the DC1.yml I will continue with the NETWORK_SERVICES.yml. This will configure the respective VRF VNI/VXLAN mappings and L2 VLANS.\n--- tenants: # Definition of tenants. Additional level of abstraction to VRFs - name: TENANT1 # Number used to generate the VNI of each VLAN by adding the VLAN number in this tenant. mac_vrf_vni_base: 10000 vrfs: # VRF definitions inside the tenant. - name: VRF10 # VRF VNI definition. vrf_vni: 10 # Enable VTEP Network diagnostics # This will create a loopback with virtual source-nat enable to perform diagnostics from the switch. vtep_diagnostic: # Loopback interface number loopback: 10 # Loopback ip range, a unique ip is derived from this ranged and assigned # to each l3 leaf based on it\u0026#39;s unique id. loopback_ip_range: 10.255.10.0/27 svis: # SVI definitions. - id: 1072 # SVI Description name: VRF10_VLAN1072 enabled: true # IP anycast gateway to be used in the SVI in every leaf. ip_address_virtual: 10.72.10.1/24 - id: 1073 name: VRF10_VLAN1073 enabled: true ip_address_virtual: 10.73.10.1/24 - name: VRF11 vrf_vni: 11 vtep_diagnostic: loopback: 11 loopback_ip_range: 10.255.11.0/27 svis: - id: 1074 name: VRF11_VLAN1074 enabled: true ip_address_virtual: 10.74.11.1/24 - id: 1075 name: VRF11_VLAN1075 enabled: true ip_address_virtual: 10.75.11.1/24 l2vlans: # These are pure L2 vlans. They do not have a SVI defined in the l3leafs and they will be bridged inside the VXLAN fabric - id: 1070 name: L2_VLAN1070 - id: 1071 name: L2_VLAN1071 And finally the CONNECTED_ENDPOINTS.yml. This will configure the actual physical access/trunk ports for host connections/endpoints.\n--- # Definition of connected endpoints in the fabric. servers: # Name of the defined server. - name: dc1-leaf1-vm-server1 # Definition of adapters on the server. adapters: # Name of the server interfaces that will be used in the description of each interface - endpoint_ports: [ VM1 ] # Device ports where the server ports are connected. switch_ports: [ Ethernet5 ] # Device names where the server ports are connected. switches: [ dc1-leaf1 ] # VLANs that will be configured on these ports. vlans: 1071 # Native VLAN to be used on these ports. #native_vlan: 4092 # L2 mode of the port. mode: access # Spanning tree portfast configuration on this port. spanning_tree_portfast: edge # Definition of the pair of ports as port channel. #port_channel: # Description of the port channel interface. #description: PortChannel dc1-leaf1-server1 # Port channel mode for LACP. #mode: active - endpoint_ports: [ VM2 ] switch_ports: [ Ethernet6 ] switches: [ dc1-leaf1 ] vlans: 1072 mode: access spanning_tree_portfast: edge - name: dc1-leaf2-server1 adapters: - endpoint_ports: [ VM3 ] switch_ports: [ Ethernet5 ] switches: [ dc1-leaf ] vlans: 1070 native_vlan: 4092 mode: access spanning_tree_portfast: edge #port_channel: # description: PortChannel dc1-leaf2-server1 # mode: active - endpoint_ports: [ VM4 ] switch_ports: [ Ethernet6 ] switches: [ dc1-leaf2 ] vlans: 1073 mode: access spanning_tree_portfast: edge Now that all the yml\u0026rsquo;s have beed edited accordingly, its the time everyone has been waiting for\u0026hellip; Applying the config above and see some magic happen.\nAnsible build and deploy # Going one folder back up, I will find two files of interest (root of my chosen example folder single-dc-l3ls) called build.yml and deploy.yml.\nBefore sending the actual configuration to the devices, I will start by running the build.yml as a dry-run for error checking etc, but also building out the configuration for me to inspect before configuring the devices themselves. It will also create some dedicated files under the documentation folder (more on that later).\nLets run the the build.yaml:\n(arista_avd) andreasm@linuxmgmt01:~/arista/andreas-spine-leaf$ ansible-playbook build.yml PLAY [Build Configurations and Documentation] ******************************************************************************************************************************************************************* TASK [arista.avd.eos_designs : Verify Requirements] ************************************************************************************************************************************************************* AVD version 4.8.0 Use -v for details. [WARNING]: Collection arista.cvp does not support Ansible version 2.17.0 ok: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Create required output directories if not present] ******************************************************************************************************************************* ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/intended/structured_configs) ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/documentation/fabric) TASK [arista.avd.eos_designs : Set eos_designs facts] *********************************************************************************************************************************************************** ok: [dc1-spine1] TASK [arista.avd.eos_designs : Generate device configuration in structured format] ****************************************************************************************************************************** ok: [dc1-borderleaf1 -\u0026gt; localhost] ok: [dc1-spine1 -\u0026gt; localhost] ok: [dc1-spine2 -\u0026gt; localhost] ok: [dc1-leaf1 -\u0026gt; localhost] ok: [dc1-leaf2 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Generate fabric documentation] *************************************************************************************************************************************************** ok: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Generate fabric point-to-point links summary in csv format.] ********************************************************************************************************************* ok: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Generate fabric topology in csv format.] ***************************************************************************************************************************************** ok: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_designs : Remove avd_switch_facts] ********************************************************************************************************************************************************* ok: [dc1-spine1] TASK [arista.avd.eos_cli_config_gen : Verify Requirements] ****************************************************************************************************************************************************** skipping: [dc1-spine1] TASK [arista.avd.eos_cli_config_gen : Create required output directories if not present] ************************************************************************************************************************ ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/intended/structured_configs) ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/documentation) ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/intended/configs) ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/documentation/devices) TASK [arista.avd.eos_cli_config_gen : Include device intended structure configuration variables] **************************************************************************************************************** skipping: [dc1-spine1] skipping: [dc1-spine2] skipping: [dc1-leaf1] skipping: [dc1-leaf2] skipping: [dc1-borderleaf1] TASK [arista.avd.eos_cli_config_gen : Generate eos intended configuration] ************************************************************************************************************************************** ok: [dc1-spine1 -\u0026gt; localhost] ok: [dc1-spine2 -\u0026gt; localhost] ok: [dc1-leaf2 -\u0026gt; localhost] ok: [dc1-leaf1 -\u0026gt; localhost] ok: [dc1-borderleaf1 -\u0026gt; localhost] TASK [arista.avd.eos_cli_config_gen : Generate device documentation] ******************************************************************************************************************************************** ok: [dc1-spine2 -\u0026gt; localhost] ok: [dc1-spine1 -\u0026gt; localhost] ok: [dc1-borderleaf1 -\u0026gt; localhost] ok: [dc1-leaf1 -\u0026gt; localhost] ok: [dc1-leaf2 -\u0026gt; localhost] PLAY RECAP ****************************************************************************************************************************************************************************************************** dc1-borderleaf1 : ok=3 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 dc1-leaf1 : ok=3 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 dc1-leaf2 : ok=3 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 dc1-spine1 : ok=11 changed=0 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 dc1-spine2 : ok=3 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 No errors, looking good. (PS! I have already run the build.yaml once, and as there was no changes, there is nothing for it to update/change hence the 0 in changed. If it was any changes it would have reflected that too)\nIt has now created the individual config files under the folder intented/configs for my inspection and record.\n(arista_avd) andreasm@linuxmgmt01:~/arista/andreas-spine-leaf/intended/configs$ ll total 48 drwxrwxr-x 2 andreasm andreasm 4096 Jun 10 11:49 ./ drwxrwxr-x 4 andreasm andreasm 4096 Jun 10 07:14 ../ -rw-rw-r-- 1 andreasm andreasm 6244 Jun 10 11:49 dc1-borderleaf1.cfg -rw-rw-r-- 1 andreasm andreasm 6564 Jun 10 11:49 dc1-leaf1.cfg -rw-rw-r-- 1 andreasm andreasm 6399 Jun 10 11:49 dc1-leaf2.cfg -rw-rw-r-- 1 andreasm andreasm 4386 Jun 10 11:49 dc1-spine1.cfg -rw-rw-r-- 1 andreasm andreasm 4390 Jun 10 11:49 dc1-spine2.cfg Lets continue with the deploy.yml and send the configuration to the devices themselves:\n(arista_avd) andreasm@linuxmgmt01:~/arista/andreas-spine-leaf$ ansible-playbook deploy.yml PLAY [Deploy Configurations to Devices using eAPI] ************************************************************************************************************************************************************** TASK [arista.avd.eos_config_deploy_eapi : Verify Requirements] ************************************************************************************************************************************************** AVD version 4.8.0 Use -v for details. [WARNING]: Collection arista.cvp does not support Ansible version 2.17.0 ok: [dc1-spine1 -\u0026gt; localhost] TASK [arista.avd.eos_config_deploy_eapi : Create required output directories if not present] ******************************************************************************************************************** ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/config_backup) ok: [dc1-spine1 -\u0026gt; localhost] =\u0026gt; (item=/home/andreasm/arista/andreas-spine-leaf/config_backup) TASK [arista.avd.eos_config_deploy_eapi : Replace configuration with intended configuration] ******************************************************************************************************************** [DEPRECATION WARNING]: The `ansible.module_utils.compat.importlib.import_module` function is deprecated. This feature will be removed in version 2.19. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. ok: [dc1-leaf1] ok: [dc1-spine2] ok: [dc1-spine1] ok: [dc1-leaf2] ok: [dc1-borderleaf1] PLAY RECAP ****************************************************************************************************************************************************************************************************** dc1-borderleaf1 : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-leaf1 : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-leaf2 : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-spine1 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 dc1-spine2 : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 No changes here either as the switches has already been configured by me earlier uisng AVD. No changes has been done to the exisitng devices (I will do a change further down). If a new configuration or change the configuration is written to the devices instantly and now configured as requested.\nIn the next chapters I will go over what other benefits Arista Validated Design comes with.\nAutomatically generated config files # As mentioned above, by just running the build.yml action it will automatically create all the devices configuration files. These files are placed under the folder intended/configs and includes the full configs for every devices defined in the inventory.yml.\nHaving a look inside the dc1-borderleaf1.cfg file shows me the exact config that will be deployed on the device when I run the deploy.yml:\n!RANCID-CONTENT-TYPE: arista ! vlan internal order ascending range 1100 1300 ! transceiver qsfp default-mode 4x10G ! service routing protocols model multi-agent ! hostname dc1-borderleaf1 ip name-server vrf MGMT 10.100.1.7 ! ntp local-interface vrf MGMT Management1 ntp server vrf MGMT dns-bind-01.int.guzware.net prefer ! spanning-tree mode mstp spanning-tree mst 0 priority 4096 ! no enable password no aaa root ! username admin privilege 15 role network-admin nopassword username ansible privilege 15 role network-admin secret sha512 $6$sd3XM3jppUZgDhBs$Ouqb8DdTnQ3efciJMM71Z7iSnkHwGv.CoaWvOppegdUeQ5F1cIAAbJE/D40rMhYMkjiNkAuW7ixMEEoccCXHT/ ! vlan 1070 name L2_VLAN1070 ! vlan 1071 name L2_VLAN1071 ! vlan 1072 name VRF10_VLAN1072 ! vlan 1073 name VRF10_VLAN1073 ! vlan 1074 name VRF11_VLAN1074 ! vlan 1075 name VRF11_VLAN1075 ! vrf instance MGMT ! vrf instance VRF10 ! vrf instance VRF11 ! interface Ethernet1 description P2P_LINK_TO_DC1-SPINE1_Ethernet5 no shutdown mtu 1500 no switchport ip address 192.168.0.17/31 ! interface Ethernet2 description P2P_LINK_TO_DC1-SPINE1_Ethernet6 no shutdown mtu 1500 no switchport ip address 192.168.0.19/31 ! interface Ethernet3 description P2P_LINK_TO_DC1-SPINE2_Ethernet5 no shutdown mtu 1500 no switchport ip address 192.168.0.21/31 ! interface Ethernet4 description P2P_LINK_TO_DC1-SPINE2_Ethernet6 no shutdown mtu 1500 no switchport ip address 192.168.0.23/31 ! interface Loopback0 description EVPN_Overlay_Peering no shutdown ip address 10.0.0.5/32 ! interface Loopback1 description VTEP_VXLAN_Tunnel_Source no shutdown ip address 10.255.1.5/32 ! interface Loopback10 description VRF10_VTEP_DIAGNOSTICS no shutdown vrf VRF10 ip address 10.255.10.5/32 ! interface Loopback11 description VRF11_VTEP_DIAGNOSTICS no shutdown vrf VRF11 ip address 10.255.11.5/32 ! interface Management1 description oob_management no shutdown vrf MGMT ip address 172.18.100.105/24 ! interface Vlan1072 description VRF10_VLAN1072 no shutdown vrf VRF10 ip address virtual 10.72.10.1/24 ! interface Vlan1073 description VRF10_VLAN1073 no shutdown vrf VRF10 ip address virtual 10.73.10.1/24 ! interface Vlan1074 description VRF11_VLAN1074 no shutdown vrf VRF11 ip address virtual 10.74.11.1/24 ! interface Vlan1075 description VRF11_VLAN1075 no shutdown vrf VRF11 ip address virtual 10.75.11.1/24 ! interface Vxlan1 description dc1-borderleaf1_VTEP vxlan source-interface Loopback1 vxlan udp-port 4789 vxlan vlan 1070 vni 11070 vxlan vlan 1071 vni 11071 vxlan vlan 1072 vni 11072 vxlan vlan 1073 vni 11073 vxlan vlan 1074 vni 11074 vxlan vlan 1075 vni 11075 vxlan vrf VRF10 vni 10 vxlan vrf VRF11 vni 11 ! ip virtual-router mac-address 00:1c:73:00:00:99 ! ip address virtual source-nat vrf VRF10 address 10.255.10.5 ip address virtual source-nat vrf VRF11 address 10.255.11.5 ! ip routing no ip routing vrf MGMT ip routing vrf VRF10 ip routing vrf VRF11 ! ip prefix-list PL-LOOPBACKS-EVPN-OVERLAY seq 10 permit 10.0.0.0/27 eq 32 seq 20 permit 10.255.1.0/27 eq 32 ! ip route vrf MGMT 0.0.0.0/0 172.18.100.2 ! route-map RM-CONN-2-BGP permit 10 match ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY ! router bfd multihop interval 300 min-rx 300 multiplier 3 ! router bgp 65003 router-id 10.0.0.5 maximum-paths 4 ecmp 4 no bgp default ipv4-unicast neighbor EVPN-OVERLAY-PEERS peer group neighbor EVPN-OVERLAY-PEERS update-source Loopback0 neighbor EVPN-OVERLAY-PEERS bfd neighbor EVPN-OVERLAY-PEERS ebgp-multihop 3 neighbor EVPN-OVERLAY-PEERS password 7 Q4fqtbqcZ7oQuKfuWtNGRQ== neighbor EVPN-OVERLAY-PEERS send-community neighbor EVPN-OVERLAY-PEERS maximum-routes 0 neighbor IPv4-UNDERLAY-PEERS peer group neighbor IPv4-UNDERLAY-PEERS password 7 7x4B4rnJhZB438m9+BrBfQ== neighbor IPv4-UNDERLAY-PEERS send-community neighbor IPv4-UNDERLAY-PEERS maximum-routes 12000 neighbor 10.0.0.1 peer group EVPN-OVERLAY-PEERS neighbor 10.0.0.1 remote-as 65000 neighbor 10.0.0.1 description dc1-spine1 neighbor 10.0.0.2 peer group EVPN-OVERLAY-PEERS neighbor 10.0.0.2 remote-as 65000 neighbor 10.0.0.2 description dc1-spine2 neighbor 192.168.0.16 peer group IPv4-UNDERLAY-PEERS neighbor 192.168.0.16 remote-as 65000 neighbor 192.168.0.16 description dc1-spine1_Ethernet5 neighbor 192.168.0.18 peer group IPv4-UNDERLAY-PEERS neighbor 192.168.0.18 remote-as 65000 neighbor 192.168.0.18 description dc1-spine1_Ethernet6 neighbor 192.168.0.20 peer group IPv4-UNDERLAY-PEERS neighbor 192.168.0.20 remote-as 65000 neighbor 192.168.0.20 description dc1-spine2_Ethernet5 neighbor 192.168.0.22 peer group IPv4-UNDERLAY-PEERS neighbor 192.168.0.22 remote-as 65000 neighbor 192.168.0.22 description dc1-spine2_Ethernet6 redistribute connected route-map RM-CONN-2-BGP ! vlan 1070 rd 10.0.0.5:11070 route-target both 11070:11070 redistribute learned ! vlan 1071 rd 10.0.0.5:11071 route-target both 11071:11071 redistribute learned ! vlan 1072 rd 10.0.0.5:11072 route-target both 11072:11072 redistribute learned ! vlan 1073 rd 10.0.0.5:11073 route-target both 11073:11073 redistribute learned ! vlan 1074 rd 10.0.0.5:11074 route-target both 11074:11074 redistribute learned ! vlan 1075 rd 10.0.0.5:11075 route-target both 11075:11075 redistribute learned ! address-family evpn neighbor EVPN-OVERLAY-PEERS activate ! address-family ipv4 no neighbor EVPN-OVERLAY-PEERS activate neighbor IPv4-UNDERLAY-PEERS activate ! vrf VRF10 rd 10.0.0.5:10 route-target import evpn 10:10 route-target export evpn 10:10 router-id 10.0.0.5 redistribute connected ! vrf VRF11 rd 10.0.0.5:11 route-target import evpn 11:11 route-target export evpn 11:11 router-id 10.0.0.5 redistribute connected ! management api http-commands protocol https no shutdown ! vrf MGMT no shutdown ! end It is the full config it is intending to send to my borderleaf1 device. So goes for all the other cfg files. If I dont have access to the devices, I just want to generate the config this is just perfect I can stop there and AVD has already provided the configuration files for me.\nAutomated documentation # Everyone loves documentation, but not everyone loves documenting. Creating a full documentation and keeping it up to date after changes has been done is an important but time consuming thing. Regardless of loving to document or not, it is a very important component to have in place.\nWhen using Ansible Validated design, every time running the build.yml it will automatically create the documentation for every single device that has been configured. Part of the process when running the build.yaml is creating the device configuration but lo the full documentation and Guess what\u0026hellip;\nIt updates the documentation AUTOMATICALLY every time a new change has been added \u0026#x1f603; \u0026#x1f44d;\nLets test that in the next chapter..\nDay 2 changes using AVD # Not every environment is static, changes needs to be done from time to time. In this example I need to change some downlinks on my borwderleaf-1 device. I have been asked to configure a downlink port on the leaf for a new firewall that is being connected. I will go ahead and change the yml file CONNECTED_ENDPOINTS.yml by adding this section:\n- name: dc1-borderleaf1-wan1 adapters: - endpoint_ports: [ WAN1 ] switch_ports: [ Ethernet5 ] switches: [ dc1-borderleaf1 ] vlans: 1079 mode: access spanning_tree_portfast: edge The whole content of the CONNECTED_ENDPOINTS.yml looks like this now:\n--- # Definition of connected endpoints in the fabric. servers: - name: dc1-leaf1-vm-server1 adapters: - endpoint_ports: [ VM1 ] switch_ports: [ Ethernet5 ] switches: [ dc1-leaf1 ] vlans: 1071 mode: access spanning_tree_portfast: edge - endpoint_ports: [ VM2 ] switch_ports: [ Ethernet6 ] switches: [ dc1-leaf1 ] vlans: 1072 mode: access spanning_tree_portfast: edge - name: dc1-leaf2-server1 adapters: - endpoint_ports: [ VM3 ] switch_ports: [ Ethernet5 ] switches: [ dc1-leaf2 ] vlans: 1070 native_vlan: 4092 mode: access spanning_tree_portfast: edge - endpoint_ports: [ VM4 ] switch_ports: [ Ethernet6 ] switches: [ dc1-leaf2 ] vlans: 1073 mode: access spanning_tree_portfast: edge - name: dc1-borderleaf1-wan1 ## ADDED NOW ## adapters: - endpoint_ports: [ WAN1 ] switch_ports: [ Ethernet5 ] switches: [ dc1-borderleaf1 ] vlans: 1079 mode: access spanning_tree_portfast: edge Now I just need to run the build and if I am satisified I can run the deploy.yml. Lets test. I have already sent one config to the switch as one can refer to above. Now I have done a change and want to reflect that both in the intented configs, documentation and on the device itself. First I will run build.yml:\n(arista_avd) andreasm@linuxmgmt01:~/arista/andreas-spine-leaf$ ansible-playbook build.yml PLAY [Build Configurations and Documentation] ******************************************************************************************************************************************************************* PLAY RECAP ****************************************************************************************************************************************************************************************************** dc1-borderleaf1 : ok=3 changed=3 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 I reports 3 changes. Lets check the documentation and device configuration for borderleaf1:\nThe automatically updated documentation and device configuration for borderleaf1 (shortened for easier readability)\n## Interfaces ### Ethernet Interfaces #### Ethernet Interfaces Summary ##### L2 | Interface | Description | Mode | VLANs | Native VLAN | Trunk Group | Channel-Group | | --------- | ----------- | ---- | ----- | ----------- | ----------- | ------------- | | Ethernet5 | dc1-borderleaf1-wan1_WAN1 | access | 1079 | - | - | - | *Inherited from Port-Channel Interface ##### IPv4 | Interface | Description | Type | Channel Group | IP Address | VRF | MTU | Shutdown | ACL In | ACL Out | | --------- | ----------- | -----| ------------- | ---------- | ----| ---- | -------- | ------ | ------- | | Ethernet1 | P2P_LINK_TO_DC1-SPINE1_Ethernet5 | routed | - | 192.168.0.17/31 | default | 1500 | False | - | - | | Ethernet2 | P2P_LINK_TO_DC1-SPINE1_Ethernet6 | routed | - | 192.168.0.19/31 | default | 1500 | False | - | - | | Ethernet3 | P2P_LINK_TO_DC1-SPINE2_Ethernet5 | routed | - | 192.168.0.21/31 | default | 1500 | False | - | - | | Ethernet4 | P2P_LINK_TO_DC1-SPINE2_Ethernet6 | routed | - | 192.168.0.23/31 | default | 1500 | False | - | - | #### Ethernet Interfaces Device Configuration ```eos ! interface Ethernet1 description P2P_LINK_TO_DC1-SPINE1_Ethernet5 no shutdown mtu 1500 no switchport ip address 192.168.0.17/31 ! interface Ethernet2 description P2P_LINK_TO_DC1-SPINE1_Ethernet6 no shutdown mtu 1500 no switchport ip address 192.168.0.19/31 ! interface Ethernet3 description P2P_LINK_TO_DC1-SPINE2_Ethernet5 no shutdown mtu 1500 no switchport ip address 192.168.0.21/31 ! interface Ethernet4 description P2P_LINK_TO_DC1-SPINE2_Ethernet6 no shutdown mtu 1500 no switchport ip address 192.168.0.23/31 ! interface Ethernet5 ## NEW ## description dc1-borderleaf1-wan1_WAN1 no shutdown switchport access vlan 1079 switchport mode access switchport spanning-tree portfast ``` The actual installed config on my dc1-borderleaf1 switch:\nandreasm@linuxmgmt01:~/arista/andreas-spine-leaf/intended/configs$ ssh ansible@172.18.100.105 Password: Last login: Mon Jun 10 07:07:38 2024 from 10.100.5.10 dc1-borderleaf1\u0026gt;enable dc1-borderleaf1#show running-config ! Command: show running-config ! device: dc1-borderleaf1 (vEOS-lab, EOS-4.32.1F) ! ! boot system flash:/vEOS-lab.swi ! no aaa root ! management api http-commands no shutdown ! vrf MGMT no shutdown ! interface Ethernet1 description P2P_LINK_TO_DC1-SPINE1_Ethernet5 mtu 1500 no switchport ip address 192.168.0.17/31 ! interface Ethernet2 description P2P_LINK_TO_DC1-SPINE1_Ethernet6 mtu 1500 no switchport ip address 192.168.0.19/31 ! interface Ethernet3 description P2P_LINK_TO_DC1-SPINE2_Ethernet5 mtu 1500 no switchport ip address 192.168.0.21/31 ! interface Ethernet4 description P2P_LINK_TO_DC1-SPINE2_Ethernet6 mtu 1500 no switchport ip address 192.168.0.23/31 ! interface Ethernet5 ! interface Ethernet6 ! interface Loopback0 description EVPN_Overlay_Peering ip address 10.0.0.5/32 No Ethernet5 configured yet.\nNow all I need to do is run the deploy.yml to send the updated config to the switch itself.\n(arista_avd) andreasm@linuxmgmt01:~/arista/andreas-spine-leaf$ ansible-playbook deploy.yml PLAY [Deploy Configurations to Devices using eAPI] ************************************************************************************************************************************************************** TASK [arista.avd.eos_config_deploy_eapi : Replace configuration with intended configuration] ******************************************************************************************************************** changed: [dc1-borderleaf1] RUNNING HANDLER [arista.avd.eos_config_deploy_eapi : Backup running config] ************************************************************************************************************************************* changed: [dc1-borderleaf1] PLAY RECAP ****************************************************************************************************************************************************************************************************** dc1-borderleaf1 : ok=2 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Done, now if I log into my actual borderleaf-1 switch to verify:\ndc1-borderleaf1#show running-config ! Command: show running-config ! device: dc1-borderleaf1 (vEOS-lab, EOS-4.32.1F) ! interface Ethernet1 description P2P_LINK_TO_DC1-SPINE1_Ethernet5 mtu 1500 no switchport ip address 192.168.0.17/31 ! interface Ethernet2 description P2P_LINK_TO_DC1-SPINE1_Ethernet6 mtu 1500 no switchport ip address 192.168.0.19/31 ! interface Ethernet3 description P2P_LINK_TO_DC1-SPINE2_Ethernet5 mtu 1500 no switchport ip address 192.168.0.21/31 ! interface Ethernet4 description P2P_LINK_TO_DC1-SPINE2_Ethernet6 mtu 1500 no switchport ip address 192.168.0.23/31 ! interface Ethernet5 description dc1-borderleaf1-wan1_WAN1 switchport access vlan 1079 spanning-tree portfast ! interface Ethernet6 ! interface Loopback0 description EVPN_Overlay_Peering ip address 10.0.0.5/32 The config has been added.\nI can run the build and deploy as much as I want, if there is no changes it will not do anything. As soon as I want to a change, it will respect that and add my change declaratively.\nAutomating the whole process and interact with AVD using a WEB UI # Instead of doing this whole process of installing dependencies manually etc I asked my friend ChatGPT if it not could make a script that did the whole process for me.. Below is the script. It needs to be run in linux and in a folder where I am allowed to create new folders. Create a new .sh file, copy the content, save and make it executable by running chmod +x filename.sh.\nandreasm@linuxmgmt01:~/arista-automated-avd$ vim create-avd-project.sh andreasm@linuxmgmt01:~/arista-automated-avd$ chmod +x create-avd-project.sh #!/bin/bash # Prompt for input name read -p \u0026#34;Enter the name: \u0026#34; input_name # Create a folder from the input mkdir \u0026#34;$input_name\u0026#34; # CD into the newly created folder cd \u0026#34;$input_name\u0026#34; # Create a Python virtual environment with the same name as the input python3 -m venv \u0026#34;$input_name\u0026#34; # Activate the virtual environment source \u0026#34;$input_name/bin/activate\u0026#34; # Install Ansible python3 -m pip install ansible # Install Arista AVD collection ansible-galaxy collection install arista.avd # Export ARISTA_AVD_DIR environment variable export ARISTA_AVD_DIR=$(ansible-galaxy collection list arista.avd --format yaml | head -1 | cut -d: -f1) # Install requirements from ARISTA_AVD_DIR pip3 install -r ${ARISTA_AVD_DIR}/arista/avd/requirements.txt # Install additional packages pip install flask markdown2 pandas # Run ansible-playbook arista.avd.install_examples ansible-playbook arista.avd.install_examples # Create menu echo \u0026#34;Which example do you want to select? 1. Single DC L3LS 2. Dual DC L3LS 3. Campus Fabric 4. ISIS-LDP-IPVPN 5. L2LS Fabric\u0026#34; read -p \u0026#34;Enter your choice (1-5): \u0026#34; choice # Set the folder based on choice case $choice in 1) folder=\u0026#34;single-dc-l3ls\u0026#34; ;; 2) folder=\u0026#34;dual-dc-l3ls\u0026#34; ;; 3) folder=\u0026#34;campus-fabric\u0026#34; ;; 4) folder=\u0026#34;isis-ldp-ipvpn\u0026#34; ;; 5) folder=\u0026#34;l2ls-fabric\u0026#34; ;; *) echo \u0026#34;Invalid choice\u0026#34;; exit 1 ;; esac # CD into the respective folder cd \u0026#34;$folder\u0026#34; # Create app.py with the given content cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; app.py from flask import Flask, render_template, request, jsonify, Response import os import subprocess import logging import markdown2 import pandas as pd app = Flask(__name__) ROOT_DIR = \u0026#39;.\u0026#39; # Root directory where inventory.yml is located GROUP_VARS_DIR = os.path.join(ROOT_DIR, \u0026#39;group_vars\u0026#39;) # Subfolder where other YAML files are located FABRIC_DOCS_DIR = os.path.join(ROOT_DIR, \u0026#39;documentation\u0026#39;, \u0026#39;fabric\u0026#39;) DEVICES_DOCS_DIR = os.path.join(ROOT_DIR, \u0026#39;documentation\u0026#39;, \u0026#39;devices\u0026#39;) CONFIGS_DIR = os.path.join(ROOT_DIR, \u0026#39;intended\u0026#39;, \u0026#39;configs\u0026#39;) STRUCTURED_CONFIGS_DIR = os.path.join(ROOT_DIR, \u0026#39;intended\u0026#39;, \u0026#39;structured_configs\u0026#39;) # Ensure the documentation directories exist for directory in [FABRIC_DOCS_DIR, DEVICES_DOCS_DIR, CONFIGS_DIR, STRUCTURED_CONFIGS_DIR]: if not os.path.exists(directory): os.makedirs(directory) # Set up logging logging.basicConfig(level=logging.DEBUG) @app.route(\u0026#39;/\u0026#39;) def index(): try: root_files = [f for f in os.listdir(ROOT_DIR) if f.endswith(\u0026#39;.yml\u0026#39;)] group_vars_files = [f for f in os.listdir(GROUP_VARS_DIR) if f.endswith(\u0026#39;.yml\u0026#39;)] fabric_docs_files = [f for f in os.listdir(FABRIC_DOCS_DIR) if f.endswith((\u0026#39;.md\u0026#39;, \u0026#39;.csv\u0026#39;))] devices_docs_files = [f for f in os.listdir(DEVICES_DOCS_DIR) if f.endswith((\u0026#39;.md\u0026#39;, \u0026#39;.csv\u0026#39;))] configs_files = [f for f in os.listdir(CONFIGS_DIR) if f.endswith(\u0026#39;.cfg\u0026#39;)] structured_configs_files = [f for f in os.listdir(STRUCTURED_CONFIGS_DIR) if f.endswith(\u0026#39;.yml\u0026#39;)] logging.debug(f\u0026#34;Root files: {root_files}\u0026#34;) logging.debug(f\u0026#34;Group vars files: {group_vars_files}\u0026#34;) logging.debug(f\u0026#34;Fabric docs files: {fabric_docs_files}\u0026#34;) logging.debug(f\u0026#34;Devices docs files: {devices_docs_files}\u0026#34;) logging.debug(f\u0026#34;Configs files: {configs_files}\u0026#34;) logging.debug(f\u0026#34;Structured configs files: {structured_configs_files}\u0026#34;) return render_template(\u0026#39;index.html\u0026#39;, root_files=root_files, group_vars_files=group_vars_files, fabric_docs_files=fabric_docs_files, devices_docs_files=devices_docs_files, configs_files=configs_files, structured_configs_files=structured_configs_files) except Exception as e: logging.error(f\u0026#34;Error loading file list: {e}\u0026#34;) return \u0026#34;Error loading file list\u0026#34;, 500 @app.route(\u0026#39;/load_file\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def load_file(): try: filename = request.json[\u0026#39;filename\u0026#39;] logging.debug(f\u0026#34;Loading file: {filename}\u0026#34;) if filename in os.listdir(ROOT_DIR): file_path = os.path.join(ROOT_DIR, filename) elif filename in os.listdir(GROUP_VARS_DIR): file_path = os.path.join(GROUP_VARS_DIR, filename) elif filename in os.listdir(FABRIC_DOCS_DIR): file_path = os.path.join(FABRIC_DOCS_DIR, filename) elif filename in os.listdir(DEVICES_DOCS_DIR): file_path = os.path.join(DEVICES_DOCS_DIR, filename) elif filename in os.listdir(CONFIGS_DIR): file_path = os.path.join(CONFIGS_DIR, filename) elif filename in os.listdir(STRUCTURED_CONFIGS_DIR): file_path = os.path.join(STRUCTURED_CONFIGS_DIR, filename) else: raise FileNotFoundError(f\u0026#34;File not found: {filename}\u0026#34;) logging.debug(f\u0026#34;File path: {file_path}\u0026#34;) with open(file_path, \u0026#39;r\u0026#39;) as file: content = file.read() if filename.endswith(\u0026#39;.md\u0026#39;): content = markdown2.markdown(content, extras=[\u0026#34;toc\u0026#34;, \u0026#34;fenced-code-blocks\u0026#34;, \u0026#34;header-ids\u0026#34;]) return jsonify(content=content, is_markdown=True) elif filename.endswith(\u0026#39;.csv\u0026#39;): df = pd.read_csv(file_path) content = df.to_html(index=False) return jsonify(content=content, is_csv=True) else: return jsonify(content=content) except Exception as e: logging.error(f\u0026#34;Error loading file: {e}\u0026#34;) return jsonify(error=str(e)), 500 @app.route(\u0026#39;/save_file\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def save_file(): try: filename = request.json[\u0026#39;filename\u0026#39;] content = request.json[\u0026#39;content\u0026#39;] file_path = os.path.join(ROOT_DIR, filename) if filename in os.listdir(ROOT_DIR) else os.path.join(GROUP_VARS_DIR, filename) with open(file_path, \u0026#39;w\u0026#39;) as file: file.write(content) return jsonify(success=True) except Exception as e: logging.error(f\u0026#34;Error saving file: {e}\u0026#34;) return jsonify(success=False, error=str(e)), 500 def run_ansible_playbook(playbook): process = subprocess.Popen([\u0026#39;ansible-playbook\u0026#39;, playbook], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) for line in iter(process.stdout.readline, \u0026#39;\u0026#39;): yield f\u0026#34;data: {line}\\n\\n\u0026#34; process.stdout.close() process.wait() @app.route(\u0026#39;/run_playbook_stream/\u0026lt;playbook\u0026gt;\u0026#39;) def run_playbook_stream(playbook): return Response(run_ansible_playbook(playbook), mimetype=\u0026#39;text/event-stream\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: app.run(host=\u0026#39;0.0.0.0\u0026#39;, port=5000, debug=True) EOF # Create templates directory and index.html with the given content mkdir templates cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; templates/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Edit Ansible Files\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; #editor { width: 100%; height: 80vh; } #output, #fileContent { width: 100%; height: 200px; white-space: pre-wrap; background-color: #f0f0f0; padding: 10px; border: 1px solid #ccc; overflow-y: scroll; } #fileContent { height: auto; } \u0026lt;/style\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/ace.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/ace/1.4.12/ext-language_tools.js\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.6.0.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Edit Ansible Files\u0026lt;/h1\u0026gt; \u0026lt;select id=\u0026#34;fileSelector\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;\u0026#34;\u0026gt;Select a file\u0026lt;/option\u0026gt; \u0026lt;optgroup label=\u0026#34;Root Files\u0026#34;\u0026gt; {% for file in root_files %} \u0026lt;option value=\u0026#34;{{ file }}\u0026#34;\u0026gt;{{ file }}\u0026lt;/option\u0026gt; {% endfor %} \u0026lt;/optgroup\u0026gt; \u0026lt;optgroup label=\u0026#34;Group Vars Files\u0026#34;\u0026gt; {% for file in group_vars_files %} \u0026lt;option value=\u0026#34;{{ file }}\u0026#34;\u0026gt;{{ file }}\u0026lt;/option\u0026gt; {% endfor %} \u0026lt;/optgroup\u0026gt; \u0026lt;/select\u0026gt; \u0026lt;button id=\u0026#34;saveButton\u0026#34;\u0026gt;Save\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;editor\u0026#34;\u0026gt;Select a file to load...\u0026lt;/div\u0026gt; \u0026lt;h2\u0026gt;Documentation\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Fabric\u0026lt;/h3\u0026gt; \u0026lt;div id=\u0026#34;fabricDocs\u0026#34;\u0026gt; {% for file in fabric_docs_files %} \u0026lt;button class=\u0026#34;docButton\u0026#34; data-filename=\u0026#34;{{ file }}\u0026#34;\u0026gt;{{ file }}\u0026lt;/button\u0026gt; {% endfor %} \u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt;Devices\u0026lt;/h3\u0026gt; \u0026lt;div id=\u0026#34;devicesDocs\u0026#34;\u0026gt; {% for file in devices_docs_files %} \u0026lt;button class=\u0026#34;docButton\u0026#34; data-filename=\u0026#34;{{ file }}\u0026#34;\u0026gt;{{ file }}\u0026lt;/button\u0026gt; {% endfor %} \u0026lt;/div\u0026gt; \u0026lt;h2\u0026gt;Configs\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Intended Configs\u0026lt;/h3\u0026gt; \u0026lt;div id=\u0026#34;configs\u0026#34;\u0026gt; {% for file in configs_files %} \u0026lt;button class=\u0026#34;configButton\u0026#34; data-filename=\u0026#34;{{ file }}\u0026#34;\u0026gt;{{ file }}\u0026lt;/button\u0026gt; {% endfor %} \u0026lt;/div\u0026gt; \u0026lt;h3\u0026gt;Structured Configs\u0026lt;/h3\u0026gt; \u0026lt;div id=\u0026#34;structuredConfigs\u0026#34;\u0026gt; {% for file in structured_configs_files %} \u0026lt;button class=\u0026#34;configButton\u0026#34; data-filename=\u0026#34;{{ file }}\u0026#34;\u0026gt;{{ file }}\u0026lt;/button\u0026gt; {% endfor %} \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;fileContent\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;button id=\u0026#34;runBuildButton\u0026#34;\u0026gt;Run Build Playbook\u0026lt;/button\u0026gt; \u0026lt;button id=\u0026#34;runDeployButton\u0026#34;\u0026gt;Run Deploy Playbook\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;output\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; $(document).ready(function() { var editor = ace.edit(\u0026#34;editor\u0026#34;); editor.setTheme(\u0026#34;ace/theme/monokai\u0026#34;); editor.session.setMode(\u0026#34;ace/mode/yaml\u0026#34;); $(\u0026#39;#fileSelector\u0026#39;).change(function() { var filename = $(this).val(); console.log(\u0026#34;Selected file: \u0026#34; + filename); if (filename) { $.ajax({ url: \u0026#39;/load_file\u0026#39;, type: \u0026#39;POST\u0026#39;, contentType: \u0026#39;application/json\u0026#39;, data: JSON.stringify({ filename: filename }), success: function(data) { console.log(\u0026#34;File content received:\u0026#34;, data); if (data \u0026amp;\u0026amp; data.content) { editor.setValue(data.content, -1); } else { editor.setValue(\u0026#34;Failed to load file content.\u0026#34;, -1); } }, error: function(xhr, status, error) { console.error(\u0026#34;Error loading file content:\u0026#34;, status, error); editor.setValue(\u0026#34;Error loading file content: \u0026#34; + error, -1); } }); } }); $(\u0026#39;#saveButton\u0026#39;).click(function() { var filename = $(\u0026#39;#fileSelector\u0026#39;).val(); var content = editor.getValue(); console.log(\u0026#34;Saving file: \u0026#34; + filename); if (filename) { $.ajax({ url: \u0026#39;/save_file\u0026#39;, type: \u0026#39;POST\u0026#39;, contentType: \u0026#39;application/json\u0026#39;, data: JSON.stringify({ filename: filename, content: content }), success: function(data) { if (data.success) { alert(\u0026#39;File saved successfully\u0026#39;); } else { alert(\u0026#39;Failed to save file\u0026#39;); } }, error: function(xhr, status, error) { console.error(\u0026#34;Error saving file:\u0026#34;, status, error); alert(\u0026#39;Error saving file: \u0026#39; + error); } }); } }); $(\u0026#39;.docButton, .configButton\u0026#39;).click(function() { var filename = $(this).data(\u0026#39;filename\u0026#39;); console.log(\u0026#34;Selected file: \u0026#34; + filename); $.ajax({ url: \u0026#39;/load_file\u0026#39;, type: \u0026#39;POST\u0026#39;, contentType: \u0026#39;application/json\u0026#39;, data: JSON.stringify({ filename: filename }), success: function(data) { console.log(\u0026#34;File content received:\u0026#34;, data); if (data \u0026amp;\u0026amp; data.content) { $(\u0026#39;#fileContent\u0026#39;).html(data.content); if (data.is_markdown || data.is_csv) { $(\u0026#39;#fileContent a\u0026#39;).click(function(event) { event.preventDefault(); var targetId = $(this).attr(\u0026#39;href\u0026#39;).substring(1); var targetElement = document.getElementById(targetId); if (targetElement) { targetElement.scrollIntoView(); } }); } } else { $(\u0026#39;#fileContent\u0026#39;).text(\u0026#34;Failed to load file content.\u0026#34;); } }, error: function(xhr, status, error) { console.error(\u0026#34;Error loading file content:\u0026#34;, status, error); $(\u0026#39;#fileContent\u0026#39;).text(\u0026#34;Error loading file content: \u0026#34; + error); } }); }); $(\u0026#39;#runBuildButton\u0026#39;).click(function() { runPlaybook(\u0026#39;build.yml\u0026#39;); }); $(\u0026#39;#runDeployButton\u0026#39;).click(function() { runPlaybook(\u0026#39;deploy.yml\u0026#39;); }); function runPlaybook(playbook) { var eventSource = new EventSource(\u0026#39;/run_playbook_stream/\u0026#39; + playbook); eventSource.onmessage = function(event) { $(\u0026#39;#output\u0026#39;).append(event.data + \u0026#39;\\n\u0026#39;); $(\u0026#39;#output\u0026#39;).scrollTop($(\u0026#39;#output\u0026#39;)[0].scrollHeight); }; eventSource.onerror = function() { eventSource.close(); }; } }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; EOF Run the script to automatically install dependencies # I will now execute the script in a folder where it will create a new subfolder based on the input I give it and then I will be presented with a menu asking me which example I want to use. Then the script will go ahead and install necessary components and requirements (including copying all the example collections from Arista). After a short while a new folder is created using the name I entered in the first prompt. In my example below I am using new-site-3. Inside the newly created folder all the examples and python environment will be created, and also a Python generated web-page (inside the selected example folder e.g single-dc-l3ls) that can be started and will be available on http://0.0.0.0:5000 (more on the webpage later).\nandreasm@linuxmgmt01:~/arista-automated-avd$ ./create-avd-project-v1.sh Enter the name: new-site-3 Collecting ansible Installing collected packages: resolvelib, PyYAML, pycparser, packaging, MarkupSafe, jinja2, cffi, Starting galaxy collection install process PLAY [Install Examples] ***************************************************************************************************************************************************************************************** TASK [Copy all examples to /home/andreasm/arista-automated-avd/new-site-3] ************************************************************************************************************************************** changed: [localhost] PLAY RECAP ****************************************************************************************************************************************************************************************************** localhost : ok=1 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Which example do you want to select? 1. Single DC L3LS 2. Dual DC L3LS 3. Campus Fabric 4. ISIS-LDP-IPVPN 5. L2LS Fabric Enter your choice (1-5): 1 Now after the script above has run, all I need to do is to cd into the new folder that was created based on my input name (new-site-3).\nandreasm@linuxmgmt01:~/arista-automated-avd$ ll total 52 drwxrwxr-x 5 andreasm andreasm 4096 Jun 13 06:19 ./ drwxr-xr-x 44 andreasm andreasm 4096 Jun 13 06:18 ../ -rwxrwxr-x 1 andreasm andreasm 14542 Jun 12 21:56 create-avd-project.sh* drwxrwxr-x 8 andreasm andreasm 4096 Jun 13 06:21 new-site-3/ The only thing I need to now is to activate my new Python environment also named after the input I gave (new-site-3):\nandreasm@linuxmgmt01:~/arista-automated-avd/new-site-3$ ls campus-fabric dual-dc-l3ls isis-ldp-ipvpn l2ls-fabric new-site-3 single-dc-l3ls andreasm@linuxmgmt01:~/arista-automated-avd/new-site-3$ source new-site-3/bin/activate (new-site-3) andreasm@linuxmgmt01:~/arista-automated-avd/new-site-3$ I can now cd into my example folder I want to use, edit the necessary files etc, do ansible-playbook build.yml and deploy.yml. Or even better, I will cd into the example folder I selected in the last prompt, there the script has placed a file called app.py and a new folder called templates containing a index.html file so I can start a webserver. With this webserver I can interact with the files more interactively.\nWeb-based interaction # Together with my friend ChatGPT we have also created a web page to interact with the Arista Validated Designs a bit more interactively.\nTo start the webserver I need to cd into the example folder I selected from the script above (e.g single-dc-l3ls), and from there run the following command: python app.py (the python environment needs to be active).\n(new-site-3) andreasm@linuxmgmt01:~/arista-automated-avd/new-site-3/single-dc-l3ls$ python app.py * Serving Flask app \u0026#39;app\u0026#39; * Debug mode: on INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on all addresses (0.0.0.0) * Running on http://127.0.0.1:5000 * Running on http://10.100.5.10:5000 INFO:werkzeug:Press CTRL+C to quit INFO:werkzeug: * Restarting with stat WARNING:werkzeug: * Debugger is active! INFO:werkzeug: * Debugger PIN: 129-945-984 The page is capable of editing and save all the needed yml files within its own \u0026ldquo;project/environment\u0026rdquo; (e.g single-dc-l3ls). When done editing, there is two buttons that will trigger the ansible-playbook build.yml and ansible-playbook deploy.yml commands respectively with output. After the build command has been run it is capable of showing all the auto-created documentation contents under the folders documentation/fabric and documentation/devices respectively for easy access and interactive Table of Contents.\nSee short video clip below:\nOutro # This post has been a very exciting exercise.\nThe Arista Validated Design not only made deploying complex designs an easy task, but provided also much important documentation as part of the process. Regardless of the network switches being physical, they can be automated like anything else. Day 2 configurations are a joy to perform with this approach too.\n","date":"10 June 2024","externalUrl":null,"permalink":"/2024/06/10/arista-automated-configuration-using-ansible/","section":"Posts","summary":"In this post I will deploy Arista vEOS from zero to a full Spine-Leaf topology using Arista ZeroTouch Provisining and Ansible","title":"Arista Automated Configuration using Ansible","type":"posts"},{"content":"","date":"16 April 2024","externalUrl":null,"permalink":"/tags/antrea/","section":"Tags","summary":"","title":"Antrea","type":"tags"},{"content":"","date":"16 April 2024","externalUrl":null,"permalink":"/categories/antrea/","section":"Categories","summary":"","title":"Antrea","type":"categories"},{"content":"","date":"16 April 2024","externalUrl":null,"permalink":"/tags/cni/","section":"Tags","summary":"","title":"Cni","type":"tags"},{"content":"","date":"16 April 2024","externalUrl":null,"permalink":"/categories/cni/","section":"Categories","summary":"","title":"CNI","type":"categories"},{"content":" Antrea FeatureGates # In this post, I will highlight a couple of Antrea features that I\u0026rsquo;d like to discuss. I will explain how to configure them, why they are useful, and how to use them. Below is some of the features I will cover\nEgress using VLAN AntreaProxy ServiceExternalIP L7FLowExporter NodeNetwork Policy ExternalNode Policy As usual, head over to the official Antrea documentation and Antrea Github repo for more information. This post will focus on the upstream version v1.15.1 of Antrea available on Github, my Kubernetes clusters will also be using upstream Kubernetes version 1.28.5+ running on Ubuntu nodes.\nAntrea FeatureGates is enabled/disabled using your Helm value file, or editing the Antrea ConfigMap. To quickly get a status over the feature gates, use the antctl cli tool:\nandreasm@linuxmgmt01:~$ antctl get featuregates Antrea Agent Feature Gates FEATUREGATE STATUS VERSION AntreaIPAM Disabled ALPHA AntreaPolicy Enabled BETA AntreaProxy Enabled GA CleanupStaleUDPSvcConntrack Disabled ALPHA Egress Enabled BETA EgressSeparateSubnet Enabled ALPHA EgressTrafficShaping Disabled ALPHA EndpointSlice Enabled GA ExternalNode Disabled ALPHA FlowExporter Disabled ALPHA IPsecCertAuth Disabled ALPHA L7FlowExporter Disabled ALPHA L7NetworkPolicy Disabled ALPHA LoadBalancerModeDSR Disabled ALPHA Multicast Enabled BETA Multicluster Disabled ALPHA NetworkPolicyStats Enabled BETA NodeNetworkPolicy Disabled ALPHA NodePortLocal Enabled GA SecondaryNetwork Disabled ALPHA ServiceExternalIP Enabled ALPHA SupportBundleCollection Disabled ALPHA TopologyAwareHints Enabled BETA Traceflow Enabled BETA TrafficControl Disabled ALPHA Antrea Controller Feature Gates FEATUREGATE STATUS VERSION AdminNetworkPolicy Disabled ALPHA AntreaIPAM Disabled ALPHA AntreaPolicy Enabled BETA Egress Enabled BETA IPsecCertAuth Disabled ALPHA L7NetworkPolicy Disabled ALPHA Multicast Enabled BETA Multicluster Disabled ALPHA NetworkPolicyStats Enabled BETA NodeIPAM Enabled BETA ServiceExternalIP Enabled ALPHA SupportBundleCollection Disabled ALPHA Traceflow Enabled BETA Antrea Egress using VLAN - new in Antrea v.1.15.0 # I have already covered Antrea Egress in another post here, where I use Daemonset to install FRR on the worker nodes. This allowed me to advertise the Egress addresses using BGP using a different subnet than the nodes themselves. Back then Antrea did not have support for Egress subnets in different network than the nodes. Now, in Antrea v1.15.0 there is support in Antrea Egress to use different subnets. This is a welcome update in Antrea.\nBefore configuring Antrea Egress, the default behaviour for a pod when \u0026ldquo;egressing\u0026rdquo; or communicating externally outside its Kubernetes cluster is that its source address (POD CIDR) is translated using SNAT to the Kubernetes node the pod is currently running on. See below diagram:\nEach time a pod needs to communicate outside its k8s node it will get the IP address from the node it is currently residing on.\nWith Antrea Egress I can specify a specific IP address from a pool (ExternalIPPool) I confgure for the pods to use (for the ones I need Egress configured on). Before Antrea v1.15.0 I could only use the same subnet as the Kubernetes nodes were using, meaning I had to reserve a range in that subnet for Egress usage to avoid IP overlap, unless I did some DIY stuff like static routes or as in my post here. Now with Antrea v1.15.0 Antrea supports Egress on a different subnet, it even supports using a optional VLAN tag like this:\nI assign a VLAN tag in the ExternalIPPool which uses the same physical interface as the node itself is configured with, like eth0/ens18 etc). This require my node to be placed on a network that allows vlan trunks/in guest tagging, and allows me to easily separate the Egress subnets from the node subnet and specifying different subnets for my Egresses. In simple terms, what it does is creating a vlan interface on the nodes I decide to use as Egress nodes. Like this:\n11: antrea-ext.181@eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether bc:24:11:1d:82:f5 brd ff:ff:ff:ff:ff:ff This then gives me the option create Antrea Egress IP Pools with different subnets, separating them using VLAN, for my pods to use when in need of reaching out externally. **This means I can have my nodes residing on its own VLAN, and my Antrea Egress pools separated on completely different VLANs than my nodes. **\nIn the diagram below I have configured Antrea Egress with an ExternalIPPool using VLAN and a subnet defined to be 172.18.181.0/24. The nodes themselves reside on the subnet 10.160.1.0/24. So when I apply the Egress resource on a pod (using label selectors) it will grab a free IP from the ExternalIPPool and use an address from that subnet (172.18.181.0/24) when exiting the node. The other pods not configured with Egress resources will still use SNAT and translate their IP into the residing pods ip address (10.160.1.0/24). This gives me great flexibility not only to control which pods needs to use Egress, but also the flexibility to specify dedicated IP addresses for certain pods to use when egressing, and I can even specify multiple different subnets (see note [here]((which may incur additional management overhead))) using several Antrea Egress IP pools and achieve additional separation to meet different requirements. Controlling access via my firewall makes life much easier as I can allow a specific pod/IP to reach certain services in one part of my environment, where another pod with another subnet defined reach other parts of my environment. Using VLAN also eliminate the need of configuring BGP from my worker nodes to my BGP routers (which may incur additional management overhead), a VLAN is often simpler to create and manage.\nA very simple illustration on how my network is configured, for better context.\nAll my nodes are configured on a virtual port group using VLAN 216, using subnet 10.160.1.0/24 and my Egress pool is configured using VLAN 181. My switch is both L2 and L3, its configured with gateway interfaces for some of my networks and routes between the networks configured on the switch. All networks outside the switch is routed to my firewall using BGP.\nHow to configure Antrea Egress using VLAN # Now that I have explained how this new Antrea Egress works, its time to show how to configure it in my Kubernetes cluster.\nThe first requirement is that Antrea is on version 1.15.x. To check that execute the following command:\nandreasm@linuxmgmt01:~$ k exec -it -n kube-system antrea-controller-c47585dbf-6vdsl antctl version kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. antctlVersion: v1.15.1 controllerVersion: v1.15.1 Then make sure you have configured the following feature gates in the Antrea configMap:\napiVersion: v1 kind: ConfigMap metadata: name: antrea-config namespace: kube-system data: antrea-agent.conf: | featureGates: Egress: true EgressSeparateSubnet: true antrea-controller.conf: | featureGates: Egress: true After editing the Antrea configMap, restart the Antrea controller and agents so they can fetch the updated configMap:\nk rollout restart -n kube-system deployment/antrea-controller k delete pods -n kube-system -l component=antrea-agent Now I need to create and apply an ExternalIPPool:\napiVersion: crd.antrea.io/v1beta1 kind: ExternalIPPool metadata: name: egress-vlan-external-ip-pool spec: ipRanges: - start: 172.18.181.40 end: 172.18.181.50 subnetInfo: gateway: 172.18.181.1 prefixLength: 24 vlan: 181 # notice the VLAN setting here - this is a different VLAN than my nodes are using. nodeSelector: matchLabels: egress-role: egress-node I am using a nodeSelector matching on the label egress-role=egress-node. So I will need to label the nodes I want to be used as Egress nodes, or all of them. The Egress object will failover to the next available node if the active one fails and if I have more than one node labeled accordingly. I can quickly showcase that a bit later.\nIf I check for any ExternalIPPools I should have one applied now:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get externalippools.crd.antrea.io NAME TOTAL USED AGE egress-vlan-external-ip-pool 11 0 23h Then I will label my nodes:\nk label nodes k8s-node-vm-1-cl-02 egress-role=egress-node k label nodes k8s-node-vm-2-cl-02 egress-role=egress-node k label nodes k8s-node-vm-3-cl-02 egress-role=egress-node Now my three control plane nodes can be used as Egress nodes.\nI can now apply the Egress resource itself, refering to my ExternalIPPool above, where I will select an Ubuntu pod I already have running:\napiVersion: crd.antrea.io/v1beta1 kind: Egress metadata: name: egress-vlan spec: appliedTo: podSelector: matchLabels: app: egress-1 externalIPPool: egress-vlan-external-ip-pool status: egressNode: k8s-node-vm-2-cl-02 In the appliedTo field I am using podSelector without namespace, so it is a clusterwide selection for any pods using the label app=egress-1. Then I select which ExternalIPPool to use. I am also specifying the preferred node as the Egress node, but as we will see later, if that one becomes unavailable it will failover to next available node.\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get egress NAME EGRESSIP AGE NODE egress-vlan 172.18.181.40 23h k8s-node-vm-2-cl-02 And the ExternalIPPool now shows me:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get externalippools.crd.antrea.io NAME TOTAL USED AGE egress-vlan-external-ip-pool 11 1 23h If I enter bash of my Ubuntu pod now and ping an external vm, it will use the IP address above (172.18.181.40).\nLets see:\nAnd just to show that my nodes are using subnet 10.160.1.0/24:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-cp-vm-1-cl-02 Ready control-plane 66d v1.28.5 10.160.1.21 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-102-generic containerd://1.7.13 k8s-cp-vm-2-cl-02 Ready control-plane 66d v1.28.5 10.160.1.22 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-102-generic containerd://1.7.13 k8s-cp-vm-3-cl-02 Ready control-plane 66d v1.28.5 10.160.1.23 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-102-generic containerd://1.7.13 k8s-node-vm-1-cl-02 Ready \u0026lt;none\u0026gt; 66d v1.28.5 10.160.1.25 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-102-generic containerd://1.7.13 k8s-node-vm-2-cl-02 Ready \u0026lt;none\u0026gt; 66d v1.28.5 10.160.1.26 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-102-generic containerd://1.7.13 k8s-node-vm-3-cl-02 Ready \u0026lt;none\u0026gt; 66d v1.28.5 10.160.1.27 \u0026lt;none\u0026gt; Ubuntu 22.04.3 LTS 5.15.0-102-generic containerd://1.7.13 andreasm@linuxmgmt01:~/prod-cluster-2$ k get pod -n egress -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-59d474f9df-dv9ms 1/1 Running 0 28h 10.30.67.5 k8s-node-vm-1-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Thats it. Works beautifully.\nOn the node that is currently my Egress node, I will now have a interface called antrea-ext.181@eth0:\nubuntu@k8s-node-vm-2-cl-02:~$ ip link 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether bc:24:11:1d:82:f5 brd ff:ff:ff:ff:ff:ff altname enp0s18 3: kube-ipvs0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default link/ether d6:a1:1d:13:12:8e brd ff:ff:ff:ff:ff:ff 4: ovs-system: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 6a:31:ff:05:ab:b1 brd ff:ff:ff:ff:ff:ff 5: genev_sys_6081: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 65000 qdisc noqueue master ovs-system state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 1a:53:a9:d8:ba:e2 brd ff:ff:ff:ff:ff:ff 6: antrea-gw0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/ether 62:d1:4b:2d:7b:4d brd ff:ff:ff:ff:ff:ff 7: antrea-egress0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default link/ether 72:a2:89:52:08:32 brd ff:ff:ff:ff:ff:ff 9: yelb-db--81d702@if2: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue master ovs-system state UP mode DEFAULT group default link/ether e6:8d:cb:ac:21:66 brd ff:ff:ff:ff:ff:ff link-netns cni-28ac3a0f-cfd6-4823-286a-2334edb7a11c 10: yelb-app-1534fc@if2: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue master ovs-system state UP mode DEFAULT group default link/ether c6:97:db:c1:97:dd brd ff:ff:ff:ff:ff:ff link-netns cni-3fd6788c-d5e8-8a41-4a2b-e9d622c43a9e 11: antrea-ext.181@eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether bc:24:11:1d:82:f5 brd ff:ff:ff:ff:ff:ff ubuntu@k8s-node-vm-2-cl-02:~$ ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether bc:24:11:1d:82:f5 brd ff:ff:ff:ff:ff:ff altname enp0s18 inet 10.160.1.26/24 brd 10.160.1.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::be24:11ff:fe1d:82f5/64 scope link valid_lft forever preferred_lft forever 8: antrea-ext.181@eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether bc:24:11:1d:82:f5 brd ff:ff:ff:ff:ff:ff inet 172.18.181.40/24 brd 172.18.181.255 scope global antrea-ext.181 valid_lft forever preferred_lft forever inet6 fe80::be24:11ff:fe1d:82f5/64 scope link valid_lft forever preferred_lft forever This interface uses a real VLAN tag. So the port group and the underlaying switch this node is connected to must allow VLAN tags.\nJust a comment to the above. In my environment I am using Proxmox to host my Kubernetes nodes. I am tagging the \u0026ldquo;port-group\u0026rdquo; the actual VM is residing on, but I am also allowing certain other VLANs as trunks on the same port. So if I do \u0026ldquo;guest\u0026rdquo; tagging, aka tagging inside the VM the port group needs to respect and forward that tag. The way I solved this with Proxmox was to edit the VM config adding the \u0026ldquo;trunks=181;216\u0026rdquo; in the line below:\nnet0: virtio=BC:24:11:1D:82:F5,bridge=vmbr0,firewall=0,tag=216,trunks=181;216 The VM needs to be powercycled for this to take effect.\nAntrea Egress failover # As mentioned above, if my current Egress node fails the Egress gateway will move over to next available node. Let me poweroff the current Egress node and see what happens:\n# Before powering off andreasm@linuxmgmt01:~/prod-cluster-2$ k get egress NAME EGRESSIP AGE NODE egress-vlan 172.18.181.40 23h k8s-node-vm-2-cl-02 # After powered off andreasm@linuxmgmt01:~/prod-cluster-2$ k get egress NAME EGRESSIP AGE NODE egress-vlan 172.18.181.40 23h k8s-node-vm-3-cl-02 # Node status andreasm@linuxmgmt01:~/prod-cluster-2$ k get nodes NAME STATUS ROLES AGE VERSION k8s-cp-vm-1-cl-02 Ready control-plane 66d v1.28.5 k8s-cp-vm-2-cl-02 Ready control-plane 66d v1.28.5 k8s-cp-vm-3-cl-02 Ready control-plane 66d v1.28.5 k8s-node-vm-1-cl-02 Ready \u0026lt;none\u0026gt; 66d v1.28.5 k8s-node-vm-2-cl-02 NotReady \u0026lt;none\u0026gt; 66d v1.28.5 k8s-node-vm-3-cl-02 Ready \u0026lt;none\u0026gt; 66d v1.28.5 The failover is pretty quick too.\nAs soon as the node-2 is back online again the Egress will revert back again, because I have declared it to be on the node-2 in my ExternalIPPool config:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get egress -w NAME EGRESSIP AGE NODE egress-vlan 172.18.181.40 23h k8s-node-vm-3-cl-02 egress-vlan 172.18.181.40 23h k8s-node-vm-2-cl-02 Kube-Proxy vs Antrea Proxy # This chapter is kind of important to cover before I continue with the next Antrea features as this may have impact on how some of these features operate, and can operate. One should have some understanding of Kube-Proxy and Antrea-Proxy\nComparing kube-proxy and AntreaProxy involves looking at how each handles service traffic management within a Kubernetes cluster.\nKube-Proxy\nKube-proxy is running in the Kubernetes cluster as Daemonsets - Kube-Proxy runs in Userspace Kube-proxy is a default component in Kubernetes responsible for IP address translation and load balancing services within the cluster.\nIt can use iptables or IPVS (nftables alpha in k8s 1.29), each with different performance characteristics.\nThe iptables mode is commonly used due to its reliability and moderate performance, managing rules that route traffic to backend pods. IPtables runs in userspace.\nIPVS mode is known for better performance in large-scale environments because it handles traffic at the kernel level with less overhead than iptables. IPVS runs in Kernelspace\nWhile kube-proxy is suitable for many Kubernetes environments, its performance can degrade as the number of services and endpoints increases, especially in iptables mode.\nDeeply integrated into Kubernetes, kube-proxy watches the Kubernetes API for changes to services and endpoints and updates its rules accordingly.\nAntreaProxy\nAntreaProxy is part of the Antrea CNI, designed to handle all Kubernetes service traffic using Open vSwitch (OVS) for more efficient packet processing.\nWhen enabled, it can completely replace kube-proxy by managing service traffic routing directly at the data plane level within each node.\nHandles traffic using OVS, which can be more efficient in packet processing compared to traditional iptables. This is beneficial for high-throughput and low-latency network environments.\nDesigned to offer better scalability and performance in large-scale Kubernetes deployments due to its efficient handling of traffic at the OVS level.\nIntegrates with Kubernetes but requires Antrea as the CNI. It monitors the Kubernetes API for changes to services and endpoints, similar to kube-proxy, but uses OVS flow rules instead of IPVS or iptables.\nKey Differences\nTraffic Handling: Kube-proxy can use iptables or IPVS to manage traffic, which involves modifying Linux networking rules. AntreaProxy uses OVS to direct traffic, which can bypass some of the overhead associated with iptables or IPVS. Dependency: Kube-proxy is a standalone component within Kubernetes, while AntreaProxy is part of the Antrea CNI and requires its installation. Performance and Scalability: AntreaProxy generally offers better performance and scalability due to its use of OVS, particularly in environments with high numbers of services and endpoints. The choice between kube-proxy and AntreaProxy often comes down to the specific requirements of a Kubernetes deployment, such as scale, performance, and the preferred networking approach.\nAntreaProxy AND Kube-Proxy together # AntreaProxy is enabled by default since Antrea version v0.11. When the Antrea feature gate antreaProxy is set to true Antrea will provide service loadbalancing for ClusterIP services in OVS instead of using kube-proxy and only applies to traffic coming from pods to ClusterIP services. This means we still need to have Kube-Proxy for other services, such as NodePort (external to cluster service) and node to all cluster services (including hostNetwork pods). This is a default setup. Antrea can also act as the only proxy in a Kubernetes cluster, more on that in a chapter later.\nWhen traffic enters the Kubernetes cluster, eg serviceType Loadbalancer or NodePort, or node to service it will be handled by kube-proxy. Traffic originating from a pod to a cluster-service will be handled by AntreaProxy.\nAn illustration of what AntreaProxy handles when the feature gate AntreaProxy is set to true:\nStipulated lines illustrates the actual packet-flow, while the non-stipulated lines indicate which component being involved in the Kubernetes cluster to \u0026ldquo;prepare\u0026rdquo; for the forwarding of the packet from the yelb-ui pod to the yelb-app service.\nThe below diagrams explains what kube-proxy handles when the AntreaProxy is enabled:\nNode to service\nExternal to service\nWith both AntreaProxy and kube-proxy enabled at the same time we end up having two daemons with each their ruleset to forward traffic inside the Kubernetes cluster like this:\nAntreaProxy All # As mentioned above, there is no need to have both kube-proxy and AntreaProxy. One can remove kube-proxy completely. AntreaProxy can handle all services, external and internal, node to service, external to service, NodePort etc.\nTo enable AntreaProxy all the following feature-gate needs to be enabled:\nkind: ConfigMap apiVersion: v1 metadata: name: antrea-config namespace: kube-system data: antreaProxy: proxyAll: true With AntreaProxy all is enabled kube-proxy is not automatically removed (if one have a Kubernetes cluster with kube-proxy deployed). So in a cluster with kube-proxy already running, there is a couple of steps that needs to be done to remove kube-proxy and enable AntreaProxy all.\nIf one enables AntreaProxy:all and Kube-Proxy is still there, Kube-Proxy will take precedence. There is two ways to delete kube-proxy, during cluster bootstrap (kubeadm init --skip-phases=addon/kube-proxy) or post cluster bootstrap. Below I will go through how to delete kube-proxy on a already running cluster with kube-proxy already deployed.\nI will follow the offical documentation from Antrea Github repo [here](kubectl -n kube-system delete ds/kube-proxy):\n# Delete the kube-proxy DaemonSet andreasm@linuxmgmt01:~/prod-cluster-2$ kubectl -n kube-system delete ds/kube-proxy daemonset.apps \u0026#34;kube-proxy\u0026#34; deleted # Delete the kube-proxy ConfigMap to prevent kube-proxy from being re-deployed # by kubeadm during \u0026#34;upgrade apply\u0026#34;. This workaround will not take effect for # kubeadm versions older than v1.19 as the following patch is required: # https://github.com/kubernetes/kubernetes/pull/89593 andreasm@linuxmgmt01:~/prod-cluster-2$ kubectl -n kube-system delete cm/kube-proxy configmap \u0026#34;kube-proxy\u0026#34; deleted # Delete existing kube-proxy rules; there are several options for doing that # Option 1 (if using kube-proxy in iptables mode), run the following on each Node: iptables-save | grep -v KUBE | iptables-restore # This option is not valid for me as I use IPVS # Option 2 (any mode), restart all Nodes # I chose this option # Option 3 (any mode), run the following on each Node: kube-proxy --cleanup # You can create a DaemonSet to easily run the above command on all Nodes, using # the kube-proxy container image After the steps above (icluding the restart of all nodes) I will need to edit the Antrea ConfigMap with the following:\nkind: ConfigMap apiVersion: v1 metadata: name: antrea-config namespace: kube-system data: antrea-agent.conf: | kubeAPIServerOverride: \u0026#34;https://k8s-dev-cluster-3.domain.int\u0026#34; antreaProxy: proxyAll: true Last step is to restart the Antrea controller and Antrea agents and see if they come up again.\nk rollout restart -n kube-system deployment/antrea-controller k delete pods -n kube-system -l component=antrea-agent andreasm@linuxmgmt01:~/prod-cluster-2$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-7pc6r 2/2 Running 0 37s kube-system antrea-agent-b6c58 2/2 Running 0 37s kube-system antrea-agent-pj62z 2/2 Running 0 37s kube-system antrea-agent-rcckh 2/2 Running 0 37s kube-system antrea-agent-tftv5 2/2 Running 0 37s kube-system antrea-agent-twpfs 2/2 Running 0 38s kube-system antrea-controller-6bd6b9569c-vhx7n 1/1 Running 0 50s No kube-proxy pods:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-7pc6r 2/2 Running 0 72s kube-system antrea-agent-b6c58 2/2 Running 0 72s kube-system antrea-agent-pj62z 2/2 Running 0 72s kube-system antrea-agent-rcckh 2/2 Running 0 72s kube-system antrea-agent-tftv5 2/2 Running 0 72s kube-system antrea-agent-twpfs 2/2 Running 0 73s kube-system antrea-controller-6bd6b9569c-vhx7n 1/1 Running 0 85s kube-system coredns-77f7cc69db-jnh6m 1/1 Running 1 (9m36s ago) 21h kube-system coredns-77f7cc69db-wb5qs 1/1 Running 1 (9m3s ago) 21h kube-system dns-autoscaler-8576bb9f5b-5n7hv 1/1 Running 1 (9m36s ago) 21h kube-system kube-apiserver-k8s-cp-vm-1-cl-03 1/1 Running 3 (8m55s ago) 21h kube-system kube-apiserver-k8s-cp-vm-2-cl-03 1/1 Running 2 (9m36s ago) 21h kube-system kube-apiserver-k8s-cp-vm-3-cl-03 1/1 Running 2 (9m3s ago) 21h kube-system kube-controller-manager-k8s-cp-vm-1-cl-03 1/1 Running 4 (9m16s ago) 21h kube-system kube-controller-manager-k8s-cp-vm-2-cl-03 1/1 Running 4 (9m36s ago) 21h kube-system kube-controller-manager-k8s-cp-vm-3-cl-03 1/1 Running 4 (9m3s ago) 21h kube-system kube-scheduler-k8s-cp-vm-1-cl-03 1/1 Running 3 (8m45s ago) 21h kube-system kube-scheduler-k8s-cp-vm-2-cl-03 1/1 Running 2 (9m36s ago) 21h kube-system kube-scheduler-k8s-cp-vm-3-cl-03 1/1 Running 3 (9m3s ago) 21h No \u0026ldquo;kube-ipvs0\u0026rdquo; interface:\nubuntu@k8s-node-vm-1-cl-03:~$ ip addr 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether bc:24:11:f4:ee:ef brd ff:ff:ff:ff:ff:ff altname enp0s18 inet 10.160.1.35/24 brd 10.160.1.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::be24:11ff:fef4:eeef/64 scope link valid_lft forever preferred_lft forever 3: ovs-system: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether ee:a3:e7:de:7d:3c brd ff:ff:ff:ff:ff:ff 4: genev_sys_6081: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 65000 qdisc noqueue master ovs-system state UNKNOWN group default qlen 1000 link/ether e6:09:e7:ee:1f:fc brd ff:ff:ff:ff:ff:ff inet6 fe80::64c1:4aff:fe4d:f8b2/64 scope link valid_lft forever preferred_lft forever 5: antrea-gw0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000 link/ether 62:78:99:77:56:60 brd ff:ff:ff:ff:ff:ff inet 10.40.68.1/24 brd 10.40.68.255 scope global antrea-gw0 valid_lft forever preferred_lft forever inet6 fe80::6078:99ff:fe77:5660/64 scope link valid_lft forever preferred_lft forever 6: antrea-egress0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN group default link/ether 3e:77:85:e2:e2:1a brd ff:ff:ff:ff:ff:ff With that out of the way, lets continue on the Antrea features.\nAntrea ServiceExternalIP # ServiceExternalIP is a feature in Antrea to provide external IP addresses for serviceType LoadBalancer. This feature is useful when there is no external/cloud/3rd party loadbalancer available to provide these external IP addresses.\nToday ServiceExternalIP supports ip addresses allocated from the same subnet (L2) as the Kubernetes nodes, by reserving a range of addresses for this use. It also supports a different subnet (L3) than the Kubernetes nodes themselves but one have to define static routes (host routes x.x.x.x/32) in the underlaying router/gateway to one or several of the nodes in your Kubernetes Cluster. After I apply a service using ServiceExternalIP all nodes will be aware of this IP address and handle requests to the service regardless of which host receives them.\n# worker node 2 ubuntu@k8s-node-vm-2-cl-02:~$ ip addr 3: kube-ipvs0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN group default link/ether d6:a1:1d:13:12:8e brd ff:ff:ff:ff:ff:ff inet 10.169.1.40/32 scope global kube-ipvs0 # IP from my ExternalIPPool valid_lft forever preferred_lft forever # worker node 1 ubuntu@k8s-node-vm-1-cl-02:~$ ip addr 3: kube-ipvs0: \u0026lt;BROADCAST,NOARP\u0026gt; mtu 1500 qdisc noop state DOWN group default link/ether a6:bf:87:cc:53:cc brd ff:ff:ff:ff:ff:ff inet 10.169.1.40/32 scope global kube-ipvs0 valid_lft forever preferred_lft forever # IP from my ExternalIPPool And here one can see IPVS is handling this IP and is available on all nodes (if using kube-proxy with IPVS). So creating a static route and pointing to any of my Kubernetes nodes will get me to the service.\nWhen it comes to failure of a node and this happens to be the node you defined as the nexthop you will ofcourse loose access. One could add several routes using cost/distance, but this will not always solve the issue as the router/switch does not really know that the node is gone. It will be kept in the ARP table, and as long as its there (it may be drained after a while), the switch will continue forward traffic to it. This may ofcourse depend on router/switch also which features are implemented and can be used using static routes. There is no BFD, and a VM is not actually disconnecting anything if it goes down.\nServiceExternalIP will work with Kube-Proxy using IPVS or IPTables and AntreaProxy where there is no Kube-Proxy. If using Kube-Proxy with IPVS strictARP must be enabled in the kube-proxy config:\n# configMap kube-proxy apiVersion: v1 data: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 ipvs: excludeCIDRs: [] minSyncPeriod: 0s scheduler: rr strictARP: true # set to true syncPeriod: 30s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s Below I will go through ServiceExternalIP using kube-proxy and only AntreaProxy\nServiceExternalIP using Kube-Proxy and IPVS (IP Virtual Server) # When using kube-proxy with IPVS kube-proxy sets up and manage the IPVS rules. IPVS residing in the Linux Kernel is then responsible for directing incoming traffic to backend pods, as in my digram below the Yelb-UI pods.\nKube-proxy listens on incoming traffic, IPVS forwards the traffic to the corresponding backend-pods.\nThe service and corresponding endpoints:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k describe svc -n yelb yelb-ui Name: yelb-ui Namespace: yelb Labels: app=yelb-ui tier=frontend Annotations: service.antrea.io/external-ip-pool: service-external-ip-pool Selector: app=yelb-ui,tier=frontend Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.30.48.237 IPs: 10.30.48.237 LoadBalancer Ingress: 10.169.1.40 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 32611/TCP Endpoints: 10.30.67.5:80,10.30.68.6:80,10.30.69.2:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; The pods, representing the endpoints:\nandreasm@linuxmgmt01:~/prod-cluster-2$ k get pods -n yelb -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES yelb-ui-f544fc74f-mb5m6 1/1 Running 0 41m 10.30.68.6 k8s-node-vm-2-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-f544fc74f-pztht 1/1 Running 0 2m37s 10.30.67.5 k8s-node-vm-1-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-f544fc74f-r48rx 1/1 Running 0 2m37s 10.30.69.2 k8s-node-vm-3-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The IPVS loadbalancing in action:\nubuntu@k8s-node-vm-1-cl-02:~$ sudo ipvsadm -l IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP k8s-node-vm-1-cl-02:32611 rr -\u0026gt; 10-30-67-5.yelb-ui.yelb.svc. Masq 1 0 0 -\u0026gt; 10-30-68-6.yelb-ui.yelb.svc. Masq 1 0 0 -\u0026gt; 10-30-69-2.yelb-ui.yelb.svc. Masq 1 0 0 TCP k8s-node-vm-1-cl-02.cluster. rr -\u0026gt; 10-30-67-5.yelb-ui.yelb.svc. Masq 1 0 0 -\u0026gt; 10-30-68-6.yelb-ui.yelb.svc. Masq 1 0 0 -\u0026gt; 10-30-69-2.yelb-ui.yelb.svc. Masq 1 0 0 TCP k8s-node-vm-1-cl-02:http rr -\u0026gt; 10-30-67-5.yelb-ui.yelb.svc. Masq 1 0 0 -\u0026gt; 10-30-68-6.yelb-ui.yelb.svc. Masq 1 0 0 -\u0026gt; 10-30-69-2.yelb-ui.yelb.svc. Masq 1 0 0 # All IPs involved in the yelb-ui service, pod, service etc ubuntu@k8s-node-vm-3-cl-02:~$ sudo ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.30.63.48:80 rr -\u0026gt; 10.30.68.3:80 Masq 1 0 0 TCP 10.30.69.1:31925 rr -\u0026gt; 10.30.68.3:80 Masq 1 0 0 TCP 10.160.1.27:31925 rr -\u0026gt; 10.30.68.3:80 Masq 1 0 0 TCP 10.169.1.40:80 rr -\u0026gt; 10.30.68.3:80 Masq 1 0 0 UDP 10.30.0.3:53 rr -\u0026gt; 10.30.65.3:53 Masq 1 0 1 -\u0026gt; 10.30.66.2:53 Masq 1 0 1 andreasm@linuxmgmt01:~/prod-cluster-2$ k get pods -n yelb -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-84f4bf49b5-lstpm 1/1 Running 2 (4d ago) 71d 10.30.67.4 k8s-node-vm-1-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-6dc7cd98-4c6s5 1/1 Running 5 (98m ago) 71d 10.30.68.4 k8s-node-vm-2-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-84d6f6fc6c-dlp68 1/1 Running 5 (98m ago) 71d 10.30.68.2 k8s-node-vm-2-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-f544fc74f-vm2b2 1/1 Running 1 (98m ago) 123m 10.30.68.3 k8s-node-vm-2-cl-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Next chapter will cover how this looks with AntreaProxy all.\nServiceExternalIP using only AntreaProxy. # Using ServiceExternalIP with only AntreaProxy there will be no IPVS, no kube-proxy involved. All rules will be configured in OVS. If there is no kube-proxy configured in the cluster there is no pre-reqs (as above) that needs to be done. To get some insight in how that looks I will dump some output below.\nThe service and the corresponding endpoints:\nandreasm@linuxmgmt01:~$ k describe svc -n yelb yelb-ui Name: yelb-ui Namespace: yelb Labels: app=yelb-ui tier=frontend Annotations: service.antrea.io/external-ip-pool: service-external-ip-pool Selector: app=yelb-ui,tier=frontend Type: LoadBalancer IP Family Policy: SingleStack IP Families: IPv4 IP: 10.40.30.39 IPs: 10.40.30.39 LoadBalancer Ingress: 10.160.1.40 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 31535/TCP Endpoints: 10.40.67.4:80,10.40.68.3:80,10.40.69.6:80 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; The pods representing the endpoints:\nandreasm@linuxmgmt01:~$ k get pods -n yelb -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-84f4bf49b5-j6g8x 1/1 Running 0 5d22h 10.40.69.3 k8s-node-vm-3-cl-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-6dc7cd98-wlflx 1/1 Running 0 5d22h 10.40.69.2 k8s-node-vm-3-cl-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-84d6f6fc6c-qr2kv 1/1 Running 0 5d22h 10.40.67.2 k8s-node-vm-2-cl-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-f544fc74f-d84gd 1/1 Running 0 56s 10.40.69.6 k8s-node-vm-3-cl-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-f544fc74f-kvvht 1/1 Running 0 5d22h 10.40.67.4 k8s-node-vm-2-cl-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-f544fc74f-xqhrk 1/1 Running 0 56s 10.40.68.3 k8s-node-vm-1-cl-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Output from IPVS:\nubuntu@k8s-node-vm-1-cl-03:~$ sudo ipvsadm -l IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn So much empty..\nHow does the OVS tables look like:\n# endpoint pod 1: root@k8s-node-vm-1-cl-03:/# ovs-ofctl dump-flows br-int | grep 10.40.68.3 cookie=0x1010000000000, duration=433.630s, table=1, n_packets=4, n_bytes=168, idle_age=428, priority=200,arp,in_port=4,arp_spa=10.40.68.3,arp_sha=6e:4d:5b:e4:99:c9 actions=resubmit(,2) cookie=0x1010000000000, duration=433.630s, table=4, n_packets=25, n_bytes=4443, idle_age=2, priority=200,ip,in_port=4,dl_src=6e:4d:5b:e4:99:c9,nw_src=10.40.68.3 actions=resubmit(,5) cookie=0x1030000000000, duration=427.300s, table=12, n_packets=1, n_bytes=78, idle_age=10, priority=200,tcp,reg3=0xa284403,reg4=0x20050/0x7ffff actions=ct(commit,table=13,zone=65520,nat(dst=10.40.68.3:80),exec(load:0x1-\u0026gt;NXM_NX_CT_MARK[4],move:NXM_NX_REG0[0..3]-\u0026gt;NXM_NX_CT_MARK[0..3])) cookie=0x1010000000000, duration=433.630s, table=17, n_packets=28, n_bytes=5264, idle_age=2, priority=200,ip,reg0=0x200/0x200,nw_dst=10.40.68.3 actions=mod_dl_src:62:78:99:77:56:60,mod_dl_dst:6e:4d:5b:e4:99:c9,resubmit(,19) cookie=0x1030000000000, duration=427.300s, table=20, n_packets=0, n_bytes=0, idle_age=427, priority=190,ct_state=+new+trk,ip,nw_src=10.40.68.3,nw_dst=10.40.68.3 actions=ct(commit,table=21,zone=65520,exec(load:0x1-\u0026gt;NXM_NX_CT_MARK[5],load:0x1-\u0026gt;NXM_NX_CT_MARK[6])) # endpoint pod 2: root@k8s-node-vm-1-cl-03:/# ovs-ofctl dump-flows br-int | grep 10.40.67.4 cookie=0x1030000000000, duration=512942.550s, table=12, n_packets=18, n_bytes=1404, idle_age=65534, hard_age=65534, priority=200,tcp,reg3=0xa284304,reg4=0x20050/0x7ffff actions=ct(commit,table=13,zone=65520,nat(dst=10.40.67.4:80),exec(load:0x1-\u0026gt;NXM_NX_CT_MARK[4],move:NXM_NX_REG0[0..3]-\u0026gt;NXM_NX_CT_MARK[0..3])) L7FlowExporter - new in Antrea v.1.15.0 # FlowExport in Antrea is something I have covered previously here, but this time I would like to have a look at the new Layer7 FlowExport. Lets configure it and see some flows with Layer7 information.\nTo be able to use the L7FlowExporter and get the additional L7 flow information I need to enable the L7FlowExporter feature gate. So in my Antrea configMap:\napiVersion: v1 data: antrea-agent.conf: | L7FlowExporter: true Restart Antrea controller and agents:\nandreasm@linuxmgmt01:~$ k edit configmaps -n kube-system antrea-config configmap/antrea-config edited andreasm@linuxmgmt01:~$ k rollout restart -n kube-system deployment/antrea-controller deployment.apps/antrea-controller restarted andreasm@linuxmgmt01:~$ k delete pods -n kube-system -l component=antrea-agent pod \u0026#34;antrea-agent-4wtgw\u0026#34; deleted pod \u0026#34;antrea-agent-8gjvw\u0026#34; deleted pod \u0026#34;antrea-agent-h72qd\u0026#34; deleted pod \u0026#34;antrea-agent-lg52n\u0026#34; deleted pod \u0026#34;antrea-agent-z8knw\u0026#34; deleted pod \u0026#34;antrea-agent-zf9l6\u0026#34; deleted To further enable the L7 flow information I need to annotate either my pods or a whole namespace with the following key: visibility.antrea.io/l7-export=X the values can be ìngress, egress or both as it depends on which direction I want the L7 flow information. In the example below I will annotate my Ubuntu pod using the value both\napiVersion: apps/v1 kind: Deployment metadata: annotations: visibility.antrea.io/l7-export: both name: ubuntu-20-04 # or just annotate the pod directly k annotate pod -n egress ubuntu-20-04-59d474f9df-dv9ms visibility.antrea.io/l7-export=both Let see what kind of information I get when I try to access a web page using HTTP from the Ubuntu pod itself.\nI will use Theia as my tool to receive and visualize my IPFIX flows, I have already covered Theia here.\nWhen enabling the L7FlowExport the current Theia installation does not have the correct columns in place in the Clickhouse DB leading to FlowAggregator pod failing. It will complain about the missing columns. The following tables default.flows and default.flows_local needs to add the following columns:\nappProtocolName httpVals egressNodeName Inside the chi-clickhouse-clickhouse-0-0-0 pod execute the following:\nroot@chi-clickhouse-clickhouse-0-0-0:/# clickhouse-client\nchi-clickhouse-clickhouse-0-0-0.chi-clickhouse-clickhouse-0-0.flow-visibility.svc.cluster.local :) ALTER TABLE default.flows ADD COLUMN appProtocolName String;\nRepeat for the other two columns, in both tables\nScreenshot from the Grafana dashboard:\nI can see the HTTP protocol and the httpVals column is displaying this information:\n{\u0026#34;0\u0026#34;:{\u0026#34;hostname\u0026#34;:\u0026#34;www.vg.no\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;/\u0026#34;,\u0026#34;http_user_agent\u0026#34;:\u0026#34;curl/7.68.0\u0026#34;,\u0026#34;http_content_type\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;http_method\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;protocol\u0026#34;:\u0026#34;HTTP/1.1\u0026#34;,\u0026#34;status\u0026#34;:301,\u0026#34;length\u0026#34;:0}} I have remove a lot of columns in the screenshot above for clarity. For more information on Flow-Export head over here and here.\nNode NetworkPolicy and ExternalNode Policy # In this section I will have a look at two features that makes it possible to secure the Kubernetes nodes themselves using Antrea and external nodes, not being a Kubernetes node but a VM outside of the Kubernetes cluster. There can be many usecases for such a feature. If there is no physical firewall in between the segments or the nodes, still we need to enforce some kind of security to restrict certain lateral movement between nodes and or other VMs or bare-metal servers in the environment. Being able to do microsegmentation is important.\nNode NetworkPolicy - new in Antrea v1.15 # The diagram above tries to illustrate a communication pattern where the Kubernetes nodes is hosting a service running on port 80/TCP and some external resources is trying to reach those services. I will define a policy that will only allow access originating from certain subnets and drop all else. If I am exposing the service using an external LoadBalancer I could potentially restrict access to this service coming only from the LoadBalancer itself, nothing else. Lets have a look at the rule itself and apply it in i a real Kubernetes cluster.\nThe example rule I am using in this case is taken from the official Antrea documentation here with some modifications by me.\nThe rule (notice the kind, it is a ClusterNetworkPolicy):\napiVersion: crd.antrea.io/v1beta1 kind: ClusterNetworkPolicy metadata: name: restrict-http-to-node spec: priority: 5 tier: application appliedTo: - nodeSelector: matchExpressions: - key: \u0026#34;kubernetes.io/hostname\u0026#34; operator: \u0026#34;In\u0026#34; values: [\u0026#34;k8s-node-vm-1-cl-02\u0026#34;, \u0026#34;k8s-node-vm-2-cl-02\u0026#34;, \u0026#34;k8s-node-vm-3-cl-02\u0026#34;] ingress: - name: allow-cidr action: Allow from: - ipBlock: cidr: 10.100.5.0/24 - ipBlock: cidr: 172.18.6.0/24 ports: - protocol: TCP port: 80 - name: drop-other action: Drop ports: - protocol: TCP port: 80 The policy above will be applied to all nodes having the key/value label kubernetes.io/hostname: k8s-worker-nodeallowing port 80/TCP from 10.100.5.0/24 and 172.18.6.0/24 and block all else trying to reach port 80/TCP.\nNow before I can go ahead and apply this policy and I need to enable the feature gate NodeNetworkPolicy:\napiVersion: v1 kind: ConfigMap metadata: name: antrea-config namespace: kube-system data: antrea-agent.conf: | featureGates: NodeNetworkPolicy: true Restart all Antrea pods:\nandreasm@linuxmgmt01:~$ k delete pods -n kube-system -l app=antrea pod \u0026#34;antrea-agent-gz9nt\u0026#34; deleted pod \u0026#34;antrea-agent-mcck5\u0026#34; deleted pod \u0026#34;antrea-agent-qqr6l\u0026#34; deleted pod \u0026#34;antrea-agent-spmz8\u0026#34; deleted pod \u0026#34;antrea-agent-vnll7\u0026#34; deleted pod \u0026#34;antrea-agent-xrr2p\u0026#34; deleted pod \u0026#34;antrea-controller-576fdb765-h7tbc\u0026#34; deleted After the policy has been applied I can check easily if it is in effect by using the command below:\nandreasm@linuxmgmt01:~/prod-cluster-2/antrea-policies$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE restrict-http-to-node application 5 3 3 16m My cluster consists of 3 control plane nodes and 3 worker nodes. I can see my policy as been applied to at least 3, which I assume is my worker nodes. Lets dig into that deeper a bit later. But lets see if the policy is being enforced.\nFrom my laptop or from my linux vm, from both subnets that should be allowed:\nandreasm@linuxmgmt01:~$ curl http://10.169.1.40 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; # From my laptop andreasm@laptop:~$ curl http://10.169.1.40 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; From any other subnet (other machines) from any other subnet not defined as allowed:\nandreasm@linuxmgmt02:~$ ip addr 2: ens18: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether f6:97:82:3b:9a:9e brd ff:ff:ff:ff:ff:ff inet 172.18.5.40/24 brd 172.18.5.255 scope global ens18 valid_lft forever preferred_lft forever andreasm@linuxmgmt02:~$ curl http://10.169.1.40 curl: (28) Failed to connect to 10.169.1.40 port 80: Connection timed out Another example can be to drop certain ports between the nodes in a Kubernetes cluster itself with an example policy like this:\napiVersion: crd.antrea.io/v1beta1 kind: ClusterNetworkPolicy metadata: name: egress-drop-node-to-node spec: priority: 5 tier: application appliedTo: - nodeSelector: matchExpressions: - key: \u0026#34;kubernetes.io/hostname\u0026#34; operator: \u0026#34;In\u0026#34; values: [\u0026#34;k8s-node-vm-1-cl-02\u0026#34;, \u0026#34;k8s-node-vm-2-cl-02\u0026#34;, \u0026#34;k8s-node-vm-3-cl-02\u0026#34;] egress: - name: drop-22 action: Drop to: - nodeSelector: matchExpressions: - key: \u0026#34;kubernetes.io/hostname\u0026#34; operator: \u0026#34;In\u0026#34; values: [\u0026#34;k8s-node-vm-1-cl-02\u0026#34;, \u0026#34;k8s-node-vm-2-cl-02\u0026#34;, \u0026#34;k8s-node-vm-3-cl-02\u0026#34;] ports: - protocol: TCP port: 22 Now if I try to reach the any of my Kubernetes worker nodes from any of my Kubernetes nodes using SSH:\n# node 1 to node 2 over SSH ubuntu@k8s-node-vm-1-cl-02:~$ ssh ubuntu@10.160.1.26 ssh: connect to host 10.160.1.26 port 22: Connection timed out # node 1 to any other machine over SSH ubuntu@k8s-node-vm-1-cl-02:~$ ssh andreasm@10.100.5.10 The authenticity of host \u0026#39;10.100.5.10 (10.100.5.10)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:Ue0UyrCk679JwN8QbaRLviCdEfxg7lnu+3RXVryujA8. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? Play around with this new feature making it possible to also apply and enforce security policies on the nodes themselves opens up for a much more granual and controlled environment if one have these requirements. On edge usecases this can also be very valuable.\nExternalNode Policy # I just want to highlight this feature in its own chapter as I feel it is relevant, and may be a very good combination with the NodePolicy feature. A Kubernetes cluster with its running workload is often depending on external backend services, like external DB/DNS/NTP services hosted somewhere else in the datacenter.\nWith Antrea ExternalNode policy it is possible to manage security policies on non Kubernetes workload, like any regular virtual machine or bare-metal server using Antrea NetworkPolicy from your Kubernetes/API. See illustration below:\nFor information on how to configure this, head over to the following pages here and here.\nOutro # Thats it for this time. Some features I find interesting in Antrea, head over to the official Antrea docs page here and Github repo here for always updated and more information. Speaking of updated information, in time of writing this blog post Antrea 2.0 is released! Antrea v2.0\n","date":"16 April 2024","externalUrl":null,"permalink":"/2024/04/16/exploring-some-antrea-features/","section":"Posts","summary":"In this post I will go through a couple of Antrea features I find interesting, some new features and some older features","title":"Exploring some Antrea Features","type":"posts"},{"content":"","date":"16 April 2024","externalUrl":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes","type":"tags"},{"content":"","date":"16 April 2024","externalUrl":null,"permalink":"/categories/kubernetes/","section":"Categories","summary":"","title":"Kubernetes","type":"categories"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/tags/avi/","section":"Tags","summary":"","title":"Avi","type":"tags"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/categories/avi/","section":"Categories","summary":"","title":"AVI","type":"categories"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/tags/loadbalancing/","section":"Tags","summary":"","title":"Loadbalancing","type":"tags"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/tags/nsx/","section":"Tags","summary":"","title":"Nsx","type":"tags"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/categories/nsx/","section":"Categories","summary":"","title":"NSX","type":"categories"},{"content":"","date":"8 April 2024","externalUrl":null,"permalink":"/categories/tanzu/","section":"Categories","summary":"","title":"Tanzu","type":"categories"},{"content":" Introduction: # Since vSphere 8 U2 it is possible to combine NSX and Avi as the networking and loadbalancing stack for vSphere with Tanzu. For more information on how to configure that head over to this post. In that post I also go through how this new integration works. But I only use one vSphere with Tanzu deployment in that post, meaning only a single Supervisor cluster is deployed. I have been contacted several times recently regarding how this new integration functions when planning to deploy multiple Supervisors (more than one vSphere cluster where vSphere with Tanzu is enabled on), while using the same NSX manager and Avi controller for all Supervisor clusters.\nSome context to these requests and before I dive into the actual configurations and deployment:\nThe plan is to have vSphere with Tanzu enabled on two or more vSphere clusters, but I dont want them to share any Avi Service Engines or NSX Edge clusters. The only thing they have in common is that they are being managed by the same vCenter Server, NSX manager and Avi controller. The keyword is as much separation on compute and network as possible where sharing some common management instances like NSX manager cluster, vCenter Server and Avi controller cluster.\nIn this post I will go through how to accomplish this, including some explanations how the components involved makes the decision which NSX cloud in Avi to use for the respective Supervisor Cluster. My lab consist of the following:\n1x NSX manager cluster, managing both my vSphere clusters 1x vCenter Server, managing both my vSphere clusters 2x vSphere clusters, Supervisor installed on both of them 1x Avi controller cluster To make this post a bit shorter I am skipping all the necessary requirements/preparation/installation of NSX and Avi and will just refer to my previous posts how to do that:\nvSphere with Tanzu - NSX and Avi\nNSX and vSphere with Tanzu\nBefore jumping in, this is a diagram of what I want to end up with:\nSo lets dig in to it and see how that goes.\nPreparations in NSX # In NSX the important part is to configure each vSphere cluster that will have their own instance of vSphere with Tanzu with their own unique NSX Overlay Transport Zone. This is a requirement for Avi to create dedicated Service Engine groups which will ultimately serve my two vSphere with Tanzu instances.\nTransport Zone: Collection of transport nodes that defines the maximum span for logical switches. A transport zone represents a set of similarly provisioned hypervisors and the logical switches that connect VMs on those hypervisors. It also has been registered with the NSX management plane and has NSX modules installed. For a hypervisor host or NSX Edge to be part of the NSX overlay, it must be added to the NSX transport zone.\nSo in my lab I have two vSphere clusters, both configured with NSX, but using two different NSX Overlay Transport Zones. It does not matter if they are utilizing the same TEP VLAN (*defined in your host uplink profile), but the logical boundary, as an Overlay Transport Zone is, is limited to only involve the clusters isolated.\n(*) Tunnel Endpoint (TEP) Profile: Each transport node has a Tunnel Endpoint (TEP) responsible for encapsulating the overlay VM traffic inside a VLAN header and routing the packet to a destination TEP for further processing. TEPs are the source and destination IP addresses used in the external IP header to identify the ESXi hosts that originate and end the NSX encapsulation of overlay frames. Traffic can be routed to another TEP on a different host or the NSX Edge gateway to access the physical network. TEPs create a GENEVE tunnel between the source and destination endpoints.\nLike this:\nIn my NSX Environment I have it configured like this.\nUsing two different Transport Node Profiles:\nTransport Node Profile: This specifies the common transport parameters for network virtualization components in an NSX environment. It includes configurations for both the overlay and underlay network settings, such as N-VDS (NSX Virtual Distributed Switch) settings, teaming policies, and uplink profiles. Transport profiles are used to streamline the deployment and management of transport nodes by applying a consistent set of networking settings, enhancing the network\u0026rsquo;s efficiency and reliability.\nWhere in the respective Transport Node Profiles two different Overlay Transport Zones have been defined:\nCluster-1:\nCluster-2:\nNSX Edge Clusters # For many customers they have reasons to separate not only the compute part (different vSphere clusters for different departments, environments and customers (tenancy)) but also network separation. So I decided to deploy a dedicated Edge cluster per vSphere compute cluster. So I am in full control of the traffic coming in and out of my different vSphere clusters, not only logical but also physical. What I end up with is one Tier-0 router per vSphere cluster/Supervisor cluster.\nvSphere with Tanzu supports multiple Tier-0 routers even in the same Supervisor cluster see this post how to configure vSphere with Tanzu using different Tier-0 routers.\nSo here is my network topology in NSX:\nIn the picture above, I have my \u0026ldquo;Cluster-1\u0026rdquo; on the left where the Tier-0 sfo-m01-ec01-to-gw01 is managing ingress and egress for my NSX overlay networks and on the right the Tier-0 \u0026ldquo;\u0026rdquo;workload-tier-0\u0026quot;\u0026quot; managing ingress and egress for my \u0026ldquo;Cluster-2\u0026rdquo;.\nThats it for the NSX preparations. Next up is the Avi preparations.\nAVI preparations # In Avi, I need to configure two NSX clouds using the same NSX manager cluster, but for Avi it will be considered as two different clouds. The reason for that is because the Supervisor installation is using the NSX Overlay Transport Zone as an identifier for which NSX cloud to use when it sends requests to Avi to create a Virtual Service to provide my K8s api endpoints.\nAVI Clouds # In my Avi controller I have configured my two NSX clouds, using the same NSX manager, but as two different clouds in Avi.\nIf we take a quick peek inside both of them, starting with the nsx-m01 cloud, we will find some common values and some unique.\nnsx-m01\nThe fields in red are common settings between the two clouds. The other values are unique. notice the different Transport Zones which should match the Transport Zone for each respective cluster. Under DNS Profile one could also define a custom DNS profile of course, I just went with the same DNS profile in my lab. It would make sense to have two different DNS profiles also to keep the red line of separation complete.\nnsx-m01-2\nAvi Cloud Resource - Service Engine Group and Networks # Under Avi Cloud Resources I have defined the Default Service Engine group according to the vSphere Clusters/NSX clouds they belong to. Cloud nsx-m01 is using cluster sfo-m01-cl01 so I have configured the Default SE group accordingly so they take SE placement into consideration. Meaning the SEs will be placed in the right vSphere Cluster. This is important as the Default Service Engine Group will be used as a template for vSphere with Tanzu to create the dedicated Service Engine group per cluster.\nCloud nsx-m01\nThe important factor is under Scope here: Here I have defined the scope for where the SEs should be placed for the Supervisor cluster deployed in my vSphere Cluster sfo-m01-cl01.\nCloud nsx-m01-2\nOn my next nsx-m01-2 cloud I have configured the placement for this vSphere Cluster, which is the vSphere Cluster\nIts the same vCenter again, but different vSphere Cluster:\nUnder networks I have configured the necessary mgmt network and data network for my SEs accordingly. Meaning I have dedicated mgmt network and data network for my SEs in the nsx-m01 cloud and nsx-m01-2 cloud. The data network is only if you decide to also share or use the same SEs for other services, like Layer 7 from the TKC clusters. The vSphere with Tanzu installation will create its own data network name something like avi-domain-xxx.\nnsx-m01\nManagement network is provided with DHCP, therefore empty in here. And I also have my Avi DNS service configured in this cloud.\nnsx-m01-2\nThats it for the Avi configs.\nvSphere with Tanzu setup # In my vCenter I have two vSphere clusters, where I have enabled the Workload Management, or the Supervisor using the above config to support my needs. This is how it looks, whith some explanations.\nvSphere cluster sfo-m01-cl01 # vSphere cluster workload-1 # Supervisors # AVI virtual services for my k8s api endpoints # Notice the domain-cXXX ids. They reflect my respective vSphere cluster domain ids. All managed by the same Avi controller.\nThey are provisioned with different VIPs, not using the same VIP (or ingress as defined when installing vSphere with Tanzu). Lets have a look at the Service Engines, and inspect the networking for them as well.\nAs we can see, they are being handled by two different pairs of Service Engines. We see that in vCenter also, see screenshot above.\nFor my vSphere Cluster sfo-m01-cl01\nAnd the vSphere Cluster workload-1\nNSX # In NSX I can see two Distributed loadbalancers being created to serve as a kube-proxy replacement for the services internal to the Supervisor cluster itself (the Supervisor Controle Plane and ESXi worker nodes).\nNote\nWhen I provisioned my second Supervisor Cluster I did not overlap the Services CIDR by using the same range as defined in my first Supervisor Cluster. See screenshot below:\nFrom my svc-1\nFrom my svc-2\nOutro # Thats it, it is possible to use 1 NSX manager cluster, 1 vCenter Server and 1 AVI controller cluster to manage multiple Supervisor Clusters, and still maintain a high level of separation, except on the mgmt layer. If that is a requirement, then one have to install these components separately also. But that was not the point of this post.\n","date":"8 April 2024","externalUrl":null,"permalink":"/2024/04/08/vsphere-with-tanzu-avi-and-multiple-supervisors/","section":"Posts","summary":"In this post I will try to describe how to use the new NSX and Avi together integration in vSphere with Tanzu in combination with several Supervisor clusters using only 1 NSX manager cluster and 1 Avi controller cluster","title":"vSphere with Tanzu - Avi and Multiple Supervisors","type":"posts"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/tags/ako/","section":"Tags","summary":"","title":"Ako","type":"tags"},{"content":" Argo CD as a Supervisor service in vSphere with Tanzu # I was really motivated to create this post after I read my colleague Navneet\u0026rsquo;s two blog posts Authoring and Installing ArgoCD-Operator as a Supervisor Service on vCenter where he creates an Argo CD Supervisor service using Argo CD Operator and Tanzu Kubernetes Cluster lifecycle on vSphere Supervisor with ArgoCD where he uses the Argo CD supervisor service he created in the first post to provision and lifecycle a Tanzu Kubernetes Cluster with a set of applications.\nThe idea of using a Argo CD as a vSphere with Tanzu Supervisor service is just brilliant so I just had to test this out myself. A big thanks to Navneet for making this possible and all credits for my post goes to Navneet. Without his effort and help (yes I did have some calls with him to help me get started and help me with some troubleshooting) this post would not have been possible.\nNavneet\u0026rsquo;s two blog posts above is very much a pre-requisite to have read and performed if you want to follow what I do in this post as I am basing this whole post on his work.\nAs I am very often involved in PoCs on vSphere with Tanzu involving products like Antrea, Avi with AKO and NSX I wanted to explore how I could utilize Navneet\u0026rsquo;s work to deploy and lifecycle Tanzu Kubernetes Clusters including Antrea and AKO as part of the tkc deployment. The goal was to make the Antrea and AKO configuration easy and part of the TKC bringup. So in this post I will go through how I did it and if I succeeded in making it easier.\nUsing a declarative approach is Kubernetes nature, and managing all the configs and applications declaratively from one place using gitops is not only fun but a single place of truth when it comes to keeping a cluster in the defined state at all time.\nI am not saying this is a definitive guide on how to do this. There is certainly room for improvement, like I how I handle the secret (more on the below), but hopefully it triggers some ideas whats possible.\nThe goal # The goal of this post is to:\nUse Argo CD/gitops to deploy and maintain a TKC cluster in my vSphere with Tanzu environment. Use Argo CD/gitops to deploy and maintain Antrea Feature Gates by adding the AntreaConfig as part of the deployment Use Argo CD/gitops to deploy and maintain AKO installation and AKO Tunables by adding AKO value yaml as part of the deployment Use Argo CD/gitops to add and manage Antrea Network Policies A menu-driven approach to dynamically create all the needed deployment manifests to form my \u0026ldquo;basic-tkc-cluster\u0026rdquo; that can be sent to Argo CD. This is just what I wanted to add into the mix as a \u0026ldquo;basic\u0026rdquo; for my TKC clusters, one could very easily add Antrea Security Policies as an example into the mix also so that all deployed TKC clusters comes with a predefined set of minimum security policies applied. Not only does this give the comfort of securing a cluster before anyone has started consuming it, but it also adds an easy way to manage the security policies in general. I will in the last sections of this post quickly go through how easy it is to add more applications in form of Antrea security policies.\nLets get started\nMy github repo # This post is relying on my Github repo: https://github.com/andreasm80/argo-gitops-ns-1\nI have created my Github repo here that contains the folder structure for all my Argo CD manifests, including a script to generate the tkc-cluster deployment manifests. I have tried to keep it as simple, logic and manageable as possible. At least for me \u0026#x1f604;.\nBelow is the initial folder structure, this will change ofcourse when adding new clusters, applications etc as it is a live repo during the lifetime of a tkc cluster.\n. ├── README.md ├── setup-cluster.sh ├── argocd │ ├── argo-tkc-cluster-1-deployment.yaml │ ├── argo-tkc-cluster-2-deployment.yaml │ └── projects │ └── myproject.yaml ├── tkc-cluster-template │ ├── antrea-config-1.yaml │ ├── kustomization.yaml │ ├── tkgs-cluster-class-noaz-2.yaml │ ├── argocd-tkc-1-base-app-env-3.yaml │ ├── argocd-tkc-1-base-app-cm-5.yaml │ └── applications │ ├── gatekeeper-tkc-1 │ │ ├── create-ns.yaml │ │ ├── gatekeeper.yaml │ │ ├── kustomization.ya │ │ └── mutation-psa-policy.yaml │ └── foundational │ ├── ako │ │ ├── ako-inject-secret-5.yaml │ │ ├── ako-secret-role-2.yaml │ │ ├── ako-secret-rolebinding-3.yaml │ │ ├── ako-secret-sa-1.yaml │ │ ├── kustomization.yaml │ │ └── tkc-pv-pvc-4.yaml │ └── repos │ ├── argocd-ako-repo.yaml │ └── argocd-nfs-repo.yaml ├── tkc-cluster-1 │ ├── antrea-config-1.yaml │ ├── kustomization.yaml │ ├── tkgs-cluster-class-noaz-2.yaml │ ├── argocd-tkc-1-base-app-env-3.yaml │ ├── argocd-tkc-1-base-app-cm-5.yaml │ └── applications │ ├── gatekeeper-tkc-1 │ │ ├── create-ns.yaml │ │ ├── gatekeeper.yaml │ │ ├── kustomization.ya │ │ └── mutation-psa-policy.yaml │ └── foundational │ ├── ako │ │ ├── ako-inject-secret-5.yaml │ │ ├── ako-secret-role-2.yaml │ │ ├── ako-secret-rolebinding-3.yaml │ │ ├── ako-secret-sa-1.yaml │ │ ├── kustomization.yaml │ │ └── tkc-pv-pvc-4.yaml │ └── repos │ ├── argocd-ako-repo.yaml │ └── argocd-nfs-repo.yaml To get a better understanding of the folder structure I will go through them below.\nThe root folder # . ├── README.md ├── setup-cluster.sh The only relevant files located here is the setup-cluster.sh and the README.md. The setup-cluster.sh is the file that allows me to easily create new TKC deployments by issuing a couple of prompts relevant for the deployment like cluster name, vSphere Namespace, AKO settings etc.\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ ./setup-cluster.sh What is the cluster name? tkc-cluster-5 What is the vSphere Namespace Name? ns-1 What is the path to your NSX Tier-1 router for AKO configuration - (full path eg \u0026#34;/infra/tier-1s/tier-x\u0026#34;) /infra/tier-1s/tier-1 What is the VIP network name configured in Avi? vip-l7 What is the VIP network cidr? 10.141.1.0/24 Do you want AKO to be the default ingress controller? true/false true What is the Service Engine Group name domain-c1006:5de34cdc-8b14-46a3-b1c9-b627d574cdf0 What is the Cloud name in Avi you want to use? nsx-t What is the controller IP/DNS? 172.18.5.51 Updated antrea-config-1.yaml Updated argocd-tkc-base-app-env-3.yaml Updated tkgs-cluster-class-2.yaml Updated argocd-tkc-base-app-cm-5.yaml including AKO configuration Created and updated argo-tkc-cluster-5-deployment.yaml based on cluster name and namespace inputs in the \u0026#39;argocd\u0026#39; folder. Renamed gatekeeper folder to gatekeeper-tkc-cluster-5 within the \u0026#39;tkc-cluster-5/applications\u0026#39; directory. Updated the name field in argocd-ako-repo.yaml to avi-ako-helm-repo-tkc-cluster-5. Updated the name field in argocd-nfs-repo.yaml to nfs-helm-repo-tkc-cluster-5. The script copies the content from the tkc-cluster-template including all the yaml files I have defined to be part of my \u0026ldquo;basic\u0026rdquo; tkc deployment to a folder with the same name as the input from the prompt \u0026ldquo;What is the cluster name\u0026rdquo;. It then update all the relevant yaml files with unique values depending on the inputs from the corresponding prompts. Then it creates an application yaml for Argo CD I can use to create the application/tkc-cluster and place it in the folder argocd. And this takes me to the first folder argocd.\nThe argocd folder # . ├── README.md ├── argocd │ ├── argo-tkc-cluster-1-deployment.yaml │ ├── argocd-instance.ns-1.yaml │ ├── argocd-sup-svc-manifest.yaml │ ├── argocd-tkc-deploy.yaml │ ├── projects │ ├── my-projects.yaml In the argocd folder I am keeping all the yaml files related to ArgoCD itself. Lets start with the first file, the argo-tkc-cluster-1-deployment. This file is created by the setup-cluster.sh script using the file argocd-tkc-deploy.yaml as a template file.\nThe file argocd-sup-svc-manifest.yaml is the ArgoCD Operator Supervisor Service generated by following Navneet\u0026rsquo;s blog here. The file argocd-instance.ns-1.yaml is my ArgoCD server instance defining in which vSphere Namespace I want to deploy it in and how I want to expose it.\nThe projects folder should contain the yamls that define ArgoCD projects, more on that here.\nThe files in this folder are applied in-cluster or local-cluster from a ArgoCD perspective (more on that later).\nThe tkc-cluster-template folder # ├── tkc-cluster-template │ ├── antrea-config-1.yaml │ ├── kustomization.yaml │ ├── tkgs-cluster-class-noaz-2.yaml │ ├── argocd-tkc-1-base-app-env-3.yaml │ ├── argocd-tkc-1-base-app-cm-5.yaml │ └── applications │ ├── gatekeeper-tkc-1 │ │ ├── create-ns.yaml │ │ ├── gatekeeper.yaml │ │ ├── kustomization.ya │ │ └── mutation-psa-policy.yaml │ └── foundational │ ├── ako │ │ ├── ako-inject-secret-5.yaml │ │ ├── ako-secret-role-2.yaml │ │ ├── ako-secret-rolebinding-3.yaml │ │ ├── ako-secret-sa-1.yaml │ │ ├── kustomization.yaml │ │ └── tkc-pv-pvc-4.yaml │ └── repos │ ├── argocd-ako-repo.yaml │ └── argocd-nfs-repo.yaml I will not go through every file in this folder by detail as they are readily available for inspection in my Github repo, instead I will quickly go through what they do.\nKustomization.yaml: This first instance of kustomization.yaml refers to all the yamls that should be applied in-cluster (on the vSphere Namespace) like ArgoCD secrets, configmaps, the tkc-cluster itself. All objects that needs to run and be applied at the vSphere NS level (antreconfig etc). On top of that it is very easy to customize/override specific values by using Kustomize. This file is placed in the root of the tkc-cluster-template folder and this folder in turn is being referred to in the argo-deployment.yaml generated by the setup-cluster.sh script. All the files in the root of the tkc-cluster-template folder is being referenced in the kustomize.yaml file, including a couple of other files that needs to be applied in the same vSphere NS. antrea-config-1.yaml: This file contains the Antrea specific feature gates currently available in the latest TKR of vSphere 8 U2. By adding this into the tkc-creation makes it very easy before deployment and during the lifecycle of the TKC cluster to adjust and manage the Antrea Feature Gates. One may have specific requirements in a TKC cluster that requires a specific feature enabled in Antrea. One may also discover during the lifecycle of the TKC cluster that a specific Antrea Feature needs to be enabled or disabled. tkgs-cluster-class-noaz-2.yaml: This file contains the actual TKC cluster definition. argocd-tkc-1-base-app-env-3.yaml: This file contains a job that runs a script to get the TKC cluster\u0026rsquo;s secret, generates and applies a secret for ArgoCD to add the TKC cluster as a Kubernetes cluster in ArgoCD. argocd-tkc-1-base-app-cm-5.yaml: This file creates the configmap for ArgoCD for which applications to be installed in the TKC cluster. In other words, everything you want to deploy in the TKC cluster itself (remote cluster for ArgoCD). The argocd-tkc-1-base-app-cm-5.yaml itself will be applied in-cluster (or in the vSphere NS) but the configmap itself contains references to the remote cluster (the TKC cluster) for where to perform the actions/applications inside the configmap. This could be other yamls, helm installs or folders containing other kustomization.yamls. Like Gatekeeper as I will go show next. The application folder # │ └── applications │ ├── gatekeeper-tkc-1 │ │ ├── create-ns.yaml │ │ ├── gatekeeper.yaml │ │ ├── kustomization.ya │ │ └── mutation-psa-policy.yaml │ └── foundational │ ├── ako │ │ ├── ako-inject-secret-5.yaml │ │ ├── ako-secret-role-2.yaml │ │ ├── ako-secret-rolebinding-3.yaml │ │ ├── ako-secret-sa-1.yaml │ │ ├── kustomization.yaml │ │ └── tkc-pv-pvc-4.yaml │ └── repos │ ├── argocd-ako-repo.yaml │ └── argocd-nfs-repo.yaml This folder should be considered a \u0026ldquo;top-folder\u0026rdquo; for all the required applications and the corresponding applications dependencies for your specifc TKC cluster. In this folder I have three subfolders called: gatekeeper-tkc-1, foundationaland repos. The gatekeeper folder contains a kustomization.yaml that in turn refers to the files inside the gatekeeper folder. Gatekeeper here is used to manage the admission controller so I am allowed to something in the cluster when it is applied.\nThe foundationalfolder contains the subfolders for the specific applications, like in my case AKO and everything involved in getting AKO deployed in my TKC cluster. This folder also contains a kustomization.yaml that refers to all the fles inside the AKO folder. Why so many files in the AKO folder as AKO is easily installed using Helm. Well, as I hinted to initially this most likely will and can be handled much better but this is how I solved it right now. The reason is, when I deploy AKO using Helm the Helm chart expects me to populate the field usernameand passwordor authtoken. And during the Helm installation of AKO a avi-secret is generated based upon that information for AKO to use to interact with my Avi controller. As this is a public repo I dont want to expose any sensitive information, so what I do is mounting a PVC located on a NFS share with a predefined secret. As soon as the AKO installation is done, it will complain about having a secret without username and password. Then I have job defined to read the secret on the PVC and apply a new avi-secret containing the correct username and password. Then it will reboot th AKO pod for it to read the new secret, and voila the AKO pod is running and happy to serve.\nNote: The AKO settings being prompted is just the most important settings to get AKO up and running. There are several parametes that can be adjusted and configured. Have a look at the AKO Helm values.yaml for more information.\nNote: If the ako application should overwrite the avi-secret, I just need to delete the job that is dormant in my TKC cluster and it will automatically be synced back to a working state again.\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ k get jobs -n avi-system NAME COMPLETIONS DURATION AGE apply-secret-job 1/1 80s 7h12m The last folder repo contains all the Helm repos I need for my applications to be deployed in my TKC cluster.\nWhy this folder structure and all the kustomize.yamls? Well it makes my ArgoCD configmap \u0026ldquo;lighter\u0026rdquo; as I can just point the configmap in Argo CD for my application referencing the folder for where my specific app kustomization.yaml is located. Then in turn I will have kustomize to refer the needed yamls that build up my application. Easier to change/manage applications individually.\nNote I have included the NFS provisioner as part of my TKC cluster bringup, but it is merely optional as long as you already have a PVC created and defined before upbringing. In my deployment this PVC containing the avi-secret is needed.\nDeploying my TKC cluster with Argo CD # Now that I have gone through how I have my folder structure for Argo CD to deploy and manage my TKC clusters its time to create a TKC cluster using Argo CD.\nBut before I quickly go through the steps of deploying my TKC cluster using ArgoCD I need to explain the two levels where things are being applied, the different folders/yamls explained aboive.\nFrom a Argo CD perspective, Argo CD can be deployed in the same cluster as the applications it is managing, or it can be installed on a different Kubernetes cluster. When managing applications with ArgoCD on the same cluster as ArgoCD itself this is being refered to as \u0026ldquo;in-cluster\u0026rdquo; or \u0026ldquo;cluster-local\u0026rdquo;. So that means all destinations defined in ArgoCD will reference actual namespaces inside that cluster not a defined cluster-ip or Kubernetes API endpoint as ArgoCD is already running in the same cluster. If ArgoCD is running on a dedicated cluster and the defined ArgoCD managed applications are being applied to another cluster, this cluster is then referred to as a \u0026ldquo;remote-cluster\u0026rdquo;.\nIn my environment I will be using both \u0026ldquo;in-cluster\u0026rdquo; and \u0026ldquo;remote-cluster\u0026rdquo; as Argo CD is running as a instance (vSphere Pods in my Supervisor Cluster) in my vSphere Namespace \u0026ldquo;NS-1\u0026rdquo; where some of the yamls needs to be applied (explained above) and when my TKC cluster is up and running the applications that is is supposed to be installed in my TKC cluster is then a remote-cluster and I need to define a destination using:\ndestination: server: ${CLUSTER_IP} Where ${CLUSTER_IP} is the Kubernetes API endpoint of my TKC cluster. Then in my application yaml definition itself I will be referencing the correct Kubernetes namespace inside my TKC cluster (${CLUSTER_IP} destination).\nThis is just something to have in mind that there is two \u0026ldquo;layers\u0026rdquo; where ArgoCD configs needs to be applied. The way ArgoCD is deployed using the Operator from Navneet, it is not supposed to manage other TKC clusters in different vSphere Namespaces. ArgoCD will only manage TKC clusters in its own vSphere Namespace. That can also be tied to a RBAC constraint where I can define in the vSphere Namespace easily what and which personas are allowed to do.\nTKC-Cluster deployment # To get things started I will go to the root folder of my Github repo https://github.com/andreasm80/argo-gitops-ns-1 on my workstation and execute the setup-cluster.sh answering the prompts to create a new tkc-cluster-x folder:\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ ./setup-cluster.sh What is the cluster name? tkc-cluster-4 What is the vSphere Namespace Name? ns-1 What is the path to your NSX Tier-1 router for AKO configuration - (full path eg \u0026#34;/infra/tier-1s/tier-x\u0026#34;) /infra/tier-1s/tier-1 What is the VIP network name configured in Avi? vip-l7 What is the VIP network cidr? 10.141.1.0/24 Do you want AKO to be the default ingress controller? true/false true What is the Service Engine Group name domain-c1006:5de34cdc-8b14-46a3-b1c9-b627d574cdf0 What is the Cloud name in Avi you want to use? nsx-t What is the controller IP/DNS? 172.18.5.51 Updated antrea-config-1.yaml Updated argocd-tkc-base-app-env-3.yaml Updated tkgs-cluster-class-2.yaml Updated argocd-tkc-base-app-cm-5.yaml including AKO configuration Created and updated argo-tkc-cluster-4-deployment.yaml based on cluster name and namespace inputs in the \u0026#39;argocd\u0026#39; folder. Renamed gatekeeper folder to gatekeeper-tkc-cluster-4 within the \u0026#39;tkc-cluster-4/applications\u0026#39; directory. Updated the name field in argocd-ako-repo.yaml to avi-ako-helm-repo-tkc-cluster-4. Updated the name field in argocd-nfs-repo.yaml to nfs-helm-repo-tkc-cluster-4. Then I need to do a git add, commit and push:\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git add . andreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git commit -s -m \u0026#34;tkc-cluster-4\u0026#34; [main 1af9b40] tkc-cluster-4 19 files changed, 4662 insertions(+) create mode 100644 argocd/argo-tkc-cluster-4-deployment.yaml create mode 100644 tkc-cluster-4/antrea-config-1.yaml create mode 100644 tkc-cluster-4/applications/foundational/ako/ako-inject-secret-5.yaml create mode 100644 tkc-cluster-4/applications/foundational/ako/ako-secret-role-2.yaml create mode 100644 tkc-cluster-4/applications/foundational/ako/ako-secret-rolebinding-3.yaml create mode 100644 tkc-cluster-4/applications/foundational/ako/ako-secret-sa-1.yaml create mode 100644 tkc-cluster-4/applications/foundational/ako/kustomization.yaml create mode 100644 tkc-cluster-4/applications/foundational/ako/tkc-pv-pvc-4.yaml create mode 100644 tkc-cluster-4/applications/foundational/repos/argocd-ako-repo.yaml create mode 100644 tkc-cluster-4/applications/foundational/repos/argocd-nfs-repo.yaml create mode 100644 tkc-cluster-4/applications/gatekeeper-tkc-cluster-4/create-ns.yaml create mode 100644 tkc-cluster-4/applications/gatekeeper-tkc-cluster-4/gatekeeper.yaml create mode 100644 tkc-cluster-4/applications/gatekeeper-tkc-cluster-4/kustomization.yaml create mode 100644 tkc-cluster-4/applications/gatekeeper-tkc-cluster-4/mutation-psa-policy.yaml create mode 100644 tkc-cluster-4/argocd-tkc-base-app-cm-5.yaml create mode 100644 tkc-cluster-4/argocd-tkc-base-app-env-3.yaml create mode 100644 tkc-cluster-4/kustomization.yaml create mode 100644 tkc-cluster-4/tkgs-cluster-class-2.yaml andreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git push Enumerating objects: 30, done. Counting objects: 100% (30/30), done. Delta compression using up to 16 threads Compressing objects: 100% (28/28), done. Writing objects: 100% (28/28), 20.83 KiB | 1.60 MiB/s, done. Total 28 (delta 3), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (3/3), completed with 2 local objects. To github.com:andreasm80/argo-gitops-ns-1.git b933ff5..1af9b40 main -\u0026gt; main Now I need to to the argocd folder and execute the following command (requring me to be logged into my correct ArgoCD instance):\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1/argocd (main)$ argocd app create -f argo-tkc-cluster-4-deployment.yaml application \u0026#39;tkc-cluster-4-deploy\u0026#39; created After a few seconds in my ArgoCD web UI I should see my application being deployed. And after a very small coffee cup (depending on the underlaying hardware ofcourse) my newly created TKC cluster should be up and running:\nBrilliant, 100% hands of. Just one command to deploy my TKC cluster with all the \u0026ldquo;required\u0026rdquo; settings and tools ready to use. If I want to create another cluster I just repeat the same tasks above, give it a new name, and other values you want to change (also vSphere namespace if managed by another ArgoCD instance in another vSphere NS).\nTwo clusters up and running:\nNow I can start consuming my newly created TKC clusters. In the next chapters I will go through how I can manage and add \u0026ldquo;features\u0026rdquo; aka applications to an already deployed cluster.\nManage my TKC cluster using gitops # By using gitops its all about doing a change in my git repo that holds the entire state of my system and commit. ArgoCD will then notice the change and act accordingly, update the previous declared state to the new declared state. My git repo will always be the place to define how my system should look like. This means how my applications in my TKC clusters are configured, but also how my TKC clusters are configured. If I try to do it another way around, changing something directly in the cluster or the application, Argo will notice this change, complain that it differs from the source of truth (my git repo) and reconcile back to the defined state. And dont forget the other benefits of using gitops like, auditing, revison control, compliance.\nSo the gitops approach gives me one place to manage and configure my whole estate. Lets do one simple test. Scale my tkc-cluster-5.\nScale TKC cluster\u0026rsquo;s worker nodes # Lets first have a look at the cluster in its current state:\nandreasm@linuxmgmt01:~$ k config use-context tkc-cluster-5 Switched to context \u0026#34;tkc-cluster-5\u0026#34;. andreasm@linuxmgmt01:~$ k get nodes NAME STATUS ROLES AGE VERSION tkc-cluster-5-node-pool-1-5fdrj-67755499fc-cq92c Ready \u0026lt;none\u0026gt; 6h10m v1.26.5+vmware.2-fips.1 tkc-cluster-5-tjtnw-mm62h Ready control-plane 6h20m v1.26.5+vmware.2-fips.1 Ok, one control plane node and one worker.\nNow I will head over to my git repo that holds the config for this TKC cluster and adjust it from 1 worker node to 2 worker nodes.\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1/tkc-cluster-5 (main)$ vim tkgs-cluster-class-2.yaml --- apiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: tkc-cluster-5 annotations: argocd.argoproj.io/sync-wave: \u0026#34;-1\u0026#34; namespace: ns-1 spec: clusterNetwork: services: cidrBlocks: [\u0026#34;192.168.32.0/20\u0026#34;] pods: cidrBlocks: [\u0026#34;192.168.0.0/20\u0026#34;] serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.26.5---vmware.2-fips.1-tkg.1 controlPlane: replicas: 1 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: machineDeployments: - class: node-pool name: node-pool-1 replicas: 2 # from 1 to 2 Save and quit VIM, git add, commit and push:\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git commit -s -m \u0026#34;scale-tkc-5\u0026#34; [main f7aae3c] scale-tkc-5 4 files changed, 27 insertions(+), 66 deletions(-) delete mode 100644 argocd/argocd.yaml delete mode 100644 argocd/tkc-cluster-1.yaml andreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git push Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 16 threads Compressing objects: 100% (6/6), done. Writing objects: 100% (6/6), 602 bytes | 602.00 KiB/s, done. Total 6 (delta 5), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (5/5), completed with 5 local objects. To github.com:andreasm80/argo-gitops-ns-1.git 1af9b40..f7aae3c main -\u0026gt; main And after a couple of minutes (polls every three minutes):\nIt starts to change my TKC cluster according to the change I have done in my cluster manifest.\nMy second worker node on its way up:\nandreasm@linuxmgmt01:~$ k get nodes NAME STATUS ROLES AGE VERSION tkc-cluster-5-node-pool-1-5fdrj-67755499fc-cq92c Ready \u0026lt;none\u0026gt; 6h23m v1.26.5+vmware.2-fips.1 tkc-cluster-5-node-pool-1-5fdrj-848b6875bf-26hq5 NotReady \u0026lt;none\u0026gt; 4s v1.26.5+vmware.2-fips.1 tkc-cluster-5-tjtnw-mm62h Ready control-plane 6h32m v1.26.5+vmware.2-fips.1 I can do a edit on every yaml/file/setting in my git repo that involves my Argo managed clusters/application. This was just quick and easy example. What about adding something completely new to my TKC cluster?\nAdd Antrea Network Policies # I need security in my TKC cluster, and as a minimum it should treat every Kubernetes \u0026ldquo;non-system\u0026rdquo; namespaces as \u0026ldquo;not-trusted\u0026rdquo;. That means I need to apply a Antrea Cluster Network policy that implies no communication between namespaces allowed by default unless stated otherwise. So let me update my git repo to accommodate this policy.\nThis piece could as well be part of the \u0026ldquo;basic\u0026rdquo; deployment of my TKC clusters, as it provide a set of security out of the box without any manual intervening before handing over the TKC cluster to the users consuming the TKC cluster.\nThis step involves editing the ArgoCD configmap adding a new application. I will create a new folder under ./applications/foundational/ called network policies. Create a kustomization.yamlreferencing my needed Antrea policy definition files. Git add, commit and push.\nSo lets do this.\nFirst check my cluster for any existing network policies:\nandreasm@linuxmgmt01:~$ k get acnp No resources found andreasm@linuxmgmt01:~$ k get anp -A No resources found Lets create the folder, the Antrea policies and the kustomization.yaml in the folder mentioned above.\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1/tkc-cluster-5/applications/foundational (main)$ ls ako network-policies repos Add my Antrea policy block any Kubernetes namespace \u0026ldquo;interaction\u0026rdquo;.\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: strict-ns-isolation-except-system-ns spec: priority: 9 tier: securityops appliedTo: - namespaceSelector: # Selects all non-system Namespaces in the cluster matchExpressions: - {key: kubernetes.io/metadata.name, operator: NotIn, values: [nfs-storage,gatekeeper-system,avi-system,default,kube-node-lease,kube-public,kube-system,secretgen-controller,tanzu-continuousdelivery-resources,tanzu-fluxcd-packageinstalls,tanzu-kustomize-controller,tanzu-source-controller,tkg-system,vmware-system-antrea,vmware-system-auth,vmware-system-cloud-provider,vmware-system-csi,vmware-system-tkg,vmware-system-tmc]} ingress: - action: Pass from: - namespaces: match: Self # Skip ACNP evaluation for traffic from Pods in the same Namespace name: PassFromSameNS - action: Drop from: - namespaceSelector: {} # Drop from Pods from all other Namespaces name: DropFromAllOtherNS egress: - action: Pass to: - namespaces: match: Self # Skip ACNP evaluation for traffic to Pods in the same Namespace name: PassToSameNS - action: Drop to: - namespaceSelector: {} # Drop to Pods from all other Namespaces name: DropToAllOtherNS I will be saving this yaml as antrea-block-ns.yamlblo\nThen the kustomization.yaml:\napiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization resources: - antrea-block-ns.yaml Now the last file to configure is the argocd-tkc-base-app-cm-5.yaml in the root folder of my tkc-cluster-5:\nI will add this section at the end:\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tkc-addons-${CLUSTER_NAME}-antrea-policy spec: project: default source: repoURL: \u0026#39;https://github.com/andreasm80/argo-gitops-ns-1.git\u0026#39; path: tkc-cluster-5/applications/foundational/network-policies/ targetRevision: HEAD destination: server: ${CLUSTER_IP} syncPolicy: automated: prune: true And the last step:\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1/tkc-cluster-5 (main)$ cd .. andreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git add . andreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ git commit -s -m \u0026#34;antrea-policy-tkc-5\u0026#34; [main 653d451] antrea-policy-tkc-5 3 files changed, 52 insertions(+) create mode 100644 tkc-cluster-5/applications/foundational/network-policies/antrea-block-ns.yaml create mode 100644 tkc-cluster-5/applications/foundational/network-policies/kustomization.yaml amarqvardsen@amarqvards1MD6T:~/github_repos/argo-gitops-ns-1 (main)$ git push Enumerating objects: 14, done. Counting objects: 100% (14/14), done. Delta compression using up to 16 threads Compressing objects: 100% (9/9), done. Writing objects: 100% (9/9), 1.39 KiB | 710.00 KiB/s, done. Total 9 (delta 3), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (3/3), completed with 3 local objects. To github.com:andreasm80/argo-gitops-ns-1.git f7aae3c..653d451 main -\u0026gt; main Now its just waiting for Argo to apply my change (Antrea policy)\u0026hellip;\nA change has been detected:\nAnd I got a new application deployed:\nHas the Antrea policies been applied in my cluster?\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE strict-ns-isolation-except-system-ns securityops 9 0 0 3m50s As there is no namespace actually affected by this policy yet. Lets create two namespaces and apply a pod in each:\nandreasm@linuxmgmt01:~/github_repos/argo-gitops-ns-1 (main)$ k run dev-pod --image nginx/nginx -n dev pod/dev-pod created amarqvardsen@amarqvards1MD6T:~/github_repos/argo-gitops-ns-1 (main)$ k run prod-pod --image nginx/nginx -n prod pod/prod-pod created amarqvardsen@amarqvards1MD6T:~/github_repos/argo-gitops-ns-1 (main)$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE strict-ns-isolation-except-system-ns securityops 9 1 1 7m23s The policy is now in effect and drops all attempts between these two namespaces devand prod.\nOutstanding. As mentioned this could also be part of the initial tkc-deployment, not necessarily being added post tkc creation. But it was nice and easy to add it post tkc creation also. Now when I have added it though it is very easy to just continue add additional Antrea policy yaml definitions in the folder network-policies commit and the changes are applied.\nConclusion # Did I succeed in making it easier to maintain a TKC cluster, AntreaConfigs, AKO installation and post-adding and managing Antrea Security Policies?\nYes, I think so. Everything is defined in my respective tkc-cluster-x folder in my gitrepo, its very easy to change any of the settings in any of the files as they are available and easy accessible. Argo CD will detect any update and ensure it is being applied. It even gives me a nice overview at all times whether there is something wrong with my cluster, my applications. I always maintain the source of truth in my Github repo, and if there are any configuration drifts, they will be reverted back.\nI am exposing the AntreaConfig for easier management of the respective Antrea Feature Gates, I am exposing AKO values for easier installation and configuration settings. I am exposing Antrea Network Policies for easier deployment and management of security policies. All changes are registered, all version changes are tracked so I know exactly whom, when and what has been changed. And if something goes wrong.. I can rollback to the last working state.\nThanks to Navneet for not only providing the Supervisor Service ArgoCD Operator, but also the tools how to create it and other Supervisor Services, read more on his blog post here.\n","date":"4 February 2024","externalUrl":null,"permalink":"/2024/02/04/argo-cd-vsphere-with-tanzu/","section":"Posts","summary":"In this post I am using ArgoCD bootstrapping Tanzu Kubernetes Clusters with AKO and Antrea configs","title":"Argo CD \u0026 vSphere with Tanzu","type":"posts"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/tags/argocd/","section":"Tags","summary":"","title":"Argocd","type":"tags"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/categories/argocd/","section":"Categories","summary":"","title":"ArgoCD","type":"categories"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/tags/gitops/","section":"Tags","summary":"","title":"Gitops","type":"tags"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/categories/gitops/","section":"Categories","summary":"","title":"GitOps","type":"categories"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/tags/tanzu/","section":"Tags","summary":"","title":"Tanzu","type":"tags"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/tags/vsphere/","section":"Tags","summary":"","title":"Vsphere","type":"tags"},{"content":"","date":"4 February 2024","externalUrl":null,"permalink":"/categories/vsphere/","section":"Categories","summary":"","title":"VSphere","type":"categories"},{"content":"","date":"15 January 2024","externalUrl":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible","type":"tags"},{"content":"","date":"15 January 2024","externalUrl":null,"permalink":"/tags/kubespray/","section":"Tags","summary":"","title":"Kubespray","type":"tags"},{"content":"","date":"15 January 2024","externalUrl":null,"permalink":"/tags/opentofu/","section":"Tags","summary":"","title":"Opentofu","type":"tags"},{"content":"","date":"15 January 2024","externalUrl":null,"permalink":"/tags/proxmox/","section":"Tags","summary":"","title":"Proxmox","type":"tags"},{"content":" I need a Kubernetes cluster, again # I am using a lot of my spare time playing around with my lab exploring different topics. Very much of that time again is spent with Kubernetes. It has been so many times have I deployed a new Kubernetes cluster, then after some time decommissioned it again. They way I have typically done it is using a Ubuntu template I have created, cloned it, manually adjusted all clones as needed, then manually installed Kubernetes. This takes a lot of time overall and it also stops me doing certain tasks as sometimes I just think nah.. not again. Maybe another day and I do something else instead. And, when a human being is set to do a repetive task it is destined to fail at some stage, forgot a setting, why is this one vm not working as the other vms and so on. Now the time has come to automate these tasks with the following goal in mind:\nReduce deployment time to a minum Eliminate human errors Consistent outcome every time Make it even more fun to deploy a Kubernetes cluster I have been running Home Assistant for many years, there I have a bunch of automations automating all kinds of things in my home which just makes my everyday life a bit happier. Most of these automations just work in the background doing their stuff as a good automation should. Automating my lab deployments is something I have been thinking of getting into several times, and now I decided to get this show started. So, as usual, a lot of tinkering, trial and error until I managed to get something working the way I wanted. Probably room for improvement in several areas, that is something I will most likely use my time on \u0026ldquo;post\u0026rdquo; this blog post. Speaking of blog post, after using some time on this I had to create a blog post on it. My goal later on is a fully automated lab from VM creation, Kubernetes runtime, and applications. And when they are decommisioned I can easily spin up everything again with persistent data etc. Lets see.\nFor now, this post will cover what I have done and configured so far to be able to automatically deploy the VMs for my Kubernetes clusters then the provisioning of Kubernetes itself.\nMy lab # My lab consists of two fairly spec\u0026rsquo;ed servers with a bunch of CPU cores, a lot of RAM, and a decent amount of SSDs. In regards to network they are using 10GB ethernet. In terms of power usage they are kind of friendly to my electricity bill with my \u0026ldquo;required\u0026rdquo; vms running on them, but can potentially ruin that if I throw a lot of stuff at them to chew on.\nThe total power usage above includes my switch, UPS and some other small devices. So its not that bad considering how much I can get out of it.\nBut with automation I can easily spin up some resources to be consumed for a certain period and delete again if not needed. My required VMs, that are always on, are things like Bind dns servers, PfSense, Truenas, Frigate, DCS server, a couple of linux \u0026ldquo;mgmt\u0026rdquo; vms, Home Assistant, a couple of Kubernetes clusters hosting my Unifi controller, Traefik Proxy, Grafana etc.\nFor the virtualization layer on my two servers I am using Proxmox, one of the reason is that it supports PCI passthrough of my Coral TPU. I have been running Proxmox for many years and I find it to be a very decent alternative. It does what I want it to do and Proxmox has a great community!\nProxmox has been configured with these two servers as a cluster. I have not configured any VMs in HA, but with a Proxmox cluster I can easily migrate VMs between the hosts, even without shared storage, and single web-ui to manage both servers. To be \u0026ldquo;quorate\u0026rdquo; I have a cute little RPi3 with its beefy 32 GB SD card, 2GB RAM and 1GB ethernet as a Qdevice\nOn that note, one does not necessarily need have them in a cluster to do vm migration. I recently moved a bunch of VMs over two these two new servers and it was easy peasy using this command:\n# executed on the source Proxmox node qm remote-migrate \u0026lt;src vm_id\u0026gt; \u0026lt;dst vm_id\u0026gt; \u0026#39;apitoken=PVEAPIToken=root@pam!migrate=\u0026lt;token\u0026gt;,host=\u0026lt;dst-host-ip\u0026gt;,fingerprint=\u0026lt;thumprint\u0026gt;\u0026#39; --target-bridge vmbr0 --target-storage \u0026#39;raid10-node01\u0026#39; --online I found this with the great help of the Proxmox community here\nBoth Proxmox servers have their own local ZFS storage. In both of them I have created a dedicated zfs pool with identical name which I use for zfs replication for some of the more \u0026ldquo;critical\u0026rdquo; VMs, and as a bonus it also reduces the migration time drastically for these VMs when I move them between my servers. The \u0026ldquo;cluster\u0026rdquo; network (vmbr1) is directly connected 2x 10GB ethernet (not through my physical switch). The other 2x 10GB interfaces are connected to my switch for all my other network needs like VM network (vmbr0) and Proxmox management.\nThroughout this post I will be using a dedicated linux vm for all my commands, interactions. So every time I install something, it is on this linux vm.\nBut I am digressing, enough of the intro, get on with the automation part you were supposed to write about. Got it.\nIn this post I will use the following products:\nProxmox - homepage Ubuntu - homepage OpenTofu - homepage bpg/proxmox terraform provider - registry and github Ansible - homepage Kubespray - homepage and github Provision VMs in Proxmox using OpenTofu # Terraform is something I have been involved in many times in my professional work, but never had the chance to actually use it myself other than know about it, and how the solutions I work with can work together with Terraform. I did notice even the Proxmox community had several post around using Terraform, so I decided to just go with Terraform. I already knew that HashiCorp had announced their change of Terraform license from MPL to BSL and that OpenTofu is an OpenSource fork of Terraform. So instead of basing my automations using Terraform, I will be using OpenTofu. Read more about OpenTofu here. For now OpenTofu is Terraform \u0026ldquo;compatible\u0026rdquo; and uses Terraform registries, so providers etc for Terraform works with OpenTofu. You will notice that further down, all my configs will be using terraform constructs.\nIn OpenTofu/Terraform there are several concepts that one needs to know about, I will use some of them in this post like providers, resources, provisioners and variables. It is smart to read about them a bit more here how and why to use them.\nTo get started with OpenTofu there are some preparations do be done. Lets start with these\nInstall OpenTofu # To get started with OpenTofu I deployed it on my linux machine using Snap, but several other alternatives is available. See more on the official OpenTofu docs page.\nsudo snap install --classic opentofu Now OpenTofu is installed and I can start using it. I also installed the bash autocompletion like this:\ntofu -install-autocomplete # restart shell session... I decided to create a dedicated folder for my \u0026ldquo;projects\u0026rdquo; to live in so I have created a folder in my home folder called *proxmox\u0026quot; where I have different subfolders depending on certain tasks or resources I will use OpenTofu for.\nproxmox/ ├── k8s-cluster-02 ├── proxmox-images OpenTofu Proxmox provider # To be able to use OpenTofu with Proxmox I need a provider that can use the Proxmox API. I did some quick research on the different options out there and landed on this provider: bpg/proxmox. Seems very active and are recently updated (according to the git repo here)\nAn OpenTofu/Terraform provider is being defined like this, and the example below is configured to install the bpg/proxmox provider I need to interact with Proxmox.\nterraform { required_providers { proxmox = { source = \u0026#34;bpg/proxmox\u0026#34; version = \u0026#34;0.43.2\u0026#34; } } } provider \u0026#34;proxmox\u0026#34; { endpoint = var.proxmox_api_endpoint api_token = var.proxmox_api_token insecure = true ssh { agent = true username = \u0026#34;root\u0026#34; } } I will save this content in a filed called providers.tf\nFirst a short explanation of the two sections above. The terraform sections instructs OpenTofu/Terraform which provider to be downloaded and enabled. The version field defines a specific version to use, not the latest but this version. Using this field will make sure that your automation will not break if there is an update in the provider version that introduces some API changes.\nThe provider section configures the proxmox provider how it should interact with Proxmox. Instead of using a regular username and password I have opted for using API token. Here I in the endpoint and api-token keys am using a variable value that are defined in another file called variables.tf and a credentials.auto.tfvars. There are also certain tasks in my automation that requires SSH interaction with Proxmox so I have also enabled that by configuring the ssh field.\nFor more information on the bpg/proxmox provider, head over here\nPrepare Proxmox with an API token for the OpenTofu bpg/proxmox provider # To be able to use the above configured provider with Proxmox I need to prepare Proxmox to use API token. I followed the bpg/proxmox provider documentation here\nOn my \u0026ldquo;leader\u0026rdquo; Proxmox node:\n# Create the user sudo pveum user add terraform@pve # Create a role for the user above sudo pveum role add Terraform -privs \u0026#34;Datastore.Allocate Datastore.AllocateSpace Datastore.AllocateTemplate Datastore.Audit Pool.Allocate Sys.Audit Sys.Console Sys.Modify SDN.Use VM.Allocate VM.Audit VM.Clone VM.Config.CDROM VM.Config.Cloudinit VM.Config.CPU VM.Config.Disk VM.Config.HWType VM.Config.Memory VM.Config.Network VM.Config.Options VM.Migrate VM.Monitor VM.PowerMgmt User.Modify\u0026#34; # Assign the terraform user to the above role sudo pveum aclmod / -user terraform@pve -role Terraform # Create the token sudo pveum user token add terraform@pve provider --privsep=0 ┌──────────────┬──────────────────────────────────────┐ │ key │ value │ ╞══════════════╪══════════════════════════════════════╡ │ full-tokenid │ terraform@pve!provider │ ├──────────────┼──────────────────────────────────────┤ │ info │ {\u0026#34;privsep\u0026#34;:\u0026#34;0\u0026#34;} │ ├──────────────┼──────────────────────────────────────┤ │ value │ \u0026lt;token\u0026gt; │ └──────────────┴──────────────────────────────────────┘ # make a backup of the token Now I have create the API user to be used with my bpg/proxmox provider. Though it is not sufficient as I also need to define a ssh keypair on my Linux jumphost for passwordless SSH authentication which I will need to copy to my Proxmox nodes. This is done like this:\nandreasm@linuxmgmt01:~$ ssh-copy-id -i id_rsa.pub root@172.18.5.102 # -i pointing to the pub key I want to use and specify the root user Now, I can log into my Proxmox node using SSH without password from my Linux jumphost. But, when used in combination with opentofu its not sufficient. I need to load the key into the keystore. If I dont do that the automations that require SSH access will fail with this error message:\nError: failed to open SSH client: unable to authenticate user \u0026#34;root\u0026#34; over SSH to \u0026#34;172.18.5.102:22\u0026#34;. Please verify that ssh-agent is correctly loaded with an authorized key via \u0026#39;ssh-add -L\u0026#39; (NOTE: configurations in ~/.ssh/config are not considered by golang\u0026#39;s ssh implementation). The exact error from ssh.Dial: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none password], no supported methods remain Ah, I can just configure the .ssh/config with this:\nHost 172.18.5.102 AddKeysToAgent yes IdentityFile ~/.ssh/id_rsa Nope, cant do.. Look at the error message again:\n(NOTE: configurations in ~/.ssh/config are not considered by golang\u0026#39;s ssh implementation) Ah, I can just do this then:\nandreasm@linuxmgmt01:~$ eval `ssh-agent -s` andreasm@linuxmgmt01:~$ ssh-add id_rsa Yes, but that is not persistent between sessions, so you need to do this every time you log on to your Linux jumphost again. I couldnt figure out how to do this the \u0026ldquo;correct\u0026rdquo; way by following the expected Golang approach so instead I went with this approach.\n# I added this in my .bashrc file if [ -z \u0026#34;$SSH_AUTH_SOCK\u0026#34; ] ; then eval `ssh-agent -s` ssh-add ~/.ssh/id_rsa fi OpenTofu variables and credentials # Instead of exposing username/tokens and other information directly in my provider/resource *.tf\u0026rsquo;s I can refer to them by declaring them in a variables.tf and credentials in a credentials.auto.tfvars. The variables.tf defines the variables to use, and the credentials.auto.tfvars maps the value to a variable. I am using this content in my variable.tf\nvariable \u0026#34;proxmox_api_endpoint\u0026#34; { type = string description = \u0026#34;Proxmox cluster API endpoint https://proxmox-01.my-domain.net:8006\u0026#34; } variable \u0026#34;proxmox_api_token\u0026#34; { type = string description = \u0026#34;Proxmox API token bpg proxmox provider with ID and token\u0026#34; } Then in my credentials.auto.tfvars file I have this content:\nproxmox_api_endpoint = \u0026#34;https://proxmox-01.mu-domain.net:8006\u0026#34; proxmox_api_token = \u0026#34;terraform@pve!provider=\u0026lt;token-from-earlier\u0026gt;\u0026#34; To read more about these files head over here. One can also use the varible.tf to generate prompts if I have created variables being referred to in any of my resources but do not map to a value (credentials.tfvars etc..). Then it will ask you to enter a value when doing a plan or apply.\nPrepare a Ubuntu cloud image - opentofu resource # In my automations with OpenTofu I will use a cloud image instead of a VM template in Proxmox.\nI have been using Ubuntu as my preferred Linux distribution for many years and dont see any reason to change that now. Ubuntu/Canonical do provide cloud images and they can be found here\nBut I will not download them using my browser, instead I will be using OpenTofu with the bpg/proxmox provider to download and upload them to my Proxmox nodes.\nIn my proxmox folder I have a dedicated folder for this project called proxmox-images\nproxmox/ ├── proxmox-images Inside that folder I will start by creating the following files: provider.tf file, variable.tf and credentials.auto.tfvars with the content described above.\nproxmox-images/ ├── credentials.auto.tfvars ├── provider.tf └── variables.tf 0 directories, 3 files But these files dont to anything other than provides the relevant information to connect and interact with Proxmox. What I need is a OpenTofu resource with a task that needs to be done. So I will prepare a file called ubuntu_cloud_image.tf with the following content:\nresource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;ubuntu_cloud_image\u0026#34; { content_type = \u0026#34;iso\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;proxmox-02\u0026#34; source_file { # you may download this image locally on your workstation and then use the local path instead of the remote URL path = \u0026#34;https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img\u0026#34; # you may also use the SHA256 checksum of the image to verify its integrity checksum = \u0026#34;1d82c1db56e7e55e75344f1ff875b51706efe171ff55299542c29abba3a20823\u0026#34; } } What this file does is instructing OpenTofu where to grab my Ubuntu cloud image from, then upload it to my Proxmox node 2 and a specific datastore on that node. So I have defined a resource to define this task. More info on this here.\nWithin this same .tf file I can define several resources to download several cloud images. I could have called the file cloud-images.tf instead and just defined all the images I wanted to download/upload to Proxmox. I have decided to keep this task as a separate project/folder.\nWhen I save this file I will now have 4 files in my proxmox-image folder:\nproxmox-images/ ├── credentials.auto.tfvars ├── provider.tf ├── ubuntu_cloud_image.tf └── variables.tf 0 directories, 4 files Thats all the files I need to start my first OpenTofu task/automation.\nLets do a quick recap. The provider.tf instructs OpenTofu to download and enable the bpg/proxmox provider in this folder, and configure the provider to interact with my Proxmox nodes.\nThen it is the variables.tf and the credentials.auto.tfvars that provides keys and values to be used in the provider.tf. The ubuntu_cloud_image.tf contains the task I want OpenTofu to perform.\nSo now I am ready to execute the first OpenTofu command tofu init. This command initiates OpenTofu in the respective folder proxmox-images, downloads and enables the provider I have configured. More info here.\nSo lets try it:\nandreasm@linuxmgmt01:~/terraform/proxmox/proxmox-images$ tofu init Initializing the backend... Initializing provider plugins... - Finding bpg/proxmox versions matching \u0026#34;0.43.2\u0026#34;... - Installing bpg/proxmox v0.43.2... - Installed bpg/proxmox v0.43.2 (signed, key ID DAA1958557A27403) Providers are signed by their developers. If you\u0026#39;d like to know more about provider signing, you can read about it here: https://opentofu.org/docs/cli/plugins/signing/ OpenTofu has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that OpenTofu can guarantee to make the same selections by default when you run \u0026#34;tofu init\u0026#34; in the future. OpenTofu has been successfully initialized! You may now begin working with OpenTofu. Try running \u0026#34;tofu plan\u0026#34; to see any changes that are required for your infrastructure. All OpenTofu commands should now work. If you ever set or change modules or backend configuration for OpenTofu, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Cool, now I have initialized my proxmox-image folder. Lets continue with creating a plan.\nandreasm@linuxmgmt01:~/terraform/proxmox/proxmox-images$ tofu plan -out plan OpenTofu used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create OpenTofu will perform the following actions: # proxmox_virtual_environment_file.ubuntu_cloud_image will be created + resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;ubuntu_cloud_image\u0026#34; { + content_type = \u0026#34;iso\u0026#34; + datastore_id = \u0026#34;local\u0026#34; + file_modification_date = (known after apply) + file_name = (known after apply) + file_size = (known after apply) + file_tag = (known after apply) + id = (known after apply) + node_name = \u0026#34;proxmox-02\u0026#34; + overwrite = true + timeout_upload = 1800 + source_file { + changed = false + checksum = \u0026#34;1d82c1db56e7e55e75344f1ff875b51706efe171ff55299542c29abba3a20823\u0026#34; + insecure = false + path = \u0026#34;https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img\u0026#34; } } Plan: 1 to add, 0 to change, 0 to destroy. ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Saved the plan to: plan To perform exactly these actions, run the following command to apply: tofu apply \u0026#34;plan\u0026#34; I am using the -out plan to just keep a record of the plan. It could be called whatever I wanted. Now OpenTofu tells me what its intention are, and if I am happy with the plan, I just need to apply it.\nSo lets apply it:\nandreasm@linuxmgmt01:~/terraform/proxmox/proxmox-images$ tofu apply plan proxmox_virtual_environment_file.ubuntu_cloud_image: Creating... proxmox_virtual_environment_file.ubuntu_cloud_image: Still creating... [10s elapsed] proxmox_virtual_environment_file.ubuntu_cloud_image: Creation complete after 20s [id=local:iso/jammy-server-cloudimg-amd64.img] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. And after 20 seconds I have my Ubuntu cloud image uploaded to my Proxmox node (jammy-server-cloudimg-amd64.img):\nThats great. Now I have my iso image to use for my VM deployments.\nNext up is to deploy a bunch of VMs to be used for a Kubernetes cluster\nDeploy VMs using OpenTofu and install Kubernetes using Ansible and Kubespray # In this section I will automate everything from deploying VMs to install Kubernetes on these VMs. This task will involve OpenTofu, Ansible and Kubespray. It will deploy 6 virtual machines, with two different resource configurations. It will deploy 3 vms intended to be used as Kubernetes controlplane nodes, and 3 vms intended to be Kubernetes worker nodes. Thats the task for OpenTofu, as soon as OpenTofu has done its part it will trigger an Ansible playbook to initiate Kubespray to install Kubernetes on all my nodes. This should then end up in a fully automated task from me just executing tofu apply plan to a ready Kubernetes cluster.\nI will start by creating another subfolder in my proxmox folder called k8s-cluster-02. In this folder I will reuse the following files:\nk8s-cluster-02/ ├── credentials.auto.tfvars ├── provider.tf └── variables.tf 0 directories, 3 files These files I will just copy from my proxmox-images folder. I will also need define and add a couple of other files. When I am ready with this task I will end up with these files:\nk8s-cluster-02/ ├── ansible.tf ├── credentials.auto.tfvars ├── k8s-cluster-02.tf ├── provider.tf ├── ubuntu_cloud_config.tf └── variables.tf 0 directories, 7 files I will go through all the files one by one. But there is still some preparations to be done to be able to complete this whole task.\nPrepare OpenTofu to deploy my VMs # In addition to the three common files I have copied from the previous project, I will need to create two proxmox_virtual_environment_vm resource definitions (one for each VM type). The content of this file will be saved as k8s-cluster-02.tf and looks like this:\nresource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;k8s-cp-vms-cl02\u0026#34; { count = 3 name = \u0026#34;k8s-cp-vm-${count.index + 1}-cl-02\u0026#34; description = \u0026#34;Managed by Terraform\u0026#34; tags = [\u0026#34;terraform\u0026#34;, \u0026#34;ubuntu\u0026#34;, \u0026#34;k8s-cp\u0026#34;] node_name = \u0026#34;proxmox-02\u0026#34; vm_id = \u0026#34;100${count.index + 1}\u0026#34; cpu { cores = 2 type = \u0026#34;host\u0026#34; } memory { dedicated = 2048 } agent { # read \u0026#39;Qemu guest agent\u0026#39; section, change to true only when ready enabled = true } startup { order = \u0026#34;3\u0026#34; up_delay = \u0026#34;60\u0026#34; down_delay = \u0026#34;60\u0026#34; } disk { datastore_id = \u0026#34;raid-10-node02\u0026#34; file_id = \u0026#34;local:iso/jammy-server-cloudimg-amd64.img\u0026#34; interface = \u0026#34;virtio0\u0026#34; iothread = true discard = \u0026#34;on\u0026#34; size = 40 file_format = \u0026#34;raw\u0026#34; } initialization { dns { servers = [\u0026#34;10.100.1.7\u0026#34;, \u0026#34;10.100.1.6\u0026#34;] domain = \u0026#34;my-domain.net\u0026#34; } ip_config { ipv4 { address = \u0026#34;10.160.1.2${count.index + 1}/24\u0026#34; gateway = \u0026#34;10.160.1.1\u0026#34; } } datastore_id = \u0026#34;raid-10-node02\u0026#34; user_data_file_id = proxmox_virtual_environment_file.ubuntu_cloud_init.id } network_device { bridge = \u0026#34;vmbr0\u0026#34; vlan_id = \u0026#34;216\u0026#34; } operating_system { type = \u0026#34;l26\u0026#34; } keyboard_layout = \u0026#34;no\u0026#34; lifecycle { ignore_changes = [ network_device, ] } } resource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;k8s-worker-vms-cl02\u0026#34; { count = 3 name = \u0026#34;k8s-node-vm-${count.index + 1}-cl-02\u0026#34; description = \u0026#34;Managed by Terraform\u0026#34; tags = [\u0026#34;terraform\u0026#34;, \u0026#34;ubuntu\u0026#34;, \u0026#34;k8s-node\u0026#34;] node_name = \u0026#34;proxmox-02\u0026#34; vm_id = \u0026#34;100${count.index + 5}\u0026#34; cpu { cores = 4 type = \u0026#34;host\u0026#34; } memory { dedicated = 4096 } agent { # read \u0026#39;Qemu guest agent\u0026#39; section, change to true only when ready enabled = true } startup { order = \u0026#34;3\u0026#34; up_delay = \u0026#34;60\u0026#34; down_delay = \u0026#34;60\u0026#34; } disk { datastore_id = \u0026#34;raid-10-node02\u0026#34; file_id = \u0026#34;local:iso/jammy-server-cloudimg-amd64.img\u0026#34; interface = \u0026#34;virtio0\u0026#34; iothread = true discard = \u0026#34;on\u0026#34; size = 60 file_format = \u0026#34;raw\u0026#34; } initialization { dns { servers = [\u0026#34;10.100.1.7\u0026#34;, \u0026#34;10.100.1.6\u0026#34;] domain = \u0026#34;my-domain.net\u0026#34; } ip_config { ipv4 { address = \u0026#34;10.160.1.2${count.index + 5}/24\u0026#34; gateway = \u0026#34;10.160.1.1\u0026#34; } } datastore_id = \u0026#34;raid-10-node02\u0026#34; user_data_file_id = proxmox_virtual_environment_file.ubuntu_cloud_init.id } network_device { bridge = \u0026#34;vmbr0\u0026#34; vlan_id = \u0026#34;216\u0026#34; } operating_system { type = \u0026#34;l26\u0026#34; } keyboard_layout = \u0026#34;no\u0026#34; lifecycle { ignore_changes = [ network_device, ] } } In this file I have define two proxmox_virtual_environment_vm resources called k8s-cp-vms-cl02 and k8s-worker-vms-cl02 respectively. I am using count.index in some of the fields where I need to automatically generate an increasing number. Like IP address, VM_ID, Name. It is also referring to my Ubuntu cloud image as installation source, then a user_data_file_id (will be shown next) to do a simple cloud-init on the VMs.\nThen I need to configure a proxmox_virtual_environment_file resource called ubuntu_cloud_init to trigger a cloud-init task to configure some basic initial config on my VMs.\nThis is the content of this file:\nresource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;ubuntu_cloud_init\u0026#34; { content_type = \u0026#34;snippets\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;proxmox-02\u0026#34; source_raw { data = \u0026lt;\u0026lt;EOF #cloud-config chpasswd: list: | ubuntu:ubuntu expire: false packages: - qemu-guest-agent timezone: Europe/Oslo users: - default - name: ubuntu groups: sudo shell: /bin/bash ssh-authorized-keys: - ${trimspace(\u0026#34;ssh-rsa \u0026lt;sha356\u0026gt; user@mail.com\u0026#34;)} sudo: ALL=(ALL) NOPASSWD:ALL power_state: delay: now mode: reboot message: Rebooting after cloud-init completion condition: true EOF file_name = \u0026#34;ubuntu.cloud-config.yaml\u0026#34; } } This will be used by all my OpenTofu deployed VMs in this task. It will install the qemu-guest-agent, configure a timezone, copy my Linux jumphost public key so I can log in to them using SSH without password. Then a quick reboot after all task completed to get the qemu-agent to report correctly to Proxmox. This will be uploaded to my Proxmox server as a Snippet. For Snippets to work. One need to enable this here under Datacenter -\u0026gt; Storage\nNow I can go ahead and perform tofu init in this folder, but I am still not ready. I Kubernetes to be deployed also remember. For that I will use Kubespray.\nInstall and configure Kubespray # If you have not heard about Kubespray before, head over here for more info. I have been following the guides from Kubespray to get it working, and its very well documented.\nKubespray is a really powerful way to deploy Kubernetes with a lot of options and customizations. I just want to highlight Kubespray in this section as it does a really great job in automating Kubernetes deployment.\nA quote from the Kubespray pages:\nComparison # Kubespray vs Kops # Kubespray runs on bare metal and most clouds, using Ansible as its substrate for provisioning and orchestration. Kops performs the provisioning and orchestration itself, and as such is less flexible in deployment platforms. For people with familiarity with Ansible, existing Ansible deployments or the desire to run a Kubernetes cluster across multiple platforms, Kubespray is a good choice. Kops, however, is more tightly integrated with the unique features of the clouds it supports so it could be a better choice if you know that you will only be using one platform for the foreseeable future.\nKubespray vs Kubeadm # Kubeadm provides domain Knowledge of Kubernetes clusters\u0026rsquo; life cycle management, including self-hosted layouts, dynamic discovery services and so on. Had it belonged to the new operators world, it may have been named a \u0026ldquo;Kubernetes cluster operator\u0026rdquo;. Kubespray however, does generic configuration management tasks from the \u0026ldquo;OS operators\u0026rdquo; ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping.\nKubespray has started using kubeadm internally for cluster creation since v2.3 in order to consume life cycle management domain knowledge from it and offload generic OS configuration things from it, which hopefully benefits both sides.\nOn my Linux jumphost I have done the following to prepare for Kubespray\n# clone the Kubespray repo andreasm@linuxmgmt01:~/terraform/proxmox$ git clone https://github.com/kubernetes-sigs/kubespray.git # kubespray folder created under my proxmox folder andreasm@linuxmgmt01:~/terraform/proxmox$ ls k8s-cluster-02 kubespray proxmox-images # Python dependencies - these I am not sure are needed but I installed them anyways. sudo apt install software-properties-common sudo add-apt-repository ppa:deadsnakes/ppa # used this repo to get a newer Python 3 than default repo sudo apt update sudo apt install python3.12 python3-pip python3-virtualenv Don\u0026rsquo;t try to install Ansible in the systemwide, it will not work. Follow the Kubespray documentation to install ansible in a Python Virtual Environment.\nandreasm@linuxmgmt01:~/terraform/proxmox$ VENVDIR=kubespray-venv andreasm@linuxmgmt01:~/terraform/proxmox$ KUBESPRAYDIR=kubespray andreasm@linuxmgmt01:~/terraform/proxmox$ python3 -m venv $VENVDIR andreasm@linuxmgmt01:~/terraform/proxmox$ source $VENVDIR/bin/activate (kubespray-venv) andreasm@linuxmgmt01:~/terraform/proxmox$ cd $KUBESPRAYDIR (kubespray-venv) andreasm@linuxmgmt01:~/terraform/proxmox/kubespray$ pip install -U -r requirements.txt To exit out of the python environment type deactivate.\nNow I have these subfolders under my proxmox folder:\nandreasm@linuxmgmt01:~/terraform/proxmox$ tree -L 1 . ├── k8s-cluster-02 ├── kubespray ├── kubespray-venv ├── proxmox-images 4 directories, 0 files Next thing I need to do is to copy a sample folder inside the kubespray folder to a folder that reflects the kubernetes cluster name I am planning to deploy.\nandreasm@linuxmgmt01:~/terraform/proxmox/kubespray$ cp -rfp inventory/sample inventory/k8s-cluster-02 This sample folder contains a couple of files and directories. The file I am interested in right now is the inventory.ini file. I need to populate this file with the nodes, including the control plane nodes, that will form my Kubernetes cluster. This is how it looks like now, default:\n# ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. [all] # node1 ansible_host=95.54.0.12 # ip=10.3.0.1 etcd_member_name=etcd1 # node2 ansible_host=95.54.0.13 # ip=10.3.0.2 etcd_member_name=etcd2 # node3 ansible_host=95.54.0.14 # ip=10.3.0.3 etcd_member_name=etcd3 # node4 ansible_host=95.54.0.15 # ip=10.3.0.4 etcd_member_name=etcd4 # node5 ansible_host=95.54.0.16 # ip=10.3.0.5 etcd_member_name=etcd5 # node6 ansible_host=95.54.0.17 # ip=10.3.0.6 etcd_member_name=etcd6 # ## configure a bastion host if your nodes are not directly reachable # [bastion] # bastion ansible_host=x.x.x.x ansible_user=some_user [kube_control_plane] # node1 # node2 # node3 [etcd] # node1 # node2 # node3 [kube_node] # node2 # node3 # node4 # node5 # node6 [calico_rr] [k8s_cluster:children] kube_control_plane kube_node calico_rr This is just a sample, but nice to know how it should be defined. I will create a OpenTofu task to create this inventory file for me with the corresponding VMs I am deploying in the same task/project. This will autopopulate this inventory.ini file with all the necessary information. So I can just go ahead and delete the inventory.ini file that has been copied from the sample folder to my new k8s-cluster-02 folder. The other folders and files contains several variables/settings I can adjust to my liking. Like different CNIs, Kubernetes version etc. But I will not cover these here, head over to the official Kubespray docs for more info on that. These are the files/folder:\nandreasm@linuxmgmt01:~/terraform/proxmox/kubespray/inventory/k8s-cluster-02/group_vars$ tree -L 1 . ├── all ├── etcd.yml └── k8s_cluster 2 directories, 1 file I will deploy my Kubernetes cluster \u0026ldquo;stock\u0026rdquo; so these files are left untouched for now, except the inventory.ini file.\nNow Kubespray is ready to execute Ansible to configure my Kubernetes cluster as soon as my OpenTofu provisioned VMs has been deployed. The last step I need to do is to confgiure a new .tf file to create this inventory.ini file and kick of the Ansible command to fire up Kubespray.\nConfigure OpenTofu to kick off Kubespray # The last .tf file in my fully automated Kubernetes installation is the ansible.tf file that contains this information:\n# Generate inventory file resource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { filename = \u0026#34;/home/andreasm/terraform/proxmox/kubespray/inventory/k8s-cluster-02/inventory.ini\u0026#34; content = \u0026lt;\u0026lt;-EOF [all] ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0].name} ansible_host=${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0].ipv4_addresses[1][0]} ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1].name} ansible_host=${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1].ipv4_addresses[1][0]} ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2].name} ansible_host=${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2].ipv4_addresses[1][0]} ${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0].name} ansible_host=${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0].ipv4_addresses[1][0]} ${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1].name} ansible_host=${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1].ipv4_addresses[1][0]} ${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2].name} ansible_host=${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2].ipv4_addresses[1][0]} [kube_control_plane] ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0].name} ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1].name} ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2].name} [etcd] ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0].name} ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1].name} ${proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2].name} [kube_node] ${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0].name} ${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1].name} ${proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2].name} [k8s_cluster:children] kube_node kube_control_plane EOF } resource \u0026#34;null_resource\u0026#34; \u0026#34;ansible_command\u0026#34; { provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;./kubespray.k8s-cluster-02.sh \u0026gt; k8s-cluster-02/ansible_output.log 2\u0026gt;\u0026amp;1\u0026#34; interpreter = [\u0026#34;/bin/bash\u0026#34;, \u0026#34;-c\u0026#34;] working_dir = \u0026#34;/home/andreasm/terraform/proxmox\u0026#34; } depends_on = [proxmox_virtual_environment_vm.k8s-cp-vms-cl02, proxmox_virtual_environment_vm.k8s-worker-vms-cl02, local_file.ansible_inventory] } This will create the inventory.ini in the /proxmox/kubespray/inventory/k8s-cluster-02/ for Kubespray to use. Then it will fire a command refering to a bash script (more on that further down) to trigger the Ansible command:\nansible-playbook -i inventory/k8s-cluster-02/inventory.ini --become --become-user=root cluster.yml -u ubuntu For this to work I had to create a bash script that activated the Kubespray virtual environment:\n#!/bin/bash # Set working directory WORKING_DIR=\u0026#34;/home/andreasm/terraform/proxmox\u0026#34; cd \u0026#34;$WORKING_DIR\u0026#34; || exit # Set virtual environment variables VENVDIR=\u0026#34;kubespray-venv\u0026#34; KUBESPRAYDIR=\u0026#34;kubespray\u0026#34; # Activate virtual environment source \u0026#34;$VENVDIR/bin/activate\u0026#34; || exit # Change to Kubespray directory cd \u0026#34;$KUBESPRAYDIR\u0026#34; || exit # Run Ansible playbook ansible-playbook -i inventory/k8s-cluster-02/inventory.ini --become --become-user=root cluster.yml -u ubuntu I will create and save this file in my proxmox folder.\nThis is now the content of my proxmox folder:\nandreasm@linuxmgmt01:~/terraform/proxmox$ tree -L 1 . ├── k8s-cluster-02 ├── kubespray ├── kubespray-venv ├── kubespray.k8s-cluster-02.sh └── proxmox-images 4 directories, 1 file And this is the content in the k8s-cluster-02 folder where I have my OpenTofu tasks defined:\nandreasm@linuxmgmt01:~/terraform/proxmox/k8s-cluster-02$ tree -L 1 . ├── ansible.tf ├── credentials.auto.tfvars ├── k8s-cluster-02.tf ├── provider.tf ├── ubuntu_cloud_config.tf └── variables.tf 0 directories, 6 files Its time to put it all to a test\nA fully automated provisioning of Kubernetes on Proxmox using OpenTofu, Ansible and Kubespray # To get this show started, it is the same procedure as it was with my cloud image task. Need to run tofu init, then create a plan, check the output and finally approve it. So lets see how this goes.\nFrom within my ./proxmox/k8s-cluster-02 folder:\nandreasm@linuxmgmt01:~/terraform/proxmox/k8s-cluster-02$ tofu init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/null... - Finding bpg/proxmox versions matching \u0026#34;0.43.2\u0026#34;... - Finding latest version of hashicorp/local... - Installing hashicorp/null v3.2.2... - Installed hashicorp/null v3.2.2 (signed, key ID 0C0AF313E5FD9F80) - Installing bpg/proxmox v0.43.2... - Installed bpg/proxmox v0.43.2 (signed, key ID DAA1958557A27403) - Installing hashicorp/local v2.4.1... - Installed hashicorp/local v2.4.1 (signed, key ID 0C0AF313E5FD9F80) Providers are signed by their developers. If you\u0026#39;d like to know more about provider signing, you can read about it here: https://opentofu.org/docs/cli/plugins/signing/ OpenTofu has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that OpenTofu can guarantee to make the same selections by default when you run \u0026#34;tofu init\u0026#34; in the future. OpenTofu has been successfully initialized! You may now begin working with OpenTofu. Try running \u0026#34;tofu plan\u0026#34; to see any changes that are required for your infrastructure. All OpenTofu commands should now work. If you ever set or change modules or backend configuration for OpenTofu, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Then create the plan:\nOpenTofu used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create OpenTofu will perform the following actions: # local_file.ansible_inventory will be created + resource \u0026#34;local_file\u0026#34; \u0026#34;ansible_inventory\u0026#34; { + content = (known after apply) + content_base64sha256 = (known after apply) + content_base64sha512 = (known after apply) + content_md5 = (known after apply) + content_sha1 = (known after apply) + content_sha256 = (known after apply) + content_sha512 = (known after apply) + directory_permission = \u0026#34;0777\u0026#34; + file_permission = \u0026#34;0777\u0026#34; + filename = \u0026#34;/home/andreasm/terraform/proxmox/kubespray/inventory/k8s-cluster-02/inventory.ini\u0026#34; + id = (known after apply) } # null_resource.ansible_command will be created + resource \u0026#34;null_resource\u0026#34; \u0026#34;ansible_command\u0026#34; { + id = (known after apply) } # proxmox_virtual_environment_file.ubuntu_cloud_init will be created + resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;ubuntu_cloud_init\u0026#34; { + content_type = \u0026#34;snippets\u0026#34; + datastore_id = \u0026#34;local\u0026#34; + file_modification_date = (known after apply) + file_name = (known after apply) + file_size = (known after apply) + file_tag = (known after apply) + id = (known after apply) + node_name = \u0026#34;proxmox-02\u0026#34; + overwrite = true + timeout_upload = 1800 \u0026lt;\u0026lt;redacted\u0026gt;\u0026gt; Plan: 9 to add, 0 to change, 0 to destroy. ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Saved the plan to: plan To perform exactly these actions, run the following command to apply: tofu apply \u0026#34;plan\u0026#34; It looks good to me, lets apply it:\nandreasm@linuxmgmt01:~/terraform/proxmox/k8s-cluster-02$ tofu apply plan proxmox_virtual_environment_file.ubuntu_cloud_init: Creating... proxmox_virtual_environment_file.ubuntu_cloud_init: Creation complete after 1s [id=local:snippets/ubuntu.cloud-config.yaml] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2]: Creating... proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1]: Creating... proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0]: Creating... proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Creating... proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Creating... proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Creating... proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2]: Still creating... [10s elapsed] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1]: Still creating... [10s elapsed] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0]: Still creating... [10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still creating... [10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Still creating... [10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still creating... [10s elapsed] \u0026lt;\u0026lt;redacted\u0026gt;\u0026gt; proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2]: Still creating... [1m30s elapsed] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1]: Still creating... [1m30s elapsed] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0]: Still creating... [1m30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still creating... [1m30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Still creating... [1m30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still creating... [1m30s elapsed] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0]: Creation complete after 1m32s [id=1005] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Creation complete after 1m32s [id=1002] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1]: Creation complete after 1m33s [id=1006] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2]: Creation complete after 1m33s [id=1007] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Creation complete after 1m34s [id=1003] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Creation complete after 1m37s [id=1001] 1m37s to create my 6 VMs, ready for the Kubernetes installation.\nAnd in Proxmox I have 6 new VMs with correct name, vm_id, tags and all:\nLet me check if the inventory.ini file has been created correct:\nandreasm@linuxmgmt01:~/terraform/proxmox/kubespray/inventory/k8s-cluster-02$ cat inventory.ini [all] k8s-cp-vm-1-cl-02 ansible_host=10.160.1.21 k8s-cp-vm-2-cl-02 ansible_host=10.160.1.22 k8s-cp-vm-3-cl-02 ansible_host=10.160.1.23 k8s-node-vm-1-cl-02 ansible_host=10.160.1.25 k8s-node-vm-2-cl-02 ansible_host=10.160.1.26 k8s-node-vm-3-cl-02 ansible_host=10.160.1.27 [kube_control_plane] k8s-cp-vm-1-cl-02 k8s-cp-vm-2-cl-02 k8s-cp-vm-3-cl-02 [etcd] k8s-cp-vm-1-cl-02 k8s-cp-vm-2-cl-02 k8s-cp-vm-3-cl-02 [kube_node] k8s-node-vm-1-cl-02 k8s-node-vm-2-cl-02 k8s-node-vm-3-cl-02 [k8s_cluster:children] kube_node kube_control_plane Now the last task the ansible_command:\nlocal_file.ansible_inventory: Creating... local_file.ansible_inventory: Creation complete after 0s [id=1d19a8be76746178f26336defc9ce96c6e82a791] null_resource.ansible_command: Creating... null_resource.ansible_command: Provisioning with \u0026#39;local-exec\u0026#39;... null_resource.ansible_command (local-exec): Executing: [\u0026#34;/bin/bash\u0026#34; \u0026#34;-c\u0026#34; \u0026#34;./kubespray.k8s-cluster-02.sh \u0026gt; k8s-cluster-02/ansible_output.log 2\u0026gt;\u0026amp;1\u0026#34;] null_resource.ansible_command: Still creating... [10s elapsed] null_resource.ansible_command: Still creating... [20s elapsed] null_resource.ansible_command: Still creating... [30s elapsed] null_resource.ansible_command: Still creating... [40s elapsed] null_resource.ansible_command: Still creating... [50s elapsed] \u0026lt;\u0026lt;redacted\u0026gt;\u0026gt; null_resource.ansible_command: Still creating... [16m50s elapsed] null_resource.ansible_command: Still creating... [17m0s elapsed] null_resource.ansible_command: Creation complete after 17m4s [id=5215143035945962122] Apply complete! Resources: 9 added, 0 changed, 0 destroyed. That took 17m4s to deploy a fully working Kubernetes cluster with no intervention from me at all.\nFrom the ansible_output.log:\nPLAY RECAP ********************************************************************* k8s-cp-vm-1-cl-02 : ok=691 changed=139 unreachable=0 failed=0 skipped=1080 rescued=0 ignored=6 k8s-cp-vm-2-cl-02 : ok=646 changed=131 unreachable=0 failed=0 skipped=1050 rescued=0 ignored=3 k8s-cp-vm-3-cl-02 : ok=648 changed=132 unreachable=0 failed=0 skipped=1048 rescued=0 ignored=3 k8s-node-vm-1-cl-02 : ok=553 changed=93 unreachable=0 failed=0 skipped=840 rescued=0 ignored=1 k8s-node-vm-2-cl-02 : ok=509 changed=90 unreachable=0 failed=0 skipped=739 rescued=0 ignored=1 k8s-node-vm-3-cl-02 : ok=509 changed=90 unreachable=0 failed=0 skipped=739 rescued=0 ignored=1 localhost : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Monday 15 January 2024 21:59:02 +0000 (0:00:00.288) 0:17:01.376 ******** =============================================================================== download : Download_file | Download item ------------------------------- 71.29s download : Download_file | Download item ------------------------------- 36.04s kubernetes/kubeadm : Join to cluster ----------------------------------- 19.06s container-engine/containerd : Download_file | Download item ------------ 16.73s download : Download_container | Download image if required ------------- 15.65s container-engine/runc : Download_file | Download item ------------------ 15.53s container-engine/nerdctl : Download_file | Download item --------------- 14.75s container-engine/crictl : Download_file | Download item ---------------- 14.66s download : Download_container | Download image if required ------------- 13.75s container-engine/crictl : Extract_file | Unpacking archive ------------- 13.35s kubernetes/control-plane : Kubeadm | Initialize first master ----------- 10.16s container-engine/nerdctl : Extract_file | Unpacking archive ------------- 9.99s kubernetes/preinstall : Install packages requirements ------------------- 9.93s kubernetes/control-plane : Joining control plane node to the cluster. --- 9.21s container-engine/runc : Download_file | Validate mirrors ---------------- 9.20s container-engine/crictl : Download_file | Validate mirrors -------------- 9.15s container-engine/nerdctl : Download_file | Validate mirrors ------------- 9.12s container-engine/containerd : Download_file | Validate mirrors ---------- 9.06s container-engine/containerd : Containerd | Unpack containerd archive ---- 8.99s download : Download_container | Download image if required -------------- 8.19s I guess I am gonna spin up a couple of Kubernetes clusters going forward \u0026#x1f604;\nIf something should fail I have configured the ansible_command to pipe the output to a file called ansible_output.log I can check. This pipes out the whole Kubespray operation. Some simple tests to do is also checking if Ansible can reach the VMs (after they have been deployed ofcourse). This command needs to be run within the python environment again.\n# activate the environment andreasm@linuxmgmt01:~/terraform/proxmox$ source $VENVDIR/bin/activate # ping all nodes (kubespray-venv) andreasm@linuxmgmt01:~/terraform/proxmox$ ansible -i inventory/k8s-cluster-02/inventory.ini -m ping all -u ubuntu Now let me log into one of the Kubernetes Control plane and check if there is a Kubernetes cluster ready or not:\nroot@k8s-cp-vm-1-cl-02:/home/ubuntu# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-cp-vm-1-cl-02 Ready control-plane 6m13s v1.28.5 k8s-cp-vm-2-cl-02 Ready control-plane 6m1s v1.28.5 k8s-cp-vm-3-cl-02 Ready control-plane 5m57s v1.28.5 k8s-node-vm-1-cl-02 Ready \u0026lt;none\u0026gt; 5m24s v1.28.5 k8s-node-vm-2-cl-02 Ready \u0026lt;none\u0026gt; 5m23s v1.28.5 k8s-node-vm-3-cl-02 Ready \u0026lt;none\u0026gt; 5m19s v1.28.5 root@k8s-cp-vm-1-cl-02:/home/ubuntu# kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-648dffd99-lr82q 1/1 Running 0 4m25s kube-system calico-node-b25gf 1/1 Running 0 4m52s kube-system calico-node-h7lpr 1/1 Running 0 4m52s kube-system calico-node-jdkb9 1/1 Running 0 4m52s kube-system calico-node-vsgqq 1/1 Running 0 4m52s kube-system calico-node-w6vrc 1/1 Running 0 4m52s kube-system calico-node-x95mh 1/1 Running 0 4m52s kube-system coredns-77f7cc69db-29t6b 1/1 Running 0 4m13s kube-system coredns-77f7cc69db-2ph9d 1/1 Running 0 4m10s kube-system dns-autoscaler-8576bb9f5b-8bzqp 1/1 Running 0 4m11s kube-system kube-apiserver-k8s-cp-vm-1-cl-02 1/1 Running 1 6m14s kube-system kube-apiserver-k8s-cp-vm-2-cl-02 1/1 Running 1 5m55s kube-system kube-apiserver-k8s-cp-vm-3-cl-02 1/1 Running 1 6m1s kube-system kube-controller-manager-k8s-cp-vm-1-cl-02 1/1 Running 2 6m16s kube-system kube-controller-manager-k8s-cp-vm-2-cl-02 1/1 Running 2 5m56s kube-system kube-controller-manager-k8s-cp-vm-3-cl-02 1/1 Running 2 6m1s kube-system kube-proxy-6jt89 1/1 Running 0 5m22s kube-system kube-proxy-9w5f2 1/1 Running 0 5m22s kube-system kube-proxy-k7l9g 1/1 Running 0 5m22s kube-system kube-proxy-p7wqt 1/1 Running 0 5m22s kube-system kube-proxy-qfmg5 1/1 Running 0 5m22s kube-system kube-proxy-v6tcn 1/1 Running 0 5m22s kube-system kube-scheduler-k8s-cp-vm-1-cl-02 1/1 Running 1 6m14s kube-system kube-scheduler-k8s-cp-vm-2-cl-02 1/1 Running 1 5m56s kube-system kube-scheduler-k8s-cp-vm-3-cl-02 1/1 Running 1 6m kube-system nginx-proxy-k8s-node-vm-1-cl-02 1/1 Running 0 5m25s kube-system nginx-proxy-k8s-node-vm-2-cl-02 1/1 Running 0 5m20s kube-system nginx-proxy-k8s-node-vm-3-cl-02 1/1 Running 0 5m20s kube-system nodelocaldns-4z72v 1/1 Running 0 4m9s kube-system nodelocaldns-8wv4j 1/1 Running 0 4m9s kube-system nodelocaldns-kl6fw 1/1 Running 0 4m9s kube-system nodelocaldns-pqxpj 1/1 Running 0 4m9s kube-system nodelocaldns-qmrq8 1/1 Running 0 4m9s kube-system nodelocaldns-vqnbd 1/1 Running 0 4m9s root@k8s-cp-vm-1-cl-02:/home/ubuntu# Nice nice nice. Now I can go ahead and grab the kubeconfig, configure my loadbalancer to loadbalance the Kubernetes API and start using my newly decomposable provisioned cluster.\nCleaning up # When I am done with my Kubernetes cluster its just about doing a tofu destroy command and everything is neatly cleaned up. I have not configured any persistent storage yet. So if I have deployed some apps on this cluster and decides to delete it any data that has been created I want to keep will be deleted. So its wise to look into how to keep certain data even after the deletion of my cluster.\nThere are two ways I can clean up. If I want to keep the nodes running but just reset the Kubernetes installation I can execute the following command:\n# from the Kubespray environment (kubespray-venv) andreasm@linuxmgmt01:~/terraform/proxmox/kubespray$ ansible-playbook -i inventory/k8s-cluster-02/hosts.yaml --become --become-user=root reset.yml -u ubuntu Or a full wipe, including the deployed VMs:\n# Inside the k8s-cluster-02 OpenTofu project folder andreasm@linuxmgmt01:~/terraform/proxmox/k8s-cluster-02$ tofu destroy Do you really want to destroy all resources? OpenTofu will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value:yes null_resource.ansible_command: Destroying... [id=5215143035945962122] null_resource.ansible_command: Destruction complete after 0s local_file.ansible_inventory: Destroying... [id=1d19a8be76746178f26336defc9ce96c6e82a791] local_file.ansible_inventory: Destruction complete after 0s proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Destroying... [id=1002] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Destroying... [id=1003] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0]: Destroying... [id=1005] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2]: Destroying... [id=1007] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1]: Destroying... [id=1006] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Destroying... [id=1001] proxmox_virtual_environment_vm.k8s-worker-vms-cl02[0]: Destruction complete after 7s proxmox_virtual_environment_vm.k8s-worker-vms-cl02[2]: Destruction complete after 7s proxmox_virtual_environment_vm.k8s-worker-vms-cl02[1]: Destruction complete after 9s proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Still destroying... [id=1001, 10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[0]: Destruction complete after 12s proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 20s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 20s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 40s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 40s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 50s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 50s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 1m0s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 1m0s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 1m10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 1m10s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 1m20s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 1m20s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Still destroying... [id=1002, 1m30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Still destroying... [id=1003, 1m30s elapsed] proxmox_virtual_environment_vm.k8s-cp-vms-cl02[1]: Destruction complete after 1m37s proxmox_virtual_environment_vm.k8s-cp-vms-cl02[2]: Destruction complete after 1m37s proxmox_virtual_environment_file.ubuntu_cloud_init: Destroying... [id=local:snippets/ubuntu.cloud-config.yaml] proxmox_virtual_environment_file.ubuntu_cloud_init: Destruction complete after 0s Destroy complete! Resources: 9 destroyed. Now all the VMs, everything that has been deployed with OpenTofu is gone again. And it takes a couple of seconds or minutes depending on whether I have configured the provider to do a shutdown or stop of the VMs on tofu destroy. I am currently using shutdown.\nPreparing a bunch of hosts for Rancher RKE2 clusters # I decided to give Rancher a run in another post (post this post) and discovered I needed to solve hostnames in the VMs being created. RKE2 requires all nodes have unique hostnames, see here Kubespray took care of that for me above, but if I just wanted to deploy a bunch of VMs with nothing more than the OS itself deployed, that means only provisioned using OpenTofu, I needed some way to also update the hostname inside the VMs also. Below is two updated resource that handles this for me. So now I can just deploy as many VMs I want, hostname is being taken care of by the OpenTofu deployment, and I can just hand them to Rancher to create my RKE2 clusters on.\nProxmox VM resource # resource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;rke2-cp-vms-cl01\u0026#34; { count = 3 name = \u0026#34;rke2-cp-vm-${count.index + 1}-cl-01\u0026#34; description = \u0026#34;Managed by Terraform\u0026#34; tags = [\u0026#34;terraform\u0026#34;, \u0026#34;ubuntu\u0026#34;, \u0026#34;k8s-cp\u0026#34;] timeout_clone = 180 timeout_create = 180 timeout_migrate = 180 timeout_reboot = 180 timeout_shutdown_vm = 180 timeout_start_vm = 180 timeout_stop_vm = 180 node_name = \u0026#34;proxmox-02\u0026#34; vm_id = \u0026#34;102${count.index + 1}\u0026#34; cpu { cores = 4 type = \u0026#34;host\u0026#34; } memory { dedicated = 6144 } agent { # read \u0026#39;Qemu guest agent\u0026#39; section, change to true only when ready enabled = true } startup { order = \u0026#34;3\u0026#34; up_delay = \u0026#34;60\u0026#34; down_delay = \u0026#34;60\u0026#34; } disk { datastore_id = \u0026#34;raid-10-node02\u0026#34; file_id = \u0026#34;local:iso/jammy-server-cloudimg-amd64.img\u0026#34; interface = \u0026#34;virtio0\u0026#34; iothread = true discard = \u0026#34;on\u0026#34; size = 40 file_format = \u0026#34;raw\u0026#34; } initialization { dns { servers = [\u0026#34;10.100.1.7\u0026#34;, \u0026#34;10.100.1.6\u0026#34;] domain = \u0026#34;my-domain.net\u0026#34; } ip_config { ipv4 { address = \u0026#34;10.170.0.1${count.index + 1}/24\u0026#34; gateway = \u0026#34;10.170.0.1\u0026#34; } } datastore_id = \u0026#34;raid-10-node02\u0026#34; user_data_file_id = proxmox_virtual_environment_file.rke2-cp-vms-cl01[count.index].id } network_device { bridge = \u0026#34;vmbr0\u0026#34; vlan_id = \u0026#34;217\u0026#34; } operating_system { type = \u0026#34;l26\u0026#34; } keyboard_layout = \u0026#34;no\u0026#34; lifecycle { ignore_changes = [ network_device, ] } depends_on = [proxmox_virtual_environment_file.rke2-cp-vms-cl01] } resource \u0026#34;proxmox_virtual_environment_vm\u0026#34; \u0026#34;rke2-worker-vms-cl01\u0026#34; { count = 3 name = \u0026#34;rke2-node-vm-${count.index + 1}-cl-01\u0026#34; description = \u0026#34;Managed by Terraform\u0026#34; tags = [\u0026#34;terraform\u0026#34;, \u0026#34;ubuntu\u0026#34;, \u0026#34;k8s-node\u0026#34;] node_name = \u0026#34;proxmox-02\u0026#34; vm_id = \u0026#34;102${count.index + 5}\u0026#34; timeout_clone = 180 timeout_create = 180 timeout_migrate = 180 timeout_reboot = 180 timeout_shutdown_vm = 180 timeout_start_vm = 180 timeout_stop_vm = 180 cpu { cores = 4 type = \u0026#34;host\u0026#34; } memory { dedicated = 6144 } agent { # read \u0026#39;Qemu guest agent\u0026#39; section, change to true only when ready enabled = true } startup { order = \u0026#34;3\u0026#34; up_delay = \u0026#34;60\u0026#34; down_delay = \u0026#34;60\u0026#34; } disk { datastore_id = \u0026#34;raid-10-node02\u0026#34; file_id = \u0026#34;local:iso/jammy-server-cloudimg-amd64.img\u0026#34; interface = \u0026#34;virtio0\u0026#34; iothread = true discard = \u0026#34;on\u0026#34; size = 60 file_format = \u0026#34;raw\u0026#34; } initialization { dns { servers = [\u0026#34;10.100.1.7\u0026#34;, \u0026#34;10.100.1.6\u0026#34;] domain = \u0026#34;my-domain.net\u0026#34; } ip_config { ipv4 { address = \u0026#34;10.170.0.1${count.index + 5}/24\u0026#34; gateway = \u0026#34;10.170.0.1\u0026#34; } } datastore_id = \u0026#34;raid-10-node02\u0026#34; user_data_file_id = proxmox_virtual_environment_file.rke2-worker-vms-cl01[count.index].id } network_device { bridge = \u0026#34;vmbr0\u0026#34; vlan_id = \u0026#34;217\u0026#34; } operating_system { type = \u0026#34;l26\u0026#34; } keyboard_layout = \u0026#34;no\u0026#34; lifecycle { ignore_changes = [ network_device, ] } depends_on = [proxmox_virtual_environment_file.rke2-worker-vms-cl01] } resource \u0026#34;null_resource\u0026#34; \u0026#34;rke2-cp-vms-cl01\u0026#34; { count = 3 provisioner \u0026#34;remote-exec\u0026#34; { inline = [\u0026#34;sudo hostnamectl set-hostname rke2-cp-vm-${count.index + 1}-cl-01.my-domain.net\u0026#34;] connection { type = \u0026#34;ssh\u0026#34; user = \u0026#34;ubuntu\u0026#34; # or another user private_key = file(\u0026#34;${var.private_key_path}\u0026#34;) host = element([for ip in flatten(proxmox_virtual_environment_vm.rke2-cp-vms-cl01[count.index].ipv4_addresses) : ip if ip != \u0026#34;127.0.0.1\u0026#34;], 0) } } depends_on = [proxmox_virtual_environment_vm.rke2-cp-vms-cl01] } resource \u0026#34;null_resource\u0026#34; \u0026#34;rke2-worker-vms-cl01\u0026#34; { count = 3 provisioner \u0026#34;remote-exec\u0026#34; { inline = [\u0026#34;sudo hostnamectl set-hostname rke2-node-vm-${count.index + 1}-cl-01.my-domain.net\u0026#34;] connection { type = \u0026#34;ssh\u0026#34; user = \u0026#34;ubuntu\u0026#34; private_key = file(\u0026#34;${var.private_key_path}\u0026#34;) host = element([for ip in flatten(proxmox_virtual_environment_vm.rke2-worker-vms-cl01[count.index].ipv4_addresses) : ip if ip != \u0026#34;127.0.0.1\u0026#34;], 0) } } depends_on = [proxmox_virtual_environment_vm.rke2-worker-vms-cl01] } output \u0026#34;usable_cp_vm_ipv4_addresses\u0026#34; { value = [for ip in flatten(proxmox_virtual_environment_vm.rke2-cp-vms-cl01[*].ipv4_addresses) : ip if ip != \u0026#34;127.0.0.1\u0026#34;] } output \u0026#34;usable_worker_vm_ipv4_addresses\u0026#34; { value = [for ip in flatten(proxmox_virtual_environment_vm.rke2-worker-vms-cl01[*].ipv4_addresses) : ip if ip != \u0026#34;127.0.0.1\u0026#34;] } I have added a remote-exec resource to remotely log in to the provisioned VMs, execute the hostname set-hostname command. To get around cycle dependencies I had to add a \u0026ldquo;null_resource\u0026rdquo; before the remote-exec. I have also divided the vm resource config to separate the type of VMs from each other so I can configure them different from each other (control plane nodes and worker nodes). The two last entries was only added to verify if it could pick up the correct IP address from the VMs.\nProxmox virtual environment file # # CP VMs resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;rke2-cp-vms-cl01\u0026#34; { count = 3 # adjust pr total amount of VMs content_type = \u0026#34;snippets\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;proxmox-02\u0026#34; source_raw { data = \u0026lt;\u0026lt;EOF #cloud-config chpasswd: list: | ubuntu:ubuntu expire: false packages: - qemu-guest-agent timezone: Europe/Oslo users: - default - name: ubuntu groups: sudo shell: /bin/bash ssh-authorized-keys: - ${trimspace(\u0026#34;ssh-rsa E8lMzi2QtaV6FbEGQG41sKUetP4IfQ9OKQb4n3pIleyuFySijxWS37krexsd9E2rkJFjz0rhh1idWb4vfzQH15lsBIaA1JpcYTqJWp6QwJ8oV2psQUi/knwVNfn3EckKrkNsGwUw6+d\u0026#34;)} sudo: ALL=(ALL) NOPASSWD:ALL power_state: delay: now mode: reboot message: Rebooting after cloud-init completion condition: true EOF file_name = \u0026#34;rke2-cp-vms-${count.index + 1}-cl01.yaml\u0026#34; } } # Node VMs resource \u0026#34;proxmox_virtual_environment_file\u0026#34; \u0026#34;rke2-worker-vms-cl01\u0026#34; { count = 3 # adjust pr total amount of VMs content_type = \u0026#34;snippets\u0026#34; datastore_id = \u0026#34;local\u0026#34; node_name = \u0026#34;proxmox-02\u0026#34; source_raw { data = \u0026lt;\u0026lt;EOF #cloud-config chpasswd: list: | ubuntu:ubuntu expire: false packages: - qemu-guest-agent timezone: Europe/Oslo users: - default - name: ubuntu groups: sudo shell: /bin/bash ssh-authorized-keys: - ${trimspace(\u0026#34;ssh-rsa AAAAB3NxTSy6hioyUwiRpVUKYDf9zsU4P87zIqasRHMPfoj2PI0YCPihDpQj/e0VtkQaBhyfLoFuLa+zTEDjR5nYt1P0MRWPRuOxY/ls04VCpVvA9mUSYF8ftAXf2SXRY7sqQE3dg4Bav7FdHe1labQH4logd1N5ra9PS+bVGcBDstSH/t7Zkf/Na1EMqN75M5PKiFzHpde7xFnvaRbcVdzr64xTXP2vVj+jTlcMBRAoJQHIO4703jy3Ma2fJbYxipSsl1TGDgUFxf3rDjW/gKOWQhbCVheDMGC94\u0026#34;)} sudo: ALL=(ALL) NOPASSWD:ALL power_state: delay: now mode: reboot message: Rebooting after cloud-init completion condition: true EOF file_name = \u0026#34;rke2-worker-vms-${count.index + 1}-cl01.yaml\u0026#34; } } In the cloud init environment resource I have also added to entries to separate the control plane nodes from the worker nodes so I can easier configure them differently. Every file_name created will accomodate the actual name of the node it belongs to. So the Proxmox snippets being created will now be 1 snippet pr VM created. They will be removed when the project is deleted by tofu destroy.\nSummary # There is so much more that can be adjusted, improved and explored in general. But this post is just how I have done it now to solve a task I have been waiting to finally get some time to do. I will maybe do a follow up post where I do some improvements after some time using it and gained more experience on it. This includes improving the OpenTofu configs and Kubespray.\n","date":"15 January 2024","externalUrl":null,"permalink":"/2024/01/15/proxmox-with-opentofu-kubespray-and-kubernetes/","section":"Posts","summary":"In this post I will quickly go through how I made use of OpenTofu to provision VMs on my Proxmox cluster and Ansible using Kubespray to deploy my Kubernetes clusters on demand.","title":"Proxmox with OpenTofu Kubespray and Kubernetes","type":"posts"},{"content":"","date":"15 January 2024","externalUrl":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform","type":"tags"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/cert-manager/","section":"Tags","summary":"","title":"Cert-Manager","type":"tags"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/ingress/","section":"Tags","summary":"","title":"Ingress","type":"tags"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/categories/ingress/","section":"Categories","summary":"","title":"Ingress","type":"categories"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/categories/loadbalancing/","section":"Categories","summary":"","title":"LoadBalancing","type":"categories"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/categories/network/","section":"Categories","summary":"","title":"Network","type":"categories"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/proxy/","section":"Tags","summary":"","title":"Proxy","type":"tags"},{"content":"","date":"26 December 2023","externalUrl":null,"permalink":"/tags/traefik/","section":"Tags","summary":"","title":"Traefik","type":"tags"},{"content":" Traefik (\u0026rsquo;træfik\u0026rsquo;) Proxy # First thing first, this is probably the first time in any of my blog posts that I can use a letter that is part of my native alphabet, namely the \u0026ldquo;æ\u0026rdquo;. I tried to look up whether I should stick to Træfik or not, so I ended up on this post by Traefik themselves, its not the letter \u0026ldquo;Æ\u0026rdquo; itself that is the reason behind the use of it, but it is the phonetic pronunciation of Traefik = \u0026rsquo;træfik\u0026rsquo;. Nice that the letter \u0026ldquo;æ\u0026rdquo; has some use internationally though \u0026#x1f604; A fun and nice post though. For the rest of the post I will stick to using Traefik instead of Træfik as Træfik is just the logo and how Traefik is pronounced, it is called Traefik (and to be kind to the non native \u0026ldquo;æ\u0026rdquo; speakers out there).\nFrom the offical Traefik homepage:\nTraefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them.\nWhat sets Traefik apart, besides its many features, is that it automatically discovers the right configuration for your services. The magic happens when Traefik inspects your infrastructure, where it finds relevant information and discovers which service serves which request.\nTraefik is natively compliant with every major cluster technology, such as Kubernetes, Docker, Docker Swarm, AWS, Mesos, Marathon, and the list goes on; and can handle many at the same time. (It even works for legacy software running on bare metal.)\nWhy Traefik\u0026hellip;\nI needed an advanced reverse proxy for my lab that could cover all kinds of backends, from Kubernetes services, services running in regular workload such as virtual machines. I wanted it to be highly available and to solve one of my challenges when exposing services on the the Internet with one public IP and multiple services using the same port. After some quick research I ended up with Traefik. I am not sure why I exactly landed on Traefik, it could have been Nginx or HAProxy just to mention some of the bigger ones out there, or was it the \u0026ldquo;Æ\u0026rdquo;? Traefik offers both paid Enterprise editions, and free open source alternatives. I did not want to use time on a product that has some basic features included in their free open source edition and as soon as I wanted a more advanced feature I had to upgrade to a enterprise solution. After some reading Traefik seemed to have all the features I wanted in their open source product Traefik Proxy.\nI decided to write this post as I wanted to document all the configurations I have done so far with Traefik. By searching in different forums, blog pages etc some say it is very easy to manage Traefik. I cant say I found it very easy to begin with, but as with everything new one need to learn how to master it. The official Traefik documentation is very good at describing and explaining all the possibilites with Traefik, but several times I was missing some \u0026ldquo;real life\u0026rdquo; example configs. But with the help of the great community out there I managed to solve the challenges I had and make them work with Traefik. So thanks to all the blog pages, forums with people asking questions and people willing to answer and explain. This is much appreciated as always.\nSo lets begin this post wth some high level explanations on some terminology used in Traefik, then the installation and how I have configured Traefik to serve as a reverse proxy for some of my services.\nImportant terminology used in Traefik Proxy # Entrypoints # EntryPoints are the network entry points into Traefik. They define the port which will receive the packets, and whether to listen for TCP or UDP.\nIn other words either an externally exposed service (NodePort or loadBalancer) or internal service (ClusterIP) defined, the destination endpoints for these entrypoints will here be the Traefik pods responsible for listening to any requests coming their way and do something useful with the traffic if configured.\nSee more here\nRouters # A router is in charge of connecting incoming requests to the services that can handle them. In the process, routers may use pieces of middleware to update the request, or act before forwarding the request to the service.\nSo this is the actual component that knows which service to forward the requests to based on for example host header.\nSee more here\nMiddleware # Attached to the routers, pieces of middleware are a means of tweaking the requests before they are sent to your service (or before the answer from the services are sent to the clients).\nAn example can be the redirectscheme to redirect all http requests to https. For a full list of options, hava a look here\nServices # The Services are responsible for configuring how to reach the actual services that will eventually handle the incoming requests.\nServices here can be of servicetype loadBalancer, ClusterIP, ExternalName etc\nProviders # Configuration discovery in Traefik is achieved through Providers.\nThe providers are infrastructure components, whether orchestrators, container engines, cloud providers, or key-value stores. The idea is that Traefik queries the provider APIs in order to find relevant information about routing, and when Traefik detects a change, it dynamically updates the routes.\nMore info on providers can be found here\nMy lab # Before getting into the actual installaton and configuration of Traefik, a quick context. My lab in this post:\nA physical server running Proxmox A physical switch with VLAN and routing support Virtual PfSense firewall Kubernetes version 1.28.2 3x Control Plane nodes (Ubuntu) 3x Worker nodes (Ubuntu) A management Ubuntu VM (also running on Proxmox) with all tools needed like Helm and kubectl Cert-Manager configured and installed with LetsEncrypt provider Cilium has been configured with BGP, LB IPAM pools have been defined an provide external ip addresses to servicetype loadBalancer requests in the Kubernetes cluster Deploying Traefik # Traefik can be deployed in Kubernetes using Helm. First I need to add the Traefik Helm repo:\nhelm repo add traefik https://traefik.github.io/charts helm repo update Now it would be as simple as just installing Traefik using helm install traefik traefik/traefik -n traefik, but I have done some adjustements in the values. So before I install Traefik I have adjusted the chart values for Traefik to use this config. See comments inline below. Note that I have removed all the comments from the default value.yaml and just added my own comments. The value yaml can be fetched by issuing this command: helm show values traefik/traefik \u0026gt; traefik-values.yaml\nimage: registry: docker.io repository: traefik tag: \u0026#34;\u0026#34; pullPolicy: IfNotPresent commonLabels: {} deployment: enabled: true kind: Deployment replicas: 3 ### Adjusted to three for high availability terminationGracePeriodSeconds: 60 minReadySeconds: 0 annotations: {} labels: {} podAnnotations: {} podLabels: {} additionalContainers: [] additionalVolumes: [] initContainers: # The \u0026#34;volume-permissions\u0026#34; init container is required if you run into permission issues. # Related issue: https://github.com/traefik/traefik-helm-chart/issues/396 - name: volume-permissions image: busybox:latest command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;touch /data/acme.json; chmod -v 600 /data/acme.json\u0026#34;] securityContext: runAsNonRoot: true runAsGroup: 65532 runAsUser: 65532 volumeMounts: - name: data mountPath: /data shareProcessNamespace: false dnsConfig: {} imagePullSecrets: [] lifecycle: {} podDisruptionBudget: enabled: false ingressClass: enabled: true isDefaultClass: false # I have set this to false as I also have Cilium IngressController experimental: plugins: {} kubernetesGateway: enabled: false ingressRoute: dashboard: enabled: false # I will enable this later annotations: {} labels: {} matchRule: PathPrefix(`/dashboard`) || PathPrefix(`/api`) entryPoints: [\u0026#34;traefik\u0026#34;] middlewares: [] tls: {} healthcheck: enabled: false annotations: {} labels: {} matchRule: PathPrefix(`/ping`) entryPoints: [\u0026#34;traefik\u0026#34;] middlewares: [] tls: {} updateStrategy: type: RollingUpdate rollingUpdate: maxUnavailable: 0 maxSurge: 1 readinessProbe: failureThreshold: 1 initialDelaySeconds: 2 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 livenessProbe: failureThreshold: 3 initialDelaySeconds: 2 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 2 startupProbe: providers: kubernetesCRD: enabled: true # set to true allowCrossNamespace: true # set to true allowExternalNameServices: true # set to true also allowEmptyServices: false namespaces: [] kubernetesIngress: enabled: true # set to true allowExternalNameServices: true # set to true allowEmptyServices: false namespaces: [] publishedService: enabled: false file: enabled: false watch: true content: \u0026#34;\u0026#34; volumes: [] additionalVolumeMounts: [] logs: general: level: ERROR access: enabled: false filters: {} fields: general: defaultmode: keep names: {} headers: defaultmode: drop names: {} metrics: prometheus: entryPoint: metrics addEntryPointsLabels: true # set to true addRoutersLabels: true # set to true addServicesLabels: true # set to true buckets: \u0026#34;0.1,0.3,1.2,5.0,10.0\u0026#34; # adjusted according to the official docs tracing: {} globalArguments: - \u0026#34;--global.checknewversion\u0026#34; - \u0026#34;--global.sendanonymoususage\u0026#34; additionalArguments: [] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace envFrom: [] ports: # these are the entrypoints traefik: port: 9000 expose: false exposedPort: 9000 protocol: TCP web: port: 8000 expose: true exposedPort: 80 protocol: TCP websecure: port: 8443 expose: true exposedPort: 443 protocol: TCP http3: enabled: false tls: enabled: true options: \u0026#34;\u0026#34; certResolver: \u0026#34;\u0026#34; domains: [] middlewares: [] metrics: port: 9100 expose: true exposedPort: 9100 protocol: TCP tlsOptions: {} tlsStore: {} service: enabled: false # I will create this later, set to false, all values below will be ignored single: true type: LoadBalancer annotations: {} annotationsTCP: {} annotationsUDP: {} labels: env: prod spec: loadBalancerIP: \u0026#34;10.150.11.11\u0026#34; loadBalancerSourceRanges: [] externalIPs: [] autoscaling: enabled: false #This is interesting, need to test persistence: enabled: true resourcePolicy: \u0026#34;keep\u0026#34; # I have added this to keep the PVC even after uninstall name: data accessMode: ReadWriteOnce size: 128Mi path: /data annotations: {} certResolvers: {} hostNetwork: false rbac: enabled: true namespaced: false podSecurityPolicy: enabled: false serviceAccount: name: \u0026#34;\u0026#34; serviceAccountAnnotations: {} resources: {} nodeSelector: {} tolerations: [] topologySpreadConstraints: [] priorityClassName: \u0026#34;\u0026#34; securityContext: capabilities: drop: [ALL] readOnlyRootFilesystem: true allowPrivilegeEscalation: false podSecurityContext: fsGroupChangePolicy: \u0026#34;OnRootMismatch\u0026#34; runAsGroup: 65532 runAsNonRoot: true runAsUser: 65532 extraObjects: [] Now I can install Traefik using the following command:\nhelm install traefik traefik/traefik -f traefik-values.yaml -n traefik # or helm upgrade -i traefik traefik/traefik -f traefik-values.yaml -n traefik When updating changes etc. or just use from the start. After a successful installation we should see this message:\nRelease \u0026#34;traefik\u0026#34; has been upgraded. Happy Helming! NAME: traefik LAST DEPLOYED: Wed Dec 27 20:36:23 2023 NAMESPACE: traefik STATUS: deployed REVISION: 15 TEST SUITE: None NOTES: Traefik Proxy v2.10.6 has been deployed successfully on traefik namespace ! 🚨 When enabling persistence for certificates, permissions on acme.json can be lost when Traefik restarts. You can ensure correct permissions with an initContainer. See https://github.com/traefik/traefik-helm-chart/issues/396 for more info. 🚨 Now I should also have a bunch of CRDs, an additional IngressClass (if you have a couple from before as I did).\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik$ k get crd NAME CREATED AT ingressroutes.traefik.containo.us 2023-12-24T08:53:45Z ingressroutes.traefik.io 2023-12-24T08:53:45Z ingressroutetcps.traefik.containo.us 2023-12-24T08:53:45Z ingressroutetcps.traefik.io 2023-12-24T08:53:45Z ingressrouteudps.traefik.containo.us 2023-12-24T08:53:45Z ingressrouteudps.traefik.io 2023-12-24T08:53:45Z middlewares.traefik.containo.us 2023-12-24T08:53:45Z middlewares.traefik.io 2023-12-24T08:53:45Z middlewaretcps.traefik.containo.us 2023-12-24T08:53:45Z middlewaretcps.traefik.io 2023-12-24T08:53:46Z serverstransports.traefik.containo.us 2023-12-24T08:53:45Z serverstransports.traefik.io 2023-12-24T08:53:46Z serverstransporttcps.traefik.io 2023-12-24T08:53:46Z tlsoptions.traefik.containo.us 2023-12-24T08:53:45Z tlsoptions.traefik.io 2023-12-24T08:53:46Z tlsstores.traefik.containo.us 2023-12-24T08:53:45Z tlsstores.traefik.io 2023-12-24T08:53:46Z traefikservices.traefik.containo.us 2023-12-24T08:53:45Z traefikservices.traefik.io 2023-12-24T08:53:46Z A note on this list of CRDs above. The former Traefik APIs used the traefik.containo.us but from version Traefik 2.x they are now using the APIs traefik.io the former APIs are there for backward compatibility.\nBelow I can see the new Traefik Ingress controller.\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik$ k get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE cilium cilium.io/ingress-controller \u0026lt;none\u0026gt; 10d traefik traefik.io/ingress-controller \u0026lt;none\u0026gt; 59s Deployment info:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik$ k get all -n traefik NAME READY STATUS RESTARTS AGE pod/traefik-59657c9c59-75cxg 1/1 Running 0 27h pod/traefik-59657c9c59-p2kdv 1/1 Running 0 27h pod/traefik-59657c9c59-tqcrm 1/1 Running 0 27h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # No services... NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/traefik 3/3 3 3 2d11h NAME DESIRED CURRENT READY AGE replicaset.apps/traefik-59657c9c59 3 3 3 27h Now it is all about configuring Traefik to receieve the requests, configure routes, middleware and services. I will start by getting the Traefik dashboard up.\nEntryPoints # As I have disabled all the services in the Helm value yaml none are created, therefore I need to create these entrypoints before anything can reach Traefik.\nA quick explanation why wanted to create these myself. One can have multiple entrypoints to Traefik, even in the same Kubernetes cluster. Assume I want to use different IP addresses and subnets for certain services, some may even call it VIPs, for IP separation, easier physical firewall creation etc. Then I need to create these services to expose the entrypoints I want to use. The Helm chart enables 4 entrypoints by default: web port 8000 (http), websecure port 8443 (https), traefik port 9000 and metrics port 9100 TCP. But these are only configured on the Traefik pods themselves, there is no service to expose them either internally in the cluster or outside. So I need to create these external or internal services to expose these entrypoints.\nDescribe the pod to see the ports and labels:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik$ k describe pod -n traefik traefik-59657c9c59-75cxg Name: traefik-59657c9c59-75cxg Namespace: traefik Priority: 0 Service Account: traefik Node: k8s-prod-node-01/10.160.1.114 Start Time: Tue, 26 Dec 2023 17:03:23 +0000 Labels: app.kubernetes.io/instance=traefik-traefik app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=traefik traefik: Container ID: containerd://b6059c9c6cdf45469403fb153ee8ddd263a870d3e5917a79e0181f543775a302 Image: docker.io/traefik:v2.10.6 Image ID: docker.io/library/traefik@sha256:1957e3314f435c85b3a19f7babd53c630996aa1af65d1f479d75539251b1e112 Ports: 9100/TCP, 9000/TCP, 8000/TCP, 8443/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP Args: --global.checknewversion --global.sendanonymoususage --entrypoints.metrics.address=:9100/tcp --entrypoints.traefik.address=:9000/tcp --entrypoints.web.address=:8000/tcp --entrypoints.websecure.address=:8443/tcp --api.dashboard=true --ping=true --metrics.prometheus=true --metrics.prometheus.entrypoint=metrics --metrics.prometheus.addRoutersLabels=true --metrics.prometheus.addEntryPointsLabels=true --metrics.prometheus.addServicesLabels=true --metrics.prometheus.buckets=0.1,0.3,1.2,5.0,10.0 --providers.kubernetescrd --providers.kubernetescrd.allowCrossNamespace=true --providers.kubernetescrd.allowExternalNameServices=true --providers.kubernetesingress --providers.kubernetesingress.allowExternalNameServices=true --entrypoints.websecure.http.tls=true My first service I define and apply will primarily be used for management, interacting with Traefik internal services using the correct label selector to select the Traefik pods and refering to the two entrypoint web and websecure. This is how the first entrypoint is defined:\napiVersion: v1 kind: Service metadata: annotations: io.cilium/lb-ipam-ips: \u0026#34;10.150.11.11\u0026#34; name: traefik-mgmt labels: env: prod namespace: traefik spec: ports: - name: web port: 80 protocol: TCP targetPort: web - name: websecure port: 443 protocol: TCP targetPort: websecure selector: app.kubernetes.io/name: traefik type: LoadBalancer This will create a servicetype LoadBalancer, the IP address is fixed by using the annotation and my confiigured Cilium LB-IPAM pool will provide the IP address for the service and BGP control plane will take care of advertising the IP address for me.\nLets apply the above yaml and check the service:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik$ k get svc -n traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE traefik-mgmt LoadBalancer 10.21.183.19 10.150.11.11 80:30343/TCP,443:30564/TCP 49s This means I can now start registering relevant DNS records to this external IP and Traefik will receieve requests coming to this address/service.\nBut as I would like to separate out services by type/function using different ip addresses I have created another service using the same entrypoints but with a different external-ip.\napiVersion: v1 kind: Service metadata: annotations: io.cilium/lb-ipam-ips: \u0026#34;10.150.16.10\u0026#34; name: traefik-exposed-pool-1 labels: env: traefik-pool-1 namespace: traefik spec: ports: - name: web port: 80 protocol: TCP targetPort: web - name: websecure port: 443 protocol: TCP targetPort: websecure selector: app.kubernetes.io/name: traefik type: LoadBalancer I can go ahead and register DNS records against this IP address also and they will be forwarded to Traefik to handle.\nThe beauty of this is that I can create as many services I want, using different external-ip addresses, and even specify different Traefik entrypoints. In my physical firewall I can more easily create firewall rules allowing or denying which source is allowed to reach these ip-addresses and then separate out apps from apps, services from services. Like in the next chapter when I expose the Traefik Dashboard.\nTraefik Dashbord # Traffic comes with a nice dashboard which gives a quick overview of services enabled, status and detailed information:\nAs I have not enabled any services I will need to define these to make the Dashboard accessible. I also want it accessible from outside my Kubernetes cluster using basic authentication.\nI will prepare three yaml files. The first one will be the secret for the authentication part, the second the middleware config to enable basic authentication and the third and final the actual IngressRoute.\nFor the secret I used the following command to generate a base64 encoded string containing both username and password:\nandreasm@linuxmgmt01:~/temp$ htpasswd -nb admin \u0026#39;password\u0026#39; | openssl base64 YWRtaW46JGFwcjEkmlBejdSYnZW5uN1oualB1Lm1LOUo0dVhqVDB3LgoK Then I created the 01-secret.yaml and pasted the bas64 output above\napiVersion: v1 kind: Secret metadata: name: traefik-dashboard-auth namespace: traefik data: users: YWRtaW4JGFwcEkdmlBejdSYnEkZW5uN1oualB1Lm1LOUo0dVhqVDB3LgoK The second yaml, the 02-middleware.yaml, to enable basic authentication:\napiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: traefik-dashboard-basicauth namespace: traefik spec: basicAuth: secret: traefik-dashboard-auth Then the last yaml, the dashboard IngressRoute:\napiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: traefik-dashboard namespace: traefik spec: entryPoints: - websecure routes: - match: Host(`traefik-ui.my-domain.net`) kind: Rule middlewares: - name: traefik-dashboard-basicauth namespace: traefik services: # - name: traefik-mgmt - name: api@internal kind: TraefikService tls: secretName: my-doamin-net-tls-prod Notice I refer to a tls secret? More on this just a tad later.\nLets see the three objects created.\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik/traefik-dashboard$ k get secrets -n traefik NAME TYPE DATA AGE traefik-dashboard-auth Opaque 1 39s andreasm@linuxmgmt01:~/prod-cluster-1/traefik/traefik-dashboard$ k get middleware.traefik.io -n traefik NAME AGE traefik-dashboard-basicauth 55s andreasm@linuxmgmt01:~/prod-cluster-1/traefik/traefik-dashboard$ k get ingressroutes.traefik.io -n traefik NAME AGE traefik-dashboard 1m I have created a DNS record to point to external-ip of the traefik-mgmt service and made sure the Host definition in the IngressRoute matches this dns.\nNow the dashboard is available and prompting for username and password.\nThe official doc here for more info.\nCert-Manager # Instead of configuring Traefik to generate the certificates I need for my HTTPS services I have already configured Cert-Manager to create the certificates I need, you can read how I have done it here. I use mostly wildcard certificates, and dont see the need to request certificates all the time.\nThen I use reflector to share/sync the certificates across namespaces. Read more on reflector here and here.\nMonitoring with Prometheus and Grafana # Another nice feature is Traefik\u0026rsquo;s built in Prometheus metrics. These Prometheus metrics can then be used as datasource in Grafana. So here is how I configured Prometheus and Grafana.\nI followed these two blog post\u0026rsquo;s here and here, used them in combination to configure Traefik with Prometheus.\nPrometheus # I will start by getting Prometheus up and running, then Grafana\nIn my Traefik value.yaml I made these changes before I ran helm upgrade on the Traefik installation:\nmetrics: ## -- Prometheus is enabled by default. ## -- It can be disabled by setting \u0026#34;prometheus: null\u0026#34; prometheus: # -- Entry point used to expose metrics. entryPoint: metrics ## Enable metrics on entry points. Default=true addEntryPointsLabels: true ## Enable metrics on routers. Default=false addRoutersLabels: true ## Enable metrics on services. Default=true addServicesLabels: true ## Buckets for latency metrics. Default=\u0026#34;0.1,0.3,1.2,5.0\u0026#34; buckets: \u0026#34;0.1,0.3,1.2,5.0,10.0\u0026#34; First I registered a DNS record on the external-ip service below with the name prometheus-traefik.my-domain.net as I consider this also a service that belongs within the management category. Now I have two dns records pointing to the same IP (the traefik-ui above included).\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) traefik-mgmt LoadBalancer 10.21.183.19 10.150.11.11 80:30343/TCP,443:30564/TCP I will prepare 6 different yaml files. All files will be explained below. First yaml traefik-metrics-service:\napiVersion: v1 kind: Service metadata: name: traefik-metrics namespace: traefik spec: ports: - name: metrics protocol: TCP port: 9100 targetPort: metrics selector: app.kubernetes.io/instance: traefik-traefik app.kubernetes.io/name: traefik type: ClusterIP As I followed the two blogs above there is a couple of approaches to make this work. One approach is to to expose the metrics using ClusterIP by applying the yaml above. Then the Prometheus target is refering to this svc (requires Prometheus to be runnning on same cluster). The other approach is to configure Prometheus scraping the Traefik pods.\nOne can also use this ClusterIP service later on with an IngressRoute to expose it outside its Kubernetes cluster for an easy way to just check whether there is metrics coming or need to access this metrics externally. If scraping the pods, this service is not needed as Prometheus will scrape the Traefik pods directly.\nThen I need to create a Prometheus configMap telling Promethus what and how to scrape. I will paste below two ways Prometheus can scrape the metrics. The first yaml will scrape the pods directly using the kubernetes_sd_config and filter on the annotations on the Traefik pods.\napiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: prometheus data: prometheus.yml: | global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: \u0026#39;traefik\u0026#39; kubernetes_sd_configs: - role: pod selectors: - role: pod label: \u0026#34;app.kubernetes.io/name=traefik\u0026#34; relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\\\d+)?;(\\\\d+) replacement: $1:$2 target_label: __address__ - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) This approach will scrape the metrics from the relevant Traefik pods, using annotation. But it also means I need to give the Prometheus pod access to scrape pods not in its own namespace. So I will go ahead and create a service account, role and role binding for that:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik/traefik-dashboard$ kubectl -n prometheus create serviceaccount prometheus serviceaccount/prometheus created andreasm@linuxmgmt01:~/prod-cluster-1/traefik/traefik-dashboard$ k create clusterrole prometheus --verb=get,list,watch --resource=pods,services,endpoints clusterrole.rbac.authorization.k8s.io/prometheus created andreasm@linuxmgmt01:~/prod-cluster-1/traefik/traefik-dashboard$ kubectl create clusterrolebinding prometheus --clusterrole=prometheus --serviceaccount=prometheus:prometheus clusterrolebinding.rbac.authorization.k8s.io/prometheus created The second approach is to point to the ClusterIP metrics service (defined above) and let Prometheus scrape this service instead. This approach does not need the serviceAccount.\napiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: prometheus data: prometheus.yml: | global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: \u0026#39;traefik\u0026#39; static_configs: - targets: [\u0026#39;traefik-metrics.traefik.svc.cluster.local:9100\u0026#39;] Then I created the third yaml file that creates the PersistentVolumeClaim for my Prometheus instance:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: prometheus-storage-persistence namespace: prometheus spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi The fourth file is the actual Promethus deployment, refering to the objects created in the previous yamls:\napiVersion: apps/v1 kind: Deployment metadata: name: prometheus namespace: prometheus spec: selector: matchLabels: app: prometheus replicas: 1 template: metadata: labels: app: prometheus spec: serviceAccountName: prometheus # Remember to add the serviceaccount for scrape access containers: - name: prometheus image: prom/prometheus:latest ports: - containerPort: 9090 name: default volumeMounts: - name: prometheus-storage mountPath: /prometheus - name: config-volume mountPath: /etc/prometheus volumes: - name: prometheus-storage persistentVolumeClaim: claimName: prometheus-storage-persistence - name: config-volume configMap: name: prometheus-config The fifth yaml file is the Prometheus service where I expose Prometheus internally in the cluster:\nkind: Service apiVersion: v1 metadata: name: prometheus namespace: prometheus spec: selector: app: prometheus type: ClusterIP ports: - protocol: TCP port: 9090 targetPort: 9090 The last yaml is the IngressRoute if I want to access Promethus outside my Kubernetes Cluster. Strictly optional if Grafana is also deployed in the same cluster as it can then just use the previously created ClusterIP service. But nice to have if in need to troubleshoot etc.\napiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: prometheus namespace: prometheus spec: entryPoints: - websecure routes: - kind: Rule match: Host(`prometheus-traefik.my-domain.net`) services: - kind: Service name: prometheus port: 9090 Here comes the DNS record into play, the record I created earlier. Now after I have applied all the above yamls Prometheus should be up and running and I can use the IngressRoute to access the Prometheus Dashboard from my laptop.\nScrenshot below is when scraping the pods directly\nScreenshot below is scraping the metrics-service:\nGrafana # Now I more or less just need to install Grafana, add the Prometheus ClusterIP as datasource. To install Grafana, that is easily done by using Helm. Below is the steps I did to install Grafana:\nhelm repo add grafana https://grafana.github.io/helm-charts helm repo update ## grabbing the default values helm show values grafana/grafana \u0026gt; grafana-values.yaml Below is the changes I have done in the value.yaml I am using to install Grafana:\n## Configure grafana datasources ## ref: http://docs.grafana.org/administration/provisioning/#datasources ## datasources: datasources.yaml: apiVersion: 1 datasources: - name: Prometheus-traefik type: prometheus url: http://prometheus.prometheus.svc.cluster.local:9090 access: proxy editable: true orgId: 1 version: 1 isDefault: true ingress: enabled: false persistence: # type: pvc enabled: true # resourcePolicy: \u0026#34;keep\u0026#34; # storageClassName: default accessModes: - ReadWriteOnce size: 10Gi annotations: helm.sh/resource-policy: \u0026#34;keep\u0026#34; finalizers: - kubernetes.io/pvc-protection # selectorLabels: {} ## Sub-directory of the PV to mount. Can be templated. # subPath: \u0026#34;\u0026#34; ## Name of an existing PVC. Can be templated. # existingClaim: ## Extra labels to apply to a PVC. extraPvcLabels: {} ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service). ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it. ## ref: http://kubernetes.io/docs/user-guide/services/ ## service: enabled: true type: ClusterIP port: 80 targetPort: 3000 # targetPort: 4181 To be used with a proxy extraContainer ## Service annotations. Can be templated. annotations: {} labels: {} portName: service # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: \u0026#34;http\u0026#34; or \u0026#34;tcp\u0026#34; appProtocol: \u0026#34;\u0026#34; # Administrator credentials when not using an existing secret (see below) adminUser: admin adminPassword: \u0026#39;password\u0026#39; This will deploy Grafana with a pvc, not deleted if the Helm installation of Grafana is uninstalled, it will create a ClusterIP exposing the Grafana UI internally in the cluster. So I need to create an IngressRoute to expose it outside the cluster using Traefik.\nBelow is the IngressRoute for this:\napiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: grafana-ingressroute namespace: grafana spec: entryPoints: - web routes: - kind: Rule match: Host(`grafana-prod.my-domain.net`) services: - kind: Service name: grafana passHostHeader: true namespace: grafana port: 80 Again, the match Host DNS is already registered using the same ExternalIP as the Prometheus one.\nNow Grafana should be up and running.\nDashboard depicted above is the Traefik Official Standalone Dashboard which can be imported from here or use the following ID: 17346.\nThats it for monitoring with Prometheus and Grafana. Now onto just a simple web application.\nTest application Yelb # I wanted to just expose my test application Yelb deployed twice, but using two different DNS records. I also wanted these services to be exposed using a completely different subnet, to create the IP separation I have mentioned a couple of times. I have already deployed the Yelb application twice in my cluster in their own respective namespaces:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik/grafana$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-84f4bf49b5-fq26l 1/1 Running 0 13d yelb-appserver-6dc7cd98-s6kt7 1/1 Running 0 13d yelb-db-84d6f6fc6c-m7xvd 1/1 Running 0 13d yelb-ui-6fbbcc4c87-qjdzg 1/1 Running 0 2d20h andreasm@linuxmgmt01:~/prod-cluster-1/traefik/grafana$ k get pods -n yelb-2 NAME READY STATUS RESTARTS AGE redis-server-84f4bf49b5-4sx7f 1/1 Running 0 2d16h yelb-appserver-6dc7cd98-tqkkh 1/1 Running 0 2d16h yelb-db-84d6f6fc6c-t4td2 1/1 Running 0 2d16h yelb-ui-2-84cc897d6d-64r9x 1/1 Running 0 2d16h I want to expose the yelb-ui in both namespaces on their different DNS records using IngressRoutes. I also want to use a completely different external IP address than what I have been using so far under the management category. So this time I will be using this external-ip:\napiVersion: v1 kind: Service metadata: annotations: io.cilium/lb-ipam-ips: \u0026#34;10.150.16.10\u0026#34; name: traefik-exposed-pool-1 labels: env: traefik-pool-1 namespace: traefik spec: ports: - name: web port: 80 protocol: TCP targetPort: web - name: websecure port: 443 protocol: TCP targetPort: websecure selector: app.kubernetes.io/name: traefik type: LoadBalancer So I will need to register two DNS records against the IP above: 10.150.16.10 with the following names: yellb-1.my-domain-net\u0026quot; and yelb-2.my-domain.net\nThen I can expose the Yelb UI services from both the namespaces yelb and yelb-2 with the following IngressRoutes:\napiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: yelb-ingressroute-1 namespace: yelb spec: entryPoints: - web routes: - kind: Rule match: Host(`yelb-1.my-domain.net`) services: - kind: Service name: yelb-ui-1 passHostHeader: true namespace: yelb port: 80 apiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: yelb-ingressroute-2 namespace: yelb-2 spec: entryPoints: - web routes: - kind: Rule match: Host(`yelb-2.my-domain.net`) services: - kind: Service name: yelb-ui-2 passHostHeader: true namespace: yelb-2 port: 80 The two IngressRoutes applied:\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium/test-apps/yelb$ k get ingressroutes.traefik.io -n yelb NAME AGE yelb-ingressroute-1 2d20h andreasm@linuxmgmt01:~/prod-cluster-1/cilium/test-apps/yelb$ k get ingressroutes.traefik.io -n yelb-2 NAME AGE yelb-ingressroute-2 2d16h Now I can access both of them using their own dns records:\nYelb-1\nYelb-2\nTraefik in front of my Home Assistant server # Another requirement I had was to expose my Home Assistant server using Traefik, including MQTT. This is how I configured Traefik to handle this.\nHome Assistant port 8123 # As Home Assistant is running outside my Kubernetes cluster I needed to create an ExternalName service in my Kubernetes cluster for Traefik to use when forwarding requests to my \u0026ldquo;external\u0026rdquo; Home Assistant server.\nkind: Service apiVersion: v1 metadata: labels: k8s-app: external-homeassistant name: external-homeassistant namespace: traefik spec: type: ExternalName ports: - name: homeassistant port: 8123 targetPort: 8123 protocol: TCP externalName: 10.100.2.14 selector: app.kubernetes.io/instance: traefik app.kubernetes.io/name: traefik The IP is the IP of my Home Assistant server and the port it is listening on. I decided to place the service in the same namespace as Traefik as Home Assistant is not residing in any namespaces in my Kubernetes cluster.\nFor this to work I needed to make sure my Traefik installation had this value enabled in my value.yaml config before running the helm upgrade of the Traefik installation:\nproviders: kubernetesCRD: # -- Allows to reference ExternalName services in IngressRoute allowExternalNameServices: true Here is the service after it has been applied:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik/hass$ k get svc -n traefik NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE external-homeassistant ExternalName \u0026lt;none\u0026gt; 10.100.2.14 8123/TCP 42h Now I needed to create a middleware to redirect all http requests to https:\napiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: hass-redirectscheme namespace: traefik spec: redirectScheme: scheme: https permanent: true And finally the IngressRoute which routes the requests to the HomeAssistant Externalname service and TLS termination using my wildcard certificate:\napiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: homeassistant-ingressroute namespace: traefik spec: entryPoints: - websecure routes: - match: Host(`hass.my-domain.net`) kind: Rule middlewares: - name: hass-redirectscheme namespace: traefik services: - name: external-homeassistant kind: Service port: 8123 tls: secretName: net-tls-prod Thats it, now I can access my Home Assistant over Traefik with TLS termination. And I dont have to worry about certificate expiration as the certificate will be automatically updated by Cert-Manager.\nThe DNS record is pointing to the ip I have decided to use for this purpose. Same concept as earlier.\nHome Assistant MQTT 1883 # I am also running MQTT in Home Assistant to support a bunch of devices, even remote devices (not in the same house). So I wanted to use Traefik for that also. This is how I configured Traefik to handle that:\nI needed to create a new entrypoint in Traefik with port 1883 called mqtt. So I edited the Traefik value yaml and updated it accordingly. Then ran Helm upgrade on the Traefik installation. Below is te config I added:\nports: mqtt: port: 1883 protocol: TCP expose: true exposedPort: 1883 Now my Traefik pods also includes the port 1883:\nContainers: traefik: Container ID: containerd://edf07e67ade4b005e7a7f8ac8a0991b2793c9320cabc35b6a5ea3c6271d63e6d Image: docker.io/traefik:v2.10.6 Image ID: docker.io/library/traefik@sha256:1957e3314f435c85b3a19f7babd53c630996aa1af65d1f479d75539251b1e112 Ports: 9100/TCP, 1883/TCP, 9000/TCP, 8000/TCP, 8443/TCP Host Ports: 0/TCP, 0/TCP, 0/TCP, 0/TCP, 0/TCP Args: --global.checknewversion --global.sendanonymoususage --entrypoints.metrics.address=:9100/tcp --entrypoints.mqtt.address=:1883/tcp --entrypoints.traefik.address=:9000/tcp --entrypoints.web.address=:8000/tcp --entrypoints.websecure.address=:8443/tcp --api.dashboard=true --ping=true --metrics.prometheus=true --metrics.prometheus.entrypoint=metrics --metrics.prometheus.addRoutersLabels=true --metrics.prometheus.addEntryPointsLabels=true --metrics.prometheus.addServicesLabels=true --metrics.prometheus.buckets=0.1,0.3,1.2,5.0,10.0 --providers.kubernetescrd --providers.kubernetescrd.allowCrossNamespace=true --providers.kubernetescrd.allowExternalNameServices=true --providers.kubernetesingress --providers.kubernetesingress.allowExternalNameServices=true --entrypoints.websecure.http.tls=true This service is not exposed to the internet, so I decided to create a third Service using another subnet for internal services, that is services within my network, but not exposed to the internet.\nI then created a DNS record for the mqtt service in this IP address. Below is the service I am using for mqtt:\napiVersion: v1 kind: Service metadata: annotations: io.cilium/lb-ipam-ips: \u0026#34;10.150.20.10\u0026#34; name: traefik-internal-pool-2 labels: env: traefik-pool-2 namespace: traefik spec: ports: - name: web port: 80 protocol: TCP targetPort: web - name: websecure port: 443 protocol: TCP targetPort: websecure - name: mqtt port: 1883 protocol: TCP targetPort: mqtt selector: app.kubernetes.io/name: traefik type: LoadBalancer This Service includes the entrypoints web 80, websecure 443 AND the newly created entrypoiint mqtt 1883. Then I can reuse it for other internal purposes also.\nNow I can go ahead and create another ExternalName:\nkind: Service apiVersion: v1 metadata: labels: k8s-app: mqtt-homeassistant name: mqtt-homeassistant namespace: traefik spec: type: ExternalName ports: - name: mqtt-homeassistant port: 1883 targetPort: 1883 protocol: TCP externalName: 10.100.2.14 selector: app.kubernetes.io/instance: traefik app.kubernetes.io/name: traefik This is also pointing to the IP of my Home Assistant server but using port 1883 instead.\nLast step is to create a TCP IngressRoute like this:\napiVersion: traefik.io/v1alpha1 kind: IngressRouteTCP metadata: name: homeassistant-mqtt-ingressroute namespace: traefik spec: entryPoints: - mqtt routes: - match: ClientIP(`172.20.1.0/24`) services: - name: mqtt-homeassistant port: 1883 I can now go ahead and repoint all my mqtt clients to point to the DNS record I have created using the external IP above.\nTraefik and Harbor Registry # The last usecase I had for Traefik this round is my Harbor registry. I will quickly show how I done that here.\nI deploy Harbor using Helm, below is the steps to add the repo and my value.yaml I am using:\nhelm repo add harbor https://helm.goharbor.io helm repo update Here is my Harbor Helm value yaml file:\nexpose: type: clusterIP tls: enabled: false certSource: secret secret: secretName: \u0026#34;net-tls-prod\u0026#34; auto: commonName: registry.my-domain.net clusterIP: name: harbor ports: httpPort: 80 httpsPort: 443 externalURL: \u0026#34;https://registry.my-domain.net\u0026#34; harborAdminPassword: \u0026#34;password\u0026#34; persistence: enabled: true # Setting it to \u0026#34;keep\u0026#34; to avoid removing PVCs during a helm delete # operation. Leaving it empty will delete PVCs after the chart deleted # (this does not apply for PVCs that are created for internal database # and redis components, i.e. they are never deleted automatically) resourcePolicy: \u0026#34;keep\u0026#34; persistentVolumeClaim: registry: # Use the existing PVC which must be created manually before bound, # and specify the \u0026#34;subPath\u0026#34; if the PVC is shared with other components existingClaim: \u0026#34;\u0026#34; # Specify the \u0026#34;storageClass\u0026#34; used to provision the volume. Or the default # StorageClass will be used (the default). # Set it to \u0026#34;-\u0026#34; to disable dynamic provisioning storageClass: \u0026#34;nfs-client\u0026#34; subPath: \u0026#34;\u0026#34; accessMode: ReadWriteOnce size: 50Gi annotations: {} database: existingClaim: \u0026#34;\u0026#34; storageClass: \u0026#34;nfs-client\u0026#34; subPath: \u0026#34;postgres-storage\u0026#34; accessMode: ReadWriteOnce size: 1Gi annotations: {} portal: tls: existingSecret: net-tls-prod Then I install Harbor using Helm, and it should end up like this, only ClusterIP services:\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik/harbor$ k get svc -n harbor NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE harbor ClusterIP 10.21.152.146 \u0026lt;none\u0026gt; 80/TCP 29h harbor-core ClusterIP 10.21.89.119 \u0026lt;none\u0026gt; 80/TCP 29h harbor-database ClusterIP 10.21.174.146 \u0026lt;none\u0026gt; 5432/TCP 29h harbor-jobservice ClusterIP 10.21.191.45 \u0026lt;none\u0026gt; 80/TCP 29h harbor-portal ClusterIP 10.21.71.241 \u0026lt;none\u0026gt; 80/TCP 29h harbor-redis ClusterIP 10.21.131.55 \u0026lt;none\u0026gt; 6379/TCP 29h harbor-registry ClusterIP 10.21.90.29 \u0026lt;none\u0026gt; 5000/TCP,8080/TCP 29h harbor-trivy ClusterIP 10.21.6.124 \u0026lt;none\u0026gt; 8080/TCP 29h I want to expose my Harbor registry to the Internet so I will be using the Service with the corresponding externalIP I am using to expose things to Internet. This will also be the same externalIP as I am using for my Home Automation exposure. This means I can expose several services to Internet using the same port, like 443, no need to create custom ports etc. Traefik will happily handle the requests coming to the respective DNS records as long as I have configured it to listen \u0026#x1f604;\nNow I just need to create a middleware to redirect all http to https and the IngressRoute itself.\napiVersion: traefik.io/v1alpha1 kind: Middleware metadata: name: harbor-redirectscheme namespace: harbor spec: redirectScheme: scheme: https permanent: true Then the IngressRoute:\napiVersion: traefik.io/v1alpha1 kind: IngressRoute metadata: name: harbor-ingressroute namespace: harbor spec: entryPoints: - websecure routes: - match: Host(`registry.my-domain.net`) kind: Rule middlewares: - name: harbor-redirectscheme namespace: harbor services: - name: harbor-portal kind: Service port: 80 - match: Host(`registry.my-domain.net`) \u0026amp;\u0026amp; PathPrefix(`/api/`, `/c/`, `/chartrepo/`, `/service/`, `/v2/`) kind: Rule middlewares: - name: harbor-redirectscheme namespace: harbor services: - name: harbor kind: Service port: 80 tls: secretName: net-tls-prod Now, let me see if I can reach Harbor:\nAnd can I login via Docker?\nandreasm@linuxmgmt01:~/prod-cluster-1/traefik/harbor$ docker login registry.my-domain.net Username: andreasm Password: WARNING! Your password will be stored unencrypted in /home/andreasm/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Summary # I found Traefik in combination with Cilium a very pleasant experience. The ease of creating IP Pools in Cilium and using BGP to advertise the host routes. How I could configure and manage Traefik to use different external IP entrypoints covering my needs like ip separation. The built-in Traefik dashboard, using Grafana for dashboard creation using Prometheus metrics was very nice. I feel very confident that Traefik is one of my go-to reverse proxies going forward. By deploying Traefik on my Kubernetes cluster I also achieved high-availability and scalability. When I started out with Traefik I found it a bit \u0026ldquo;difficult\u0026rdquo;, as I mention in the beginning of this post also, but after playing around with it for a while and got the terminology under my skin I find Traefik to be quite easy to manage and operate. Traefik has a good community out there, which also helped out getting the help I needed when I was stuck.\nThis post is not meant to be an exhaustive list of Traefik capabilities, this post is just scraping the surface of what Traefik is capable of, so I will most likely create a follow up post when I dive into more deeper and advanced topics with Traefik.\n","date":"26 December 2023","externalUrl":null,"permalink":"/2023/12/26/traefik-proxy-in-kubernetes/","section":"Posts","summary":"In this post I will go through how I have configured and run Traefik in my Kubernetes lab.","title":"Traefik Proxy in Kubernetes","type":"posts"},{"content":" About Cilium # Instead of me using my own words I will just copy the text from the official Cilium website:\neBPF-based Networking, Observability, Security # Cilium is an open source, cloud native solution for providing, securing, and observing network connectivity between workloads, fueled by the revolutionary Kernel technology eBPF\nNow what is eBPF?\nFrom ebpf.io\nDynamically program the kernel for efficient networking, observability, tracing, and security # What is eBPF? # eBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in a privileged context such as the operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules.\nAs it is always interesting to learn new technology I thought writing a post about Cilium was about time. At first look Cilium is kind of a Swiss Army knife with a lot interesting features. I will go through this post beginning with a basic installation of Cilium on a new cluster (upstream K8s based on Ubuntu nodes). Then I will continune with some of the features I found interesting, and needed myself in my lab, and how to enable and configure them.\nThis post will be divided into dedicated sections for the installtion part and the different features respectively, starting with the installation of Cilium as the CNI in my Kubernetes cluster.\nPreparations # This post assumes the following:\nAlready prepared the Kubernetes nodes with all the software installed ready to do the kubeadm init. A jumphost or Linux mgmt vm/server to operate from Helm installed and configured on the Linux jumphost Kubectl installed on the Linux jumphost Cilium CLI installed on the Linux jumphost Cilium-cli is installed using this command:\nCILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt) CLI_ARCH=amd64 if [ \u0026#34;$(uname -m)\u0026#34; = \u0026#34;aarch64\u0026#34; ]; then CLI_ARCH=arm64; fi curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum} For more information have a look here\nBelow is my lab\u0026rsquo;s topology for this post:\nInstallation of Cilium # Cililum can be installed using Helm or using Ciliums nifty cilium-cli tool.\nOne can use Helm to configure/install features but also the Cilium cli tool. In my post I will mostly use Helm when adding some features or changing certain settings and cilium-cli for others just to showcase how easy it is to use cilium cli for certain features/tasks.\nAccording to the official docs:\nInstall the latest version of the Cilium CLI. The Cilium CLI can be used to install Cilium, inspect the state of a Cilium installation, and enable/disable various features (e.g. clustermesh, Hubble).\nThe first feature of Cilium in this post is how it can fully replace kube-proxy by providing distributed load balancing using eBPF. Naturally I would like to use this feature. This means I need to deploy my Kubernetes cluster without kube-proxy. That is easiest done during the initial upbringing of the Kubernetes cluster. It can be done post-upringing also, see more info here\nkubeadm init with no-kube-proxy # To bring up my Kubernetes cluster without kube-proxy, this is the command I will use on my first control-plane node:\nsudo kubeadm init --pod-network-cidr=10.22.0.0/16 --service-cidr=10.23.0.0/16 --control-plane-endpoint \u0026#34;test-cluster-1.my-doamin.net\u0026#34; --upload-certs --skip-phases=addon/kube-proxy --cri-socket unix:///var/run/containerd/containerd.sock This is the parameter to disable kube-proxy \u0026ndash;skip-phases=addon/kube-proxy\nI1219 14:08:17.376790 13327 version.go:256] remote version is much newer: v1.29.0; falling back to: stable-1.28 [init] Using Kubernetes version: v1.28.4 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; W1219 14:08:33.520592 13327 checks.go:835] detected that the sandbox image \u0026#34;registry.k8s.io/pause:3.5\u0026#34; of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \u0026#34;registry.k8s.io/pause:3.9\u0026#34; as the CRI sandbox image. [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master-01 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local test-cluster-1.my-domain.net] and IPs [10.23.0.1 10.160.1.10] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master-01 localhost] and IPs [10.160.1.10 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master-01 localhost] and IPs [10.160.1.10 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [kubelet-check] Initial timeout of 40s passed. [apiclient] All control plane components are healthy after 106.047476 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Storing the certificates in Secret \u0026#34;kubeadm-certs\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [upload-certs] Using certificate key: 3c9fa959a7538baaaf484e931ade45fbad07934dc40d456cae54839a7d888715 [mark-control-plane] Marking the node k8s-master-01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node k8s-master-01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: q495cj.apdasczda14j87tc [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join test-cluster-1.my-domain.net:6443 --token q4da14j87tc \\ --discovery-token-ca-cert-hash sha256: \\ --control-plane --certificate-key Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \u0026#34;kubeadm init phase upload-certs --upload-certs\u0026#34; to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join test-cluster-1.my-domain.net:6443 --token q4aczda14j87tc \\ --discovery-token-ca-cert-hash sha256: NB, notice that it \u0026ldquo;complains\u0026rdquo; No kubeproxy.config.k8s.io/v1alpha1 config is loaded. Continuing without it: configmaps \u0026ldquo;kube-proxy\u0026rdquo;\nNow on my worker nodes:\nkubeadm join test-cluster-1.my-domain.net:6443 --token q414j87tc \\ --discovery-token-ca-cert-hash sha256:edf4d18883f94f0b5aa646001606147 [preflight] Running pre-flight checks [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -o yaml\u0026#39; W1219 16:25:57.203844 1279 configset.go:78] Warning: No kubeproxy.config.k8s.io/v1alpha1 config is loaded. Continuing without it: configmaps \u0026#34;kube-proxy\u0026#34; is forbidden: User \u0026#34;system:bootstrap:q495cj\u0026#34; cannot get resource \u0026#34;configmaps\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;kube-system\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Starting the kubelet [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run \u0026#39;kubectl get nodes\u0026#39; on the control-plane to see this node join the cluster. NB, notice that it \u0026ldquo;complains\u0026rdquo; No kubeproxy.config.k8s.io/v1alpha1 config is loaded. Continuing without it: configmaps \u0026ldquo;kube-proxy\u0026rdquo;\nWhen all worker nodes has been joined:\nandreasm@linuxmgmt01:~/test-cluster-1$ k get nodes NAME STATUS ROLES AGE VERSION k8s-master-01 Ready control-plane 135m v1.28.2 k8s-worker-01 Ready \u0026lt;none\u0026gt; 12s v1.28.2 k8s-worker-02 Ready \u0026lt;none\u0026gt; 38s v1.28.2 k8s-worker-03 Ready \u0026lt;none\u0026gt; 4m28s v1.28.2 Notice they are not ready. CoreDNS is pending and there is no CNI in place to cover IPAM etc..\nandreasm@linuxmgmt01:~/test-cluster-1$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-5dd5756b68-c5xml 0/1 Pending 0 35m kube-system coredns-5dd5756b68-fgdzj 0/1 Pending 0 35m kube-system etcd-k8s-master-01 1/1 Running 0 35m kube-system kube-apiserver-k8s-master-01 1/1 Running 0 35m kube-system kube-controller-manager-k8s-master-01 1/1 Running 1 (19m ago) 35m kube-system kube-scheduler-k8s-master-01 1/1 Running 1 (19m ago) 35m Now its time to jump over to my jumphost where I will do all the remaining configurations/interactions with my test-cluster-1.\nInstall Cilium CNI # From my jumphost I already have all the tools I need to deploy Cilium. To install the Cilium CNI I will just use the cilium-cli tool as it is so easy. With a very short command it will automatically install Cilium on all my worker/control-plane nodes. The cilium-cli will act according to the kube context you are in, so make sure you are in the correct context (the context that needs Cilium to be installed):\nandreasm@linuxmgmt01:~/test-cluster-1$ k config current-context test-cluster-1-admin@kubernetes andreasm@linuxmgmt01:~/test-cluster-1$ cilium install --version 1.14.5 ℹ️ Using Cilium version 1.14.5 🔮 Auto-detected cluster name: test-cluster-1 🔮 Auto-detected kube-proxy has not been installed ℹ️ Cilium will fully replace all functionalities of kube-proxy Thats it\u0026hellip;. \u0026#x1f604;\nVersion 1.14.5 is the latest stable at the writing of this post.\nInstall Cilium on a cluster with no kube-proxy # If I have prepared my cluster as above with no kube-proxy I need to install Cilium using the following command:\nAPI_SERVER_IP=10.160.1.111 API_SERVER_PORT=6443 helm install cilium cilium/cilium --version 1.14.5 -f cilium.1.14.5.values-prod-cluster-1.yaml \\ --namespace kube-system \\ --set kubeProxyReplacement=strict \\ --set k8sServiceHost=${API_SERVER_IP} \\ --set k8sServicePort=${API_SERVER_PORT} Where the API_SERVER_PORT is one of my k8s control plane node (I did try to use the loadbalanced IP for the k8s api endpoint as I have 3 control plane nodes but that did not work out so I went with the IP of my first cp node). The value file is the value file I am using to set all the Cilium settings, more on that later.\nNow, whats inside my Kubernetes cluster now:\nandreasm@linuxmgmt01:~/test-cluster-1$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system cilium-6gx5b 1/1 Running 0 11m kube-system cilium-bsqzw 1/1 Running 1 11m kube-system cilium-ct8n4 1/1 Running 0 53s kube-system cilium-operator-545dc68d55-fsh6s 1/1 Running 1 11m kube-system cilium-v7rdz 1/1 Running 0 5m9s kube-system coredns-5dd5756b68-j9vwm 1/1 Running 0 77s kube-system coredns-5dd5756b68-mjbzk 1/1 Running 0 78s kube-system etcd-k8s-master-01 1/1 Running 0 136m kube-system hubble-relay-d478c79c8-pbn4v 1/1 Running 0 16m kube-system kube-apiserver-k8s-master-01 1/1 Running 0 136m kube-system kube-controller-manager-k8s-master-01 1/1 Running 1 (120m ago) 136m kube-system kube-scheduler-k8s-master-01 1/1 Running 1 (120m ago) 136m Everything is up and running. The Cilium cli contains a lot of useful features. Like checking the status of Cilium. Lets test that:\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium status /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: disabled \\__/ ClusterMesh: disabled DaemonSet cilium Desired: 4, Ready: 4/4, Available: 4/4 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 4 cilium-operator Running: 1 Cluster Pods: 2/2 managed by Cilium Helm chart version: 1.14.5 Image versions cilium-operator quay.io/cilium/operator-generic:v1.14.5@sha256:303f9076bdc73b3fc32aaedee64a14f6f44c8bb08ee9e3956d443021103ebe7a: 1 cilium quay.io/cilium/cilium:v1.14.5@sha256:d3b287029755b6a47dee01420e2ea469469f1b174a2089c10af7e5e9289ef05b: 4 Looks great\nDid you notice above that the Cilium installer discovered there was no kube-proxy and that it told me it will replace all the feature of kube-proxy? Well it did.. Lets check the config of Cilium and see if that is also reflected there. Look after this key-value:\nkube-proxy-replacement strict\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium config view agent-not-ready-taint-key node.cilium.io/agent-not-ready arping-refresh-period 30s auto-direct-node-routes false bpf-lb-external-clusterip false bpf-lb-map-max 65536 bpf-lb-sock false bpf-map-dynamic-size-ratio 0.0025 bpf-policy-map-max 16384 bpf-root /sys/fs/bpf cgroup-root /run/cilium/cgroupv2 cilium-endpoint-gc-interval 5m0s cluster-id 0 cluster-name test-cluster-1 cluster-pool-ipv4-cidr 10.0.0.0/8 cluster-pool-ipv4-mask-size 24 cni-exclusive true cni-log-file /var/run/cilium/cilium-cni.log cnp-node-status-gc-interval 0s custom-cni-conf false debug false debug-verbose disable-cnp-status-updates true egress-gateway-reconciliation-trigger-interval 1s enable-auto-protect-node-port-range true enable-bgp-control-plane false enable-bpf-clock-probe false enable-endpoint-health-checking true enable-health-check-nodeport true enable-health-checking true enable-hubble true enable-ipv4 true enable-ipv4-big-tcp false enable-ipv4-masquerade true enable-ipv6 false enable-ipv6-big-tcp false enable-ipv6-masquerade true enable-k8s-networkpolicy true enable-k8s-terminating-endpoint true enable-l2-neigh-discovery true enable-l7-proxy true enable-local-redirect-policy false enable-policy default enable-remote-node-identity true enable-sctp false enable-svc-source-range-check true enable-vtep false enable-well-known-identities false enable-xt-socket-fallback true external-envoy-proxy false hubble-disable-tls false hubble-listen-address :4244 hubble-socket-path /var/run/cilium/hubble.sock hubble-tls-cert-file /var/lib/cilium/tls/hubble/server.crt hubble-tls-client-ca-files /var/lib/cilium/tls/hubble/client-ca.crt hubble-tls-key-file /var/lib/cilium/tls/hubble/server.key identity-allocation-mode crd identity-gc-interval 15m0s identity-heartbeat-timeout 30m0s install-no-conntrack-iptables-rules false ipam cluster-pool ipam-cilium-node-update-rate 15s k8s-client-burst 10 k8s-client-qps 5 kube-proxy-replacement strict kube-proxy-replacement-healthz-bind-address mesh-auth-enabled true mesh-auth-gc-interval 5m0s mesh-auth-queue-size 1024 mesh-auth-rotated-identities-queue-size 1024 monitor-aggregation medium monitor-aggregation-flags all monitor-aggregation-interval 5s node-port-bind-protection true nodes-gc-interval 5m0s operator-api-serve-addr 127.0.0.1:9234 preallocate-bpf-maps false procfs /host/proc proxy-connect-timeout 2 proxy-max-connection-duration-seconds 0 proxy-max-requests-per-connection 0 proxy-prometheus-port 9964 remove-cilium-node-taints true routing-mode tunnel set-cilium-is-up-condition true set-cilium-node-taints true sidecar-istio-proxy-image cilium/istio_proxy skip-cnp-status-startup-clean false synchronize-k8s-nodes true tofqdns-dns-reject-response-code refused tofqdns-enable-dns-compression true tofqdns-endpoint-max-ip-per-hostname 50 tofqdns-idle-connection-grace-period 0s tofqdns-max-deferred-connection-deletes 10000 tofqdns-proxy-response-max-delay 100ms tunnel-protocol vxlan unmanaged-pod-watcher-interval 15 vtep-cidr vtep-endpoint vtep-mac vtep-mask write-cni-conf-when-ready /host/etc/cni/net.d/05-cilium.conflist One can also verify with this command:\nandreasm@linuxmgmt01:~/test-cluster-1$ kubectl -n kube-system exec ds/cilium -- cilium status | grep KubeProxyReplacement Defaulted container \u0026#34;cilium-agent\u0026#34; out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) KubeProxyReplacement: Strict [ens18 10.160.1.11 (Direct Routing)] The feature to easily list all features and the status on them is valuable and a really helpful feature.\nIt took a couple of seconds and Cilium CNI was installed. Now the fun begins to explore some of the features. Lets tag along\nEnabling features using Helm # When I installed Cilium using the cilium-cli tool, it actually deploys using Helm in the background. Lets see if there is a Helm manifest in the kube-system:\nandreasm@linuxmgmt01:~/test-cluster-1$ helm list -n kube-system NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION cilium\tkube-system\t1 2023-12-19 13:50:40.121866679 +0000 UTC\tdeployed\tcilium-1.14.5\t1.14.5 Well there it is..\nThat makes it all more interesting. As I will use Helm to update certain parameters going forward in this post I will take a \u0026ldquo;snapshot\u0026rdquo; of the current values in the manifest above and altered in the next sections when I enabel additional features. How does the values look like now?\nFrom the configMap cilium-config apiVersion: v1 data: agent-not-ready-taint-key: node.cilium.io/agent-not-ready arping-refresh-period: 30s auto-direct-node-routes: \u0026#34;false\u0026#34; bpf-lb-external-clusterip: \u0026#34;false\u0026#34; bpf-lb-map-max: \u0026#34;65536\u0026#34; bpf-lb-sock: \u0026#34;false\u0026#34; bpf-map-dynamic-size-ratio: \u0026#34;0.0025\u0026#34; bpf-policy-map-max: \u0026#34;16384\u0026#34; bpf-root: /sys/fs/bpf cgroup-root: /run/cilium/cgroupv2 cilium-endpoint-gc-interval: 5m0s cluster-id: \u0026#34;0\u0026#34; cluster-name: test-cluster-1 cluster-pool-ipv4-cidr: 10.0.0.0/8 cluster-pool-ipv4-mask-size: \u0026#34;24\u0026#34; cni-exclusive: \u0026#34;true\u0026#34; cni-log-file: /var/run/cilium/cilium-cni.log cnp-node-status-gc-interval: 0s custom-cni-conf: \u0026#34;false\u0026#34; debug: \u0026#34;false\u0026#34; debug-verbose: \u0026#34;\u0026#34; disable-cnp-status-updates: \u0026#34;true\u0026#34; egress-gateway-reconciliation-trigger-interval: 1s enable-auto-protect-node-port-range: \u0026#34;true\u0026#34; enable-bgp-control-plane: \u0026#34;false\u0026#34; enable-bpf-clock-probe: \u0026#34;false\u0026#34; enable-endpoint-health-checking: \u0026#34;true\u0026#34; enable-health-check-nodeport: \u0026#34;true\u0026#34; enable-health-checking: \u0026#34;true\u0026#34; enable-hubble: \u0026#34;true\u0026#34; enable-ipv4: \u0026#34;true\u0026#34; enable-ipv4-big-tcp: \u0026#34;false\u0026#34; enable-ipv4-masquerade: \u0026#34;true\u0026#34; enable-ipv6: \u0026#34;false\u0026#34; enable-ipv6-big-tcp: \u0026#34;false\u0026#34; enable-ipv6-masquerade: \u0026#34;true\u0026#34; enable-k8s-networkpolicy: \u0026#34;true\u0026#34; enable-k8s-terminating-endpoint: \u0026#34;true\u0026#34; enable-l2-neigh-discovery: \u0026#34;true\u0026#34; enable-l7-proxy: \u0026#34;true\u0026#34; enable-local-redirect-policy: \u0026#34;false\u0026#34; enable-policy: default enable-remote-node-identity: \u0026#34;true\u0026#34; enable-sctp: \u0026#34;false\u0026#34; enable-svc-source-range-check: \u0026#34;true\u0026#34; enable-vtep: \u0026#34;false\u0026#34; enable-well-known-identities: \u0026#34;false\u0026#34; enable-xt-socket-fallback: \u0026#34;true\u0026#34; external-envoy-proxy: \u0026#34;false\u0026#34; hubble-disable-tls: \u0026#34;false\u0026#34; hubble-listen-address: :4244 hubble-socket-path: /var/run/cilium/hubble.sock hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key identity-allocation-mode: crd identity-gc-interval: 15m0s identity-heartbeat-timeout: 30m0s install-no-conntrack-iptables-rules: \u0026#34;false\u0026#34; ipam: cluster-pool ipam-cilium-node-update-rate: 15s k8s-client-burst: \u0026#34;10\u0026#34; k8s-client-qps: \u0026#34;5\u0026#34; kube-proxy-replacement: strict kube-proxy-replacement-healthz-bind-address: \u0026#34;\u0026#34; mesh-auth-enabled: \u0026#34;true\u0026#34; mesh-auth-gc-interval: 5m0s mesh-auth-queue-size: \u0026#34;1024\u0026#34; mesh-auth-rotated-identities-queue-size: \u0026#34;1024\u0026#34; monitor-aggregation: medium monitor-aggregation-flags: all monitor-aggregation-interval: 5s node-port-bind-protection: \u0026#34;true\u0026#34; nodes-gc-interval: 5m0s operator-api-serve-addr: 127.0.0.1:9234 preallocate-bpf-maps: \u0026#34;false\u0026#34; procfs: /host/proc proxy-connect-timeout: \u0026#34;2\u0026#34; proxy-max-connection-duration-seconds: \u0026#34;0\u0026#34; proxy-max-requests-per-connection: \u0026#34;0\u0026#34; proxy-prometheus-port: \u0026#34;9964\u0026#34; remove-cilium-node-taints: \u0026#34;true\u0026#34; routing-mode: tunnel set-cilium-is-up-condition: \u0026#34;true\u0026#34; set-cilium-node-taints: \u0026#34;true\u0026#34; sidecar-istio-proxy-image: cilium/istio_proxy skip-cnp-status-startup-clean: \u0026#34;false\u0026#34; synchronize-k8s-nodes: \u0026#34;true\u0026#34; tofqdns-dns-reject-response-code: refused tofqdns-enable-dns-compression: \u0026#34;true\u0026#34; tofqdns-endpoint-max-ip-per-hostname: \u0026#34;50\u0026#34; tofqdns-idle-connection-grace-period: 0s tofqdns-max-deferred-connection-deletes: \u0026#34;10000\u0026#34; tofqdns-proxy-response-max-delay: 100ms tunnel-protocol: vxlan unmanaged-pod-watcher-interval: \u0026#34;15\u0026#34; vtep-cidr: \u0026#34;\u0026#34; vtep-endpoint: \u0026#34;\u0026#34; vtep-mac: \u0026#34;\u0026#34; vtep-mask: \u0026#34;\u0026#34; write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: cilium meta.helm.sh/release-namespace: kube-system creationTimestamp: \u0026#34;2023-12-19T13:50:42Z\u0026#34; labels: app.kubernetes.io/managed-by: Helm name: cilium-config namespace: kube-system resourceVersion: \u0026#34;4589\u0026#34; uid: f501a3d0-8b33-43af-9fae-63625dcd6df1 These are the two settings that have been changed from being completely default:\ncluster-name: test-cluster-1 kube-proxy-replacement: strict I prefer editing the changes in a dedicated value.yaml file and run helm upgrade -f value.yaml each time I want to do a change so going forward I will be the adding/changing certain settings in this value.yaml file to update the settings in Cilium.\nI grabbed the default value yaml from the Helm repo and use that to alter the settings in the next sections.\nEnabling features using Cilium cli # The cilium-cli can also be used to enable disable features certain features like Hubble and clustermesh. An example on how to install Hubble with cilium-cli is shown below in the next chapter, but I can also use Helm to achieve the same. I enable Hubble using cilium-cli just to show how easy it is.\nBut as I mention above, I prefer using the Helm method as I can keep better track of the settings and have them consistent each time I alter an update and refering to my value.yaml file.\nObservability and flow-monitoring - Hubble Observability # Cilium comes with a very neat monitor tool out of the box called Hubble. It is enabled by default but I need to enable the Hubble Relay and Hubble UI feature to get the information from my nodes, pods etc available in a nice dashboard (Hubble UI), so this is a feature I certainly want to enable as one of the first features to test out.\nMore details here and here\nIf I do a quick check with cilium cli now I can see the Hubble Relay is disabled.\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium status /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: disabled \\__/ ClusterMesh: disabled DaemonSet cilium Desired: 4, Ready: 4/4, Available: 4/4 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 4 cilium-operator Running: 1 Cluster Pods: 2/2 managed by Cilium Helm chart version: 1.14.5 Image versions cilium quay.io/cilium/cilium:v1.14.5@sha256:d3b287029755b6a47dee01420e2ea469469f1b174a2089c10af7e5e9289ef05b: 4 cilium-operator quay.io/cilium/operator-generic:v1.14.5@sha256:303f9076bdc73b3fc32aaedee64a14f6f44c8bb08ee9e3956d443021103ebe7a: 1 andreasm@linuxmgmt01:~/test-cluster-1$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-peer ClusterIP 10.23.182.223 \u0026lt;none\u0026gt; 443/TCP 69m kube-dns ClusterIP 10.23.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 109m To enable it, it is as simple as running this command:\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium hubble enable This command enables the Hubble Relay.\nLets check the stats of Cilium:\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium status /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: OK \\__/ ClusterMesh: disabled DaemonSet cilium Desired: 4, Ready: 4/4, Available: 4/4 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 4 cilium-operator Running: 1 hubble-relay Running: 1 Cluster Pods: 3/3 managed by Cilium Helm chart version: 1.14.5 Image versions cilium quay.io/cilium/cilium:v1.14.5@sha256:d3b287029755b6a47dee01420e2ea469469f1b174a2089c10af7e5e9289ef05b: 4 cilium-operator quay.io/cilium/operator-generic:v1.14.5@sha256:303f9076bdc73b3fc32aaedee64a14f6f44c8bb08ee9e3956d443021103ebe7a: 1 hubble-relay quay.io/cilium/hubble-relay:v1.14.5@sha256:dbef89f924a927043d02b40c18e417c1ea0e8f58b44523b80fef7e3652db24d4: 1 Hubble Relay OK\nNow I need to enable the Hubble UI.\nAgain, uisng Cilium-CLI its a very quick and simple operation:\nandreasm@linuxmgmt01:~$ cilium hubble enable --ui Lets check the services in my cluster:\nandreasm@linuxmgmt01:~$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-peer ClusterIP 10.23.182.223 \u0026lt;none\u0026gt; 443/TCP 6h19m hubble-relay ClusterIP 10.23.182.76 \u0026lt;none\u0026gt; 80/TCP 4h34m hubble-ui ClusterIP 10.23.31.4 \u0026lt;none\u0026gt; 80/TCP 42s kube-dns ClusterIP 10.23.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 6h59m Hubble Relay and Hubble UI service is enabled. The issue though is that they are exposed using clusterIP, I need to reach them from the outside of my cluster. Lets continue with the next feature to test: LB-IPAM.\nUsing Helm to enable Hubble Relay and Hubble-UI\nInstead of using clilium cli I would have enabled the Relay and UI in my value.yaml file and run the following command:\nandreasm@linuxmgmt01:~/test-cluster-1$ helm upgrade -n kube-system cilium cilium/cilium --version 1.14.5 -f cilium-values-feature-by-feature.yaml Release \u0026#34;cilium\u0026#34; has been upgraded. Happy Helming! NAME: cilium LAST DEPLOYED: Tue Dec 19 20:32:12 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 13 TEST SUITE: None NOTES: You have successfully installed Cilium with Hubble Relay and Hubble UI. Your release version is 1.14.5. For any further help, visit https://docs.cilium.io/en/v1.14/gettinghelp Where I have changed these settings in the value.yaml:\nrelay: # -- Enable Hubble Relay (requires hubble.enabled=true) enabled: true ...... ui: # -- Whether to enable the Hubble UI. enabled: true ........ hubble: # -- Enable Hubble (true by default). enabled: true ............ # -- Buffer size of the channel Hubble uses to receive monitor events. If this # value is not set, the queue size is set to the default monitor queue size. # eventQueueSize: \u0026#34;\u0026#34; # -- Number of recent flows for Hubble to cache. Defaults to 4095. # Possible values are: # 1, 3, 7, 15, 31, 63, 127, 255, 511, 1023, # 2047, 4095, 8191, 16383, 32767, 65535 # eventBufferCapacity: \u0026#34;4095\u0026#34; # -- Hubble metrics configuration. # See https://docs.cilium.io/en/stable/observability/metrics/#hubble-metrics # for more comprehensive documentation about Hubble metrics. metrics: # -- Configures the list of metrics to collect. If empty or null, metrics # are disabled. # Example: # # enabled: # - dns:query;ignoreAAAA # - drop # - tcp # - flow # - icmp # - http # # You can specify the list of metrics from the helm CLI: # # --set metrics.enabled=\u0026#34;{dns:query;ignoreAAAA,drop,tcp,flow,icmp,http}\u0026#34; # enabled: - dns:query;ignoreAAAA ### added these - drop ### added these - tcp ### added these - flow ### added these - icmp ### added these - http ### added these ......... andreasm@linuxmgmt01:~/test-cluster-1$ cilium status /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: OK \\__/ ClusterMesh: disabled Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 DaemonSet cilium Desired: 4, Ready: 4/4, Available: 4/4 Deployment hubble-ui Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 4 hubble-ui Running: 1 hubble-relay Running: 1 cilium-operator Running: 2 Cluster Pods: 4/4 managed by Cilium Helm chart version: 1.14.5 Image versions cilium quay.io/cilium/cilium:v1.14.5@sha256:d3b287029755b6a47dee01420e2ea469469f1b174a2089c10af7e5e9289ef05b: 4 hubble-ui quay.io/cilium/hubble-ui:v0.12.1@sha256:9e5f81ee747866480ea1ac4630eb6975ff9227f9782b7c93919c081c33f38267: 1 hubble-ui quay.io/cilium/hubble-ui-backend:v0.12.1@sha256:1f86f3400827a0451e6332262467f894eeb7caf0eb8779bd951e2caa9d027cbe: 1 hubble-relay quay.io/cilium/hubble-relay:v1.14.5@sha256:dbef89f924a927043d02b40c18e417c1ea0e8f58b44523b80fef7e3652db24d4: 1 cilium-operator quay.io/cilium/operator-generic:v1.14.5@sha256:303f9076bdc73b3fc32aaedee64a14f6f44c8bb08ee9e3956d443021103ebe7a: 2 I will get back to the Hubble UI later\u0026hellip;\nLB-IPAM # Exposing a service from Kubernetes to be accessible from outside the cluster can be done in a couple of ways:\nExporting the service by binding it to a node using NodePort (not scalable and manageable). Exporting the service using a servicetype of loadBalancer, only Layer4, though scalable. Usually requires external load balancer installed or some additional component installed and configured to support your Kubernetes platform. Exporting using Ingress, Layer7, requires a loadbalancer to provide exernal IP address Exporting using GatewayAPI (Ingress successor), requires a loadbalancer to provide exernal IP address. Cilium has really made it simple here, it comes with a built in LoadBalancer-IPAM. More info here.\nThis is already enabled, no feature to install or enable. The only thing I need to do is to configure an IP pool that will provide ip addresses from a defined subnet when I request a serviceType loadBalancer, Ingress or Gateway. We can configure multiple pools with different subnets, and configure a serviceSelector matching on labels or expressions.\nIn my lab I have already configured a couple of IP pools, using different subnets and different serviceSelectors so I can control which service gets IP addresses from which pool.\nA couple of example pools from my lab:\napiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumLoadBalancerIPPool metadata: name: \u0026#34;gateway-api-pool-10.150.14.x\u0026#34; spec: cidrs: - cidr: \u0026#34;10.150.14.0/24\u0026#34; serviceSelector: matchExpressions: - {key: io.kubernetes.service.namespace, operator: In, values: [harbor, booking]} --- apiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumLoadBalancerIPPool metadata: name: \u0026#34;lb-pool-prod.10.150.11.x\u0026#34; spec: cidrs: - cidr: \u0026#34;10.150.11.0/24\u0026#34; serviceSelector: matchExpressions: - {key: env, operator: In, values: [prod]} The first pool will only provide IP addresses to services being deployed in any of the two namespaces \u0026ldquo;harbor\u0026rdquo; or \u0026ldquo;booking\u0026rdquo;. This is an \u0026ldquo;OR\u0026rdquo; selection, not AND, meaning it can be deployed in any of the namespaces, not both. The second will use lablels and match on the key-value: env=prod.\nBear in mind that these IP Pools will only listen for services (serviceType loadBalancer) not Ingress pr say. That means each time you create an Ingress or a Gateway the serviceType loadBalancer will be auto-created as a reaction to the Ingress/Gateway creation. So if you try to create labels on the Ingress/Gatewat object it will not be noticed by the LB-IPAM pool. Instead you can adjust the selection based on the namespace you know it will be created in, or use this label that is auto-created on the svc: \u0026ldquo;Labels: io.cilium.gateway/owning-gateway=\u0026ldquo;name-of-gateway\u0026rdquo;\u0026rdquo;\nAs soon as you have created an ip-pool, applied it, it will immediately start to serve requests by providing IP addresses to them. This is very nice. There is a small catch though. If I create IP Pools, as above, which is outside of my nodes subnet how does my network know how to reach these subnets? Creating static routes and pointing to my nodes that potentially holds these ip addresses? Nah.. Not scalable, nor manageable. Some kind of dynamic routing protocol would be best here, BGP or OSPF. Did I mention that Cilium also includes support for BGP out of the box?\nBGP Control Plane # Yes, you guessed it, Cilium includes BGP. A brilliant way of advertising all my IP pools. Creating many IP pools with a bunch of subnets have never been more fun. This is the same concept as I write about here, the biggest difference is that with Cilium this only needs to be enabled as a feature and then define a yaml to confgure the bgp settings. Nothing additional to install, just Plug\u0026rsquo;nPlay.\nFor more info on the BGP control plane, read here.\nFirst out, enable the BGP control plane feature. To enable it I will alter my Helm value.yaml file with this setting:\n# -- This feature set enables virtual BGP routers to be created via # CiliumBGPPeeringPolicy CRDs. bgpControlPlane: # -- Enables the BGP control plane. enabled: true Then run the command:\nhelm upgrade -n kube-system cilium cilium/cilium --version 1.14.5 -f cilium-values-feature-by-feature.yaml This will enable the bgp control plane feature:\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium config view agent-not-ready-taint-key node.cilium.io/agent-not-ready arping-refresh-period 30s auto-direct-node-routes false bpf-lb-external-clusterip false bpf-lb-map-max 65536 bpf-lb-sock false bpf-map-dynamic-size-ratio 0.0025 bpf-policy-map-max 16384 bpf-root /sys/fs/bpf cgroup-root /run/cilium/cgroupv2 cilium-endpoint-gc-interval 5m0s cluster-id 0 cluster-name test-cluster-1 cluster-pool-ipv4-cidr 10.0.0.0/8 cluster-pool-ipv4-mask-size 24 cni-exclusive true cni-log-file /var/run/cilium/cilium-cni.log cnp-node-status-gc-interval 0s custom-cni-conf false debug false disable-cnp-status-updates true egress-gateway-reconciliation-trigger-interval 1s enable-auto-protect-node-port-range true enable-bgp-control-plane true #### Here it is Now I need to create a yaml that contains the BGP peering info I need for my workers to peer to my upstream router. For reference I will paste my lab topology here again:\nWhen I apply my below BGPPeeringPolicy yaml, my nodes will enable a BGP peering session to the switch (their upstream bgp neighbor) they are connected to in the diagram above. This switch has also been configured to allow them as BGP neigbors. Please take into consideration creating some ip-prefix/route-maps so we dont accidentally advertise routes that confilcts, or should not be advertised into the network to prevent BGP blackholes etc\u0026hellip;\nHere is my BGP config I apply on my cluster:\napiVersion: \u0026#34;cilium.io/v2alpha1\u0026#34; kind: CiliumBGPPeeringPolicy metadata: name: 01-bgp-peering-policy spec: nodeSelector: matchLabels: bgp-policy: worker-nodes virtualRouters: - localASN: 64520 serviceSelector: matchExpressions: - {key: somekey, operator: NotIn, values: [\u0026#39;never-used-value\u0026#39;]} exportPodCIDR: false neighbors: - peerAddress: \u0026#39;10.160.1.1/24\u0026#39; peerASN: 64512 eBGPMultihopTTL: 10 connectRetryTimeSeconds: 120 holdTimeSeconds: 12 keepAliveTimeSeconds: 4 gracefulRestart: enabled: true restartTimeSeconds: 120 Here we can also configure a serviceSelector to prevent services we dont want to be advertised. I used used the example from the official docs to allow everything. If I also have a good BGP route-map config on my switch side or upstream bgp neighbour subnets that are not allowed will never be advertised.\nNow that I have applied it I can check the bgp peering status using the Cilium cli:\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium$ cilium bgp peers Node Local AS Peer AS Peer Address Session State Uptime Family Received Advertised k8s-prod-node-01 64520 64512 10.160.1.1 established 13h2m53s ipv4/unicast 47 6 ipv6/unicast 0 0 k8s-prod-node-02 64520 64512 10.160.1.1 established 13h2m25s ipv4/unicast 45 6 ipv6/unicast 0 0 k8s-prod-node-03 64520 64512 10.160.1.1 established 13h2m27s ipv4/unicast 43 6 ipv6/unicast 0 0 I can see some prefixes being Advertised and some being Received and the Session State is Established. I can also confirm that on my switch, and the routes they advertise:\nGUZ-SW-01# show ip bgp summary Peer Information Remote Address Remote-AS Local-AS State Admin Status --------------- --------- -------- ------------- ------------ 10.160.1.114 64520 64512 Established Start 10.160.1.115 64520 64512 Established Start 10.160.1.116 64520 64512 Established Start 172.18.1.1 64500 64512 Established Start GUZ-SW-01# show ip bgp Local AS : 64512 Local Router-id : 172.18.1.2 BGP Table Version : 1706 Status codes: * - valid, \u0026gt; - best, i - internal, e - external, s - stale Origin codes: i - IGP, e - EGP, ? - incomplete Network Nexthop Metric LocalPref Weight AsPath ------------------ --------------- ---------- ---------- ------ --------- * e 10.150.11.10/32 10.160.1.114 0 0 64520 i *\u0026gt;e 10.150.11.10/32 10.160.1.115 0 0 64520 i * e 10.150.11.10/32 10.160.1.116 0 0 64520 i * e 10.150.11.199/32 10.160.1.114 0 0 64520 i * e 10.150.11.199/32 10.160.1.115 0 0 64520 i *\u0026gt;e 10.150.11.199/32 10.160.1.116 0 0 64520 i * e 10.150.12.4/32 10.160.1.114 0 0 64520 i * e 10.150.12.4/32 10.160.1.115 0 0 64520 i *\u0026gt;e 10.150.12.4/32 10.160.1.116 0 0 64520 i * e 10.150.14.32/32 10.160.1.114 0 0 64520 i * e 10.150.14.32/32 10.160.1.115 0 0 64520 i *\u0026gt;e 10.150.14.32/32 10.160.1.116 0 0 64520 i * e 10.150.14.150/32 10.160.1.114 0 0 64520 i *\u0026gt;e 10.150.14.150/32 10.160.1.115 0 0 64520 i * e 10.150.14.150/32 10.160.1.116 0 0 64520 i * e 10.150.15.100/32 10.160.1.114 0 0 64520 i * e 10.150.15.100/32 10.160.1.115 0 0 64520 i *\u0026gt;e 10.150.15.100/32 10.160.1.116 0 0 64520 i Now I can just create my IP Pools, create some services and they should be immediately advertised and reachable in my network (unless they are being stopped by some route-maps ofcourse).\nNote, it will only advertise ip-addresses in use by a service, not the whole subnet I define in my IP-Pools. That means I will only see host-routes advertised (as seen above).\nLB-IPAM - does it actually loadbalance? # It says LoadBalancer IPAM, but does it actually loadbalance? Let me quicly put that to a test.\nI have exposed a web service using serviceType loadBalancer consisting of three simple nginx web pods.\nHere is the yaml I am using (think I grabbed it from the offical Cilium docs)\napiVersion: v1 kind: Service metadata: name: test-lb namespace: example labels: env: prod #### added this label to match with my ip pool spec: type: LoadBalancer ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: svc: test-lb --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: example spec: selector: matchLabels: svc: test-lb template: metadata: labels: svc: test-lb spec: containers: - name: web image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 readinessProbe: httpGet: path: / port: 80 Initially it deploys one pod, I will scale it up to three\nThey are running here, perfectly distributed across all my worker nodes:\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium$ k get pods -n example -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-698447f456-5xczj 1/1 Running 0 18s 10.0.0.239 k8s-prod-node-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-698447f456-plknk 1/1 Running 0 117s 10.0.4.167 k8s-prod-node-02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-698447f456-xs4jq 1/1 Running 0 18s 10.0.5.226 k8s-prod-node-03 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; And here is the LB service:\nexample test-lb LoadBalancer 10.21.69.190 10.150.11.48 80:31745/TCP Now let me do a curl against the LoadBalancer IP and see if something changes:\nEvery 0.5s: curl http://10.150.11.48 linuxmgmt01: Wed Dec 20 07:58:14 2023 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 567 100 567 0 0 184k 0 --:--:-- --:--:-- --:--:-- 276k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; Pod 2 ##### Notice this \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Pod 2 \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Every 0.5s: curl http://10.150.11.48 linuxmgmt01: Wed Dec 20 07:59:15 2023 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 567 100 567 0 0 110k 0 --:--:-- --:--:-- --:--:-- 138k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; Pod 1 ##### Notice this \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Pod 1 \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Every 0.5s: curl http://10.150.11.48 linuxmgmt01: Wed Dec 20 08:01:02 2023 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 567 100 567 0 0 553k 0 --:--:-- --:--:-- --:--:-- 553k \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; Pod 3 ##### Notice this \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Pod 3 \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Well, it is actually load-balancing the requests to the three different pods, running on three different nodes. And it took me about 5 seconds to apply the ip-pool yaml and the bgppeeringpolicy yaml and I had a fully functioning load-balancer.\nA bit more info on this feature from the offical Cilium docs:\nLB IPAM works in conjunction with features like the Cilium BGP Control Plane (Beta). Where LB IPAM is responsible for allocation and assigning of IPs to Service objects and other features are responsible for load balancing and/or advertisement of these IPs.\nSo I assume the actual loadbalancing is done by BGP here.\nCilium Ingress # As I covered above serviceType loadBalancer, let me quickly cover how to enable Cilium IngressController. More info can be found here\nI will head into my Helm value.yaml and edit the following:\ningressController: # -- Enable cilium ingress controller # This will automatically set enable-envoy-config as well. enabled: true # -- Set cilium ingress controller to be the default ingress controller # This will let cilium ingress controller route entries without ingress class set default: false # -- Default ingress load balancer mode # Supported values: shared, dedicated # For granular control, use the following annotations on the ingress resource # ingress.cilium.io/loadbalancer-mode: shared|dedicated, loadbalancerMode: dedicated The Cilium Ingress controller can be dedicated or shared, meaning that it can support a shared IP for multiple Ingress objects. Nice if we are IP limited etc. Additionally we can edit the shared Ingress to configured with a specific IP like this:\n# -- Load-balancer service in shared mode. # This is a single load-balancer service for all Ingress resources. service: # -- Service name name: cilium-ingress # -- Labels to be added for the shared LB service labels: {} # -- Annotations to be added for the shared LB service annotations: {} # -- Service type for the shared LB service type: LoadBalancer # -- Configure a specific nodePort for insecure HTTP traffic on the shared LB service insecureNodePort: ~ # -- Configure a specific nodePort for secure HTTPS traffic on the shared LB service secureNodePort : ~ # -- Configure a specific loadBalancerClass on the shared LB service (requires Kubernetes 1.24+) loadBalancerClass: ~ # -- Configure a specific loadBalancerIP on the shared LB service loadBalancerIP : 10.150.11.100 ### Set your preferred IP here # -- Configure if node port allocation is required for LB service # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation allocateLoadBalancerNodePorts: ~ This will dictate that the shared Ingress object will get this IP address.\nNow save changes and run the helm upgrade command:\nandreasm@linuxmgmt01:~/test-cluster-1$ helm upgrade -n kube-system cilium cilium/cilium --version 1.14.5 -f cilium-values-feature-by-feature.yaml Release \u0026#34;cilium\u0026#34; has been upgraded. Happy Helming! NAME: cilium LAST DEPLOYED: Wed Dec 20 08:18:58 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 15 TEST SUITE: None NOTES: You have successfully installed Cilium with Hubble Relay and Hubble UI. Your release version is 1.14.5. For any further help, visit https://docs.cilium.io/en/v1.14/gettinghelp Now is also a good time to restart the Cilium Operator and Cilium Agents to re-read the new configMap.\nandreasm@linuxmgmt01:~/test-cluster-1$ kubectl -n kube-system rollout restart deployment/cilium-operator deployment.apps/cilium-operator restarted andreasm@linuxmgmt01:~/test-cluster-1$ kubectl -n kube-system rollout restart ds/cilium daemonset.apps/cilium restarted Also check the Cilium status by running this:\nandreasm@linuxmgmt01:~/test-cluster-1$ cilium status /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: OK \\__/ ClusterMesh: disabled Deployment hubble-ui Desired: 1, Ready: 1/1, Available: 1/1 DaemonSet cilium Desired: 4, Ready: 4/4, Available: 4/4 Deployment cilium-operator Desired: 2, Ready: 2/2, Available: 2/2 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Containers: cilium Running: 4 hubble-ui Running: 1 cilium-operator Running: 2 hubble-relay Running: 1 Cluster Pods: 4/4 managed by Cilium Helm chart version: 1.14.5 Image versions hubble-ui quay.io/cilium/hubble-ui:v0.12.1@sha256:9e5f81ee747866480ea1ac4630eb6975ff9227f9782b7c93919c081c33f38267: 1 hubble-ui quay.io/cilium/hubble-ui-backend:v0.12.1@sha256:1f86f3400827a0451e6332262467f894eeb7caf0eb8779bd951e2caa9d027cbe: 1 cilium-operator quay.io/cilium/operator-generic:v1.14.5@sha256:303f9076bdc73b3fc32aaedee64a14f6f44c8bb08ee9e3956d443021103ebe7a: 2 hubble-relay quay.io/cilium/hubble-relay:v1.14.5@sha256:dbef89f924a927043d02b40c18e417c1ea0e8f58b44523b80fef7e3652db24d4: 1 cilium quay.io/cilium/cilium:v1.14.5@sha256:d3b287029755b6a47dee01420e2ea469469f1b174a2089c10af7e5e9289ef05b: 4 As soon as I enable the Ingress controller it will create this object for me, and provide an IngressClass in my cluster.\nandreasm@linuxmgmt01:~/test-cluster-1$ k get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE cilium cilium.io/ingress-controller \u0026lt;none\u0026gt; 86s Now I suddenly have an IngressController also. Let me deploy a test app to test this.\nFirst I deploy two pods with their corresponding clusterIP services:\nkind: Pod apiVersion: v1 metadata: name: apple-app labels: app: apple namespace: fruit spec: containers: - name: apple-app image: hashicorp/http-echo args: - \u0026#34;-text=apple\u0026#34; --- kind: Service apiVersion: v1 metadata: name: apple-service namespace: fruit spec: selector: app: apple ports: - port: 5678 # Default port for image kind: Pod apiVersion: v1 metadata: name: banana-app labels: app: banana namespace: fruit spec: containers: - name: banana-app image: hashicorp/http-echo args: - \u0026#34;-text=banana\u0026#34; --- kind: Service apiVersion: v1 metadata: name: banana-service namespace: fruit spec: selector: app: banana ports: - port: 5678 # Default port for image And then the Ingress pointing to the two services apple and banana:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-example namespace: fruit labels: env: test annotations: ingress.cilium.io/loadbalancer-mode: dedicated spec: ingressClassName: cilium rules: - host: fruit.my-domain.net http: paths: - path: /apple pathType: Prefix backend: service: name: apple-service port: number: 5678 - path: /banana pathType: Prefix backend: service: name: banana-service port: number: 5678 Notice the only annotation I have used is the loadbalancer-mode: dedicated. The other value that is accepted is shared. By using this annotation I can choose on specific Ingresses whether they should be using the ip from the shared Ingress object or if I want it to be a dedicated one with its own IP address. If I dont want this Ingress to consume a specific IP address I will use shared, if I want to create a dedicated IP for this Ingress I can use dedicated. The shared service for Ingress object is automatically created when enabling the IngressController. You can see this here:\nandreasm@linuxmgmt01:~$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cilium-shared-ingress LoadBalancer 10.21.104.15 10.150.15.100 80:30810/TCP,443:31104/TCP 46h I have configured this shared-ingress to use a specific ip-address. When using dedicated it will create a cilium-ingress-name-of-Ingress on a new IP address (as can be seen below).\nAs soon as this has been applied Cilium will automatically take care of the serviceType loadBalancer object by getting an IP address from one of the IP pools that matches my serviceSelections (depending on shared or dedicated ofcourse). Then BGP will automatically advertise the host-route to my BGP router. And the Ingress object should now be listening on HTTP requests on this IP. Here is the services/objects created:\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium$ k get ingress -n fruit NAME CLASS HOSTS ADDRESS PORTS AGE ingress-example cilium fruit.my-domain.net 10.150.12.4 80 44h andreasm@linuxmgmt01:~/prod-cluster-1/cilium$ k get svc -n fruit NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apple-service ClusterIP 10.21.243.103 \u0026lt;none\u0026gt; 5678/TCP 4d9h banana-service ClusterIP 10.21.124.111 \u0026lt;none\u0026gt; 5678/TCP 4d9h cilium-ingress-ingress-example LoadBalancer 10.21.50.107 10.150.12.4 80:30792/TCP,443:31553/TCP 43h Let me see if the Ingress responds to my http requests (I have registered the IP above with a DNS record so I can resolve it):\nandreasm@linuxmgmt01:~$ curl http://fruit.my-domain.net/apple apple andreasm@linuxmgmt01:~$ curl http://fruit.my-domain.net/banana banana The Ingress works. Again for more information on Cilium IngressController (like supported annotations etc) head over here\nCilium Gateway API # Another ingress solution to use is Gateway API, read more about that here\nGateway API is an \u0026ldquo;evolution\u0026rdquo; of the regular Ingress, so it would be natural to take this into consideration going forward. Again Cilium supports Gateway API out of the box, I will use Helm to enable it and it just needs a couple of CRDs to be installed. Read more on Cilium API support here.\nTo enable Cilium Gateway API I did the following:\nEdit my Helm value.yaml with the following setting: gatewayAPI: # -- Enable support for Gateway API in cilium # This will automatically set enable-envoy-config as well. enabled: true Installed these CRDs before I ran the Helm upgrade command $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.7.0/config/crd/standard/gateway.networking.k8s.io_gatewayclasses.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.7.0/config/crd/standard/gateway.networking.k8s.io_gateways.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.7.0/config/crd/standard/gateway.networking.k8s.io_httproutes.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.7.0/config/crd/standard/gateway.networking.k8s.io_referencegrants.yaml $ kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/gateway-api/v0.7.0/config/crd/experimental/gateway.networking.k8s.io_tlsroutes.yaml Run the helm upgrade command: andreasm@linuxmgmt01:~/test-cluster-1$ helm upgrade -n kube-system cilium cilium/cilium --version 1.14.5 -f cilium-values-feature-by-feature.yaml Release \u0026#34;cilium\u0026#34; has been upgraded. Happy Helming! NAME: cilium LAST DEPLOYED: Wed Dec 20 11:12:55 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 16 TEST SUITE: None NOTES: You have successfully installed Cilium with Hubble Relay and Hubble UI. Your release version is 1.14.5. For any further help, visit https://docs.cilium.io/en/v1.14/gettinghelp andreasm@linuxmgmt01:~/test-cluster-1$ kubectl -n kube-system rollout restart deployment/cilium-operator deployment.apps/cilium-operator restarted andreasm@linuxmgmt01:~/test-cluster-1$ kubectl -n kube-system rollout restart ds/cilium daemonset.apps/cilium restarted It is very important to install the above CRDs first before attempting to enable the GatewayAPI in Cilium. Otherwise it will create any gatewayclass, aka no GatewayAPI realized.\nNow I should have a gatewayClass:\nandreasm@linuxmgmt01:~/test-cluster-1$ k get gatewayclasses.gateway.networking.k8s.io NAME CONTROLLER ACCEPTED AGE cilium io.cilium/gateway-controller True 96s Now I can just go ahead and create a gateway and some httproutes. When it comes to providing an external IP address for my gateway, this is provided by my ip-pools the same way as for the IngressController.\nLets go ahead and create a gateway, and for this excercise I will be creating a gateway with corresponding httproutes to support my Harbor registry installation.\nBelow is the config I have used, this has also been configured to do a https redirect (from http to https):\napiVersion: gateway.networking.k8s.io/v1beta1 kind: Gateway metadata: name: harbor-tls-gateway namespace: harbor spec: gatewayClassName: cilium listeners: - name: http protocol: HTTP port: 80 hostname: \u0026#34;registry.my-domain.net\u0026#34; - name: https # allowedRoutes: # namespaces: # from: Same protocol: HTTPS port: 443 hostname: \u0026#34;registry.my-domain.net\u0026#34; # allowedRoutes: # namespaces: # from: Same tls: mode: Terminate certificateRefs: - kind: Secret name: harbor-tls-prod namespace: harbor --- apiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: harbor-tls-redirect namespace: harbor spec: parentRefs: - name: harbor-tls-gateway sectionName: http namespace: harbor hostnames: - registry.my-domain.net rules: - filters: - type: RequestRedirect requestRedirect: scheme: https port: 443 --- apiVersion: gateway.networking.k8s.io/v1beta1 kind: HTTPRoute metadata: name: harbor-api-route namespace: harbor spec: parentRefs: - name: harbor-tls-gateway sectionName: https namespace: harbor hostnames: - registry.my-domain.net rules: - matches: - path: type: PathPrefix value: /api/ backendRefs: - name: harbor port: 80 - matches: - path: type: PathPrefix value: /service/ backendRefs: - name: harbor port: 80 - matches: - path: type: PathPrefix value: /v2/ backendRefs: - name: harbor port: 80 - matches: - path: type: PathPrefix value: /chartrepo/ backendRefs: - name: harbor port: 80 - matches: - path: type: PathPrefix value: /c/ backendRefs: - name: harbor port: 80 - matches: - path: type: PathPrefix value: / backendRefs: - name: harbor-portal port: 80 I have already created the certificate as the secret I refer to in the yaml above.\nLets have a look at the gateway, httproutes and the svc that provides the external IP address , also the Harbor services the httproutes refer to:\n#### gateway created ##### andreasm@linuxmgmt01:~/prod-cluster-1/harbor$ k get gateway -n harbor NAME CLASS ADDRESS PROGRAMMED AGE harbor-tls-gateway cilium 10.150.14.32 True 28h #### HTTPROUTES #### andreasm@linuxmgmt01:~/prod-cluster-1/harbor$ k get httproutes.gateway.networking.k8s.io -n harbor NAME HOSTNAMES AGE harbor-api-route [\u0026#34;registry.my-domain.net\u0026#34;] 28h harbor-tls-redirect [\u0026#34;registry.my-domain.net\u0026#34;] 28h andreasm@linuxmgmt01:~/prod-cluster-1/harbor$ k get svc -n harbor NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE cilium-gateway-harbor-tls-gateway LoadBalancer 10.21.27.25 10.150.14.32 80:32393/TCP,443:31932/TCP 28h Then the services that Harbor installs:\nandreasm@linuxmgmt01:~/prod-cluster-1/harbor$ k get svc -n harbor NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE harbor ClusterIP 10.21.101.75 \u0026lt;none\u0026gt; 80/TCP 40h harbor-core ClusterIP 10.21.1.68 \u0026lt;none\u0026gt; 80/TCP 40h harbor-database ClusterIP 10.21.193.216 \u0026lt;none\u0026gt; 5432/TCP 40h harbor-jobservice ClusterIP 10.21.120.54 \u0026lt;none\u0026gt; 80/TCP 40h harbor-portal ClusterIP 10.21.64.138 \u0026lt;none\u0026gt; 80/TCP 40h harbor-redis ClusterIP 10.21.213.160 \u0026lt;none\u0026gt; 6379/TCP 40h harbor-registry ClusterIP 10.21.212.118 \u0026lt;none\u0026gt; 5000/TCP,8080/TCP 40h harbor-trivy ClusterIP 10.21.138.224 \u0026lt;none\u0026gt; 8080/TCP 40h Now I can reach my Harbor using the UI and docker cli all through the Gateway API\u0026hellip;\nI will use Harbor in the next chapter with Hubble UI..\nHubble UI # As you may recall, I did enable the two features Hubble Relay and Hubble UI as we can se below:\nandreasm@linuxmgmt01:~$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-metrics ClusterIP None \u0026lt;none\u0026gt; 9965/TCP 35h hubble-peer ClusterIP 10.23.182.223 \u0026lt;none\u0026gt; 443/TCP 42h hubble-relay ClusterIP 10.23.182.76 \u0026lt;none\u0026gt; 80/TCP 40h hubble-ui ClusterIP 10.23.31.4 \u0026lt;none\u0026gt; 80/TCP 36h It is not exposed so I can reach from outside the Kubernetes cluster. So let me first start by just creating a serviceType loadBalancer service to expose the Hubble UI clusterIP service. Below is the yaml I use for that:\napiVersion: v1 kind: Service metadata: name: hubble-ui-lb namespace: kube-system labels: env: prod annotations: \u0026#34;io.cilium/lb-ipam-ips\u0026#34;: \u0026#34;10.150.11.10\u0026#34; spec: type: LoadBalancer ports: - port: 8081 targetPort: 8081 protocol: TCP name: http selector: k8s-app: hubble-ui Apply it and I should see the service:\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium/services$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hubble-ui-lb LoadBalancer 10.21.47.47 10.150.11.10 8081:32328/TCP 3d11h There it is, now open my browser and point to this ip:port\nNow let me go to a test application I have deployed in the yelb namespace. Click on it from the list or the dropdown top left corner:\nSoo much empty\u0026hellip;\nI can see the pods are running:\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium/services$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-84f4bf49b5-fq26l 1/1 Running 0 5d18h yelb-appserver-6dc7cd98-s6kt7 1/1 Running 0 5d18h yelb-db-84d6f6fc6c-m7xvd 1/1 Running 0 5d18h They are probably not so interested in talking to each other unless they have to. Let me deploy the Fronted service and create some interactions.\nandreasm@linuxmgmt01:~/prod-cluster-1/cilium$ k apply -f yelb-lb-frontend.yaml service/yelb-ui created deployment.apps/yelb-ui created andreasm@linuxmgmt01:~/prod-cluster-1/cilium$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-server ClusterIP 10.21.67.23 \u0026lt;none\u0026gt; 6379/TCP 5d18h yelb-appserver ClusterIP 10.21.81.95 \u0026lt;none\u0026gt; 4567/TCP 5d18h yelb-db ClusterIP 10.21.188.43 \u0026lt;none\u0026gt; 5432/TCP 5d18h yelb-ui LoadBalancer 10.21.207.114 10.150.11.221 80:32335/TCP 49s I will now open the Yelb UI and do some quick \u0026ldquo;votes\u0026rdquo;\nInstantly, even by just opening the yelb webpage I get a lot of flow information in Hubble. And not only that, it automatically creates a \u0026ldquo;service-map\u0026rdquo; so I can see the involved services in the Yelb app.\nThis will only show me L4 information. What about Layer 7? Lets test that also by heading over to Harbor\nIn Hubble I will switch to the namespace harbor\nA nice diagram with all involced services, but no L7 Information yet. Well there is, but I have no recent interactions to Harbor using the Gateway API, as soon as I use docker or web-ui against harbor what happens then?\nWhats this, an ingress object?\nNow when I click on the ingress object:\nLook at the L7 info coming there.\nI logged out from Harbor:\nLogged back in:\nBrowsing the Harbor Projects/repositories:\nVery rich set of information presented in a very snappy and responsive dashbord. Its instantly updated as soon as there is a request coming.\nFor now, this concludes this post.\nIt has been a nice experience getting a bit more under the hood of Cilium, and so far I must say it looks very good.\nThings I have not covered yet wich I will at a later stage # I will update this post with some other features at a later stage. Some of the features I am interested looking at is:\nSecurity policies with Cilium - just have quick look here many interesting topics, Host Firewall? Egress Cilium Multi-Cluster ","date":"18 December 2023","externalUrl":null,"permalink":"/2023/12/18/a-quick-glance-at-cilium-cni/","section":"Posts","summary":"In this post I will deploy Cilium in my k8s cluster and test some features and how it is to manage","title":"A Quick Glance at Cilium CNI","type":"posts"},{"content":"","date":"18 December 2023","externalUrl":null,"permalink":"/tags/cilium/","section":"Tags","summary":"","title":"Cilium","type":"tags"},{"content":"","date":"18 December 2023","externalUrl":null,"permalink":"/tags/ebpf/","section":"Tags","summary":"","title":"Ebpf","type":"tags"},{"content":"","date":"18 December 2023","externalUrl":null,"permalink":"/categories/ebpf/","section":"Categories","summary":"","title":"EBPF","type":"categories"},{"content":"","date":"18 December 2023","externalUrl":null,"permalink":"/categories/monitoring/","section":"Categories","summary":"","title":"Monitoring","type":"categories"},{"content":"","date":"18 December 2023","externalUrl":null,"permalink":"/categories/security/","section":"Categories","summary":"","title":"Security","type":"categories"},{"content":"","date":"23 November 2023","externalUrl":null,"permalink":"/tags/nsx-alb/","section":"Tags","summary":"","title":"Nsx-Alb","type":"tags"},{"content":"","date":"23 November 2023","externalUrl":null,"permalink":"/tags/tkgi/","section":"Tags","summary":"","title":"Tkgi","type":"tags"},{"content":"","date":"23 November 2023","externalUrl":null,"permalink":"/categories/tkgi/","section":"Categories","summary":"","title":"TKGi","type":"categories"},{"content":" TKGi # I realized I have not covered any posts around TKGi, so it is about high time to cover TKGi also. TKGi for me has a special place in my IT heart. Back in the days when it was called PKS I was all over the place doing a lot of PoCs and some production installations on TKGi. It was, and is, a very good Kubernetes platform very integrated with NSX. Back then NSX Advanced Loadbalancer, or Avi Networks, were not part of VMware. TKGi relied 100% on NSX-T providing both L4 and L7 loadbalancer services and it had a very tight integration with NCP (NSX Container Plugin) as the CNI for the Kubernetes clusters. Now we can also use Antrea as the CNI and we can also use NSX Advanced LoadBalancer for both Layer 4 and Layer 7 services in combination with NSX (different ways how NSX is being used, more on that later) as the underlaying network platform. So this will be an exiting post for me to go through. TKGi is very much alive and keeps going forward.\nInstalling TKGi # TKGi involves several components like vSphere, vCenter, NSX, and now also NSX Advanced LoadBalancer and the actual TKGi components themselves. The actual installing of TKGi can be done a couple of ways. One way (the method I prefer) is to use the TKGi Management Console. The MC is a rather large OVA image that contains all the binaries needed to install TKGi, and provides a nice UI. More on that later. The other approach is to build TKGi by downloading all the binaries from the Ops Manager, Bosh Stemcells, TGKi, etc. But before we can deploy TKGi there is some preparations that needs to be done. A note on how NSX can be prepared for use in TKGi. NSX can be the underlaying network provider for TKGi in three ways. We can use NSX as a regular underlay, no integration with TKGi, we will create the NSX networks as for regular VM workload with segments, gateway IPs and select them to be used respectively in TKGi. The other option is to use fully automated way where TKGi creates and configures all the needed NSX network components on demand and the third option is to Bring Your Own Topology meaning we have created some components like Tier-0 ourselves, and TKGi will use these objects and attach its own created Tier1s and segments according to the config. I will go through both of these two options to to show the differencem where the Antrea approach just leverage NSX as the underlaying network stack with no TKGi integration and the \u0026ldquo;BYOT\u0026rdquo; will automate many of the networking tasks. I will not show how the \u0026ldquo;fully\u0026rdquo; automated NSX network integrations works, this also configures the Tier-0 which I already have in place in my infra.\nI will be basing this post on TKGi version 1.18.0. This version supports the NSX Policy API, but is not so relevant when selecting the Antrea approach. I was not able to deploy TKGi using NSX Policy API in my lab so it to use Manager API. But the biggest difference is just how to NSX objects are being created in NSX, and for the sake of it probably some support statements that may differ.\nSo this post will cover to two ways of deplying TKGI where the first is using the EPMC, BYOT with NSX-T using NSX manager API, then the Antrea approach (still using TKGiMC) with NSX-T as just a network fabric managed completely outside of TKGi. Both approaches will incorporate the use of NSX Advanced Loadbalancer for both L4 services and L7 services inside my Kubernetes clusters. In the Antrea approach I will also configure Virtual Services in NSX Advanced Loadbalancer to provide the Kubernetes APi endpoint.\nPreparations before deploying TKGi # A fully functioning vSphere cluster with vCenter NSX installed and configured with required network topology(see drawing below) NSX-ALB controller deployed, configured and NSX-T cloud configured. Downloaded the TKGi Management Console from my.vmware.com (13GB) Network topology NSX configurations # Within NSX I have configured my Tier-0, then a Tier-1 for a couple of segments like a \u0026ldquo;management segment\u0026rdquo; and \u0026ldquo;avi-se-generic-data\u0026rdquo; for SE \u0026ldquo;generic\u0026rdquo; datanic, then a second Tier for my Avi DNS service where the DNS SE data nic will be placed. If BYOT then the T1s and segments for TKGi will be created by TKGi, for the Antrea approach, you need to manually or create the segments/Tier1s in NSX yourselves, these segments is at a minumun the management network for TKGi management components (OpsMan, Bosh) and a service network for the Kubernetes nodes. By using network profiles you can add more service networks to accommodate different networks for different Kubernetes clusters.\nTier-0\nTier-1s\nSegments\nIP Pools/Blocks\nNSX-ALB configurations # In my NSX-ALB controller I have done the following configurations\nClouds:\nService Engine Group\nI have created two SE groups, one for all \u0026ldquo;usecases\u0026rdquo; (se-group-generic) and one for the Avi DNS service (se-dns-group).\nNetworks\nHere I have defined networks for the SE data and different VIP networks. All these have been added to the IPAM template used by my NSX cloud. The SE data networks are only defining SE vNICS, the VIP networks are defined as only VIP networks.\nVRF Contexts\nHere I have configured a static route under the global VRF for my mgmt subnet using the gateway for the the SE mgmt network as nextop. The other two VRFs has been configured with a default route using the gateway in their respective datanetworks gateway.\nIPAM/DNS profiles\nThen my DNS service\nIn system settings DNS service is configured to use my deployed DNS VS.\nI have configured my backend DNS server to forward requests to this service (10.146.100.53) for my dns sones configured in my NSX ALB DNS service.\nDeploy the TKGi Management Console (Formerly the EPMC for Enterprise PKS Management Console) # After downloading the ova, I will deploy it in my vCenter server as all other OVA deployments. I just want to point out a setting that one should be aware of and that is the docker-network. In the initial days of this EPMC installer it just took for granted that we could use the default docker network inside the EPMC appliance. But not every customer are using 192.168.0.0/16 or 10.0.0.0/16 they may use 172.16.0.0/12.. And the Docker default network is using 172.16.0.0/17. If your client network that interacts with the deployed MC appliance happens to be on this subnet also, you are unable to reach the installer as the container network will not try to route outside. I made the EPMC developers aware of this back then, created a guide how to override it manually by adjusting the docker network inside the appliance itself, and shortly after they added the field in the OVA for you to select your own Docker network during provisioning. Below is the deployment of the EPMC OVA.\nSelecting my mgmt network created in NSX.\nEnter password for the root user of the epmc appliance, then leave SSH selected for troubleshooting scenarios.\nHere it is important to change the docker network to something that does not conflict with subnet your are coming from when interacting with the EPMC appliance.\nThen Next and FINISH\nSit back and wait, as soon as it is deployed power on and wat a couple more minutes. Then it is time to enter the IP address in our browser to start the TKGi installation\u0026hellip;\nInstalling TKGi using the Management Console # Open your preferred browser and enter the given IP address.\nLogin with username root and the password provided in the OVA deployment process.\nI want to do a new install, so I will select INSTALL, but first I will change the theme to dark. Much nicer on my eyes.\nHere I can import an already existing config, or start creating a new one. I dont have an existing so I will do a new one selection Start Configuration\nEnter my relevant vCenter info, click connect.\nvCenter connected, selecting my Datacenter, then Next.\nAnd its here we can do our topology choices. I will start with the BYOT approach, NSX-T Bring Your Own Topology.\nNSX-T Bring Your Own Topology # In this topology I will point to my existing Tier0, then my already created segment ls-tkgi-mgmt, the floating IP pool (api endpoints), ip blocks for node and pod. I deselect NAT as I want to keep the Kubernetes nodes routable. This also mean the POD network is not NAT\u0026rsquo;ed, be aware of this otherwise you may end up advertising a bunch of subnets you may not want to re-distribute.\nAccording to the official documentation here NO NAT should give me routable management VMs (Bosh, OpsMan etc) and routable Kubernetes Nodes, but I discovered it also gave me routable pods, so my pod cidr was exposed in my network via BGP. To keep this network inside NSX I had to add a route-map on my Tier-0 blocking this cidr. This only stop my POD cidr from being advertised outside NSX (Tier0) but not inside NSX. I would like to have the option to at least place the nat rules on the Tier1 for only the pod CIDR.\nHere is the topology diagrams, the Cluster Network (I assume is the pod-cidr) is following the kubernetes nodes topology.\nMore on that later.\nAlso I was not able to enable the NSX-T Policy API using the TKGI MC, it could not find any of my NSX objects created using Policy API, so I decided to continue using manager api. Maybe using Ops Man, the manual approach, will make this easier. But again, this is my lab and there may be some small config it does not like there. In other real world environments it may behave different.\nThis is because it requires the T1 where I connect my tkgi-mgmt segment to be centralised, and placed on an edge cluster. That is most likely because it needs the option to create stateful objects like NAT rules..\nAfter creating my new Tier1 router as active standby, selecting my edge cluster repoint my tkgi mgmt segment to the new Tier1 router it validates fine.\nAntrea Topology # Here I will select Antrea. I will do that because I want to use NSX ALB as the loadbalancer provider and Antrea as the CNI. I know I will not get the benefit from NCP automatically creating the networks for me in NSX and some other NCP features as bridging, easy choice whether I want to NAT or no NAT etc. But this blog post is about running TKGi together with NSX, NSX ALB and Antrea. NSX will still be my main network provider in this setup as networks are easily created in NSX and I have the benefit of using the DFW for my worker nodes and the Antrea NSX Integration I have written about here.\nFor more information on the differnet network topologies have a look here\nThis first installation of TKGi can be called a foundation, that means we can have several TKGi installations with their own Ops Managers, Bosh instances etc, sharing some common components as NSX (not using same Tier0 but dedicated Tier0s pr foundation). The benefits of running multiple TKGi foundations is that we can have unique configurations done pr foundation. See more here\nNow that I have selected the Topology Antrea, I will fill out the necessary network information accordingly. The Deployment Network Resource is for the TKGi management components and the Service Network Resource is for the Kubernetes Cluster nodes.\nCommon settings independent of topology choice # Click next\nadd a name to the TKGi API endpoint.\nBefore I populate the next section, I have already created four resource pools in my vCenter to \u0026ldquo;simulate\u0026rdquo; 3 availability zones and one for management.\nNext\nUnder Resources and Storage I have enabled vSphere CSI Driver Integration. Selected my vSAN Storage for Ephemeral and permanent as it will default used if not specifying otherwise.\nPlans\nHere I have only adjusted the worker nodes in the medium and large plans to have 2 cpu 8gb ram and 4 cpus and 16gb ram respectively.\nNext\nI dont have any integrations\nI already have a Harbor instance running.. So I disable it here.\nGenerate Configuration\nNow I can Apply the config, but before that I will export the config.\nContinue\u0026hellip; Sit back and well\u0026hellip; It will take some time.\nAs soon as the OpsMgr has been deployed it is possible to monitor the deployment process further. Like when it gets to this step below, and begin to wonder if it has stopped completely.\nAs soon as I logged into OpsManager I could see it was still doing a lot of things.\nGrab the password from here:\nNow after some waiting the TKGi Management Console reports a successful deployment\nNow the fun starts.\nDeploy workload clusters using NSX-T BYOT (Manager API) # From the TKGI Management Console click continue and you should get to the \u0026ldquo;welcome page\u0026rdquo; for your TKGi foundation with all the details of the components installed. Notice that the NSX Policy API is disabled. I could not get the TKGi MC to use Policy API during deployment in my lab. So I decided to leave that unchecked. The biggest impact of this is that certain objects created in NSX by TKGi (NCP) will be created using Manager API.\nThe plan now is to to deploy a workload cluster using the plan Small. but before I do that I will create a Network Profile from the TKGi MC that disables the NSX-T loadbalancer for both L4 and L7 for the Kubernetes cluster. What this means is that NSX-T loadbalancer will be used only to provide a loadbalanced API endpoint for my workload cluster control planes. But from within the workload clusters themselves (Kubernetes clusters) I will need to provide my own layer4 and layer7 provider. And guess what that will be.. Yes, the NSX Advanced LoadBalancer.\nCreate Network Profile from TKGi MC # I will go to Profiles in the TKGi MC left menu and create a new Network Profile like this:\nClick create profile\nThen I will give the profile the name ako-provider\nThen I will expand the Container Network section, and set Use NSX-T L4 Virtual Server For K8s LoadBalancer to No and Use NSX-T L7 Virtual Server As The Ingress Controller For K8s Cluster to No\nThis is done as I only want NSX ALB AKO to provide L4 and L7 services from my Kubernetes Clusters.\nScroll all the way down and click save as there are no more settings I want to adjust now.\nDeploy workload cluster/Kubernetes cluster # I prefer the easy way to deploy the clusters, so from the TKGi MC I will head here and create a cluster:\nClick create cluster:\nHere I will give the cluster a name, then I will use the same cluster name also in my fqdn name. I will go with 3 Worker Nodes. Then the important step is to select my newly created ako-provider Network Profile. If one want to see what its in the network profile click on show more on the right side under Network Profile Parameters\nThen click Create and another session of waiting.\nIt will now start to deploy the vms in my vCenter. Then moving them to the correct Resource Groups and network (Using the Node pool I created earlier)\nAfter a little while (depending on fast the underlaying hardware is) it should be ready:\nNow next step is to log into the newly created cluster and deploy AKO for the L4 and L7 parts inside that cluster\nAs I did deploy this cluster using NSX Manager API, the only way of configuring the Cloud in the NSX Advanced Loadbalancer is vCenter Cloud. According to the offical documentation found here. I will use a NSX Cloud anyway, so consider this a non-supported approach. I will not use the NCP/TKGi created Segments using Manager API for any of the Avi components, I have created my own T1 and segments for my Avi cloud using Policy API. So this would be interesting to explorer further. But with TKGi in Policy mode it is supported. In the link above one of the concerns is that SEs needs to traverse the Tier-0 to reach the backends. But this will be the case even if using policy api as the T1s created by TKGi/NCP will be created as centralized tier1s and will be handled by the NSX Edges, so I am not so sure how this can be done differently.\nNSX objects created by the manager API # Now that my first Kubernetes cluster is up and running, lets have a look inside NSX and see what is has created. For that I need to switch my NSX UI into Manager\nThen I will first have a look at the Tier 1\nThen the segments:\nThis one is interesting. If you are used to any other CNIs than NCP you will notice here that all the Kubernetes Namespaces created in your Kubernetes clusters reflects a NSX segment. Example the segment pks-0b7d0725-c67b-4e14-b6d0-52f799cb3556-kube-system, contains all the pods as logical ports in that namespace. A nice feature of NCP. This is really useful in strict IPAM cases, isolation and maintaning control of any pod in the Kubernetes clusters. All Logical ports are enforced by the NSX Distributed Firewall also.\nNAT rules: Here is a bunch of SNAT rules translating all the pod cidrs in use to ip addresses from the Floating IP pool.\nDistributed Firewall rules\nThese are the major objects being created in manager mode in NSX by NCP.\nSpeaking of loadbalancing, lets get some loadbalancing done from the Kubernetes cluster I just created. Isnt this post about TKGi and NSX Advanced Loadbalancer? \u0026hellip; \u0026#x1f604;\nConfigure NSX Advanced Loadbalancer as Layer 4 and Layer 7 provider inside workload clusters. # I have already configured my NSX Advanced Loadbalancer using a NSX cloud, configured a Tier1 for both generic SE data and my DNS SEs using the policy API. But again according to the official documentation here this is not supported. But as I will not be putting any of my SEs on the segments created by TKGi, and I am doing a BYOT installation where I have created some NSX objects myself) I dont feel this should be an issue. What this approach will do though is forcing SE traffic to reach the backend (Kubernetes nodes on the NCP created segments using manager API) via the T0. This could potentially reduce performance, as the T1s may be centralized. If the T1 for the SEs are distributed they will just use the DR T0, not via the Edges. Another statement from the official documentation around this scenario where the SE datanics needs to traverse the T0 to reach the backends here:\nFor a given virtual service, the logical segment of the pool servers and the logical segment of the VIP must belong to the same tier-1 router. If the VIP and the pool are connected to different tier-1, the traffic may pass through the tier-0 and hence through the NSX-T edge (depending on the NSX-T services configured by admin). This reduces the data path performance and hence must be avoided.\nSo, lets see how the traffic goes when all this is up and running.\nFirst I need to grab the config for my Kubernetes cluster so I can interact with it. I will just grab the config from the TKGI MC here:\nThis assumes we have configured a DNS record for the FQDN I entered when I created the cluster. Have I done that, yes of course.\nNow I will just paste this in my linux machine where I have kubectl installed.\nandreasm@ubuntu02:~/tkgi/cluster-1$ kubectl config set-cluster tkgi-cluster-1 --server=https://tkgi-cluster-1.this-is-a.domain.net:8443 \u0026amp;\u0026amp; \\ kubectl config set clusters.tkgi-cluster-1.certificate-authority-data Ci0tLS0tQkVHSU4gQ0VSVElGSUNBVEUtLS0tLQpNSUlET3pDQ0FpT2dBd0lCQWdJVU9EQXNVemVuRk1JYVBhajhoZWpOZ21FL2JXOHdEUVlKS29aSWh2Y05BUUVMCkJRQXdMVEVMTUFrR0ExVUVBeE1DUTBFeERUQUxCZ05WQkFzVEJGUkxSMGt4RHpBTkJnTlZCQW9UQmxaTmQyRnkKWlRBZUZ3MHlNekV4TWpjd09ETTVORFJhRncweU56RXhNamN3T0RNNU5EUmFNQzB4Q3pBSkJnTlZCQU1UQWtOQgpNUTB3Q3dZRFZRUUxFd1JVUzBkSk1ROHdEUVlEVlFRS0V3WldUWGRoY21Vd2dnRWlNQTBHQ1NxR1NJYjNEUUVCCkFRVUFBNElCRHdBd2dnRUtBb0lCQVFEaURlVTBaOXF3MVFDblltNDRUNnFmOWJDeVA5YXJPd2ltZjFaL09paVoKRFhDYzhMT214YTRWbHh1NXB0OGVzMEt5QmdLd1hjMEVnVDl2NmluV083RWpsZmNHYy9ySThPdjFkOEFuT0tzWQpiRXUzQlo5VS9KOGZNNnpGV3M5bGdzZVBsOURldEFLWXZpcFgzV28yVHpkbjhOVnU5NlJKTmNNdTlNVGNZU05DCjNUbUpVMWdFOVRVak45TXpyRWpsNVo3RkFaMVZnMXl3ZmpCU3h4WlpJcUN6UDFhaWw3OFZBR2VHLzhoN3VXUVIKd0oyTFM5Vm90bTlGQ0FFMlN1ZzUzQVEyUzNPalNBMExNWVI2d0dBeGk0a0xhcld3TjU1d2owcm9zWnBKREJzcQo4OURHYkI5YjdNaHZYcWhMQ3dOT2NiUFppVFFCUkJUZnVOZTRFQjhvTFZxcEFnTUJBQUdqVXpCUk1CMEdBMVVkCkRnUVdCQlFLVEV0d3ZVZEdRMVAzU01aNWtCV0xOcTJWOWpBZkJnTlZIU01FR0RBV2dCUUtURXR3dlVkR1ExUDMKU01aNWtCV0xOcTJWOWpBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFCNQpoT3Q3aW5GZy94Vkg5d0prY3M0QVlNQnJpT2tabkl5RHBzQWl0MmJqMkNlUzdEMHdlNE5saWUrNmxzdFhHQ1B4ClU5YmFRTld0a3EwWERxNEhKT0hJd0lXUTE1V2N0ZFZmYks4SFo2QWtZN2QvTWNaTEJUeEZPc2JtSVJnRTZMVW4Kc1BBMHcyRkloMWk5TFkrVE5lV2l2K1YrZi9kZmpVaWh4citzZFRkOHhUSmdxZldjSUl2TmVpcitNZmg2YW1IcgpUVDhDK1dRSm5xc29SMjBLdTU4VGNPbzFsQmYyUjNKZUNJeXIwZExpSnJZV2RZckxteHpORU4xRndjVU9MMHAyCnI4dXl5RkZqYWUyNXp0VFFIR09YU3lzV2xSSHJ2MmdqSW03MlFGZnVUdGhzOVpKRGtYSFhKdnBYcEdoelhqZVgKLzQwVnhzR3Qwd3dwa2c2b2dqUTkKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= \u0026amp;\u0026amp; \\ kubectl config set-credentials c3905e0f-b588-40d9-973f-fba55b6dd6a0 --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IlJNWU5pdFdwV3RZUDE2dk9RSHFkQ3p3VnFZV0ttTFJPYllMYmMyM1BkQzAifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImMzOTA1ZTBmLWI1ODgtNDBkOS05NzNmLWZiYTU1YjZkZDZhMCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJjMzkwNWUwZi1iNTg4LTQwZDktOTczZi1mYmE1NWI2ZGQ2YTAiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2YzIxNDRmNi1iNTRjLTQ4NmItOTU3NS0yNDk4YTQ5YzZjZmIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDpjMzkwNWUwZi1iNTg4LTQwZDktOTczZi1mYmE1NWI2ZGQ2YTAifQ.K9qjIqBUMlwag_-XCK9kbiGjdXflKSRLmOol2AvWlu3HV5yEOAdh7B4-CrTDsk-RA1ajBqUerc8IngbyaWELN2APTKoe2gQoVqvszw4sBqL_VnmNGL9zBl1AyEB8EuUSY17W3XGTWjVRaDskqOkyB36vx3nksnM07du7tRa6evQlvAqPQFuWaX-d7yeTTRRurxzhycPL_AdKFry7V8XSFxvLFaAK93EvWsKQbknxmq7YT4HM7HbGnBzBceUz2ZCjQ_ilMpup-Rxvu1y7Fgf4-dx09PVQXT_SqSaqfSN2sxesNZOFESW-EFEgXytDO4VNQjKVgEv0SBHNwzhhAl8wNw \u0026amp;\u0026amp; \\ kubectl config set-context tkgi-cluster-1 --cluster=tkgi-cluster-1 --user=c3905e0f-b588-40d9-973f-fba55b6dd6a0 \u0026amp;\u0026amp; \\ kubectl config use-context tkgi-cluster-1 Cluster \u0026#34;tkgi-cluster-1\u0026#34; set. Property \u0026#34;clusters.tkgi-cluster-1.certificate-authority-data\u0026#34; set. User \u0026#34;c3905e0f-b588-40d9-973f-fba55b6dd6a0\u0026#34; set. Context \u0026#34;tkgi-cluster-1\u0026#34; created. Switched to context \u0026#34;tkgi-cluster-1\u0026#34;. Lets test connectivity\nandreasm@ubuntu02:~/tkgi/cluster-1-nsx-manager-api$ k get nodes NAME STATUS ROLES AGE VERSION a0e22550-80fe-40d8-9b0f-0dcb2d92c58a Ready \u0026lt;none\u0026gt; 4h16m v1.27.5+vmware.1 bc69d2aa-4ddc-41b8-8ede-b8a94929496b Ready \u0026lt;none\u0026gt; 4h12m v1.27.5+vmware.1 d90a5da4-343a-4040-bef3-1f2f148159e0 Ready \u0026lt;none\u0026gt; 4h9m v1.27.5+vmware.1 There are my worker nodes. You have to wonder where the control plane nodes is.. Well a quick question to my friend Robert Guske the simple explanation is that the kubelet is not running on the control plane nodes in TKGi. To view the control plane nodes we need to use bosh. See here.\nNow I just need to deploy AKO and create some test applications. But before I do that I will need to check if there are any ingress or loadbalancer classes available in my cluster.\nandreasm@ubuntu02:~/tkgi/cluster-1-nsx-manager-api$ k get ingressclasses.networking.k8s.io No resources found ## There is no loadbalancerclass crd available... Now I will grab the values for AKO\nandreasm@ubuntu02:~/tkgi/cluster-1-nsx-manager-api$ helm show values oci://projects.registry.vmware.com/ako/helm-charts/ako --version 1.10.3 \u0026gt; values.yaml Then I will edit the values.yaml according to my environment:\n# Default values for ako. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: projects.registry.vmware.com/ako/ako pullPolicy: IfNotPresent ### This section outlines the generic AKO settings AKOSettings: primaryInstance: true # Defines AKO instance is primary or not. Value `true` indicates that AKO instance is primary. In a multiple AKO deployment in a cluster, only one AKO instance should be primary. Default value: true. enableEvents: \u0026#39;true\u0026#39; # Enables/disables Event broadcasting via AKO logLevel: WARN # enum: INFO|DEBUG|WARN|ERROR fullSyncFrequency: \u0026#39;1800\u0026#39; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. apiServerPort: 8080 # Internal port for AKO\u0026#39;s API server for the liveness probe of the AKO pod default=8080 deleteConfig: \u0026#39;false\u0026#39; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI disableStaticRouteSync: \u0026#39;false\u0026#39; # If the POD networks are reachable from the Avi SE, set this knob to true. clusterName: tkgi-cluster-1 # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT cniPlugin: \u0026#39;ncp\u0026#39; # Set the string if your CNI is calico or openshift or ovn-kubernetes. For Cilium CNI, set the string as cilium only when using Cluster Scope mode for IPAM and leave it empty if using Kubernetes Host Scope mode for IPAM. enum: calico|canal|flannel|openshift|antrea|ncp|ovn-kubernetes|cilium enableEVH: false # This enables the Enhanced Virtual Hosting Model in Avi Controller for the Virtual Services layer7Only: false # If this flag is switched on, then AKO will only do layer 7 loadbalancing. # NamespaceSelector contains label key and value used for namespacemigration # Same label has to be present on namespace/s which needs migration/sync to AKO namespaceSelector: labelKey: \u0026#39;\u0026#39; labelValue: \u0026#39;\u0026#39; servicesAPI: false # Flag that enables AKO in services API mode: https://kubernetes-sigs.github.io/service-apis/. Currently implemented only for L4. This flag uses the upstream GA APIs which are not backward compatible # with the advancedL4 APIs which uses a fork and a version of v1alpha1pre1 vipPerNamespace: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to create Parent VS per Namespace in EVH mode istioEnabled: false # This flag needs to be enabled when AKO is be to brought up in an Istio environment # This is the list of system namespaces from which AKO will not listen any Kubernetes or Openshift object event. blockedNamespaceList: [] # blockedNamespaceList: # - kube-system # - kube-public ipFamily: \u0026#39;\u0026#39; # This flag can take values V4 or V6 (default V4). This is for the backend pools to use ipv6 or ipv4. For frontside VS, use v6cidr useDefaultSecretsOnly: \u0026#39;false\u0026#39; # If this flag is set to true, AKO will only handle default secrets from the namespace where AKO is installed. # This flag is applicable only to Openshift clusters. ### This section outlines the network settings for virtualservices. NetworkSettings: ## This list of network and cidrs are used in pool placement network for vcenter cloud. ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. ## Either networkName or networkUUID should be specified. ## If duplicate networks are present for the network name, networkUUID should be used for appropriate network. nodeNetworkList: - networkName: \u0026#34;pks-0b7d0725-c67b-4e14-b6d0-52f799cb3556\u0026#34; cidrs: - 10.146.42.0/24 # - 11.0.0.1/24 enableRHI: false # This is a cluster wide setting for BGP peering. nsxtT1LR: \u0026#39;/infra/tier-1s/The-Tier-1\u0026#39; # T1 Logical Segment mapping for backend network. Only applies to NSX-T cloud. bgpPeerLabels: [] # Select BGP peers using bgpPeerLabels, for selective VsVip advertisement. # bgpPeerLabels: # - peer1 # - peer2 # Network information of the VIP network. Multiple networks allowed only for AWS Cloud. # Either networkName or networkUUID should be specified. # If duplicate networks are present for the network name, networkUUID should be used for appropriate network. vipNetworkList: - networkName: vip-l4-incluster-tkgi cidr: 10.146.102.0/24 # v6cidr: 2002::1234:abcd:ffff:c0a8:101/64 # Setting this will enable the VS networks to use ipv6 ### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. L7Settings: defaultIngController: \u0026#39;true\u0026#39; noPGForSNI: false # Switching this knob to true, will get rid of poolgroups from SNI VSes. Do not use this flag, if you don\u0026#39;t want http caching. This will be deprecated once the controller support caching on PGs. serviceType: ClusterIP # enum NodePort|ClusterIP|NodePortLocal shardVSSize: SMALL # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL, DEDICATED passthroughShardSize: SMALL # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL enableMCI: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to start processing multi-cluster ingress objects. ### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. L4Settings: defaultDomain: \u0026#39;\u0026#39; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. autoFQDN: default # ENUM: default(\u0026lt;svc\u0026gt;.\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), flat (\u0026lt;svc\u0026gt;-\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), \u0026#34;disabled\u0026#34; If the value is disabled then the FQDN generation is disabled. ### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. ControllerSettings: serviceEngineGroupName: se-group-generic # Name of the ServiceEngine Group. controllerVersion: \u0026#39;22.1.4\u0026#39; # The controller API version cloudName: nsx-t-lhr # The configured cloud name on the Avi controller. controllerHost: \u0026#39;172.60.146.50\u0026#39; # IP address or Hostname of Avi Controller tenantName: admin # Name of the tenant where all the AKO objects will be created in AVI. nodePortSelector: # Only applicable if serviceType is NodePort key: \u0026#39;\u0026#39; value: \u0026#39;\u0026#39; resources: limits: cpu: 350m memory: 400Mi requests: cpu: 200m memory: 300Mi securityContext: {} podSecurityContext: {} rbac: # Creates the pod security policy if set to true pspEnable: false avicredentials: username: \u0026#39;\u0026#39; password: \u0026#39;\u0026#39; authtoken: certificateAuthorityData: persistentVolumeClaim: \u0026#39;\u0026#39; mountPath: /log logFile: avi.log Now when I am done editing the values file its time to deploy AKO.\nandreasm@ubuntu02:~/tkgi/cluster-1-nsx-manager-api$ helm install --generate-name oci://projects.registry.vmware.com/ako/helm-charts/ako --version 1.10.3 -f ako-values.1.10.3.yaml --namespace avi-system Pulled: projects.registry.vmware.com/ako/helm-charts/ako:1.10.3 Digest: sha256:1f2f9b89f4166737ed0d0acf4ebfb5853fb6f67b08ca1c8dae48e1dd99d31ab6 NAME: ako-1701092267 LAST DEPLOYED: Mon Nov 27 13:37:48 2023 NAMESPACE: avi-system STATUS: deployed REVISION: 1 TEST SUITE: None Its up and running and no errors in the logs. So far so good\nandreasm@ubuntu02:~/tkgi/cluster-1-nsx-manager-api$ k get pods -n avi-system NAME READY STATUS RESTARTS AGE ako-0 1/1 Running 0 81s andreasm@ubuntu02:~/tkgi/cluster-1-nsx-manager-api$ k logs -n avi-system ako-0 2023-11-27T13:38:04.192Z\tINFO\tapi/api.go:52\tSetting route for GET /api/status 2023-11-27T13:38:04.193Z\tINFO\tako-main/main.go:72\tAKO is running with version: v1.10.3 2023-11-27T13:38:04.193Z\tINFO\tako-main/main.go:82\tWe are running inside kubernetes cluster. Won\u0026#39;t use kubeconfig files. 2023-11-27T13:38:04.193Z\tINFO\tapi/api.go:110\tStarting API server at :8080 2023-11-27T13:38:04.278Z\tINFO\tako-main/main.go:157\tKubernetes cluster apiserver version 1.27 2023-11-27T13:38:04.285Z\tINFO\tutils/utils.go:168\tInitializing configmap informer in avi-system 2023-11-27T13:38:04.285Z\tINFO\tlib/dynamic_client.go:137\tSkipped initializing dynamic informers for cniPlugin ncp 2023-11-27T13:38:05.660Z\tINFO\tutils/avi_rest_utils.go:116\tOverwriting the controller version 22.1.4 to max Avi version 22.1.3 2023-11-27T13:38:05.660Z\tINFO\tutils/avi_rest_utils.go:119\tSetting the client version to the current controller version 22.1.3 2023-11-27T13:38:06.849Z\tINFO\tcache/controller_obj_cache.go:2345\tAvi cluster state is CLUSTER_UP_NO_HA 2023-11-27T13:38:07.019Z\tINFO\tcache/controller_obj_cache.go:3116\tSetting cloud vType: CLOUD_NSXT 2023-11-27T13:38:07.019Z\tINFO\tcache/controller_obj_cache.go:3119\tSetting cloud uuid: cloud-b739e6b0-f0c8-4259-b36b-450d227c633c 2023-11-27T13:38:07.019Z\tINFO\tlib/lib.go:291\tSetting AKOUser: ako-tkgi-cluster-1 for Avi Objects 2023-11-27T13:38:07.019Z\tINFO\tcache/controller_obj_cache.go:2855\tSkipping the check for SE group labels 2023-11-27T13:38:07.019Z\tINFO\tcache/controller_obj_cache.go:3395\tSkipping the check for Node Network 2023-11-27T13:38:07.134Z\tINFO\tcache/controller_obj_cache.go:3526\tSetting VRF The-Tier-1 found from network vip-l4-incluster-tkgi 2023-11-27T13:38:07.135Z\tINFO\trecord/event.go:282\tEvent(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;avi-system\u0026#34;, Name:\u0026#34;ako-0\u0026#34;, UID:\u0026#34;e0a321af-42c9-49cb-a3b5-cba17cf41805\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;65342\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ValidatedUserInput\u0026#39; User input validation completed. 2023-11-27T13:38:07.139Z\tINFO\tlib/lib.go:230\tSetting Disable Sync to: false 2023-11-27T13:38:07.143Z\tINFO\tk8s/ako_init.go:271\tavi k8s configmap created 2023-11-27T13:38:08.007Z\tWARN\trest/dequeue_nodes.go:65\tkey: admin/DummyVSForStaleData, msg: no model found for the key Do I have an Ingressclass now?\nandreasm@ubuntu02:~/tkgi/examples$ k get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 13m Yes I do \u0026#x1f604;\nTime to deploy some test applications, first out my favourite demo app, Yelb.\nThis just need a servicetype loadbalancer.\nBackend\nandreasm@ubuntu02:~/tkgi/examples$ k apply -f yelb-lb-backend.yaml service/redis-server created service/yelb-db created service/yelb-appserver created deployment.apps/redis-server created deployment.apps/yelb-db created deployment.apps/yelb-appserver created andreasm@ubuntu02:~/tkgi/examples$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-6cf478df95-vf7t8 0/1 ContainerCreating 0 5s yelb-appserver-bf75dbb5b-g6bmf 0/1 ContainerCreating 0 5s yelb-db-d4c64d9c-687sb 0/1 ContainerCreating 0 5s Frontend, where the request for a loadbalancer comes from.\nandreasm@ubuntu02:~/tkgi/examples$ k apply -f yelb-lb-frontend.yaml service/yelb-ui created deployment.apps/yelb-ui created andreasm@ubuntu02:~/tkgi/examples$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-server ClusterIP 10.100.200.110 \u0026lt;none\u0026gt; 6379/TCP 8m46s yelb-appserver ClusterIP 10.100.200.170 \u0026lt;none\u0026gt; 4567/TCP 8m46s yelb-db ClusterIP 10.100.200.84 \u0026lt;none\u0026gt; 5432/TCP 8m46s yelb-ui LoadBalancer 10.100.200.123 10.146.102.100 80:30400/TCP 31s There it is..\nNow what happens in the NSX ALB ui?\nIn vCenter the first SE has been deployed.\nAnd the Service is UP\nCan I access it?\nLet me just use the automatically dns record created by the AKO integration with NSX ALB DNS service. This means I have enabled the DNS service in NSX ALB to auto create DNS records for me whenever I create servicetype Loabalancer or Ingress objects. So I dont have to think about that.\nIn this case the FQDN of my yelb ui service is yelb-ui-yelb.this-is-a.domain.net\nI will grab the ip of the service and do a curl\nandreasm@ubuntu02:~/tkgi/examples$ curl yelb-ui-yelb.this-is-a.domain.net \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Nice\nNow lets test the Ingress.\nI will deploy two pods with corresponding ClusterIP services that has one purpose in life and that is displaying apple and banana respectively. Then I will deploy an Ingress pointing to these two services.\nandreasm@ubuntu02:~/tkgi/examples$ k apply -f apple.yaml -f banana.yaml pod/apple-app created service/apple-service created pod/banana-app created service/banana-service created andreasm@ubuntu02:~/tkgi/examples$ k get svc -n fruit NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apple-service ClusterIP 10.100.200.33 \u0026lt;none\u0026gt; 5678/TCP 26s banana-service ClusterIP 10.100.200.231 \u0026lt;none\u0026gt; 5678/TCP 26s andreasm@ubuntu02:~/tkgi/examples$ k apply -f ingress-example-generic.yaml ingress.networking.k8s.io/ingress-example created andreasm@ubuntu02:~/tkgi/examples$ k get ingress -n fruit NAME CLASS HOSTS ADDRESS PORTS AGE ingress-example avi-lb fruit-tkgi.this-is-a.domain.net 10.146.102.101 80 7s And in my NSX ALB UI\nNow can I reach it? Lets test using the FQDN:\nandreasm@ubuntu02:~/tkgi/examples$ curl fruit-tkgi.this-is-a.domain.net/apple apple andreasm@ubuntu02:~/tkgi/examples$ curl fruit-tkgi.this-is-a.domain.net/banana banana Works like a charm\u0026hellip;\nNow back to my notice above about doing this in an unsupported way and placing the SEs on different T1s (which I cant understand how is possible to do anyway). By just doing a traceflow from one of the SEs data nic to the worker nodes of my Kubernetes cluster, it will involve the Edge cluster as the destination T1 is centralised (being active/passive and placed on an Edge cluster). But when using products like TKGi, TKGs the auto created Tier1 routers tend to be created as centralized routers, so we will alway have this issue when traffic is going through the Tier-0/Edges.\nA quick diagram of how this topology looks like networking wise:\nHere my Tier1 routers for the PKS segments will be created by TKGi and I cant configre Avi to use somehting that does not exist.\nWell, it was a nice test. It worked.. Let me tear it all down and go ahead with an installation using Antrea as the CNI.\nDeploy workload clusters using Antrea # In this approach there will be no automatic assignement of a L4 loadbalancer for the Kubernetes API endpoints, that is something I need to manually provide myself, and I will be using NSX ALB for that purpose also.\nHere is my TKGi installation for this section:\nFirst I will need to create a Kubernetes cluster. I will not use the TKGi MC to deploy the cluster now, I will use the TKGi CLI to to that, just to make it a bit more interesting. I still have the option to use TKGi MC, but I have already used that approach above. More info on TKGi CLI here\nLogin to the TKGi API\nandreasm@ubuntu02:~/tkgi$ tkgi login -a tkgi-api.this-is-a.domain.net -u admin -p \u0026#39;password\u0026#39; --ca-cert ca/tkgi-ca.crt API Endpoint: tkgi-api.this-is-a.domain.net User: admin Login successful. All the relevant information can be found in the TKGi MC Deployment Metadata page.\nNow its just all about creating the same cluster as I did in TKGi MC just using cli instead. There is no need to create any network profiles this time as there is no NSX-T that provides any loadbalancer functionality. It is a \u0026ldquo;vanilla\u0026rdquo; Kubernetes cluster with Antrea as the CNI, so I will need to add the services I need. I will start by listing the plans available. I will go with the medium plan, deploy 3 control plane nodes and 3 worker nodes. The reason for three control plane nodes is because I would like to utilize the Avi LB for the the Kubernetes API endpoint and the it is much more interesting to have more than 1 cp node as the loadbalancer should have something to loadbalance. The purpose of a loadbalancer\u0026hellip;.\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ tkgi network-profiles Name Description # No profiles as its not needed andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ tkgi plans Name ID Description Small 8A0E21A8-8072-4D80-B365-D1F502085560 This plan will configure a lightweight Kubernetes cluster. Not recommended for production workloads. Medium 58375a45-17f7-4291-acf1-455bfdc8e371 Example: This plan will configure a medium sized Kubernetes cluster, suitable for more pods. Large 241118e5-69b2-4ef9-b47f-4d2ab071aff5 Example: This plan will configure a large Kubernetes cluster for resource heavy workloads, or a high number of workloads. Now, lets create a cluster\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ tkgi create-cluster tkgi-cluster-1-antrea --external-hostname tkgi-cluster-1-antrea.this-is-a.domain.net --plan Medium --num-nodes 3 PKS Version: 1.18.0-build.46 Name: tkgi-cluster-1-antrea K8s Version: 1.27.5 Plan Name: Medium UUID: 518d850c-ca34-4163-85cb-dda5d915abda Last Action: CREATE Last Action State: in progress Last Action Description: Creating cluster Kubernetes Master Host: tkgi-cluster-1-antrea.this-is-a.domain.net Kubernetes Master Port: 8443 Worker Nodes: 3 Kubernetes Master IP(s): In Progress Network Profile Name: Kubernetes Profile Name: Compute Profile Name: NSX Policy: false Tags: Use \u0026#39;tkgi cluster tkgi-cluster-1-antrea\u0026#39; to monitor the state of your cluster In vCenter I can see that TKGi has been so kind to spread my Kubernetes cluster across all three availability zones:\nAnd in my TKGi MC: After a while it is ready:\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ tkgi clusters PKS Version Name k8s Version Plan Name UUID Status Action 1.18.0-build.46 tkgi-cluster-1-antrea 1.27.5 Medium 518d850c-ca34-4163-85cb-dda5d915abda succeeded CREATE andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ tkgi cluster tkgi-cluster-1-antrea PKS Version: 1.18.0-build.46 Name: tkgi-cluster-1-antrea K8s Version: 1.27.5 Plan Name: Medium UUID: 518d850c-ca34-4163-85cb-dda5d915abda Last Action: CREATE Last Action State: succeeded Last Action Description: Instance provisioning completed Kubernetes Master Host: tkgi-cluster-1-antrea.this-is-a.domain.net Kubernetes Master Port: 8443 Worker Nodes: 3 Kubernetes Master IP(s): 10.146.41.4, 10.146.41.2, 10.146.41.3 Network Profile Name: Kubernetes Profile Name: Compute Profile Name: NSX Policy: false Tags: In TKGiMC\nNow as I can see from the output above, I can reach my cluster using any of my 3 control plane nodes. I would like to reach them using one singe entry, and with the real external name. I will now go ahead and create the Virtual Service in NSX Advanced Loadbalancer.\nConfigure NSX Advanced Loadbalancer as Kuberntes API endpoint provider # In NSX ALB, go to Virtual Services and create a new Virtual Sevice, and select advanced.\nThen I will configure my Virtual Service accordingly below:\nThe Virtual Service Settings:\nThe pool containing my Control Plane nodes (notice the control plane nodes dont use the defaul Kubernetes 6443 port, it is using 8443):\nThen it is the VS VIP. Here I will also add a FQDN/DNS entry called tkgi-cluster-1-antrea.this-is-a.domain.net\nLast step I just select the SE group I want to use.\nFinish.\nAnd my Virtual Service should now be green and I can use this entrypoint to reach my newly created Kubernetes Cluster.\nThe FQDN/DNS record is being automatically created by the Avi DNS service. So I should be able to use DNS instead of ip:\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ ping tkgi-cluster-1-antrea.this-is-a.domain.net PING tkgi-cluster-1-antrea.this-is-a.domain.net (10.146.101.10) 56(84) bytes of data. 64 bytes from 10.146.101.10 (10.146.101.10): icmp_seq=1 ttl=63 time=0.562 ms 64 bytes from 10.146.101.10 (10.146.101.10): icmp_seq=2 ttl=63 time=0.602 ms Getting the Kubernetes context and logging in:\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ kubectl config set-cluster tkgi-cluster-1-antrea --server=https://tkgi-cluster-1-antrea.this-is-a.domain.net:8443 \u0026amp;\u0026amp; \\ kubectl config set clusters.tkgi-cluster-1-antrea.certificate-authority-data Ci0tLS0tQkVHSU4gQ0VSVElGSUNBVEUtLS0tLQpNSUlET3pDQ0FpT2dBd0lCQWdJVVlGdDNxVFJLYXQ5T3hxOS9kYzdzbkNnSXRMa3dEUVlKS29aSWh2Y05BUUVMCkJRQXdMVEVMTUFrR0ExVUVBeE1DUTBFeERUQUxCZ05WQkFzVEJGUkxSMGt4RHpBTkJnTlZCQW9UQmxaTmQyRnkKWlRBZUZ3MHlNekV5TURFd09URXpNREZhRncweU56RXlNREV3T1RFek1ERmFNQzB4Q3pBSkJnTlZCQU1UQWtOQgpNUTB3Q3dZRFZRUUxFd1JVUzBkSk1ROHdEUVlEVlFRS0V3WldUWGRoY21Vd2dnRWlNQTBHQ1NxR1NJYjNEUUVCCkFRVUFBNElCRHdBd2dnRUtBb0lCQVFDNS9lcDdFLzFQOXJPanpxNlNJVkRaN2dDTEJ0L21GZW5oQVNNQUM0WUEKUUVmUnlzWlloZUhxRkZxVENEcEpRZzFuUDFmZzNEdDhhVWNpaGR6VEEremJvS0R6WStjWk9EVy9yM1YvR2Y1bwpIdHd2SDIrWkEyT1lydlVrOGExVUwvb1hKNHdlWFBsWUdndWJoeXcrSU44WUdGdEVQQnlmaTQrSEFQdDFveWlTCnN5VUF1bHFiOUwyU29xQ3Zmc3cwY2cyN0l5RktvN1FUSmFiTnd1MXdQbnhnWCtwa2M4dTdsSnZucWJnMm1OeUYKYVpxZGRqMHN1YmplSG9VN0Z3b3U5ZHN4SVVCYlZSbWxQVkc1S1JSN3pVdWNSTjdLKzYyS1p2ZWMwRGNsaW13SApONEhqUFJSczN4OEpVRXFEajU5MWcrT0NUTnFqK0pIVm9sQnFJbi9RcUlOVkFnTUJBQUdqVXpCUk1CMEdBMVVkCkRnUVdCQlRUMVRleVlMcGtaRzZnZGxXU05pVXlIaURUeURBZkJnTlZIU01FR0RBV2dCVFQxVGV5WUxwa1pHNmcKZGxXU05pVXlIaURUeURBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFBdApqY09iOFFjUmxWM2h6YXc2R2YzdnpxeFRXYXl4b1NiK1ZFSCtUTEFuZERRaVZQMjR2OS9uektYVUl5M0pPQnlQCnVmNnJJQ2Viekt6WUtDNC9hb2JpRmJBcmRiajd3c1UyNGMvUHdPdUgya0l2SnY0Q3lwOHZ3Umh3aUdnZUZCNFoKMEdLNkJ0VGdKVW9QblhuZnYreGZrRzFMN0Jod0Z6aC9YM2lNSmp4S21tenpLcXRBZG5aajMvbFhaUlAzVUFSbgpuQTlBakVDbkpxSU5ENGhLK1p4cjhaVy81a0NtM2xWL3BRMHZMSXF6czBmN1RMS2hHYXhieFExeDBpRmxwS3JoCkIzU1lhTDdQTmJCUzYzWE50ZjlZQnNPYmFmNFErbFFlVlRMQ2ZkZGwxN0ZzaXJ5T0xQK09aK2pmUnJQSmdPbmcKR281VU8xZ0RyR1dkWHpYK1NRZmgKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo= \u0026amp;\u0026amp; \\ kubectl config set-credentials 93a89395-658d-4058-b1f2-5a47a50b51f9 --token=eyJhbGciOiJSUzI1NiIsImtpZCI6IkVCUzkyVFExNktOTzV5bF9ITG1aYWNiYzMxQk9IZnVXMHF2MFNMQ1FqU2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6IjkzYTg5Mzk1LTY1OGQtNDA1OC1iMWYyLTVhNDdhNTBiNTFmOSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiI5M2E4OTM5NS02NThkLTQwNTgtYjFmMi01YTQ3YTUwYjUxZjkiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI4NDk0YjZjNi03OTU0LTRiMWMtOWMwMC0yYTdjODFiMDkzZTIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6ZGVmYXVsdDo5M2E4OTM5NS02NThkLTQwNTgtYjFmMi01YTQ3YTUwYjUxZjkifQ.CJg_eMksCi9eaEaKsB1tjy083owleoBH9tYM2olEpRyU5GQOc3tkRE0r9SqUhe67FB9JWruyuA6QwgyOGxGfJvi1r_spoQeQg7Xzn50N1OtKPocdTDlWUVgxXeqR1OBgPE3pfSIyvX7Hhf9TN-5oM7GSrCmwtVGhFyI3SH1VukYg63PjwrsBMlwi1l-WvOkKgDmishC7-bn2sep2z0GEGMZHC6eipt7kYOVsK19QSq-U9Z2yDWfOgctmRShZx0V0BbvqXonHFej8yD79nFEasX5BsrWhUXEldjU2teKB-cQjPFju-GjSKJEOybtRf_Pu0XBjJngaHOpjMzX-s9Xolg \u0026amp;\u0026amp; \\ kubectl config set-context tkgi-cluster-1-antrea --cluster=tkgi-cluster-1-antrea --user=93a89395-658d-4058-b1f2-5a47a50b51f9 \u0026amp;\u0026amp; \\ kubectl config use-context tkgi-cluster-1-antrea Cluster \u0026#34;tkgi-cluster-1-antrea\u0026#34; set. Property \u0026#34;clusters.tkgi-cluster-1-antrea.certificate-authority-data\u0026#34; set. User \u0026#34;93a89395-658d-4058-b1f2-5a47a50b51f9\u0026#34; set. Context \u0026#34;tkgi-cluster-1-antrea\u0026#34; created. Switched to context \u0026#34;tkgi-cluster-1-antrea\u0026#34;. andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k config current-context tkgi-cluster-1-antrea andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k get nodes NAME STATUS ROLES AGE VERSION 7c0f802f-0ecf-47c7-adc6-2dbf3f8b3ccb Ready \u0026lt;none\u0026gt; 54m v1.27.5+vmware.1 9b83d6a1-402a-46b2-9645-cebfe5db5f23 Ready \u0026lt;none\u0026gt; 58m v1.27.5+vmware.1 dac2d224-22cb-4254-be93-bebf131f40e7 Ready \u0026lt;none\u0026gt; 50m v1.27.5+vmware.1 Now I just need to do the same steps as in the section where I used NSX/NCP to configure NSX ALB as my L4 and L7 provider in my Kubernetes cluster\nConfigure NSX Advanced Loadbalancer as Layer 4 and Layer 7 provider inside Kubernetes cluster # I am logged into my Kubernetes cluster tkgi-cluster-1-antrea\nI need to check whether a certain Antrea feature is enabled, the NodePortLocal. So I will check the configmap of Antrea and confirm whether this is enabled or not. I do think it is default enabled from a certain Antrea version and up, but its good to be sure.\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k get configmaps -n kube-system antrea-config -oyaml # Enable NodePortLocal feature to make the Pods reachable externally through NodePort NodePortLocal: true The reason I want to know this is because I can then configure AKO to use NPL instead of NodePort or ClusterIP. NPL has some benefits over the two.\nNow, prepare my AKO value yaml. Below is the the value yaml I will use when deploying AKO for this cluster:\n# Default values for ako. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: projects.registry.vmware.com/ako/ako pullPolicy: IfNotPresent ### This section outlines the generic AKO settings AKOSettings: primaryInstance: true # Defines AKO instance is primary or not. Value `true` indicates that AKO instance is primary. In a multiple AKO deployment in a cluster, only one AKO instance should be primary. Default value: true. enableEvents: \u0026#39;true\u0026#39; # Enables/disables Event broadcasting via AKO logLevel: WARN # enum: INFO|DEBUG|WARN|ERROR fullSyncFrequency: \u0026#39;1800\u0026#39; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. apiServerPort: 8080 # Internal port for AKO\u0026#39;s API server for the liveness probe of the AKO pod default=8080 deleteConfig: \u0026#39;false\u0026#39; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI disableStaticRouteSync: \u0026#39;false\u0026#39; # If the POD networks are reachable from the Avi SE, set this knob to true. clusterName: tkgi-cluster-1-antrea # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT cniPlugin: \u0026#39;antrea\u0026#39; # Set the string if your CNI is calico or openshift or ovn-kubernetes. For Cilium CNI, set the string as cilium only when using Cluster Scope mode for IPAM and leave it empty if using Kubernetes Host Scope mode for IPAM. enum: calico|canal|flannel|openshift|antrea|ncp|ovn-kubernetes|cilium enableEVH: false # This enables the Enhanced Virtual Hosting Model in Avi Controller for the Virtual Services layer7Only: false # If this flag is switched on, then AKO will only do layer 7 loadbalancing. # NamespaceSelector contains label key and value used for namespacemigration # Same label has to be present on namespace/s which needs migration/sync to AKO namespaceSelector: labelKey: \u0026#39;\u0026#39; labelValue: \u0026#39;\u0026#39; servicesAPI: false # Flag that enables AKO in services API mode: https://kubernetes-sigs.github.io/service-apis/. Currently implemented only for L4. This flag uses the upstream GA APIs which are not backward compatible # with the advancedL4 APIs which uses a fork and a version of v1alpha1pre1 vipPerNamespace: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to create Parent VS per Namespace in EVH mode istioEnabled: false # This flag needs to be enabled when AKO is be to brought up in an Istio environment # This is the list of system namespaces from which AKO will not listen any Kubernetes or Openshift object event. blockedNamespaceList: [] # blockedNamespaceList: # - kube-system # - kube-public ipFamily: \u0026#39;\u0026#39; # This flag can take values V4 or V6 (default V4). This is for the backend pools to use ipv6 or ipv4. For frontside VS, use v6cidr useDefaultSecretsOnly: \u0026#39;false\u0026#39; # If this flag is set to true, AKO will only handle default secrets from the namespace where AKO is installed. # This flag is applicable only to Openshift clusters. ### This section outlines the network settings for virtualservices. NetworkSettings: ## This list of network and cidrs are used in pool placement network for vcenter cloud. ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. ## Either networkName or networkUUID should be specified. ## If duplicate networks are present for the network name, networkUUID should be used for appropriate network. nodeNetworkList: - networkName: \u0026#34;ls-tkgi-service-network\u0026#34; cidrs: - 10.146.41.0/24 # - 11.0.0.1/24 enableRHI: false # This is a cluster wide setting for BGP peering. nsxtT1LR: \u0026#39;/infra/tier-1s/The-Tier-1\u0026#39; # T1 Logical Segment mapping for backend network. Only applies to NSX-T cloud. bgpPeerLabels: [] # Select BGP peers using bgpPeerLabels, for selective VsVip advertisement. # bgpPeerLabels: # - peer1 # - peer2 # Network information of the VIP network. Multiple networks allowed only for AWS Cloud. # Either networkName or networkUUID should be specified. # If duplicate networks are present for the network name, networkUUID should be used for appropriate network. vipNetworkList: - networkName: vip-l4-incluster-tkgi cidr: 10.146.102.0/24 # v6cidr: 2002::1234:abcd:ffff:c0a8:101/64 # Setting this will enable the VS networks to use ipv6 ### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. L7Settings: defaultIngController: \u0026#39;true\u0026#39; noPGForSNI: false # Switching this knob to true, will get rid of poolgroups from SNI VSes. Do not use this flag, if you don\u0026#39;t want http caching. This will be deprecated once the controller support caching on PGs. serviceType: NodePortLocal # enum NodePort|ClusterIP|NodePortLocal shardVSSize: SMALL # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL, DEDICATED passthroughShardSize: SMALL # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL enableMCI: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to start processing multi-cluster ingress objects. ### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. L4Settings: defaultDomain: \u0026#39;\u0026#39; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. autoFQDN: flat # ENUM: default(\u0026lt;svc\u0026gt;.\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), flat (\u0026lt;svc\u0026gt;-\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), \u0026#34;disabled\u0026#34; If the value is disabled then the FQDN generation is disabled. ### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. ControllerSettings: serviceEngineGroupName: se-group-generic # Name of the ServiceEngine Group. controllerVersion: \u0026#39;22.1.4\u0026#39; # The controller API version cloudName: nsx-t-lhr # The configured cloud name on the Avi controller. controllerHost: \u0026#39;172.60.146.50\u0026#39; # IP address or Hostname of Avi Controller tenantName: admin # Name of the tenant where all the AKO objects will be created in AVI. nodePortSelector: # Only applicable if serviceType is NodePort key: \u0026#39;\u0026#39; value: \u0026#39;\u0026#39; resources: limits: cpu: 350m memory: 400Mi requests: cpu: 200m memory: 300Mi securityContext: {} podSecurityContext: {} rbac: # Creates the pod security policy if set to true pspEnable: false Now its time do deploy AKO\nandreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k create ns avi-system namespace/avi-system created andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ helm install --generate-name oci://projects.registry.vmware.com/ako/helm-charts/ako --version 1.10.3 -f ako-values.1.10.3.yaml --namespace avi-system Pulled: projects.registry.vmware.com/ako/helm-charts/ako:1.10.3 Digest: sha256:1f2f9b89f4166737ed0d0acf4ebfb5853fb6f67b08ca1c8dae48e1dd99d31ab6 NAME: ako-1701427075 LAST DEPLOYED: Fri Dec 1 10:38:03 2023 NAMESPACE: avi-system STATUS: deployed REVISION: 1 TEST SUITE: None andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 19s andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k get pods -n avi-system NAME READY STATUS RESTARTS AGE ako-0 1/1 Running 0 36s andreasm@ubuntu02:~/tkgi/cluster-1-antrea$ k logs -n avi-system ako-0 2023-12-01T10:38:15.457Z\tINFO\tapi/api.go:52\tSetting route for GET /api/status 2023-12-01T10:38:15.458Z\tINFO\tako-main/main.go:72\tAKO is running with version: v1.10.3 2023-12-01T10:38:15.458Z\tINFO\tako-main/main.go:82\tWe are running inside kubernetes cluster. Won\u0026#39;t use kubeconfig files. 2023-12-01T10:38:15.458Z\tINFO\tapi/api.go:110\tStarting API server at :8080 2023-12-01T10:38:15.540Z\tINFO\tako-main/main.go:157\tKubernetes cluster apiserver version 1.27 2023-12-01T10:38:15.551Z\tINFO\tutils/utils.go:168\tInitializing configmap informer in avi-system 2023-12-01T10:38:15.551Z\tINFO\tlib/dynamic_client.go:137\tSkipped initializing dynamic informers for cniPlugin antrea 2023-12-01T10:38:16.029Z\tINFO\tutils/avi_rest_utils.go:116\tOverwriting the controller version 22.1.4 to max Avi version 22.1.3 2023-12-01T10:38:16.030Z\tINFO\tutils/avi_rest_utils.go:119\tSetting the client version to the current controller version 22.1.3 2023-12-01T10:38:17.176Z\tINFO\tcache/controller_obj_cache.go:2345\tAvi cluster state is CLUSTER_UP_NO_HA 2023-12-01T10:38:17.337Z\tINFO\tcache/controller_obj_cache.go:3116\tSetting cloud vType: CLOUD_NSXT 2023-12-01T10:38:17.337Z\tINFO\tcache/controller_obj_cache.go:3119\tSetting cloud uuid: cloud-b739e6b0-f0c8-4259-b36b-450d227c633c 2023-12-01T10:38:17.337Z\tINFO\tlib/lib.go:291\tSetting AKOUser: ako-tkgi-cluster-1-antrea for Avi Objects 2023-12-01T10:38:17.337Z\tINFO\tcache/controller_obj_cache.go:2855\tSkipping the check for SE group labels 2023-12-01T10:38:17.337Z\tINFO\tcache/controller_obj_cache.go:3395\tSkipping the check for Node Network 2023-12-01T10:38:17.442Z\tINFO\tcache/controller_obj_cache.go:3526\tSetting VRF The-Tier-1 found from network vip-l4-incluster-tkgi 2023-12-01T10:38:17.443Z\tINFO\trecord/event.go:282\tEvent(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;avi-system\u0026#34;, Name:\u0026#34;ako-0\u0026#34;, UID:\u0026#34;3891605f-8d92-43ed-8b4f-a03b7bf3e0d4\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;21070\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ValidatedUserInput\u0026#39; User input validation completed. 2023-12-01T10:38:17.449Z\tINFO\tlib/lib.go:230\tSetting Disable Sync to: false 2023-12-01T10:38:17.456Z\tINFO\tk8s/ako_init.go:271\tavi k8s configmap created Thats it.. Now its time for some test applications. Exact same procedure as previous. The Yelb app and the Banana/Apple Ingress application. First out the L4 ServiceType Loadbalancer (Yelb).\nandreasm@ubuntu02:~/tkgi$ k create ns yelb namespace/yelb created andreasm@ubuntu02:~/tkgi$ cd examples/ andreasm@ubuntu02:~/tkgi/examples$ k apply -f yelb-lb-backend.yaml service/redis-server created service/yelb-db created service/yelb-appserver created deployment.apps/redis-server created deployment.apps/yelb-db created deployment.apps/yelb-appserver created Waiting for them to be downloaded and started, then the frontend and I should get an service in NSX ALB created for me.\nandreasm@ubuntu02:~/tkgi/examples$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-6cf478df95-t96fn 1/1 Running 0 48s yelb-appserver-bf75dbb5b-vdrr5 1/1 Running 0 48s yelb-db-d4c64d9c-mmfbz 1/1 Running 0 48s andreasm@ubuntu02:~/tkgi/examples$ k apply -f yelb-lb-frontend yelb-lb-frontend-w-lb.class.yaml yelb-lb-frontend.yaml andreasm@ubuntu02:~/tkgi/examples$ k apply -f yelb-lb-frontend.yaml service/yelb-ui created deployment.apps/yelb-ui created andreasm@ubuntu02:~/tkgi/examples$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-server ClusterIP 10.20.99.56 \u0026lt;none\u0026gt; 6379/TCP 93s yelb-appserver ClusterIP 10.20.190.198 \u0026lt;none\u0026gt; 4567/TCP 93s yelb-db ClusterIP 10.20.241.6 \u0026lt;none\u0026gt; 5432/TCP 93s yelb-ui LoadBalancer 10.20.150.139 10.146.102.100 80:32498/TCP 4s Anything in NSX ALB?\nYes it is, does it work?\nandreasm@ubuntu02:~/tkgi/examples$ curl yelb-ui-yelb.this-is-a.domain.net \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Yes it does. Now the last step is an Ingress.\nSame procedure as previous there also.\nandreasm@ubuntu02:~/tkgi/examples$ k create ns fruit namespace/fruit created andreasm@ubuntu02:~/tkgi/examples$ k apply -f apple.yaml -f banana.yaml pod/apple-app created service/apple-service created pod/banana-app created service/banana-service created andreasm@ubuntu02:~/tkgi/examples$ k get svc -n fruit NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apple-service ClusterIP 10.20.143.112 \u0026lt;none\u0026gt; 5678/TCP 21s banana-service ClusterIP 10.20.184.169 \u0026lt;none\u0026gt; 5678/TCP 21s Now the Ingress\nandreasm@ubuntu02:~/tkgi/examples$ k apply -f ingress-example-generic.yaml ingress.networking.k8s.io/ingress-example created andreasm@ubuntu02:~/tkgi/examples$ k get ingress -n fruit NAME CLASS HOSTS ADDRESS PORTS AGE ingress-example avi-lb fruit-tkgi.this-is-a.domain.net 10.146.102.102 80 5s Anything in NSX-ALB?\nYes it is, notice even the ports and the backend IPs.. It is using NodeportLocal and pointing to the nodes holding the pods apple and banana.\nDoes it work?\nandreasm@ubuntu02:~/tkgi/examples$ curl fruit-tkgi.this-is-a.domain.net/banana banana andreasm@ubuntu02:~/tkgi/examples$ curl fruit-tkgi.this-is-a.domain.net/apple apple Yes it does. This concludes this excercise. Continue for the closout section\u0026hellip;\nA note on the VIP # A often get the question on how the VIP is being realized. In NSX ALB we can configure the VIP to arp\u0026rsquo;ed (in lack of other words) in the same network as the SEs dataplane nic. Or it can be something completely different. It is the SEs that are responsible of \u0026ldquo;advertising\u0026rdquo; the VIP for a VS. So one way or the other the network overall needs to know where this VIP is coming from. The simplest method (if not having the benefit of using NSX) is advertising the VIP inside the same network as the SE dataplane network. That means the VIP will be part of the same subnet as the SEs actual dataplane ip. This dont require any routes configured in the the network as the VIP will be arp\u0026rsquo;ed by the responsible SEs in that same network using the same subnet.\nBut what if one does not want to use the same network as the SE dataplane network. Well, that can be done also. But then we suddenly need to consider how to advertise this VIP. It can be done using a specific nic on the SE only used for VIPs (this also consumes the available nics on the SEs) or it can be used by just creating a VIP address and advertise/inform the network where it comes from and how to reach it. This approach is what I have been using in this whole post. I will quickly explain how this can be done with NSX ALB configured with a NSX-T cloud. Very basic explained. This way I will configure the SEs with min 2 nics. One for the management, to NSX ALB controller connectivity, and one as a dedicated dataplane nic. When I create a VIP IP profile in NSX-ALB I can create Virtual Services to use these VIP IP profiles as ip addresses for my VS VIPs. The benefit of using a NSX cloud here is that Avi will automatically inject static routes on the Tier-1 router to announce where this VIP comes from, where the static route is a host route /32 pointng to the SEs that are realising the VIP. So the Tier will advertise this to the Tier-0 (if confiugured in NSX to do so) and the Tier-0 will further advertise it to its BGP peer and out in the \u0026ldquo;wild\u0026rdquo; (physical network).\nIf not using NSX, Avi also supports BGP natively and can advertise the vips on its own. Meaning the SEs needs to be confgured to peer with BGP neighbours or one can use static routes. In terms of redundancy, availability BGP is the preferred method.\nA diagram using NSX cloud and VIPs not tied to any SE nic or network, but a VIP true to its name (Virtual IP).\nThats it for me..\nLets see what I will write about the next time. Something completely different maybe\u0026hellip; who knows what the future might bring.\n","date":"23 November 2023","externalUrl":null,"permalink":"/2023/11/23/tkgi-with-nsx-and-nsx-advanced-loadbalancer/","section":"Posts","summary":"In this post I will go through installation of TKGi, using the EPMC installer, then how to configure TKGi to use NSX Advanced LoadBalancer","title":"TKGi with NSX and NSX Advanced LoadBalancer","type":"posts"},{"content":" Antrea Multi-cluster # From the official Antrea.io documentation:\nAntrea Multi-cluster implements Multi-cluster Service API, which allows users to create multi-cluster Services that can be accessed cross clusters in a ClusterSet. Antrea Multi-cluster also supports Antrea ClusterNetworkPolicy replication. Multi-cluster admins can define ClusterNetworkPolicies to be replicated across the entire ClusterSet, and enforced in all member clusters.\nAn Antrea Multi-cluster ClusterSet includes a leader cluster and multiple member clusters. Antrea Multi-cluster Controller needs to be deployed in the leader and all member clusters. A cluster can serve as the leader, and meanwhile also be a member cluster of the ClusterSet.\nThe diagram below depicts a basic Antrea Multi-cluster topology with one leader cluster and two member clusters.\nIn this post I will go through how to configure Antrea Multi-cluster in TKG 2.3 and Tanzu with vSphere. As of the time I am writing this post (end of September begining of October 2023) Tanzu with vSphere does not have all the feature gates available right now to be able to configure Antrea Multi-cluster, so this will be added later. After the initial configuration and installation of Antrea Multi-cluster I will go through the different possibilities (features) with Antrea Multi-cluster, with configuration and examples in each of their own sections. The first sections involving how to enable Antrea Multi-cluster feature gate is specific for the Kubernetes \u0026ldquo;distribution\u0026rdquo; it is enabled on (TKG, vSphere with Tanzu, upstream Kubernetes etc). After this initial config the rest is generic and can be re-used for all types of Kubernetes platforms. I will go through everything step by step to learn what the different \u0026ldquo;moving\u0026rdquo; parts are doing and how they work. At the end I have a bonus chapter where I have created a menu driven script that automates or simplify the whole process.\nAntrea Feature Gates # Antrea has a set of Feature Gates that can be enabled or disabled on both the Antrea Controller and Antrea Agent, depending on the feature. These are configured using the corresponding antrea-config configMap. For a list of available features head over to the antrea.io documentation page here. Depending on the Kubernetes platform, and when the settings are applied, these features may be enabled in different ways. This post will specifically cover how to enable the Antrea Multi-cluster Feature Gate in Tanzu Kubernetes Grid and vSphere with Tanzu (not available yet).\nConfiguring Antrea Multi-cluster in TKG 2.3 with Antrea v1.11.1 # The following procedure may not at the time writing this post be officially supported - will get back and confirm this\nUsing Tanzu Kubernetes Grid the Antrea Feature Gates can be configured during provisioning of the workload clusters or post cluster provision. I will be enabling the Antrea Multi-cluster feature gate during cluster provisioning. If one need to enable these feature gates post cluster provision one must edit the antreaconfigs crd at the Management cluster level for the corresponding TKG cluster. See below.\nk get antreaconfigs.cni.tanzu.vmware.com -n tkg-ns-1 NAME TRAFFICENCAPMODE DEFAULTMTU ANTREAPROXY ANTREAPOLICY SECRETREF tkg-cluster-1 encap true true tkg-cluster-1-antrea-data-values tkg-cluster-2 encap true true tkg-cluster-2-antrea-data-values tkg-cluster-3 encap true true tkg-cluster-3-antrea-data-values If I take a look at the yaml values for any of these antreaconfigs:\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;cni.tanzu.vmware.com/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;AntreaConfig\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;tkg-cluster-1\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-ns-1\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;antrea\u0026#34;:{\u0026#34;config\u0026#34;:{\u0026#34;antreaProxy\u0026#34;:{\u0026#34;nodePortAddresses\u0026#34;:[],\u0026#34;proxyAll\u0026#34;:false,\u0026#34;proxyLoadBalancerIPs\u0026#34;:true,\u0026#34;skipServices\u0026#34;:[]},\u0026#34;cloudProvider\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;\u0026#34;},\u0026#34;disableTXChecksumOffload\u0026#34;:false,\u0026#34;disableUdpTunnelOffload\u0026#34;:false,\u0026#34;dnsServerOverride\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;egress\u0026#34;:{\u0026#34;exceptCIDRs\u0026#34;:[],\u0026#34;maxEgressIPsPerNode\u0026#34;:255},\u0026#34;enableBridgingMode\u0026#34;:null,\u0026#34;enableUsageReporting\u0026#34;:false,\u0026#34;featureGates\u0026#34;:{\u0026#34;AntreaIPAM\u0026#34;:false,\u0026#34;AntreaPolicy\u0026#34;:true,\u0026#34;AntreaProxy\u0026#34;:true,\u0026#34;AntreaTraceflow\u0026#34;:true,\u0026#34;Egress\u0026#34;:true,\u0026#34;EndpointSlice\u0026#34;:true,\u0026#34;FlowExporter\u0026#34;:false,\u0026#34;L7NetworkPolicy\u0026#34;:false,\u0026#34;Multicast\u0026#34;:false,\u0026#34;Multicluster\u0026#34;:true,\u0026#34;NetworkPolicyStats\u0026#34;:false,\u0026#34;NodePortLocal\u0026#34;:true,\u0026#34;SecondaryNetwork\u0026#34;:false,\u0026#34;ServiceExternalIP\u0026#34;:false,\u0026#34;SupportBundleCollection\u0026#34;:false,\u0026#34;TopologyAwareHints\u0026#34;:false,\u0026#34;TrafficControl\u0026#34;:false},\u0026#34;flowExporter\u0026#34;:{\u0026#34;activeFlowTimeout\u0026#34;:\u0026#34;60s\u0026#34;,\u0026#34;idleFlowTimeout\u0026#34;:\u0026#34;15s\u0026#34;,\u0026#34;pollInterval\u0026#34;:\u0026#34;5s\u0026#34;},\u0026#34;kubeAPIServerOverride\u0026#34;:null,\u0026#34;multicast\u0026#34;:{\u0026#34;igmpQueryInterval\u0026#34;:\u0026#34;125s\u0026#34;},\u0026#34;multicastInterfaces\u0026#34;:[],\u0026#34;multicluster\u0026#34;:{\u0026#34;enable\u0026#34;:true,\u0026#34;enablePodToPodConnectivity\u0026#34;:true,\u0026#34;enableStretchedNetworkPolicy\u0026#34;:true,\u0026#34;namespace\u0026#34;:\u0026#34;antrea-multicluster\u0026#34;},\u0026#34;noSNAT\u0026#34;:false,\u0026#34;nodePortLocal\u0026#34;:{\u0026#34;enabled\u0026#34;:true,\u0026#34;portRange\u0026#34;:\u0026#34;61000-62000\u0026#34;},\u0026#34;serviceCIDR\u0026#34;:\u0026#34;10.132.0.0/16\u0026#34;,\u0026#34;trafficEncapMode\u0026#34;:\u0026#34;encap\u0026#34;,\u0026#34;trafficEncryptionMode\u0026#34;:\u0026#34;none\u0026#34;,\u0026#34;transportInterface\u0026#34;:null,\u0026#34;transportInterfaceCIDRs\u0026#34;:[],\u0026#34;tunnelCsum\u0026#34;:false,\u0026#34;tunnelPort\u0026#34;:0,\u0026#34;tunnelType\u0026#34;:\u0026#34;geneve\u0026#34;,\u0026#34;wireGuard\u0026#34;:{\u0026#34;port\u0026#34;:51820}}}}} creationTimestamp: \u0026#34;2023-09-28T19:49:11Z\u0026#34; generation: 1 labels: tkg.tanzu.vmware.com/cluster-name: tkg-cluster-1 tkg.tanzu.vmware.com/package-name: antrea.tanzu.vmware.com.1.11.1---vmware.4-tkg.1-advanced name: tkg-cluster-1 namespace: tkg-ns-1 ownerReferences: - apiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster name: tkg-cluster-1 uid: f635b355-e094-471f-bfeb-63e1c10443cf - apiVersion: run.tanzu.vmware.com/v1alpha3 blockOwnerDeletion: true controller: true kind: ClusterBootstrap name: tkg-cluster-1 uid: 83b5bdd6-27c3-4c65-a9bc-f665e99c0670 resourceVersion: \u0026#34;14988446\u0026#34; uid: 7335854b-49ca-44d9-bded-2d4a09aaf5de spec: antrea: config: antreaProxy: nodePortAddresses: [] proxyAll: false proxyLoadBalancerIPs: true skipServices: [] cloudProvider: name: \u0026#34;\u0026#34; defaultMTU: \u0026#34;\u0026#34; disableTXChecksumOffload: false disableUdpTunnelOffload: false dnsServerOverride: \u0026#34;\u0026#34; egress: exceptCIDRs: [] maxEgressIPsPerNode: 255 enableBridgingMode: false enableUsageReporting: false featureGates: AntreaIPAM: false AntreaPolicy: true AntreaProxy: true AntreaTraceflow: true Egress: true EndpointSlice: true FlowExporter: false L7NetworkPolicy: false Multicast: false Multicluster: true NetworkPolicyStats: false NodePortLocal: true SecondaryNetwork: false ServiceExternalIP: false SupportBundleCollection: false TopologyAwareHints: false TrafficControl: false flowExporter: activeFlowTimeout: 60s idleFlowTimeout: 15s pollInterval: 5s multicast: igmpQueryInterval: 125s multicastInterfaces: [] multicluster: enable: true enablePodToPodConnectivity: true enableStretchedNetworkPolicy: true namespace: antrea-multicluster noSNAT: false nodePortLocal: enabled: true portRange: 61000-62000 serviceCIDR: 10.132.0.0/16 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 trafficEncapMode: encap trafficEncryptionMode: none transportInterfaceCIDRs: [] tunnelCsum: false tunnelPort: 0 tunnelType: geneve wireGuard: port: 51820 status: secretRef: tkg-cluster-1-antrea-data-values I have all the Antrea Feature Gates avaible for Antrea to use in the current version of TKG. What I dont have is the antrea-agent.conf and antrea-controller.conf sections. But enabling the Feature Gates here will enable the corresponding setting under the correct section in the native Antrea configMap.\nThe required settings or Antrea Feature-Gates that needs to be enabled are the following:\nkind: ConfigMap apiVersion: v1 metadata: name: antrea-config namespace: kube-system data: antrea-agent.conf: | featureGates: Multicluster: true multicluster: enableGateway: true namespace: \u0026#34;\u0026#34; Then I know which Feature Gates that must be enabled, but in TKG I dont have antrea-agent.conf nor antrea-controller.conf.\nUsing a class based yaml for my workload clusters in TKG the Antrea specific section look like this and to enable Antrea Multi-cluster (including two optional features) I need to enable these features (redacted):\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: tkg-cluster-1 namespace: tkg-ns-1 spec: antrea: config: antreaProxy: cloudProvider: egress: featureGates: Multicluster: true # set to true multicluster: enable: true # set to true enablePodToPodConnectivity: true # set to true enableStretchedNetworkPolicy: true # set to true namespace: \u0026#34;antrea-multicluster\u0026#34; #optional Below is the full workload cluster manifest I use, beginning with the Antrea specific settings (see my inline comments again). This will enable the Multi-cluster feature gate, and the two additional Multi-cluster features PodToPodConnectivity and StretchedNetworkPolicy upon cluster creation.\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: tkg-cluster-1 namespace: tkg-ns-1 spec: antrea: config: antreaProxy: nodePortAddresses: [] proxyAll: false proxyLoadBalancerIPs: true skipServices: [] cloudProvider: name: \u0026#34;\u0026#34; disableTXChecksumOffload: false disableUdpTunnelOffload: false dnsServerOverride: \u0026#34;\u0026#34; egress: exceptCIDRs: [] maxEgressIPsPerNode: 255 enableBridgingMode: null enableUsageReporting: false featureGates: AntreaIPAM: false AntreaPolicy: true AntreaProxy: true AntreaTraceflow: true Egress: true EndpointSlice: true FlowExporter: false L7NetworkPolicy: false Multicast: false Multicluster: true # set to true NetworkPolicyStats: false NodePortLocal: true SecondaryNetwork: false ServiceExternalIP: false SupportBundleCollection: false TopologyAwareHints: false TrafficControl: false flowExporter: activeFlowTimeout: 60s idleFlowTimeout: 15s pollInterval: 5s kubeAPIServerOverride: null multicast: igmpQueryInterval: 125s multicastInterfaces: [] multicluster: enable: true # set to true enablePodToPodConnectivity: true # set to true enableStretchedNetworkPolicy: true # set to true namespace: \u0026#34;antrea-multicluster\u0026#34; noSNAT: false nodePortLocal: enabled: true portRange: 61000-62000 serviceCIDR: 10.132.0.0/16 # if you forget to update this CIDR it will be updated according to the services cidr below trafficEncapMode: encap trafficEncryptionMode: none transportInterface: null transportInterfaceCIDRs: [] tunnelCsum: false tunnelPort: 0 tunnelType: geneve wireGuard: port: 51820 --- apiVersion: cpi.tanzu.vmware.com/v1alpha1 kind: VSphereCPIConfig metadata: name: tkg-cluster-1 namespace: tkg-ns-1 spec: vsphereCPI: ipFamily: ipv4 mode: vsphereCPI region: k8s-region tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 zone: k8s-zone --- apiVersion: csi.tanzu.vmware.com/v1alpha1 kind: VSphereCSIConfig metadata: name: tkg-cluster-1 namespace: tkg-ns-1 spec: vsphereCSI: config: datacenter: /cPod-NSXAM-WDC httpProxy: \u0026#34;\u0026#34; httpsProxy: \u0026#34;\u0026#34; insecureFlag: false noProxy: \u0026#34;\u0026#34; region: k8s-region tlsThumbprint: \u0026lt;SHA\u0026gt; useTopologyCategories: true zone: k8s-zone mode: vsphereCSI --- apiVersion: run.tanzu.vmware.com/v1alpha3 kind: ClusterBootstrap metadata: annotations: tkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.26.5---vmware.2-tkg.1 name: tkg-cluster-1 namespace: tkg-ns-1 spec: additionalPackages: - refName: metrics-server* - refName: secretgen-controller* - refName: pinniped* cni: refName: antrea* valuesFrom: providerRef: apiGroup: cni.tanzu.vmware.com kind: AntreaConfig name: tkg-cluster-1 cpi: refName: vsphere-cpi* valuesFrom: providerRef: apiGroup: cpi.tanzu.vmware.com kind: VSphereCPIConfig name: tkg-cluster-1 csi: refName: vsphere-csi* valuesFrom: providerRef: apiGroup: csi.tanzu.vmware.com kind: VSphereCSIConfig name: tkg-cluster-1 kapp: refName: kapp-controller* --- apiVersion: v1 kind: Secret metadata: name: tkg-cluster-1 namespace: tkg-ns-1 stringData: password: password username: andreasm@cpod-nsxam-wdc.domain.net --- apiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: annotations: osInfo: ubuntu,20.04,amd64 tkg/plan: dev labels: tkg.tanzu.vmware.com/cluster-name: tkg-cluster-1 name: tkg-cluster-1 namespace: tkg-ns-1 spec: clusterNetwork: pods: cidrBlocks: - 10.131.0.0/16 services: cidrBlocks: - 10.132.0.0/16 topology: class: tkg-vsphere-default-v1.1.0 controlPlane: metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu replicas: 1 variables: - name: cni value: antrea - name: controlPlaneCertificateRotation value: activate: true daysBefore: 90 - name: auditLogging value: enabled: false - name: podSecurityStandard value: audit: restricted deactivated: false warn: restricted - name: apiServerEndpoint value: \u0026#34;\u0026#34; - name: aviAPIServerHAProvider value: true - name: vcenter value: cloneMode: fullClone datacenter: /cPod-NSXAM-WDC datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 folder: /cPod-NSXAM-WDC/vm/TKGm network: /cPod-NSXAM-WDC/network/ls-tkg-mgmt resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources server: vcsa.cpod-nsxam-wdc.az-wdc.domain.net storagePolicyID: \u0026#34;\u0026#34; tlsThumbprint: \u0026lt;SHA\u0026gt; - name: user value: sshAuthorizedKeys: - ssh-rsa 2UEBx235bVRSxQ== - name: controlPlane value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: worker value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: controlPlaneZoneMatchingLabels value: region: k8s-region tkg-cp: allowed - name: security value: fileIntegrityMonitoring: enabled: false imagePolicy: pullAlways: false webhook: enabled: false spec: allowTTL: 50 defaultAllow: true denyTTL: 60 retryBackoff: 500 kubeletOptions: eventQPS: 50 streamConnectionIdleTimeout: 4h0m0s systemCryptoPolicy: default version: v1.26.5+vmware.2-tkg.1 workers: machineDeployments: - class: tkg-worker failureDomain: wdc-zone-2 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 replicas: 1 strategy: type: RollingUpdate - class: tkg-worker failureDomain: wdc-zone-3 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-1 replicas: 1 strategy: type: RollingUpdate In addition I am also setting these three additional settings:\nenablePodToPodConnectivity: true # set to true enableStretchedNetworkPolicy: true # set to true namespace: \u0026#34;antrea-multicluster\u0026#34; # the namespace will be created later, and I am not sure its even required to define anything here. Leave it blank \u0026#34;\u0026#34; As I will test out the Multi-cluster NetworkPolicy and routing pod traffic through Multi-cluster Gateways, more on that later. In the workload cluster manifest make sure the services and pod CIDR does not overlap between your clusters that will be joined to the same Multi-cluster ClusterSet.\nThe namespace section is not mandatory.\nThe services CIDRs can not overlap\nThe pod CIDR can not overlap if enabling PodToPodConnectivity\nAfter my TKG workload cluster has been provisioned with the above yaml, this will give me the following AntreaConfig in my workload cluster:\nandreasm@tkg-bootstrap:~$ k get configmaps -n kube-system antrea-config -oyaml apiVersion: v1 data: antrea-agent.conf: | featureGates: AntreaProxy: true EndpointSlice: true TopologyAwareHints: false Traceflow: true NodePortLocal: true AntreaPolicy: true FlowExporter: false NetworkPolicyStats: false Egress: true AntreaIPAM: false Multicast: false Multicluster: true #enabled SecondaryNetwork: false ServiceExternalIP: false TrafficControl: false SupportBundleCollection: false L7NetworkPolicy: false trafficEncapMode: encap noSNAT: false tunnelType: geneve tunnelPort: 0 tunnelCsum: false trafficEncryptionMode: none enableBridgingMode: false disableTXChecksumOffload: false wireGuard: port: 51820 egress: exceptCIDRs: [] maxEgressIPsPerNode: 255 serviceCIDR: 10.132.0.0/16 nodePortLocal: enable: true portRange: 61000-62000 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 multicast: {} antreaProxy: proxyAll: false nodePortAddresses: [] skipServices: [] proxyLoadBalancerIPs: true multicluster: enableGateway: true #enabled namespace: antrea-multicluster enableStretchedNetworkPolicy: true #enabled enablePodToPodConnectivity: true #enabled antrea-cni.conflist: | { \u0026#34;cniVersion\u0026#34;:\u0026#34;0.3.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;antrea\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;antrea\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34; } } , { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true} } , { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} } ] } antrea-controller.conf: | featureGates: Traceflow: true AntreaPolicy: true NetworkPolicyStats: false Multicast: false Egress: true AntreaIPAM: false ServiceExternalIP: false SupportBundleCollection: false L7NetworkPolicy: false Multicluster: true tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 nodeIPAM: null multicluster: enableStretchedNetworkPolicy: true cloudProvider: name: \u0026#34;\u0026#34; kind: ConfigMap metadata: annotations: kapp.k14s.io/identity: v1;kube-system//ConfigMap/antrea-config;v1 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;antrea-agent.conf\u0026#34;:\u0026#34;featureGates:\\n AntreaProxy: true\\n EndpointSlice: true\\n TopologyAwareHints: false\\n Traceflow: true\\n NodePortLocal: true\\n AntreaPolicy: true\\n FlowExporter: false\\n NetworkPolicyStats: false\\n Egress: true\\n AntreaIPAM: false\\n Multicast: false\\n Multicluster: true\\n SecondaryNetwork: false\\n ServiceExternalIP: false\\n TrafficControl: false\\n SupportBundleCollection: false\\n L7NetworkPolicy: false\\ntrafficEncapMode: encap\\nnoSNAT: false\\ntunnelType: geneve\\ntunnelPort: 0\\ntunnelCsum: false\\ntrafficEncryptionMode: none\\nenableBridgingMode: false\\ndisableTXChecksumOffload: false\\nwireGuard:\\n port: 51820\\negress:\\n exceptCIDRs: []\\n maxEgressIPsPerNode: 255\\nserviceCIDR: 100.20.0.0/16\\nnodePortLocal:\\n enable: true\\n portRange: 61000-62000\\ntlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384\\nmulticast: {}\\nantreaProxy:\\n proxyAll: false\\n nodePortAddresses: []\\n skipServices: []\\n proxyLoadBalancerIPs: true\\nmulticluster:\\n enableGateway: true\\n enableStretchedNetworkPolicy: true\\n enablePodToPodConnectivity: true\\n\u0026#34;,\u0026#34;antrea-cni.conflist\u0026#34;:\u0026#34;{\\n \\\u0026#34;cniVersion\\\u0026#34;:\\\u0026#34;0.3.0\\\u0026#34;,\\n \\\u0026#34;name\\\u0026#34;: \\\u0026#34;antrea\\\u0026#34;,\\n \\\u0026#34;plugins\\\u0026#34;: [\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;antrea\\\u0026#34;,\\n \\\u0026#34;ipam\\\u0026#34;: {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;host-local\\\u0026#34;\\n }\\n }\\n ,\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;portmap\\\u0026#34;,\\n \\\u0026#34;capabilities\\\u0026#34;: {\\\u0026#34;portMappings\\\u0026#34;: true}\\n }\\n ,\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;bandwidth\\\u0026#34;,\\n \\\u0026#34;capabilities\\\u0026#34;: {\\\u0026#34;bandwidth\\\u0026#34;: true}\\n }\\n ]\\n}\\n\u0026#34;,\u0026#34;antrea-controller.conf\u0026#34;:\u0026#34;featureGates:\\n Traceflow: true\\n AntreaPolicy: true\\n NetworkPolicyStats: false\\n Multicast: false\\n Egress: true\\n AntreaIPAM: false\\n ServiceExternalIP: false\\n SupportBundleCollection: false\\n L7NetworkPolicy: false\\n Multicluster: true\\ntlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384\\nnodeIPAM: null\\nmulticluster:\\n enableStretchedNetworkPolicy: true\\ncloudProvider:\\n name: \\\u0026#34;\\\u0026#34;\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1695711779258641591\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.c39c4aca919097e50452c3432329dd40\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;antrea-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}}\u0026#39; kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 creationTimestamp: \u0026#34;2023-09-26T07:03:04Z\u0026#34; labels: app: antrea kapp.k14s.io/app: \u0026#34;1695711779258641591\u0026#34; kapp.k14s.io/association: v1.c39c4aca919097e50452c3432329dd40 name: antrea-config namespace: kube-system resourceVersion: \u0026#34;902\u0026#34; uid: 49aea43b-4f8e-4f60-9d54-b299a73bdba8 Notice that the features have been enabled under the corresponding antrea-agent.conf and antrea-controller.conf sections.\napiVersion: v1 data: antrea-agent.conf: | featureGates: Multicluster: true #enabled multicluster: enableGateway: true #enabled namespace: antrea-multicluster enableStretchedNetworkPolicy: true #enabled enablePodToPodConnectivity: true #enabled antrea-controller.conf: | featureGates: Multicluster: true #enabled multicluster: enableStretchedNetworkPolicy: true #enabled That\u0026rsquo;s it for enabling Antrea Multi-cluster in TKG workload clusters.\nTanzu in vSphere and Antrea Multi-cluster # Coming later - stay tuned\nInstall and Configure Antrea Multi-cluster # All the instructions for this exercise has been taken from the offial antrea.io and the Antrea Github pages. My TKG Cluster-1 (Leader cluster) is up and running with the required Antrea Multi-cluster settings (see above) enabled. Now I will follow the user-guide which will involve installing the Antrea Multi-cluster controller in the leader cluster, create ClusterSet, Multi-cluster Gateway configuration. I will start doing all the steps that can be done on the Leader cluster, then take the member clusters next. I will be following the yaml approach. There is also another approach using antctl I may add that also or update the post at a later stage to involve these steps.\nIt is important to follow the documentation according to the version of Antrea being used. This is due to updates in api, and general configuration settings. An example. If I am on Antrea v1.11.1 I would be using the following github or antrea url: https://github.com/antrea-io/antrea/blob/release-1.11/docs/multicluster/user-guide.md https://antrea.io/docs/v1.11.1/docs/multicluster/user-guide/\nIf using Antrea v1.13.1 I would be using the following url:\nhttps://github.com/antrea-io/antrea/blob/release-1.13/docs/multicluster/user-guide.md https://antrea.io/docs/v1.13.1/docs/multicluster/user-guide/\nAntrea Multi-cluster can be configured in different topologies\nRead more here: https://github.com/antrea-io/antrea/blob/main/docs/multicluster/user-guide.md#deploy-antrea-multi-cluster-controller\nInstall the Antrea Multi-cluster controller in the dedicated leader cluster # I will start by creating an enviroment variable, exporting the Antrea version I am using, for the following yaml manifests commands to use. Just to be certain I will first check the actual Antrea version by issuing the antctl version command inside the Antrea controller pod:\nk exec -it -n kube-system antrea-controller-75b85cf45b-hvhq4 bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@tkg-cluster-1-btnn5-vzq5n:/# antctl version antctlVersion: v1.11.1-4776f66 controllerVersion: v1.11.1-4776f66 root@tkg-cluster-1-btnn5-vzq5n:/# andreasm@tkg-bootstrap:~$ export TAG=v1.11.1 Install the Multi-cluster controller in the namespace antrea-multicluster issuing the following two yamls:\nkubectl apply -f https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-leader-global.yml kubectl apply -f https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-leader-namespaced.yml Creating the namespace antrea-multicluster:\nandreasm@tkg-bootstrap:~$ kubectl create ns antrea-multicluster\rnamespace/antrea-multicluster created Applying the antrea-multicluster-leader-global.yaml:\n## Apply the antrea-multicluster-leader-global.yaml andreasm@tkg-bootstrap:~$ kubectl apply -f https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-leader-global.yml ## applied customresourcedefinition.apiextensions.k8s.io/clusterclaims.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/clustersets.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/memberclusterannounces.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/resourceexports.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/resourceimports.multicluster.crd.antrea.io created Applying the antrea-multicluster-leader-namespaced.yaml:\n## Apply the antrea-multicluster-leader-namespaced.yaml andreasm@tkg-bootstrap:~$ kubectl apply -f https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-leader-namespaced.yml ## applied serviceaccount/antrea-mc-controller created serviceaccount/antrea-mc-member-access-sa created role.rbac.authorization.k8s.io/antrea-mc-controller-role created role.rbac.authorization.k8s.io/antrea-mc-member-cluster-role created clusterrole.rbac.authorization.k8s.io/antrea-multicluster-antrea-mc-controller-webhook-role created rolebinding.rbac.authorization.k8s.io/antrea-mc-controller-rolebinding created rolebinding.rbac.authorization.k8s.io/antrea-mc-member-cluster-rolebinding created clusterrolebinding.rbac.authorization.k8s.io/antrea-multicluster-antrea-mc-controller-webhook-rolebinding created configmap/antrea-mc-controller-config created service/antrea-mc-webhook-service created Warning: would violate PodSecurity \u0026#34;restricted:v1.24\u0026#34;: host namespaces (hostNetwork=true), hostPort (container \u0026#34;antrea-mc-controller\u0026#34; uses hostPort 9443), unrestricted capabilities (container \u0026#34;antrea-mc-controller\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;antrea-mc-controller\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;antrea-mc-controller\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) deployment.apps/antrea-mc-controller created mutatingwebhookconfiguration.admissionregistration.k8s.io/antrea-multicluster-antrea-mc-mutating-webhook-configuration created validatingwebhookconfiguration.admissionregistration.k8s.io/antrea-multicluster-antrea-mc-validating-webhook-configuration created andreasm@tkg-bootstrap:~$ k get pods -n antrea-multicluster NAME READY STATUS RESTARTS AGE antrea-mc-controller-7697c8b776-dqtzp 1/1 Running 0 58s And the Antrea CRDs including the additional CRDs from the Multi-cluster installation:\naandreasm@tkg-bootstrap:~$ k get crd -A NAME CREATED AT antreaagentinfos.crd.antrea.io 2023-09-26T07:03:02Z antreacontrollerinfos.crd.antrea.io 2023-09-26T07:03:02Z clusterclaims.multicluster.crd.antrea.io 2023-09-27T06:59:07Z clustergroups.crd.antrea.io 2023-09-26T07:03:04Z clusternetworkpolicies.crd.antrea.io 2023-09-26T07:03:02Z clustersets.multicluster.crd.antrea.io 2023-09-27T06:59:07Z egresses.crd.antrea.io 2023-09-26T07:03:02Z externalentities.crd.antrea.io 2023-09-26T07:03:02Z externalippools.crd.antrea.io 2023-09-26T07:03:02Z externalnodes.crd.antrea.io 2023-09-26T07:03:02Z groups.crd.antrea.io 2023-09-26T07:03:04Z ippools.crd.antrea.io 2023-09-26T07:03:02Z memberclusterannounces.multicluster.crd.antrea.io 2023-09-27T06:59:08Z multiclusteringresses.ako.vmware.com 2023-09-26T07:03:51Z networkpolicies.crd.antrea.io 2023-09-26T07:03:02Z resourceexports.multicluster.crd.antrea.io 2023-09-27T06:59:12Z resourceimports.multicluster.crd.antrea.io 2023-09-27T06:59:15Z supportbundlecollections.crd.antrea.io 2023-09-26T07:03:03Z tierentitlementbindings.crd.antrea.tanzu.vmware.com 2023-09-26T07:03:04Z tierentitlements.crd.antrea.tanzu.vmware.com 2023-09-26T07:03:02Z tiers.crd.antrea.io 2023-09-26T07:03:03Z traceflows.crd.antrea.io 2023-09-26T07:03:03Z trafficcontrols.crd.antrea.io 2023-09-26T07:03:03Z Install the Antrea Multi-cluster controller in the member clusters # Next up is to deploy the Antrea Multi-cluster in both of my member clusters, tkg-cluster-2 and tkg-cluster-3. They have also been configured to enable the Multi-cluster feature gate. This operation involves applying the antrea-multicluster-member.yaml\nandreasm@tkg-bootstrap:~$ k config current-context tkg-cluster-2-admin@tkg-cluster-2 # Show running pods NAMESPACE NAME READY STATUS RESTARTS AGE avi-system ako-0 1/1 Running 0 19h kube-system antrea-agent-grq68 2/2 Running 0 18h kube-system antrea-agent-kcbc8 2/2 Running 0 18h kube-system antrea-agent-zvgcc 2/2 Running 0 18h kube-system antrea-controller-bc584bbcd-7hw7n 1/1 Running 0 18h kube-system coredns-75f565d4dd-48sgb 1/1 Running 0 19h kube-system coredns-75f565d4dd-wr8pl 1/1 Running 0 19h kube-system etcd-tkg-cluster-2-jzmpk-w2thj 1/1 Running 0 19h kube-system kube-apiserver-tkg-cluster-2-jzmpk-w2thj 1/1 Running 0 19h kube-system kube-controller-manager-tkg-cluster-2-jzmpk-w2thj 1/1 Running 0 19h kube-system kube-proxy-dmcb5 1/1 Running 0 19h kube-system kube-proxy-kr5hl 1/1 Running 0 19h kube-system kube-proxy-t7qqp 1/1 Running 0 19h kube-system kube-scheduler-tkg-cluster-2-jzmpk-w2thj 1/1 Running 0 19h kube-system metrics-server-5666ffccb9-p7qqw 1/1 Running 0 19h kube-system vsphere-cloud-controller-manager-6b8v2 1/1 Running 0 19h secretgen-controller secretgen-controller-69cbc65949-2x8nv 1/1 Running 0 19h tkg-system kapp-controller-5776b48998-7rkzf 2/2 Running 0 19h tkg-system tanzu-capabilities-controller-manager-77d8ffcd57-9tgzn 1/1 Running 0 19h vmware-system-antrea register-placeholder-m5bj8 0/1 Completed 0 8m41s vmware-system-csi vsphere-csi-controller-67799db966-qnfv2 7/7 Running 1 (19h ago) 19h vmware-system-csi vsphere-csi-node-dkcf7 3/3 Running 2 (19h ago) 19h vmware-system-csi vsphere-csi-node-w8dqw 3/3 Running 3 (31m ago) 19h vmware-system-csi vsphere-csi-node-zxpmz 3/3 Running 4 (19h ago) 19h Apply the multicluster-member.yaml\nandreasm@tkg-bootstrap:~$ kubectl apply -f https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-member.yml ## applied customresourcedefinition.apiextensions.k8s.io/clusterclaims.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/clusterinfoimports.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/clustersets.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/gateways.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/labelidentities.multicluster.crd.antrea.io created customresourcedefinition.apiextensions.k8s.io/serviceexports.multicluster.x-k8s.io created customresourcedefinition.apiextensions.k8s.io/serviceimports.multicluster.x-k8s.io created serviceaccount/antrea-mc-controller created clusterrole.rbac.authorization.k8s.io/antrea-mc-controller-role created clusterrolebinding.rbac.authorization.k8s.io/antrea-mc-controller-rolebinding created configmap/antrea-mc-controller-config created service/antrea-mc-webhook-service created deployment.apps/antrea-mc-controller created mutatingwebhookconfiguration.admissionregistration.k8s.io/antrea-mc-mutating-webhook-configuration created validatingwebhookconfiguration.admissionregistration.k8s.io/antrea-mc-validating-webhook-configuration created NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-mc-controller-5bb945f87f-xk287 1/1 Running 0 64s The controller is running, and it is running in the kube-system namespace.\nNow I will repeat the same operation for my tkg-cluster-3\u0026hellip;\nCreate ClusterSet # Creating ClusterSet involves a couple of steps on both Leader cluster and member clusters.\nOn the Leader Cluster - Create ServiceAccounts # First I will configure a ServiceAccount for each member to access the Leader cluster\u0026rsquo;s API.\nIn my Leader cluster I will apply the following yaml:\nMember 1, tkg-cluster-2, will be called member-blue\n## This the yaml for creating the Service Account for member-blue apiVersion: v1 kind: ServiceAccount metadata: name: member-blue namespace: antrea-multicluster --- apiVersion: v1 kind: Secret metadata: name: member-blue-token namespace: antrea-multicluster annotations: kubernetes.io/service-account.name: member-blue type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: member-blue namespace: antrea-multicluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: antrea-mc-member-cluster-role subjects: - kind: ServiceAccount name: member-blue namespace: antrea-multicluster Apply it:\nandreasm@tkg-bootstrap:~$ k apply -f sa-member-blue.yaml ## applied serviceaccount/member-blue created secret/member-blue-token created rolebinding.rbac.authorization.k8s.io/member-blue created Now create a token yaml file for member blue:\nkubectl get secret member-blue-token -n antrea-multicluster -o yaml | grep -w -e \u0026#39;^apiVersion\u0026#39; -e \u0026#39;^data\u0026#39; -e \u0026#39;^metadata\u0026#39; -e \u0026#39;^ *name:\u0026#39; -e \u0026#39;^kind\u0026#39; -e \u0026#39; ca.crt\u0026#39; -e \u0026#39; token:\u0026#39; -e \u0026#39;^type\u0026#39; -e \u0026#39; namespace\u0026#39; | sed -e \u0026#39;s/kubernetes.io\\/service-account-token/Opaque/g\u0026#39; -e \u0026#39;s/antrea-multicluster/kube-system/g\u0026#39; \u0026gt; member-blue-token.yml This should create the file member-blue-token.yaml and the content of the file:\n# cat member-blue-token.yml cat member-blue-token.yml ## output apiVersion: v1 data: ca.crt: LS0tLS1CRUdJ0tCk1JSUM2akNDQWRLZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1Ea3lOakEyTlRReU5Wb1hEVE16TURreU16QTJOVGt5TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTDh2Ck1iWHc0MFM2NERZc2dnMG9qSEVwUHlOOC93MVBkdFE2cGxSSThvbUVuWW1ramc5TThIN3NrTDhqdHR1WXRxQVkKYnorZEFsVmJBcWRrWCtkL3Q5MTZGOWRBYmRveW9Qc3pwREIraVVQdDZ5Nm5YbDhPK0xEV2JWZzdEWTVXQ3lYYQpIeEJBM1I4NUUxRkhvYUxBREZ6OFRsZ2lKR3RZYktROFJYTWlIMk1xczRaNU9Mblp3Qy9rSTNVNEEzMVFlcXl3Cm1VMjd2SDdzZjlwK0tiTE5wZldtMDJoV3hZNzlMS1hCNE1LOStLaXAyVkt4VUlQbHl6bGpXTjQrcngyclN4bVEKbnpmR3NpT0JQTWpOanpQOE44cWJub01hL2Jd1haOHJpazhnenJGN05sQQp6R1BvdC84S2Q5UXZ2Q2doVVlNQ0F3RUFBYU5GTUVNd0RnWURWUjBQQVFIL0JBUURBZ0trTUJJR0ExVWRFd0VCCi93UUlNQVlCQWY4Q0FRQXdIUVlEVlIwT0JCWUVGQk9PWHcrb1o1VGhLQ3I5RnBMWm9ySkZZWW1wTUEwR0NTcUcKU0liM0RRRUJDd1VBQTRJQkFRQ2Rpa2lWYi9pblk1RVNIdVVIcTY2YnBLK1RTQXI5eTFERnN0Qjd2eUt3UGduVAp2bGdEZnZnN1o3UTVOaFhzcFBnV1Y4NEZMMU80UUQ1WmtTYzhLcDVlM1V1ZFFvckRQS3VEOWkzNHVXVVc0TVo5ClR2UUJLNS9sRUlsclBONG5XYmEyazYrOE9tZitmWWREd3JsZTVaa3JUOHh6UnhEbEtXdE5vNVRHMWgrMElUOVgKcVMwL1hzNFpISlU2NGd5dlRsQXlwR2pPdFdxMUc0MEZ5U3dydFJLSE52a3JjTStkeDdvTEM1d003ZTZSTHg1cApnb0Y5dGZZV3ZyUzJWWjl2RUR5QllPN1RheFhEMGlaV2V1VEh0ZFJxTWVLZVAvei9lYnZJMUkvRkWS9NNGdxYjdXbGVqM0JNS051TVc2Q1AwUmFYcXJnUmpnVTAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQu= namespace: YW50cmVhLW11bHRpY2x1c3Rlcg== token: WXlKaGJHY2lPaUpTVXpJMCUVRGaWNIUlJTR0Z1VnpkTU1GVjVOalF3VWpKcVNFVXdYMFkzZDFsNmJqWXpUa3hzVUc4aWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpoYm5SeVpXRXRiWFZzZEdsamJIVnpkR1Z5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaV055WlhRdWJtRnRaU0k2SW0xbGJXSmxjaTFpYkhWbExYUnZhMlZ1SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qY5RdWJtRnRaU0k2SW0xbGJXSmxjaTFpYkhWbElpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaU1XSTVaVGd4TUdVdE5tTTNaQzAwTjJZekxXRTVaR1F0TldWbU4yTXpPVEE0T0dNNElpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbUZ1ZEhKbFlTMXRkV3gwYVdOc2RYTjBaWEk2YldWdFltVnlMV0pzZFdVaWZRLmNyU29sN0JzS2JyR2lhUVYyWmJ2OG8tUVl1eVBfWGlnZ1hfcDg2UGs0ZEpVOTBwWk1XYUZTUkFUR1ptQnBCUUJnajI5eVJHbkdvajVHRWN3YVNxWlZaV3FySk9jVTM1QXFlWHhpWm1fUl9LWDB4VUR1Y0wxQTVxNdWhOYTBGUkxmU2FKWUNOME1NTHNKVTBDU3pHUVg1dHlzTXBUN0YwVG0weS1mZFpVOE9IQmJoY0ZDWXkyYk5WdC0weU9pQUlYOHR2TVNrb2NzaHpWUm5ha1A5dmtMaXNVUGh2Vm9xMVROZ2RVRmtjc0lPRjl4ZFo5Ul9PX3NlT1ZLay1hNkhJbjB3THQzZ3FEZHRHU09Ub3BfMUh0djgxeEZQdF9zNlVRNmpldjZpejh3aFAzX1BkSGhwTlNCWFBEc3hZbEhyMlVaUK== kind: Secret metadata: name: member-blue-token namespace: kube-system type: Opaque Repeating the steps for my member cluster 2 (tkg-cluster-3)\nMember 2, tkg-cluster-3, will be called member-red (yes you guessed it right)\n## This the yaml for creating the Service Account for member-blue apiVersion: v1 kind: ServiceAccount metadata: name: member-red namespace: antrea-multicluster --- apiVersion: v1 kind: Secret metadata: name: member-red-token namespace: antrea-multicluster annotations: kubernetes.io/service-account.name: member-red type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: member-red namespace: antrea-multicluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: antrea-mc-member-cluster-role subjects: - kind: ServiceAccount name: member-red namespace: antrea-multicluster Apply it:\nandreasm@tkg-bootstrap:~$ k apply -f sa-member-red.yaml ## applied serviceaccount/member-red created secret/member-red-token created rolebinding.rbac.authorization.k8s.io/member-red created Now create a token yaml file for member red:\nkubectl get secret member-red-token -n antrea-multicluster -o yaml | grep -w -e \u0026#39;^apiVersion\u0026#39; -e \u0026#39;^data\u0026#39; -e \u0026#39;^metadata\u0026#39; -e \u0026#39;^ *name:\u0026#39; -e \u0026#39;^kind\u0026#39; -e \u0026#39; ca.crt\u0026#39; -e \u0026#39; token:\u0026#39; -e \u0026#39;^type\u0026#39; -e \u0026#39; namespace\u0026#39; | sed -e \u0026#39;s/kubernetes.io\\/service-account-token/Opaque/g\u0026#39; -e \u0026#39;s/antrea-multicluster/kube-system/g\u0026#39; \u0026gt; member-blue-token.yml This should create the file member-red-token.yaml and the content of the file:\n# cat member-red-token.yml cat member-red-token.yml ## output apiVersion: v1 data: ca.crt: LS0tLS1CRUdJ0tCk1JSUM2akNDQWRLZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1Ea3lOakEyTlRReU5Wb1hEVE16TURreU16QTJOVGt5TlZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTDh2Ck1iWHc0MFM2NERZc2dnMG9qSEVwUHlOOC93MVBkdFE2cGxSSThvbUVuWW1ramc5TThIN3NrTDhqdHR1WXRxQVkKYnorZEFsVmJBcWRrWCtkL3Q5MTZGOWRBYmRveW9Qc3pwREIraVVQdDZ5Nm5YbDhPK0xEV2JWZzdEWTVXQ3lYYQpIeEJBM1I4NUUxRkhvYUxBREZ6OFRsZ2lKR3RZYktROFJYTWlIMk1xczRaNU9Mblp3Qy9rSTNVNEEzMVFlcXl3Cm1VMjd2SDdzZjlwK0tiTE5wZldtMDJoV3hZNzlMS1hCNE1LOStLaXAyVkt4VUlQbHl6bGpXTjQrcngyclN4bVEKbnpmR3NpT0JQTWpOanpQOE44cWJub01hL2Jd1haOHJpazhnenJGN05sQQp6R1BvdC84S2Q5UXZ2Q2doVVlNQ0F3RUFBYU5GTUVNd0RnWURWUjBQQVFIL0JBUURBZ0trTUJJR0ExVWRFd0VCCi93UUlNQVlCQWY4Q0FRQXdIUVlEVlIwT0JCWUVGQk9PWHcrb1o1VGhLQ3I5RnBMWm9ySkZZWW1wTUEwR0NTcUcKU0liM0RRRUJDd1VBQTRJQkFRQ2Rpa2lWYi9pblk1RVNIdVVIcTY2YnBLK1RTQXI5eTFERnN0Qjd2eUt3UGduVAp2bGdEZnZnN1o3UTVOaFhzcFBnV1Y4NEZMMU80UUQ1WmtTYzhLcDVlM1V1ZFFvckRQS3VEOWkzNHVXVVc0TVo5ClR2UUJLNS9sRUlsclBONG5XYmEyazYrOE9tZitmWWREd3JsZTVaa3JUOHh6UnhEbEtXdE5vNVRHMWgrMElUOVgKcVMwL1hzNFpISlU2NGd5dlRsQXlwR2pPdFdxMUc0MEZ5U3dydFJLSE52a3JjTStkeDdvTEM1d003ZTZSTHg1cApnb0Y5dGZZV3ZyUzJWWjl2RUR5QllPN1RheFhEMGlaV2V1VEh0ZFJxTWVLZVAvei9lYnZJMUkvRkWS9NNGdxYjdXbGVqM0JNS051TVc2Q1AwUmFYcXJnUmpnVTAKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQu= namespace: YW50cmVhLW11bHRpY2x1c3Rlcg== token: WXlKaGJHY2lPaUpTVXpJMCUVRGaWNIUlJTR0Z1VnpkTU1GVjVOalF3VWpKcVNFVXdYMFkzZDFsNmJqWXpUa3hzVUc4aWZRLmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpoYm5SeVpXRXRiWFZzZEdsamJIVnpkR1Z5SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXpaV055WlhRdWJtRnRaU0k2SW0xbGJXSmxjaTFpYkhWbExYUnZhMlZ1SWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qY5RdWJtRnRaU0k2SW0xbGJXSmxjaTFpYkhWbElpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVkV2xrSWpvaU1XSTVaVGd4TUdVdE5tTTNaQzAwTjJZekxXRTVaR1F0TldWbU4yTXpPVEE0T0dNNElpd2ljM1ZpSWpvaWMzbHpkR1Z0T25ObGNuWnBZMlZoWTJOdmRXNTBPbUZ1ZEhKbFlTMXRkV3gwYVdOc2RYTjBaWEk2YldWdFltVnlMV0pzZFdVaWZRLmNyU29sN0JzS2JyR2lhUVYyWmJ2OG8tUVl1eVBfWGlnZ1hfcDg2UGs0ZEpVOTBwWk1XYUZTUkFUR1ptQnBCUUJnajI5eVJHbkdvajVHRWN3YVNxWlZaV3FySk9jVTM1QXFlWHhpWm1fUl9LWDB4VUR1Y0wxQTVxNdWhOYTBGUkxmU2FKWUNOME1NTHNKVTBDU3pHUVg1dHlzTXBUN0YwVG0weS1mZFpVOE9IQmJoY0ZDWXkyYk5WdC0weU9pQUlYOHR2TVNrb2NzaHpWUm5ha1A5dmtMaXNVUGh2Vm9xMVROZ2RVRmtjc0lPRjl4ZFo5Ul9PX3NlT1ZLay1hNkhJbjB3THQzZ3FEZHRHU09Ub3BfMUh0djgxeEZQdF9zNlVRNmpldjZpejh3aFAzX1BkSGhwTlNCWFBEc3hZbEhyMlVaUK== kind: Secret metadata: name: member-red-token namespace: kube-system type: Opaque On the Member Cluster apply tokens # Now on both members apply the corresponding token yaml, member-1 applies the member-blue-token.yaml and member-2 applies the member-red-token.yaml.\n## tkg-cluster-2 andreasm@tkg-bootstrap:~$ k apply -f member-blue-token.yml secret/member-blue-token created ## tkg-cluster-3 andreasm@tkg-bootstrap:~$ k apply -f member-red-token.yml secret/member-red-token created On the Leader Cluster - ClusterSet Initialization # I am using Antrea version 1.11.1 (that comes with TKG 2.3) so in this step we need to use the v1alpha1 api.\nFirst I need to create a ClusterSet in the Leader cluster by applying the below yaml:\napiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: id.k8s.io namespace: antrea-multicluster value: tkg-cluster-leader --- apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: clusterset.k8s.io namespace: antrea-multicluster value: andreasm-clusterset --- apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: ClusterSet metadata: name: andreasm-clusterset namespace: antrea-multicluster spec: leaders: - clusterID: tkg-cluster-leader ## apply it andreasm@tkg-bootstrap:~$ k apply -f clusterset-leader.yaml ### applied clusterclaim.multicluster.crd.antrea.io/id.k8s.io created clusterclaim.multicluster.crd.antrea.io/clusterset.k8s.io created clusterset.multicluster.crd.antrea.io/andreasm-clusterset created andreasm@tkg-bootstrap:~$ k get clustersets.multicluster.crd.antrea.io -A NAMESPACE NAME LEADER CLUSTER NAMESPACE TOTAL CLUSTERS READY CLUSTERS AGE antrea-multicluster andreasm-clusterset 84s andreasm@tkg-bootstrap:~$ k get clusterclaims.multicluster.crd.antrea.io -A NAMESPACE NAME VALUE AGE antrea-multicluster clusterset.k8s.io andreasm-clusterset 118s antrea-multicluster id.k8s.io tkg-cluster-leader 119s On the Member Clusters - ClusterSet Initialization # Next step is to deploy the corresponding yaml on both my member clusters (tkg-cluster-2=member-cluster-blue and tkg-cluster-3=member-cluster-red):\napiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: id.k8s.io namespace: kube-system value: member-cluster-blue --- apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: clusterset.k8s.io namespace: kube-system value: andreasm-clusterset --- apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: ClusterSet metadata: name: andreasm-clusterset namespace: kube-system spec: leaders: - clusterID: tkg-cluster-leader secret: \u0026#34;member-blue-token\u0026#34; # secret/token created earlier server: \u0026#34;https://10.101.114.100:6443\u0026#34; # reflect the correct endpoint IP for leader cluster namespace: antrea-multicluster apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: id.k8s.io namespace: kube-system value: member-cluster-red --- apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: clusterset.k8s.io namespace: kube-system value: andreasm-clusterset --- apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: ClusterSet metadata: name: andreasm-clusterset namespace: kube-system spec: leaders: - clusterID: tkg-cluster-leader secret: \u0026#34;member-red-token\u0026#34; # secret/token created earlier server: \u0026#34;https://10.101.114.100:6443\u0026#34; # reflect the correct endpoint IP for leader cluster namespace: antrea-multicluster ## tkg-cluster-2 andreasm@tkg-bootstrap:~$ k apply -f clusterclaim-member-blue.yaml ## applied clusterclaim.multicluster.crd.antrea.io/id.k8s.io created clusterclaim.multicluster.crd.antrea.io/clusterset.k8s.io created clusterset.multicluster.crd.antrea.io/andreasm-clusterset created ## tkg-cluster-3 andreasm@tkg-bootstrap:~$ k apply -f clusterclaim-member-red.yaml ## applied clusterclaim.multicluster.crd.antrea.io/id.k8s.io created clusterclaim.multicluster.crd.antrea.io/clusterset.k8s.io created clusterset.multicluster.crd.antrea.io/andreasm-clusterset created On the Member Clusters - Multi-cluster Gateway configuration # From the docs:\nMulti-cluster Gateways are required to support multi-cluster Service access across member clusters. Each member cluster should have one Node served as its Multi-cluster Gateway. Multi-cluster Service traffic is routed among clusters through the tunnels between Gateways.\nAfter a member cluster joins a ClusterSet, and the Multicluster feature is enabled on antrea-agent, you can select a Node of the cluster to serve as the Multi-cluster Gateway by adding an annotation: multicluster.antrea.io/gateway=true to the K8s Node.\nYou can annotate multiple Nodes in a member cluster as the candidates for Multi-cluster Gateway, but only one Node will be selected as the active Gateway. Before Antrea v1.9.0, the Gateway Node is just randomly selected and will never change unless the Node or its gateway annotation is deleted. Starting with Antrea v1.9.0, Antrea Multi-cluster Controller will guarantee a \u0026ldquo;ready\u0026rdquo; Node is selected as the Gateway, and when the current Gateway Node\u0026rsquo;s status changes to not \u0026ldquo;ready\u0026rdquo;, Antrea will try selecting another \u0026ldquo;ready\u0026rdquo; Node from the candidate Nodes to be the Gateway.\nI will annotate both my worker nodes in both my member clusters, blue and red.\nandreasm@tkg-bootstrap:~/Kubernetes-library/tkgm/antrea-multicluster$ k annotate node tkg-cluster-2-md-0-qhqgb-58df8c59f4xxb5q8-s7w2z multicluster.antrea.io/gateway=true node/tkg-cluster-2-md-0-qhqgb-58df8c59f4xxb5q8-s7w2z annotated ## Will repeat this for both nodes in both clusters Then check which node that has been decided in both clusters:\n## tkg cluster 2 - member blue andreasm@tkg-bootstrap:~$ k get gateway -n kube-system NAME GATEWAY IP INTERNAL IP AGE tkg-cluster-2-md-0-qhqgb-58df8c59f4xxb5q8-s7w2z 10.101.12.24 10.101.12.24 4m18s ## tkg cluster 3 - member red andreasm@tkg-bootstrap:~$ k get gateway -n kube-system NAME GATEWAY IP INTERNAL IP AGE tkg-cluster-3-md-0-82gh2-6dcf989cbcxnc8lc-z5c8g 10.101.12.26 10.101.12.26 116s And the logs from the Antrea MC controller after adding the gateway annotation:\nI0928 09:46:01.599697 1 clusterset_controller.go:111] \u0026#34;Received ClusterSet add/update\u0026#34; clusterset=\u0026#34;kube-system/andreasm-clusterset\u0026#34; I0928 09:46:01.599772 1 controller_utils.go:40] \u0026#34;Validating ClusterClaim\u0026#34; namespace=\u0026#34;kube-system\u0026#34; I0928 09:46:01.701456 1 controller_utils.go:52] \u0026#34;Processing ClusterClaim\u0026#34; name=\u0026#34;id.k8s.io\u0026#34; value=\u0026#34;member-cluster-blue\u0026#34; I0928 09:46:01.701484 1 controller_utils.go:52] \u0026#34;Processing ClusterClaim\u0026#34; name=\u0026#34;clusterset.k8s.io\u0026#34; value=\u0026#34;andreasm-clusterset\u0026#34; I0928 09:46:01.701907 1 clusterset_controller.go:204] \u0026#34;Creating RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:46:02.236829 1 remote_common_area.go:111] \u0026#34;Create a RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:46:02.237004 1 clusterset_controller.go:251] \u0026#34;Created RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:46:02.237064 1 remote_common_area.go:293] \u0026#34;Starting MemberAnnounce to RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:46:02.388473 1 remote_common_area.go:236] \u0026#34;Updating RemoteCommonArea status\u0026#34; cluster=tkg-cluster-leader connected=true I0928 09:49:17.663979 1 clusterset_controller.go:111] \u0026#34;Received ClusterSet add/update\u0026#34; clusterset=\u0026#34;kube-system/andreasm-clusterset\u0026#34; I0928 09:49:17.664066 1 controller_utils.go:40] \u0026#34;Validating ClusterClaim\u0026#34; namespace=\u0026#34;kube-system\u0026#34; I0928 09:49:17.764734 1 controller_utils.go:52] \u0026#34;Processing ClusterClaim\u0026#34; name=\u0026#34;id.k8s.io\u0026#34; value=\u0026#34;member-cluster-red\u0026#34; I0928 09:49:17.764778 1 controller_utils.go:52] \u0026#34;Processing ClusterClaim\u0026#34; name=\u0026#34;clusterset.k8s.io\u0026#34; value=\u0026#34;andreasm-clusterset\u0026#34; I0928 09:49:17.764855 1 clusterset_controller.go:204] \u0026#34;Creating RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:49:18.251398 1 remote_common_area.go:111] \u0026#34;Create a RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:49:18.251475 1 clusterset_controller.go:251] \u0026#34;Created RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:49:18.251588 1 remote_common_area.go:293] \u0026#34;Starting MemberAnnounce to RemoteCommonArea\u0026#34; cluster=tkg-cluster-leader I0928 09:49:18.363564 1 remote_common_area.go:236] \u0026#34;Updating RemoteCommonArea status\u0026#34; cluster=tkg-cluster-leader connected=true Now on both member clusters, they should have received and be aware of their fellow member clusters network information. This can be verified with the following command on both member clusters:\n## member-red - tkg-cluster-3 andreasm@tkg-bootstrap:~$ k get clusterinfoimports.multicluster.crd.antrea.io -n kube-system NAME CLUSTER ID SERVICE CIDR AGE member-cluster-blue-clusterinfo member-cluster-blue 10.134.0.0/16 69s ## member-blue - tkg-cluster-2 andreasm@tkg-bootstrap:~$ k get clusterinfoimports.multicluster.crd.antrea.io -n kube-system NAME CLUSTER ID SERVICE CIDR AGE member-cluster-red-clusterinfo member-cluster-red 10.136.0.0/16 3m10s Now that Antrea Multi-cluster has been configured, its time to head over to the section on what we can use this for.\nThis his how my environment looks like now:\nMember cluster blue is exchanging information to member cluster red via the leader cluster using their gateway currently active on worker-node-md-0 in both member clusters.\nUsing antctl # For now I will just link to the GitHub docs of Antrea how to use the antctl approach.\nFor how to use the antctl approach, click here\nTo use the antctl cli tool download the corresponding Antrea version of antctl here\nMulti-cluster Service # Imagine you have an application that consists of a frontend and a backend. The frontend must run on a dedicated cluster or multiple clusters for scalability, easier exposure, and security posture reasons. The backend services should be running in a more controlled \u0026ldquo;inner\u0026rdquo; cluster. Using Antrea Multi-cluster that is possible. Lets go through how to configure this.\nFirst I will deploy an application called Yelb source. This consists of a frontend service \u0026ldquo;yelb-ui\u0026ldquo;and three backend applications \u0026ldquo;application, redis and PostgreSQL\u0026rdquo;.\nThis is how the architecture of Yelb looks like:\nI want the yelb-ui to only run in the member cluster blue, and I want all the backends to run in the member cluster red. Like this:\nI will start with deploying the backend part of the Yelb application in my member cluster red:\n## Backend pods running in member-red andreasm@tkg-bootstrap:~$ k get pods -n yelb -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-56d97cc8c-dzbg7 1/1 Running 0 5m38s 100.10.2.157 tkg-cluster-3-md-0-82gh2-6dcf989cbcxnc8lc-z5c8g \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-65855b7ffd-nggg9 1/1 Running 0 5m36s 100.10.2.159 tkg-cluster-3-md-0-82gh2-6dcf989cbcxnc8lc-z5c8g \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-6f78dc6f8f-5dgpv 1/1 Running 0 5m37s 100.10.2.158 tkg-cluster-3-md-0-82gh2-6dcf989cbcxnc8lc-z5c8g \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ## Backend services running in member-red andreasm@tkg-bootstrap:~$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-server ClusterIP 100.20.176.130 \u0026lt;none\u0026gt; 6379/TCP 2m29s yelb-appserver ClusterIP 100.20.111.133 \u0026lt;none\u0026gt; 4567/TCP 2m27s yelb-db ClusterIP 100.20.7.160 \u0026lt;none\u0026gt; 5432/TCP 2m28s Now I will deploy the frontend service in the member cluster blue\nandreasm@tkg-bootstrap:~$ k get pods -n yelb NAME READY STATUS RESTARTS AGE yelb-ui-5c5b8d8887-wkchd 0/1 CrashLoopBackOff 3 (20s ago) 2m49s its deployed, but not running. It cant reach the backend service yelb-appserver as this is running on a completely different cluster.\nandreasm@tkg-bootstrap:~$ k logs -n yelb yelb-ui-7bc645756b-qmtbp 2023/09/29 06:16:17 [emerg] 10#10: host not found in upstream \u0026#34;antrea-mc-yelb-appserver\u0026#34; in /etc/nginx/conf.d/default.conf:5 nginx: [emerg] host not found in upstream \u0026#34;antrea-mc-yelb-appserver\u0026#34; in /etc/nginx/conf.d/default.conf:5 Its also red in my NSX-ALB environment.\nNow I will export the yelb-appserver service using Antrea Multi-cluster Service, so any pods running in member-blue can also use this service running on member-red. For the Yelb UI to work it needs to talk to the appserver service (yelb-appserver).\nFrom the source, where the yelb-appserver service is defined locally and running, I need to define a ServiceExport which is just using the name of the original service and the namespace where the service is located:\napiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: yelb-appserver ## name of service you want to export namespace: yelb ## namespace of the service Apply it\nandreasm@tkg-bootstrap:~$ k apply -f yelb-app-service-export.yaml serviceexport.multicluster.x-k8s.io/yelb-appserver created From the leader-cluster I can check all the resourceexports and resourceimports:\nandreasm@tkg-bootstrap:~$ k get resourceexports.multicluster.crd.antrea.io -n antrea-multicluster NAME CLUSTER ID KIND NAMESPACE NAME AGE member-cluster-blue-clusterinfo member-cluster-blue ClusterInfo kube-system member-cluster-blue 53m member-cluster-red-clusterinfo member-cluster-red ClusterInfo kube-system member-cluster-red 51m member-cluster-red-yelb-yelb-appserver-endpoints member-cluster-red Endpoints yelb yelb-appserver 2m59s member-cluster-red-yelb-yelb-appserver-service member-cluster-red Service yelb yelb-appserver 2m59s andreasm@tkg-bootstrap:~$ k get resourceimports.multicluster.crd.antrea.io -n antrea-multicluster NAME KIND NAMESPACE NAME AGE member-cluster-blue-clusterinfo ClusterInfo antrea-multicluster member-cluster-blue-clusterinfo 55m member-cluster-red-clusterinfo ClusterInfo antrea-multicluster member-cluster-red-clusterinfo 53m yelb-yelb-appserver-endpoints Endpoints yelb yelb-appserver 4m41s yelb-yelb-appserver-service ServiceImport yelb yelb-appserver 4m41s So my yelb-appserver service has been exported.\nWhat happens now in my member cluster blue? For the service to be imported into my member blue cluster it needs to have the namespace yelb created, otherwise it will not be imported. I have the ns yelb created as the yelb-ui is already deployed in this namespace. In my member cluster blue I can now see that I have the service yelb-appserver imported. Its there. Yay.\nandreasm@tkg-bootstrap:~$ k get serviceimports.multicluster.x-k8s.io -A NAMESPACE NAME TYPE IP AGE yelb yelb-appserver ClusterSetIP [\u0026#34;100.40.224.145\u0026#34;] 15m andreasm@tkg-bootstrap:~$ k get serviceimports.multicluster.x-k8s.io -n yelb NAME TYPE IP AGE yelb-appserver ClusterSetIP [\u0026#34;100.40.224.145\u0026#34;] 17m andreasm@tkg-bootstrap:~$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE antrea-mc-yelb-appserver ClusterIP 100.40.224.145 \u0026lt;none\u0026gt; 4567/TCP 19m yelb-ui LoadBalancer 100.40.136.62 10.101.115.100 80:30425/TCP 26m Will my yelb-ui pod figure this out then?\nThe yelb-ui pod is running:\namarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/antrea-multicluster/yelb$ k get pods -n yelb NAME READY STATUS RESTARTS AGE yelb-ui-5c5b8d8887-wkchd 1/1 Running 5 (5m5s ago) 8m17s The VS is green:\nAnd inside the application itself I can also see the app server it is currently using:\nAnd doing a quick sanity check. Where are my pods running. The yelb-ui pod is running in member cluster blue:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES yelb-ui-7bc645756b-qmtbp 1/1 Running 5 (121m ago) 124m 10.133.1.56 tkg-cluster-2-md-1-wmxhd-5998fcf669x64b9h-z7mnw \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; And the appserver pod is running:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-56d97cc8c-wvc4q 1/1 Running 0 126m 10.135.1.53 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-d9ddcdd97-z8gt9 1/1 Running 0 126m 10.135.1.55 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-749c784cb8-f6wx8 1/1 Running 0 126m 10.135.1.54 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Notice the name from the ui of the app above on the name of the pod listed. Its the same pod.\nRecap of Multi-cluster service. # In member cluster red I deployed the three backends needed for the Yelb application. These consists of 3 PODs and their corresponding services. As of now they are just local and accessible internally in the same k8s cluster. In member cluster blue I deployed the Yelb UI service, in a CrashLoopBackOff as it cant reach the necessary appserver service. Then I exported the yelb-appserver using Antrea Multi-cluster Services And the Yelb application lived happily ever after \u0026#x1f192;\nQuick troubleshooting # Just to add an easy way to verify the exported services work. I deployed a simple nginx pod in member cluster red. Exported the nginx ClusterIP service, from my member cluster blue I have deployed a Ubuntu pod. From this pod I will do a curl to the exported nginx service to see if I can reach it and if it works.\n## nginx pod running in member cluster red NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ui-59c956b95b-qhz8n 1/1 Running 0 151m 10.135.1.56 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ## nginx local clusterip service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.136.40.246 \u0026lt;none\u0026gt; 80/TCP 152m ## the exported nginx service in my member cluster blue NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE antrea-mc-nginx ClusterIP 10.134.30.220 \u0026lt;none\u0026gt; 80/TCP 147m ## from my ubuntu pod in member cluster blue root@ubuntu-20-04-cbb58d77-tcrjb:/# nslookup 10.134.30.220 220.30.134.10.in-addr.arpa\tname = antrea-mc-nginx.nginx.svc.cluster.local. ## curl the dns name root@ubuntu-20-04-cbb58d77-tcrjb:/# curl http://antrea-mc-nginx.nginx.svc.cluster.local \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Failure scenario # What happens if a node which currently holds the active gateway goes down?\nLets test that. Currently these are my active gateways in member-cluster-blue and red respectively:\n## get the current gateway in member-cluster blue k get gateway -A NAMESPACE NAME GATEWAY IP INTERNAL IP AGE kube-system tkg-cluster-2-md-0-vrt25-7f44f4798xqbk9h-tc979 10.101.12.14 10.101.12.14 37h ## list all nodes in my member-cluster blue k get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME tkg-cluster-2-md-0-vrt25-7f44f4798xqbk9h-tc979 Ready \u0026lt;none\u0026gt; 46h v1.26.5+vmware.2 10.101.12.14 10.101.12.14 Ubuntu 20.04.6 LTS 5.4.0-152-generic containerd://1.6.18-1-gdbc99e5b1 tkg-cluster-2-md-1-wmxhd-5998fcf669x64b9h-z7mnw Ready \u0026lt;none\u0026gt; 46h v1.26.5+vmware.2 10.101.12.13 10.101.12.13 Ubuntu 20.04.6 LTS 5.4.0-152-generic containerd://1.6.18-1-gdbc99e5b1 tkg-cluster-2-x2fqj-fxswx Ready control-plane 46h v1.26.5+vmware.2 10.101.12.33 10.101.12.33 Ubuntu 20.04.6 LTS 5.4.0-152-generic containerd://1.6.18-1-gdbc99e5b1 ## get the current gateway in member-cluster red k get gateway -A NAMESPACE NAME GATEWAY IP INTERNAL IP AGE kube-system tkg-cluster-3-md-0-h5ppw-9db445579xq45bn-nsq98 10.101.12.38 10.101.12.38 37h ## list all nodes in my member-cluster red k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME tkg-cluster-3-krrwq-p588j Ready control-plane 46h v1.26.5+vmware.2 10.101.12.28 10.101.12.28 Ubuntu 20.04.6 LTS 5.4.0-152-generic containerd://1.6.18-1-gdbc99e5b1 tkg-cluster-3-md-0-h5ppw-9db445579xq45bn-nsq98 Ready \u0026lt;none\u0026gt; 46h v1.26.5+vmware.2 10.101.12.38 10.101.12.38 Ubuntu 20.04.6 LTS 5.4.0-152-generic containerd://1.6.18-1-gdbc99e5b1 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr Ready \u0026lt;none\u0026gt; 46h v1.26.5+vmware.2 10.101.12.17 10.101.12.17 Ubuntu 20.04.6 LTS 5.4.0-152-generic containerd://1.6.18-1-gdbc99e5b1 Now I will shutdown the node that currently is the active gateway in my member-cluster blue. I will from my vCenter just do a \u0026ldquo;Power off\u0026rdquo; operation.\nAnd I will even delete it..\nRemember I annotated my two nodes as potential Gateway candidates?\nWell look what happened after my active gateway disappeared. It selects the next available candidate automatically. Gateway up again and all services up.\nk get gateway -A NAMESPACE NAME GATEWAY IP INTERNAL IP AGE kube-system tkg-cluster-2-md-1-wmxhd-5998fcf669x64b9h-z7mnw 10.101.12.13 10.101.12.13 37s The deleted node will be taken care of by TKG and recreated.\nRouting pod traffic through Multi-cluster Gateways # Next neat feature of Antrea Multi-cluster is the ability to allow pods from the different member clusters to reach each other on their respective pod IP addresses. Say I have POD-A running in member-cluster blue and are depending on connecting to POD-B running in member-cluster red. To achieve this a couple of steps is needed to be configured.\nThe agent.conf Feature Gate enablePodToPodConnectivity: true must be set to true (This is already done as explained in the initial chapter) Need to configure the antrea-mc-controller configMap There are two ways to edit the antrea-mc-controller configmap, edit the yaml antrea-multicluster-member.yml that was used to install the antrea-mc-controller in the member clusters or edit the configMap antrea-mc-controller-config directly in the ns kube-system. I will edit it directly and the only thing that needs to be added is the cluster configured pod CIDR.\nIf editing directly, one have to restart the pod antrea-mc-controller for it to read the new configMap.\nThe POD CIDRS between the clusters can NOT overlap\nEditing the configMap\n## member-cluster-blue (tkg-cluster-2) ## Get the pod or cluster cidr of the cluster you are editing the configMap k cluster-info dump | grep -m 1 cluster-cidr \u0026#34;--cluster-cidr=10.133.0.0/16\u0026#34;, ## edit the configmap k edit configmaps -n kube-system antrea-mc-controller-config ## the configmap # Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: controller_manager_config.yaml: | apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: MultiClusterConfig health: healthProbeBindAddress: :8080 metrics: bindAddress: \u0026#34;0\u0026#34; webhook: port: 9443 leaderElection: leaderElect: false serviceCIDR: \u0026#34;\u0026#34; podCIDRs: - \u0026#34;10.133.0.0/16\u0026#34; ## Add the cidr here from the range above gatewayIPPrecedence: \u0026#34;private\u0026#34; endpointIPType: \u0026#34;ClusterIP\u0026#34; enableStretchedNetworkPolicy: false kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;controller_manager_config.yaml\u0026#34;:\u0026#34;apiVersion: multicluster.crd.antrea.io/v1alpha1\\nkind: MultiClusterConfig\\nhealth:\\n healthProbeBindAddress: :8080\\nmetrics:\\n bindAddress: \\\u0026#34;0\\\u0026#34;\\nwebhook:\\n port: 9443\\nleaderElection:\\n leaderElect: false\\nserviceCIDR: \\\u0026#34;\\\u0026#34;\\npodCIDRs:\\n - \\\u0026#34;\\\u0026#34;\\ngatewayIPPrecedence: \\\u0026#34;private\\\u0026#34;\\nendpointIPType: \\\u0026#34;ClusterIP\\\u0026#34;\\nenableStretchedNetworkPolicy: false\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;antrea\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;antrea-mc-controller-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}} creationTimestamp: \u0026#34;2023-09-29T05:57:12Z\u0026#34; labels: app: antrea name: antrea-mc-controller-config namespace: kube-system resourceVersion: \u0026#34;98622\u0026#34; uid: fd760052-a18e-4623-b217-d9b96ae36cac ## :wq configmap/antrea-mc-controller-config edited Restart antrea-mc-controller pod\nk delete pod -n kube-system antrea-mc-controller-5bb945f87f-nfd97 pod \u0026#34;antrea-mc-controller-5bb945f87f-nfd97\u0026#34; deleted Repeat on the other clusters\nNow testing time\nI have a pod running in member-cluster red (tkg-cluster-3) with the following IP:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-cbb58d77-l25zv 1/1 Running 0 5h18m 10.135.1.58 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; I also have another POD running in my member-cluster blue with this information:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-cbb58d77-tcrjb 1/1 Running 0 5h24m 10.133.1.57 tkg-cluster-2-md-1-wmxhd-5998fcf669x64b9h-z7mnw \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Now I will execute into the last pod and do a ping to the pod in member-cluster red using the POD IP.\nroot@ubuntu-20-04-cbb58d77-tcrjb:/# ping 10.135.1.58 PING 10.135.1.58 (10.135.1.58) 56(84) bytes of data. On the destination pod I have started tcpdump to listen on icmp, and on the screenshot below I have the ping from the source on the left side to the destination on the right. Notice the IP addresses being reported by tcpdump in the destination pod.\nroot@ubuntu-20-04-cbb58d77-l25zv:/# tcpdump -i eth0 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 11:41:44.577683 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 4455, seq 1, length 64 11:41:44.577897 IP ubuntu-20-04-cbb58d77-l25zv \u0026gt; 10.101.12.14: ICMP echo reply, id 4455, seq 1, length 64 11:41:45.577012 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 4455, seq 2, length 64 11:41:45.577072 IP ubuntu-20-04-cbb58d77-l25zv \u0026gt; 10.101.12.14: ICMP echo reply, id 4455, seq 2, length 64 11:41:46.577099 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 4455, seq 3, length 64 11:41:46.577144 IP ubuntu-20-04-cbb58d77-l25zv \u0026gt; 10.101.12.14: ICMP echo reply, id 4455, seq 3, length 64 11:41:47.579227 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 4455, seq 4, length 64 11:41:47.579256 IP ubuntu-20-04-cbb58d77-l25zv \u0026gt; 10.101.12.14: ICMP echo reply, id 4455, seq 4, length 64 11:41:48.580607 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 4455, seq 5, length 64 11:41:48.580632 IP ubuntu-20-04-cbb58d77-l25zv \u0026gt; 10.101.12.14: ICMP echo reply, id 4455, seq 5, length 64 11:41:49.581900 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 4455, seq 6, length 64 The source IP address here is the gateway IP from the source member-cluster:\nandreasm@tkg-bootstrap:~$ k get gateway -A NAMESPACE NAME GATEWAY IP INTERNAL IP AGE kube-system tkg-cluster-2-md-0-vrt25-7f44f4798xqbk9h-tc979 10.101.12.14 10.101.12.14 5h37m And if I do the same operation just change the direction:\nThe source IP is the gateway IP from the source cluster:\nNAMESPACE NAME GATEWAY IP INTERNAL IP AGE kube-system tkg-cluster-3-md-0-h5ppw-9db445579xq45bn-nsq98 10.101.12.38 10.101.12.38 5h41m Using antctl traceflow:\nandreasm@tkg-bootstrap:~$ antctl traceflow -S prod/ubuntu-20-04-cbb58d77-tcrjb -D 10.135.1.58 name: prod-ubuntu-20-04-cbb58d77-tcrjb-to-10.135.1.58-5b8l7ms8 phase: Running source: prod/ubuntu-20-04-cbb58d77-tcrjb destination: 10.135.1.58 results: - node: tkg-cluster-2-md-1-wmxhd-5998fcf669x64b9h-z7mnw timestamp: 1696064979 observations: - component: SpoofGuard action: Forwarded - component: Forwarding componentInfo: Output action: Forwarded tunnelDstIP: 10.101.12.14 Error: timeout waiting for Traceflow done I know it is forwarded and delivered as I can see it on the destination pod using tcpdump while doing the traceflow:\nroot@ubuntu-20-04-cbb58d77-l25zv:/# tcpdump -i eth0 icmp tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 09:09:39.161140 IP 10.101.12.14 \u0026gt; ubuntu-20-04-cbb58d77-l25zv: ICMP echo request, id 0, seq 0, length 8 09:09:39.161411 IP ubuntu-20-04-cbb58d77-l25zv \u0026gt; 10.101.12.14: ICMP echo reply, id 0, seq 0, length 8 Next up is how Antrea Policies can be used with Antrea Multi-cluster\nMulti-cluster NetworkPolicy (ANP and ACNP) # This feature is about the possibility to create policies using ClusterSet objects as selector for ingress or egress, like exported Multi-cluster services or namespaces across the clusters in a ClusterSet. For this to work the enableStretchedNetworkPolicy Feature Gate must be set to true on both controller.conf and agent.conf. I already enabled this at cluster provisioning, but to check take a look at the antrea-config configMap:\nandreasm@tkg-bootstrap:~$ k get configmaps -n kube-system antrea-config -oyaml apiVersion: v1 data: antrea-agent.conf: | featureGates: Multicluster: true multicluster: enableGateway: true enableStretchedNetworkPolicy: true enablePodToPodConnectivity: true ... antrea-controller.conf: | featureGates: Multicluster: true multicluster: enableStretchedNetworkPolicy: true Following the GitHub docs I will use the examples there to make some policies and demonstrate them.\nStarting with the first example, an egress rule to a Multi-cluster service.\nEgress rule to Multi-cluster service\nBelow is an example taken from the official docs creating a policy that will Drop traffic to a specific Multi-cluster service from a specific pod using label selector (adjusted to fit my environment):\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-drop-ubuntu-pod-to-nginx-mc-service spec: priority: 1 tier: securityops appliedTo: - podSelector: matchLabels: role: no-nginx egress: - action: Drop toServices: - name: nginx # an exported Multi-cluster Service namespace: nginx scope: ClusterSet I will now apply it on the member-cluster-blue, where my source test pod Ubuntu is running. I can also apply it on the source cluster where service originates from and it will have the same effect:\nk apply -f egress-nginx-mc.yaml clusternetworkpolicy.crd.antrea.io/acnp-drop-ubuntu-pod-to-nginx-mc-service created k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-ubuntu-pod-to-nginx-mc-service securityops 1 0 0 32s Its applied, but not in effect as I dont have the correct labels applied yet on my test pod.\nSo from my Ubuntu test pod, I can still reach the service nginx. But I will now label the pod according to the yaml above.\nk label pod -n prod ubuntu-20-04-cbb58d77-tcrjb role=no-nginx pod/ubuntu-20-04-cbb58d77-tcrjb labeled k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-ubuntu-pod-to-nginx-mc-service securityops 1 1 1 39s Something is in effect\u0026hellip; Now from my test Ubuntu pod I will try to curl the nginx service.\nroot@ubuntu-20-04-cbb58d77-tcrjb:/# nslookup 10.134.30.220 220.30.134.10.in-addr.arpa\tname = antrea-mc-nginx.nginx.svc.cluster.local. root@ubuntu-20-04-cbb58d77-tcrjb:/# curl http://antrea-mc-nginx.nginx.svc.cluster.local curl: (28) Failed to connect to antrea-mc-nginx.nginx.svc.cluster.local port 80: Connection timed out root@ubuntu-20-04-cbb58d77-tcrjb:/# Then I can do a Antrea traceflow:\nandreasm@tkg-bootstrap:~$ antctl traceflow -S prod/ubuntu-20-04-cbb58d77-tcrjb -D nginx/antrea-mc-nginx -f tcp,tcp_dst=80 name: prod-ubuntu-20-04-cbb58d77-tcrjb-to-nginx-antrea-mc-nginx-l55dnhvf phase: Succeeded source: prod/ubuntu-20-04-cbb58d77-tcrjb destination: nginx/antrea-mc-nginx results: - node: tkg-cluster-2-md-1-wmxhd-5998fcf669x64b9h-z7mnw timestamp: 1696065339 observations: - component: SpoofGuard action: Forwarded - component: LB action: Forwarded translatedDstIP: 10.136.40.246 - component: NetworkPolicy componentInfo: EgressMetric action: Dropped networkPolicy: AntreaClusterNetworkPolicy:acnp-drop-ubuntu-pod-to-nginx-mc-service No need to troubleshoot connectivity issues, is being dropped by the above policy. \u0026#x1f6b7;\nThis policy will drop all outgoing (egress) from any pods with label role=no-nginx to the exported/imported service using Multi-cluster. Pod selection is done regardless of namespace as my selection is done using pod labels and I am using a Antrea ClusterNetworkPolicy.\nSuch a policy is applied on the \u0026ldquo;source\u0026rdquo; cluster where you want to define strict egress policies. Another way is to define ingress policies on the destination cluster (source cluster of the exported service).\nIngress rule\nBefore getting started on this chapter we need to enable the **enableStretchedNetworkPolicy\u0026rdquo; feature in the configMap of ALL antrea-mc-controllers in the ClusterSet (including the leader cluster). It is shown below\u0026hellip;\nI will have to edit the configMap in both member clusters and leader-cluster setting enableStretchedNetworkPolicy: to true\n# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: controller_manager_config.yaml: | apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: MultiClusterConfig health: healthProbeBindAddress: :8080 metrics: bindAddress: \u0026#34;0\u0026#34; webhook: port: 9443 leaderElection: leaderElect: false serviceCIDR: \u0026#34;\u0026#34; podCIDRs: - \u0026#34;10.133.0.0/16\u0026#34; gatewayIPPrecedence: \u0026#34;private\u0026#34; endpointIPType: \u0026#34;ClusterIP\u0026#34; enableStretchedNetworkPolicy: true #set to true kind: ConfigMap metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;controller_manager_config.yaml\u0026#34;:\u0026#34;apiVersion: multicluster.crd.antrea.io/v1alpha1\\nkind: MultiClusterConfig\\nhealth:\\n healthProbeBindAddress: :8080\\nmetrics:\\n bindAddress: \\\u0026#34;0\\\u0026#34;\\nwebhook:\\n port: 9443\\nleaderElection:\\n leaderElect: false\\nserviceCIDR: \\\u0026#34;\\\u0026#34;\\npodCIDRs:\\n - \\\u0026#34;\\\u0026#34;\\ngatewayIPPrecedence: \\\u0026#34;private\\\u0026#34;\\nendpointIPType: \\\u0026#34;ClusterIP\\\u0026#34;\\nenableStretchedNetworkPolicy: false\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;antrea\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;antrea-mc-controller-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}} creationTimestamp: \u0026#34;2023-09-29T05:57:12Z\u0026#34; labels: app: antrea name: antrea-mc-controller-config namespace: kube-system resourceVersion: \u0026#34;415934\u0026#34; uid: fd760052-a18e-4623-b217-d9b96ae36cac Restart the antrea-mc-controller after editing the above configMap.\nAgain I will be taking the two examples from the Antrea Gitub doc pages, adjust them to suit my environment. The first policy example will apply to namespaces with the label environment=protected where ingress will be denied/dropped from the selected pods in any namespace with the label environment=untrust where the scope is ClusterSet. This means it should filter on any traffic coming from any of the namespaces having the label environment=untrust in any member cluster in the ClusterSet. So lets see how this works.\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: drop-untrust-access-to-protected-namespace spec: appliedTo: - namespaceSelector: matchLabels: environment: protected priority: 1 tier: securityops ingress: - action: Drop from: # Select all Pods in environment=untrust Namespaces in the ClusterSet - scope: ClusterSet namespaceSelector: matchLabels: environment: untrust This policy will be applied on the the member clusters where I do have such \u0026ldquo;protected\u0026rdquo; namespaces and want to ensure no incoming (ingress) from any \u0026ldquo;untrust\u0026rdquo; environments.\nLets start with applying the policy, and check if it has been applied:\nandreasm@tkg-bootstrap:~$ k apply -f drop-untrust-to-protected.yaml clusternetworkpolicy.crd.antrea.io/drop-untrust-access-to-protected-namespace created ## check it andreasm@tkg-bootstrap:~$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE drop-untrust-access-to-protected-namespace securityops 1 0 0 54s Nothing enforced yet.\nBack to my ubuntu pod again which resided in the namespace prod in member-cluster blue (tkg-cluster-2). I will label til namespace with environment=protected.\nandreasm@tkg-bootstrap:~$ k label namespaces prod environment=protected namespace/prod labeled ## checking the policy now andreasm@tkg-bootstrap:~$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE drop-untrust-access-to-protected-namespace securityops 1 1 1 3m10s In my other member cluster, red, I will create a new namespace, spin up another ubuntu pod there, label the namespace environment=untrust.\nandreasm@tkg-bootstrap:~$ k get pods -n untrust-ns -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-cbb58d77-bl5w5 1/1 Running 0 43s 10.135.1.228 tkg-cluster-3-md-1-824bx-5bdb559f7bxqgbb8-bqwnr \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Is now running in the untrust-ns\nFrom the pod I will try to ping the \u0026ldquo;protected\u0026rdquo; pod in the member-cluster-blue before I label the namespace with untrust.\nroot@ubuntu-20-04-cbb58d77-bl5w5:/# ping 10.133.1.57 PING 10.133.1.57 (10.133.1.57) 56(84) bytes of data. 64 bytes from 10.133.1.57: icmp_seq=1 ttl=60 time=7.93 ms 64 bytes from 10.133.1.57: icmp_seq=2 ttl=60 time=4.17 ms 64 bytes from 10.133.1.57: icmp_seq=3 ttl=60 time=2.99 ms This \u0026ldquo;protected\u0026rdquo; pod is also running an nginx instance, so lets see if I can curl it from my untrust pod:\nroot@ubuntu-20-04-cbb58d77-bl5w5:/# curl 10.133.1.57 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; This works also. Now I will label the namespace accordingly.\nandreasm@tkg-bootstrap:~$ k get ns untrust-ns --show-labels NAME STATUS AGE LABELS untrust-ns Active 9m3s environment=untrust,kubernetes.io/metadata.name=untrust-ns Can I now both curl and ping the protected pod from the untrust pod?\nroot@ubuntu-20-04-cbb58d77-bl5w5:/# ping 10.133.1.57 PING 10.133.1.57 (10.133.1.57) 56(84) bytes of data. ^C --- 10.133.1.57 ping statistics --- 96 packets transmitted, 0 received, 100% packet loss, time 97275ms ## curl root@ubuntu-20-04-cbb58d77-bl5w5:/# curl 10.133.1.57 curl: (28) Failed to connect to 10.133.1.57 port 80: Connection timed out Thats a no\u0026hellip;\nHow about that. Now I would like to see the resourceImport/Exports in the leader cluster:\nandreasm@tkg-bootstrap:~$ k get resourceimports.multicluster.crd.antrea.io -A NAMESPACE NAME KIND NAMESPACE NAME AGE antrea-multicluster 07114e55523175d2 LabelIdentity 7m4s antrea-multicluster 085dd73c98e1d875 LabelIdentity 7m4s antrea-multicluster 0c56bac2726cdc89 LabelIdentity 7m4s antrea-multicluster 0f8eaa8d1ed0d024 LabelIdentity 7m4s antrea-multicluster 1742a902fef9ecf2 LabelIdentity 7m4s antrea-multicluster 1a7d18d61d0c0ee1 LabelIdentity 7m4s antrea-multicluster 1f395d26ddf2e628 LabelIdentity 7m4s antrea-multicluster 23f00caa60df7444 LabelIdentity 7m4s antrea-multicluster 2ae09744db3c2971 LabelIdentity 7m4s antrea-multicluster 2de39d651a0361e9 LabelIdentity 7m4s antrea-multicluster 2ef5bcdb8443a24c LabelIdentity 7m4s antrea-multicluster 339dbb049e2e9a92 LabelIdentity 7m4s antrea-multicluster 430e80a9621c621a LabelIdentity 7m4s antrea-multicluster 4c9c7b4329d0e128 LabelIdentity 3m55s antrea-multicluster 5629f9c0856c3bab LabelIdentity 7m4s antrea-multicluster 593cb26f6e1ae9e3 LabelIdentity 7m4s antrea-multicluster 66b072b8efc1faa7 LabelIdentity 7m4s antrea-multicluster 67410707ad7a9908 LabelIdentity 7m4s antrea-multicluster 7468af4ac6f5dfa7 LabelIdentity 7m4s antrea-multicluster 7c59020a5dcbb1b9 LabelIdentity 7m4s antrea-multicluster 7dac813f5932e57e LabelIdentity 7m4s antrea-multicluster 7f43c50b4566cd91 LabelIdentity 7m4s antrea-multicluster 8327de14325c06f9 LabelIdentity 7m4s antrea-multicluster 9227dd1f8d5eef10 LabelIdentity 7m4s antrea-multicluster 9a2e5dbff4effe99 LabelIdentity 7m4s antrea-multicluster 9a4b2085e53f890c LabelIdentity 7m4s antrea-multicluster 9b5c3a1ff3c1724f LabelIdentity 7m4s antrea-multicluster 9ba8fb64d35434a6 LabelIdentity 7m4s antrea-multicluster a59e6e24ceaabe76 LabelIdentity 7m4s antrea-multicluster a642d62a95b68860 LabelIdentity 7m4s antrea-multicluster afe73316119e5beb LabelIdentity 7m4s antrea-multicluster b07efcf6d7df9ecc LabelIdentity 7m4s antrea-multicluster b0ef5ea4e6654296 LabelIdentity 7m4s antrea-multicluster b4ab02dcfded7a88 LabelIdentity 7m4s antrea-multicluster b9f26e2c922bdfce LabelIdentity 7m4s antrea-multicluster be152630c03e5d6b LabelIdentity 7m4s antrea-multicluster c316283f47088c45 LabelIdentity 7m4s antrea-multicluster c7703628c133a9ae LabelIdentity 7m4s antrea-multicluster db564a4a19f62e39 LabelIdentity 7m4s antrea-multicluster db672f99c9b13343 LabelIdentity 7m4s antrea-multicluster db674d682cb5db88 LabelIdentity 7m4s antrea-multicluster ef097265c27216d2 LabelIdentity 7m4s antrea-multicluster f8e5f6fba3fb9a5c LabelIdentity 7m4s antrea-multicluster fc0e44265f8dce47 LabelIdentity 7m4s antrea-multicluster fc156481c7b8ebf2 LabelIdentity 7m4s antrea-multicluster member-cluster-blue-clusterinfo ClusterInfo antrea-multicluster member-cluster-blue-clusterinfo 31h antrea-multicluster member-cluster-red-clusterinfo ClusterInfo antrea-multicluster member-cluster-red-clusterinfo 31h antrea-multicluster nginx-nginx-endpoints Endpoints nginx nginx 31h antrea-multicluster nginx-nginx-service ServiceImport nginx nginx 31h antrea-multicluster yelb-yelb-appserver-endpoints Endpoints yelb yelb-appserver 31h antrea-multicluster yelb-yelb-appserver-service ServiceImport yelb yelb-appserver 31h andreasm@tkg-bootstrap:~$ k get resourceexports.multicluster.crd.antrea.io -A NAMESPACE NAME CLUSTER ID KIND NAMESPACE NAME AGE antrea-multicluster member-cluster-blue-085dd73c98e1d875 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-0f8eaa8d1ed0d024 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-23f00caa60df7444 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-2ae09744db3c2971 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-2de39d651a0361e9 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-2ef5bcdb8443a24c member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-5629f9c0856c3bab member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-593cb26f6e1ae9e3 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-7c59020a5dcbb1b9 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-9a4b2085e53f890c member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-9b5c3a1ff3c1724f member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-a642d62a95b68860 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-afe73316119e5beb member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-b07efcf6d7df9ecc member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-b0ef5ea4e6654296 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-b4ab02dcfded7a88 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-b9f26e2c922bdfce member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-c7703628c133a9ae member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-clusterinfo member-cluster-blue ClusterInfo kube-system member-cluster-blue 31h antrea-multicluster member-cluster-blue-db672f99c9b13343 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-blue-fc156481c7b8ebf2 member-cluster-blue LabelIdentity 11m antrea-multicluster member-cluster-red-07114e55523175d2 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-1742a902fef9ecf2 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-1a7d18d61d0c0ee1 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-1f395d26ddf2e628 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-339dbb049e2e9a92 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-430e80a9621c621a member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-4c9c7b4329d0e128 member-cluster-red LabelIdentity 4m23s antrea-multicluster member-cluster-red-66b072b8efc1faa7 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-67410707ad7a9908 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-7468af4ac6f5dfa7 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-7dac813f5932e57e member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-7f43c50b4566cd91 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-8327de14325c06f9 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-9227dd1f8d5eef10 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-9a2e5dbff4effe99 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-9ba8fb64d35434a6 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-a59e6e24ceaabe76 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-be152630c03e5d6b member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-c316283f47088c45 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-clusterinfo member-cluster-red ClusterInfo kube-system member-cluster-red 31h antrea-multicluster member-cluster-red-db564a4a19f62e39 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-db674d682cb5db88 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-ef097265c27216d2 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-f8e5f6fba3fb9a5c member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-fc0e44265f8dce47 member-cluster-red LabelIdentity 11m antrea-multicluster member-cluster-red-nginx-nginx-endpoints member-cluster-red Endpoints nginx nginx 31h antrea-multicluster member-cluster-red-nginx-nginx-service member-cluster-red Service nginx nginx 31h antrea-multicluster member-cluster-red-yelb-yelb-appserver-endpoints member-cluster-red Endpoints yelb yelb-appserver 31h antrea-multicluster member-cluster-red-yelb-yelb-appserver-service member-cluster-red Service yelb yelb-appserver 31h And if I describe one of them:\nk describe resourceimports.multicluster.crd.antrea.io -n antrea-multicluster 07114e55523175d2 Name: 07114e55523175d2 Namespace: antrea-multicluster Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: multicluster.crd.antrea.io/v1alpha1 Kind: ResourceImport Metadata: Creation Timestamp: 2023-09-30T13:33:02Z Generation: 1 Resource Version: 442986 UID: 4157d671-6c4d-4653-b97f-554bdfa705d9 Spec: Kind: LabelIdentity Label Identity: Id: 35 Label: ns:kubernetes.io/metadata.name=nginx\u0026amp;pod:app=nginx-ui,pod-template-hash=59c956b95b,tier=frontend Events: \u0026lt;none\u0026gt; The leader cluster is now aware of all labels, namespaces and pods, from the other member clusters which will allow us to create this ingress rule with namespace or pod selection from the other clusters. The applyTo field is only relevant for the cluster the policy is applied in.\nThe second ingress policy example below is using a \u0026ldquo;namespaced\u0026rdquo; policy (Antrea NetworkPolicy). It will be applied to a specific namespace in the \u0026ldquo;destination\u0026rdquo; cluster, using podSelector with labels app=db to select specific pods. The policy will be placed in the Application Tier, it will allow sources coming from any pods in the ClusterSet matching the label app=client to the pods in the specified namesapace with label app=db and dropping everything else.\napiVersion: crd.antrea.io/v1alpha1 kind: AntreaNetworkPolicy metadata: name: db-svc-allow-ingress-from-client-only namespace: prod-us-west spec: appliedTo: - podSelector: matchLabels: app: db priority: 1 tier: application ingress: - action: Allow from: # Select all Pods in Namespace \u0026#34;prod-us-west\u0026#34; from all clusters in the ClusterSet (if the # Namespace exists in that cluster) whose labels match app=client - scope: ClusterSet podSelector: matchLabels: app: client - action: Deny I will not test this, as it will work similarly as the first example, the only difference being is that it is applied on a namespace, not clusterwide (using Antrea ClusterNetworkPolicy).\nNext up is creating a ClusterNetworkPolicy that is replicated across all clusters.\nMulti-cluster ClusterNetworkPolicy replication (ACNP) # In this last chapter I will test out the possibility with Multi-cluster to replicate a ClusterNetworkPolicy across each members in my ClusterSet, member-cluster-blue and red.\nI will just take the example from the official Antrea Github docs page and use it as it is. Before I apply it I will just quickly check whether I have any policies applied in any of my member clusters:\n## tkg-cluster-2 - member-blue k config current-context tkg-cluster-2-admin@tkg-cluster-2 ## Any policies? k get acnp No resources found k get anp -A No resources found ## tkg-cluster-3 - member-red k config current-context tkg-cluster-3-admin@tkg-cluster-3 ## Any policies? k get acnp No resources found k get anp -A No resources found No policies.\nTo replicate a policy to all members I will switch context to the leader-cluster and create a ResourceExport and the ClusterNetworkPolicy itself and apply it on the leader-cluster.\nThe namespace for the Kind: ResourceExport needs to be in the same namespace as where the antrea-mc-controller is running!!\nBelow is the example I will be using, taken from the Antrea Github doc page:\napiVersion: multicluster.crd.antrea.io/v1alpha1 kind: ResourceExport metadata: name: strict-namespace-isolation-for-test-clusterset namespace: antrea-multicluster # Namespace that Multi-cluster Controller is deployed spec: kind: AntreaClusterNetworkPolicy name: strict-namespace-isolation # In each importing cluster, an ACNP of name antrea-mc-strict-namespace-isolation will be created with the spec below clusterNetworkPolicy: priority: 1 tier: securityops appliedTo: - namespaceSelector: {} # Selects all Namespaces in the member cluster ingress: - action: Pass from: - namespaces: match: Self # Skip drop rule for traffic from Pods in the same Namespace - podSelector: matchLabels: k8s-app: kube-dns # Skip drop rule for traffic from the core-dns components - action: Drop from: - namespaceSelector: {} # Drop from Pods from all other Namespaces Now apply it in my leader-cluster:\n## tkg-cluster-1 - leader-cluster k config current-context tkg-cluster-1-admin@tkg-cluster-1 ## apply k apply -f acnp-replicated.yaml resourceexport.multicluster.crd.antrea.io/strict-namespace-isolation-for-test-clusterset created Now I will switch over to the contexts of my member clusters and check what has happened there:\n## tkg-cluster-2 - member-blue k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE antrea-mc-strict-namespace-isolation securityops 1 3 3 47s ## tkg-cluster-3 - member-red k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE antrea-mc-strict-namespace-isolation securityops 1 3 3 108s This is really great!! Both my member cluster has gotten the policy applied.\nOn the leader-cluster?\n## tkg-cluster-1 k get acnp No resources found Nothing.\nThe only bad thing now is that this policy broke my Yelb application as my Yelb-ui can no longer reach the backends in the other cluster \u0026#x1f604; so will have to add some additional policies to support this application also. Which is perfectly normal, and I have covered a bunch of Antrea policies in this post and this post that can be re-used for this purpose.\nNow I will just do a last thing an check the status of my replicated policy from the leader-cluster if there are any alert on any of the member cluster, why they have not applied the policy etc..\nk get resourceexports.multicluster.crd.antrea.io -A NAMESPACE NAME CLUSTER ID KIND NAMESPACE NAME AGE antrea-multicluster strict-namespace-isolation-for-test-clusterset AntreaClusterNetworkPolicy strict-namespace-isolation 8m39s k describe resourceexports.multicluster.crd.antrea.io -n antrea-multicluster strict-namespace-isolation-for-test-clusterset Name: strict-namespace-isolation-for-test-clusterset Namespace: antrea-multicluster Labels: sourceKind=AntreaClusterNetworkPolicy sourceName=strict-namespace-isolation sourceNamespace= Annotations: \u0026lt;none\u0026gt; API Version: multicluster.crd.antrea.io/v1alpha1 Kind: ResourceExport Metadata: Creation Timestamp: 2023-09-30T14:02:42Z Finalizers: resourceexport.finalizers.antrea.io Generation: 1 Resource Version: 448359 UID: 3d7111dd-eecc-46bb-8307-c95d747474b0 Spec: Cluster Network Policy: Applied To: Namespace Selector: Ingress: Action: Pass Enable Logging: false From: Namespaces: Match: Self Pod Selector: Match Labels: k8s-app: kube-dns Action: Drop Enable Logging: false From: Namespace Selector: Priority: 1 Tier: securityops Kind: AntreaClusterNetworkPolicy Name: strict-namespace-isolation Status: Conditions: Last Transition Time: 2023-09-30T14:02:42Z Status: True Type: Succeeded Events: \u0026lt;none\u0026gt; Looks good.\nBonus content # With the help from my friend ChatGPT I created a menu driven \u0026ldquo;automated\u0026rdquo; way of deploying all of the above steps.\nThe pre-requisities for this script is that it expects all your Kubernetes clusters already deployed and the Antrea Multi-cluster feature gates enabled. Then the script should be executed from a machine that has all the kubernetes clusters contexts added. It will prompt for the different contexts in some of the menus and will change to these context to execute specific commands in selected contexts.\nI will just quickly go through the script/menus. When script is executed it will bring up this menu:\nMain Menu: 1. Select Antrea version 2. Install Antrea Multi-cluster on leader cluster 3. Install Antrea Multi-cluster on member cluster 4. Create member-cluster secrets 5. Apply member tokens 6. Create ClusterSet on the leader cluster 7. Create ClusterClaim on member cluster 8. Create Multi-cluster Gateway 9. Create a Multi-cluster service 10. Exit Enter your choice: Explanation of the different menu selections and what it does:\nThis will prompt you for the specific Antrea version you are using and use that as a tag for downloading and applying the correct yaml files\nThis will let you select your \u0026ldquo;Leader Cluster\u0026rdquo;, create the namespace antrea-multicluster, deploy the leader yaml manifests and deploy the antrea-mc-controller in the cluster.\nThis will let you select a member cluster to install the member yaml and the antrea-mc-controller. This needs to be done for each of the member cluster you want to install it on.\nThis will create the member-cluster secrets, asking for the leader-cluster contexts for them to be created in. And the export the token.yamls to be applied in next step\nEnter your choice: 4 Creating member-cluster secrets... 1) 10.13.90.1 2) cluster-1 3) ns-stc-1 4) Back to Main Menu Select a context as the leader cluster: 2 Switched to context \u0026#34;cluster-1\u0026#34;. Enter the name for Member Cluster (e.g., member-blue): member-red This will ask you for the context for the respective token.yamls created to be applied in. It will list all the yaml files created in the current folder for you to choose which token to be applied.\nEnter your choice: 5 Applying member tokens... 1) 10.13.90.1 2) cluster-1 3) ns-stc-1 4) Back to Main Menu Select a context to switch to: 2 Switched to context \u0026#34;cluster-1\u0026#34;. 1) member-blue-token.yaml 2) Back to Main Menu Select a YAML file to apply: This will create the clusterset prompting for the leader cluster context and ask for the ClusterID and ClusterSet name\nEnter your choice: 6 Creating ClusterSet on the leader cluster... 1) 10.13.90.1 2) cluster-1 3) ns-stc-1 4) Back to Main Menu Select the leader cluster context: 2 Switched to context \u0026#34;cluster-1\u0026#34;. Enter ClusterID (e.g., tkg-cluster-leader): leader-cluster Enter ClusterSet name (e.g., andreasm-clusterset): super-clusterset This will create the clusterclaim on the member cluster to join the cluster leader/clusterset\nEnter your choice: 7 Creating ClusterClaim on member cluster... 1) 10.13.90.1 2) cluster-1 3) ns-stc-1 4) Back to Main Menu Select a context to switch to: 2 Switched to context \u0026#34;cluster-1\u0026#34;. Enter member-cluster-name (e.g., member-cluster-red): member-cluster-blue Enter ClusterSet name (e.g., andreasm-clusterset): super-clusterset Enter Leader ClusterID (e.g., tkg-cluster-leader): leader-cluster Enter Member Token to use: member-blue-token Enter Leader cluster API endpoint (e.g., https://10.101.114.100:6443): https://10.101.115.120:6443 This will create the Multi-cluster Gateway by letting you select the which node in which cluster to annotate\nEnter your choice: 8 Creating Multi-cluster Gateway... 1) 10.13.90.1 2) cluster-1 3) ns-stc-1 4) Back to Main Menu Select a context to switch to: 2 Switched to context \u0026#34;cluster-1\u0026#34;. 1) cluster-1-f82lv-fdvw8\t3) cluster-1-node-pool-01-tb4tw-555756bd56-klgcs 2) cluster-1-node-pool-01-tb4tw-555756bd56-76qv6 4) Back to Context Menu Select a node to annotate as Multi-cluster Gateway: 2 node/cluster-1-node-pool-01-tb4tw-555756bd56-76qv6 annotated Annotated cluster-1-node-pool-01-tb4tw-555756bd56-76qv6 as Multi-cluster Gateway. Select a node to annotate as Multi-cluster Gateway: 4 Do you want to annotate another node? (yes/no): # Selecting yes brings up the node list again. Selecting no takes you back to main menu. This needs to be done on all member clusters you need to define a gateway node This will let you select a context, list all services defined in this cluster, let you select it from a menu then export is as a Multi-cluster service.\nEnter your choice: 9 Creating a Multi-cluster service... 1) 10.13.90.1 2) cluster-1 3) ns-stc-1 4) Back to Main Menu Select a context to switch to: 2 Switched to context \u0026#34;cluster-1\u0026#34;. 1) antrea-multicluster\t5) kube-public\t9) vmware-system-antrea\t13) vmware-system-tkg 2) default\t6) kube-system\t10) vmware-system-auth\t14) yelb 3) fruit\t7) secretgen-controller\t11) vmware-system-cloud-provider 15) Back to Context Menu 4) kube-node-lease\t8) tkg-system\t12) vmware-system-csi Select a namespace to list services from: 14 1) redis-server 2) yelb-appserver 3) yelb-db 4) yelb-ui 5) Back to Namespace Menu Select a service to export as Multi-cluster service: 2 ServiceExport created for yelb-appserver in namespace yelb. serviceexport.multicluster.x-k8s.io/yelb-appserver unchanged Multi-cluster service applied. Select a service to export as Multi-cluster service: # hit enter to bring up menu 1) antrea-multicluster\t5) kube-public\t9) vmware-system-antrea\t13) vmware-system-tkg 2) default\t6) kube-system\t10) vmware-system-auth\t14) yelb 3) fruit\t7) secretgen-controller\t11) vmware-system-cloud-provider 15) Back to Context Menu 4) kube-node-lease\t8) tkg-system\t12) vmware-system-csi Select a service to export as Multi-cluster service: 15 Here is the script:\n#!/bin/bash # Function to create member-cluster secrets create_member_cluster_secrets() { echo \u0026#34;Creating member-cluster secrets...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu for selecting a context as the leader cluster PS3=\u0026#34;Select a context as the leader cluster: \u0026#34; select LEADER_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$LEADER_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$LEADER_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi # Set the selected context as the leader cluster context kubectl config use-context \u0026#34;$LEADER_CONTEXT\u0026#34; read -p \u0026#34;Enter the name for Member Cluster (e.g., member-blue): \u0026#34; MEMBER_CLUSTER_NAME # Create YAML content for the member cluster cat \u0026lt;\u0026lt;EOF \u0026gt; member-cluster.yml apiVersion: v1 kind: ServiceAccount metadata: name: $MEMBER_CLUSTER_NAME namespace: antrea-multicluster --- apiVersion: v1 kind: Secret metadata: name: ${MEMBER_CLUSTER_NAME}-token namespace: antrea-multicluster annotations: kubernetes.io/service-account.name: $MEMBER_CLUSTER_NAME type: kubernetes.io/service-account-token --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: $MEMBER_CLUSTER_NAME namespace: antrea-multicluster roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: antrea-mc-member-cluster-role subjects: - kind: ServiceAccount name: $MEMBER_CLUSTER_NAME namespace: antrea-multicluster EOF # Apply the YAML content for the member cluster kubectl apply -f member-cluster.yml # Create the member cluster secret file kubectl get secret ${MEMBER_CLUSTER_NAME}-token -n antrea-multicluster -o yaml | grep -w -e \u0026#39;^apiVersion\u0026#39; -e \u0026#39;^data\u0026#39; -e \u0026#39;^metadata\u0026#39; -e \u0026#39;^ *name:\u0026#39; -e \u0026#39;^kind\u0026#39; -e \u0026#39; ca.crt\u0026#39; -e \u0026#39; token:\u0026#39; -e \u0026#39;^type\u0026#39; -e \u0026#39; namespace\u0026#39; | sed -e \u0026#39;s/kubernetes.io\\/service-account-token/Opaque/g\u0026#39; -e \u0026#34;s/antrea-multicluster/kube-system/g\u0026#34; \u0026gt; \u0026#34;${MEMBER_CLUSTER_NAME}-token.yaml\u0026#34; echo \u0026#34;Member cluster secrets created and YAML file generated: ${MEMBER_CLUSTER_NAME}-token.yaml.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done } # Function to apply member tokens apply_member_tokens() { echo \u0026#34;Applying member tokens...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu for selecting a context to switch to PS3=\u0026#34;Select a context to switch to: \u0026#34; select SWITCH_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$SWITCH_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$SWITCH_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$SWITCH_CONTEXT\u0026#34; # List YAML files in the current folder and create a menu yaml_files=($(ls *.yaml)) # Display the menu for selecting a YAML file to apply PS3=\u0026#34;Select a YAML file to apply: \u0026#34; select SELECTED_YAML in \u0026#34;${yaml_files[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$SELECTED_YAML\u0026#34; ]]; then if [ \u0026#34;$SELECTED_YAML\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl apply -f \u0026#34;$SELECTED_YAML\u0026#34; echo \u0026#34;Applied $SELECTED_YAML in context $SWITCH_CONTEXT.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a YAML file or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done } # Function to create ClusterSet on the leader cluster create_clusterset_on_leader() { echo \u0026#34;Creating ClusterSet on the leader cluster...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu for selecting the leader cluster context PS3=\u0026#34;Select the leader cluster context: \u0026#34; select LEADER_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$LEADER_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$LEADER_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$LEADER_CONTEXT\u0026#34; # Prompt for ClusterID and ClusterSet name read -p \u0026#34;Enter ClusterID (e.g., tkg-cluster-leader): \u0026#34; CLUSTER_ID read -p \u0026#34;Enter ClusterSet name (e.g., andreasm-clusterset): \u0026#34; CLUSTERSET_NAME # Create YAML content for ClusterSet cat \u0026lt;\u0026lt;EOF \u0026gt; clusterset.yaml apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: id.k8s.io namespace: antrea-multicluster value: $CLUSTER_ID --- apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: clusterset.k8s.io namespace: antrea-multicluster value: $CLUSTERSET_NAME --- apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: ClusterSet metadata: name: $CLUSTERSET_NAME namespace: antrea-multicluster spec: leaders: - clusterID: $CLUSTER_ID EOF # Apply the ClusterSet YAML kubectl apply -f clusterset.yaml echo \u0026#34;ClusterSet created on the leader cluster.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done } # Function to create ClusterClaim on member cluster create_clusterclaim_on_member() { echo \u0026#34;Creating ClusterClaim on member cluster...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu for selecting a context to switch to PS3=\u0026#34;Select a context to switch to: \u0026#34; select MEMBER_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$MEMBER_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$MEMBER_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$MEMBER_CONTEXT\u0026#34; # Prompt for ClusterClaim values read -p \u0026#34;Enter member-cluster-name (e.g., member-cluster-red): \u0026#34; MEMBER_CLUSTER_NAME read -p \u0026#34;Enter ClusterSet name (e.g., andreasm-clusterset): \u0026#34; CLUSTERSET_NAME read -p \u0026#34;Enter Leader ClusterID (e.g., tkg-cluster-leader): \u0026#34; LEADER_CLUSTER_ID read -p \u0026#34;Enter Member Token to use: \u0026#34; MEMBER_TOKEN read -p \u0026#34;Enter Leader cluster API endpoint (e.g., https://10.101.114.100:6443): \u0026#34; LEADER_ENDPOINT # Create YAML content for ClusterClaim cat \u0026lt;\u0026lt;EOF \u0026gt; \u0026#34;${MEMBER_CLUSTER_NAME}-clusterclaim.yaml\u0026#34; apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: id.k8s.io namespace: kube-system value: $MEMBER_CLUSTER_NAME --- apiVersion: multicluster.crd.antrea.io/v1alpha2 kind: ClusterClaim metadata: name: clusterset.k8s.io namespace: kube-system value: $CLUSTERSET_NAME --- apiVersion: multicluster.crd.antrea.io/v1alpha1 kind: ClusterSet metadata: name: $CLUSTERSET_NAME namespace: kube-system spec: leaders: - clusterID: $LEADER_CLUSTER_ID secret: \u0026#34;$MEMBER_TOKEN\u0026#34; server: \u0026#34;$LEADER_ENDPOINT\u0026#34; namespace: antrea-multicluster EOF # Apply the ClusterClaim YAML kubectl apply -f \u0026#34;${MEMBER_CLUSTER_NAME}-clusterclaim.yaml\u0026#34; echo \u0026#34;ClusterClaim created on member cluster.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done } # Function to create Multi-cluster Gateway create_multi_cluster_gateway() { echo \u0026#34;Creating Multi-cluster Gateway...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu for selecting a context to switch to PS3=\u0026#34;Select a context to switch to: \u0026#34; select GATEWAY_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$GATEWAY_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$GATEWAY_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$GATEWAY_CONTEXT\u0026#34; while true; do # List nodes and create a menu nodes=($(kubectl get nodes -o custom-columns=NAME:.metadata.name --no-headers)) # Display the menu for selecting a node to annotate PS3=\u0026#34;Select a node to annotate as Multi-cluster Gateway: \u0026#34; select SELECTED_NODE in \u0026#34;${nodes[@]}\u0026#34; \u0026#34;Back to Context Menu\u0026#34;; do if [[ -n \u0026#34;$SELECTED_NODE\u0026#34; ]]; then if [ \u0026#34;$SELECTED_NODE\u0026#34; == \u0026#34;Back to Context Menu\u0026#34; ]; then break fi # Annotate the selected node kubectl annotate node \u0026#34;$SELECTED_NODE\u0026#34; multicluster.antrea.io/gateway=true echo \u0026#34;Annotated $SELECTED_NODE as Multi-cluster Gateway.\u0026#34; sleep 2 else echo \u0026#34;Invalid selection. Please choose a node or \u0026#39;Back to Context Menu\u0026#39;.\u0026#34; fi done read -p \u0026#34;Do you want to annotate another node? (yes/no): \u0026#34; ANNOTATE_ANOTHER if [ \u0026#34;$ANNOTATE_ANOTHER\u0026#34; != \u0026#34;yes\u0026#34; ]; then break fi done break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done } # Function to create a Multi-cluster service create_multi_cluster_service() { echo \u0026#34;Creating a Multi-cluster service...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu for selecting a context to switch to PS3=\u0026#34;Select a context to switch to: \u0026#34; select SELECT_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$SELECT_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$SELECT_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$SELECT_CONTEXT\u0026#34; # List namespaces and create a menu namespaces=($(kubectl get namespaces -o custom-columns=NAME:.metadata.name --no-headers)) # Display the menu for selecting a namespace PS3=\u0026#34;Select a namespace to list services from: \u0026#34; select SELECTED_NAMESPACE in \u0026#34;${namespaces[@]}\u0026#34; \u0026#34;Back to Context Menu\u0026#34;; do if [[ -n \u0026#34;$SELECTED_NAMESPACE\u0026#34; ]]; then if [ \u0026#34;$SELECTED_NAMESPACE\u0026#34; == \u0026#34;Back to Context Menu\u0026#34; ]; then break fi # List services in the selected namespace and create a menu services=($(kubectl get services -n \u0026#34;$SELECTED_NAMESPACE\u0026#34; -o custom-columns=NAME:.metadata.name --no-headers)) # Display the menu for selecting a service PS3=\u0026#34;Select a service to export as Multi-cluster service: \u0026#34; select SELECTED_SERVICE in \u0026#34;${services[@]}\u0026#34; \u0026#34;Back to Namespace Menu\u0026#34;; do if [[ -n \u0026#34;$SELECTED_SERVICE\u0026#34; ]]; then if [ \u0026#34;$SELECTED_SERVICE\u0026#34; == \u0026#34;Back to Namespace Menu\u0026#34; ]; then break fi # Create YAML content for ServiceExport cat \u0026lt;\u0026lt;EOF \u0026gt; \u0026#34;${SELECTED_SERVICE}-multi-cluster-service.yaml\u0026#34; apiVersion: multicluster.x-k8s.io/v1alpha1 kind: ServiceExport metadata: name: $SELECTED_SERVICE namespace: $SELECTED_NAMESPACE EOF echo \u0026#34;ServiceExport created for $SELECTED_SERVICE in namespace $SELECTED_NAMESPACE.\u0026#34; # Apply the Multi-cluster service kubectl apply -f \u0026#34;${SELECTED_SERVICE}-multi-cluster-service.yaml\u0026#34; echo \u0026#34;Multi-cluster service applied.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a service or \u0026#39;Back to Namespace Menu\u0026#39;.\u0026#34; fi done else echo \u0026#34;Invalid selection. Please choose a namespace or \u0026#39;Back to Context Menu\u0026#39;.\u0026#34; fi done break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done } # Main menu while true; do clear echo \u0026#34;Main Menu:\u0026#34; echo \u0026#34;1. Select Antrea version\u0026#34; echo \u0026#34;2. Install Antrea Multi-cluster on leader cluster\u0026#34; echo \u0026#34;3. Install Antrea Multi-cluster on member cluster\u0026#34; echo \u0026#34;4. Create member-cluster secrets\u0026#34; echo \u0026#34;5. Apply member tokens\u0026#34; echo \u0026#34;6. Create ClusterSet on the leader cluster\u0026#34; echo \u0026#34;7. Create ClusterClaim on member cluster\u0026#34; echo \u0026#34;8. Create Multi-cluster Gateway\u0026#34; echo \u0026#34;9. Create a Multi-cluster service\u0026#34; echo \u0026#34;10. Exit\u0026#34; read -p \u0026#34;Enter your choice: \u0026#34; choice case $choice in 1) read -p \u0026#34;Enter Antrea version (e.g., v1.11.1): \u0026#34; TAG ;; 2) echo \u0026#34;Installing Antrea Multi-cluster on leader cluster...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu PS3=\u0026#34;Select a context to switch to: \u0026#34; select SWITCH_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$SWITCH_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$SWITCH_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$SWITCH_CONTEXT\u0026#34; # Create namespace if it does not exist kubectl create namespace antrea-multicluster --dry-run=client -o yaml | kubectl apply -f - # Apply leader cluster YAMLs kubectl apply -f \u0026#34;https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-leader-global.yml\u0026#34; kubectl apply -f \u0026#34;https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-leader-namespaced.yml\u0026#34; echo \u0026#34;Antrea Multi-cluster installed on leader cluster.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done ;; 3) echo \u0026#34;Installing Antrea Multi-cluster on member cluster...\u0026#34; # List available contexts and create a menu contexts=($(kubectl config get-contexts -o=name)) # Display the menu PS3=\u0026#34;Select a context to switch to: \u0026#34; select SWITCH_CONTEXT in \u0026#34;${contexts[@]}\u0026#34; \u0026#34;Back to Main Menu\u0026#34;; do if [[ -n \u0026#34;$SWITCH_CONTEXT\u0026#34; ]]; then if [ \u0026#34;$SWITCH_CONTEXT\u0026#34; == \u0026#34;Back to Main Menu\u0026#34; ]; then break fi kubectl config use-context \u0026#34;$SWITCH_CONTEXT\u0026#34; # Apply member cluster YAML kubectl apply -f \u0026#34;https://github.com/antrea-io/antrea/releases/download/$TAG/antrea-multicluster-member.yml\u0026#34; echo \u0026#34;Antrea Multi-cluster installed on member cluster.\u0026#34; sleep 2 break else echo \u0026#34;Invalid selection. Please choose a context or \u0026#39;Back to Main Menu\u0026#39;.\u0026#34; fi done ;; 4) create_member_cluster_secrets ;; 5) apply_member_tokens ;; 6) create_clusterset_on_leader ;; 7) create_clusterclaim_on_member ;; 8) create_multi_cluster_gateway ;; 9) create_multi_cluster_service ;; 10) echo \u0026#34;Exiting...\u0026#34; exit 0 ;; *) echo \u0026#34;Invalid choice. Please choose a valid option.\u0026#34; sleep 2 ;; esac done Thats it for this post. Thanks for reading.\n","date":"25 September 2023","externalUrl":null,"permalink":"/2023/09/25/antrea-multi-cluster-in-tkg-and-vsphere-with-tanzu/","section":"Posts","summary":"In this post I will go through how to configure and use the Antrea feature gate Multi-cluster in both TKGs and TKG","title":"Antrea Multi-cluster - in TKG and vSphere with Tanzu","type":"posts"},{"content":"","date":"25 September 2023","externalUrl":null,"permalink":"/tags/multi-cluster/","section":"Tags","summary":"","title":"Multi-Cluster","type":"tags"},{"content":"","date":"23 September 2023","externalUrl":null,"permalink":"/tags/network/","section":"Tags","summary":"","title":"Network","type":"tags"},{"content":"","date":"23 September 2023","externalUrl":null,"permalink":"/categories/nsx-alb/","section":"Categories","summary":"","title":"NSX-ALB","type":"categories"},{"content":" NSX and NSX Advanced Loadbalancer with TKG # Before vSphere 8 U2 it was not possible to use the NSX Advanced Loadbalancer loadbalancing the Kubernetes control plane nodes when configuring Tanzu or TKG in vSphere using NSX as the networking stack. I am talking about using NSX as an integrated part of the TKG installation (enabling Workload Management and selecting NSX as the network stack). It was of course possible to use NSX as a pure network \u0026ldquo;underlay\u0026rdquo; for TKG, and then use NSX ALB as both L4 and L7 (using AKO) provider. But then one misses out of the \u0026ldquo;magic\u0026rdquo; using NSX as the integrated network provider for TKG like automatic network creation, vrf/ip separation support, NSX policies, vSphere pods etc. One could expose services using servicetype loadBalancer (L4) from the workload clusters with NSX ALB in combination with NSX as the integrated networking stack, but one had to specify the loadBalancerClass to specify NSX-ALB for this, and not all services could be configured with this crd. The control plane nodes or the Kubernetes api endpoint was always exposed/managed by the built-in NSX-T loadbalancer. Now with the release of vSphere 8 U2 it seems like we finally can use NSX and NSX-ALB in combination where all loadbalancing needs are being managed by NSX-ALB, including the control plane nodes/kubernetes api endpoint.\nSo I am very excited to give this a try and see how it works out, the steps needed to be configured for this to work and how it look like during the installation and after. What happens in the NSX-ALB controller, what happens in the NSX environment.\nTo enable this feature in this release is only for greenfield deployments.\nWhat we should end up with in this release is using NSX for all networking and security features, NSX Advanced Loadbalancer for all loadbalancing needs.\nFrom the official release notes I have pasted below two features I will discuss in this post. The first one is the major topic of this post of course, but the second one is also a very nice feature that I actually have used a lot in this post. For the full list of new features head over to the official release notes for vSphere 8 U2 here and for all the Tanzu related features here\nThe major topic of this post:\nSupport of NSX Advanced Load Balancer for a Supervisor configured with NSX networking - You can now enable a Supervisor with NSX Advanced Load Balancer (Avi Networks) for L4 load balancing, as well as load balancing for the control plane nodes of Supervisor and Tanzu Kubernetes Grid clusters with NSX networking. Checkout the documentation page for guidance on configuring the NSX Advanced Load Balancer with NSX. And the nice feature:\nImport and export the Supervisor configuration - In previous versions, activating the Supervisor was a manual step-wise process without the ability to save any configurations. In the current release, you can now export and share the Supervisor configuration with peers in a human-readable format or within a source control system, import configurations to a new Supervisor, and replicate a standard configuration across multiple Supervisors. Checkout the documentation for details on how to export and import the Supervisor configuration. Pre-requisites and assumptions # Before getting started with the installation/enabling TKG in vSphere 8 U2 using NSX-ALB in combination with NSX some requirments need to be met.\nvSphere 8 U2 - kind of obvious but nice to mention NSX version 4.1.1 or higher NSX-ALB version 22.1.4 or higher (yes it is stated in the release notes of vSphere with Tanzu that version 22.1.3 is the supported release) NSX-ALB Enterprise license And the usual assumptions (mostly to save some time, as I have covered many of these topics several times before, saving digital ink is saving the environment \u0026#x1f604; I already have my vSphere 8 U2 environment running, I already have my NSX 4.1.1 environment configured and lastly I already have my NSX-ALB controller 22.1.4 deployed and working. My lab is used for many things so I am not deploying NSX and NSX-ALB from scratch just for this post, they are already running a bunch of other stuff including the NSX-LAB controller. In the next chapters I will go through what I had to prepare on the NSX side (if any) and the NSX-ALB side (there are a couple of steps there).\nvSphere preparations - requirements # I only needed to upgrade my current vSphere 8 U1 environment to vSphere 8 U2, as always starting with the vCenter server using the VAMI interface, then updated the ESXi image in LCM to do a rolling upgrade of the ESXi hosts to 8 U2.\nNSX-T preparations # For information how to install NSX, see my post here\nNSX-ALB preparations - requirements # Instead of going through all the steps in configuring the NSX-ALB I will only post the settings that is specific/or needed for the NSX+NSX-ALB feature to work. As I already have a working NSX-ALB environment running and in use for other needs, its not that many changes I had to do. So I will show them here in their own section starting with the cloud. For reference how to install NSX-ALB I have done a post on that here.\nNSX cloud # In my NSX-ALB I already have two clouds configured, both of them are NSX clouds. If you start from scratch all the necessary config needs to be done of course. See my posts here. In the NSX cloud I will be using for this I need to enable DHCP. That is done by editing your specific cloud and checking the box DHCP:\nIf you happen to have multiple clouds configured it will figure out wich cloud it will use. The NSX manager knows which NSX cloud it must use, most likely because it will use the cloud that is the same NSX the API comes from. In my lab I have two NSX clouds configured, and it will select the correct cloud. My two NSX clouds are two unique NSX instances, if having two NSX clouds of same NSX instance I am not sure how it can select the right cloud.\nIPAM profile # You need to make sure that you have configured an IPAM profile. Again, I already have that configured. The deployment will use this IPAM profile to configure the new usable VIP networks you define as the ingress cidr in TKG.\nIn your IPAM profile it is very important to not have the Allocate ip in VRF option selected. This must be de-selected.\nThen make sure your NSX cloud has this IPAM profile select:\nDefault Service-Engine Group # The Default-Service Engine Group in your NSX Cloud will be used as a \u0026ldquo;template\u0026rdquo; group. This means you should configure this default se-group how you want your SEs to be provisioned. From the official documentation:\nThe AKO creates one Service Engine Group for each vSphere with Tanzu cluster. The Service Engine Group configuration is derived from the Default-Group configuration. Once the Default-Group is configured with the required values, any new Service Engine Group created by the AKO will have the same settings. However, changes made to the Default-Group configuration will not reflect in an already created Service Engine Group. You must modify the configuration for an existing Service Engine Group separately.\nSo in my Default SE-Group I have this configuration:\nUnder Scope I have confgured the vSphere cluster, and shared datastore placement.\nWhen I am satisified the the settings that suits my need, save and I am done with the Default group configurations.\nCustom Certificate # Another requirement is to change the default certificate to a custom one. One can follow the official documentation here. For now it is sufficient to just prepare the certificate, dont change the settings in the UI to use the new certificate yet, that is done after the step where you register the NSX-ALB endpoint to the NSX manager.\nSummary of NSX-ALB pre-requisites # Need to have configured a NSX Cloud with the DHCP option enabled Created a custom certificate (not configured to be used yet) Created and configured an IPAM profile, updated the NSX cloud to use this profile, in the IPAM profile Allocate IP in VRF i de-selected. NSX preparations - requirements - adding the NSX-ALB controller endpoint using API # I already have a working NSX 4.1.1 environment. The only thing I need to to here is to add the NSX-ALB controller (alb endpoint) so my NSX environment is aware of my NSX-ALB controller. The reason for this is that some configurations in NSX-ALB will be automatically done by NCP during TKG deployment using api. So the NSX manager will need to know the username and password of the NSX-ALB controller or controller-cluster.\nAdding the NSX-ALB to the NSX manager # To add NSX-ALB to the NSX manager I will need to do an API call to the NSX manager using curl (or Postman or whatever preferred tool), the below is the call I will be issuing:\ncurl -k --location --request PUT \u0026#39;https://172.24.3.10/policy/api/v1/infra/alb-onboarding-workflow\u0026#39; \\ #IP NSX manager cluster IP --header \u0026#39;X-Allow-Overwrite: True\u0026#39; \\ -u admin:password \\ #password and username to the NSX manager in the format username:password --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;owned_by\u0026#34;: \u0026#34;LCM\u0026#34;, \u0026#34;cluster_ip\u0026#34;: \u0026#34;172.24.3.50\u0026#34;, # IP NSX-ALB controller or controller cluster IP \u0026#34;infra_admin_username\u0026#34; : \u0026#34;admin\u0026#34;, #username \u0026#34;infra_admin_password\u0026#34; : \u0026#34;password\u0026#34;, #password \u0026#34;dns_servers\u0026#34;: [\u0026#34;172.24.3.1\u0026#34;], #not sure why I need to add this - my ALB is already configured with this \u0026#34;ntp_servers\u0026#34;: [\u0026#34;172.24.3.1\u0026#34;] #not sure why I need to add this - my ALB is already configured with this }\u0026#39; The offical documentation is using this example:\ncurl -k --location --request PUT \u0026#39;https://\u0026lt;nsx-mgr-ip\u0026gt;/policy/api/v1/infra/alb-onboarding-workflow\u0026#39; \\ --header \u0026#39;X-Allow-Overwrite: True\u0026#39; \\ --header \u0026#39;Authorization: Basic \u0026lt;base64 encoding of username:password of NSX Mgr\u0026gt;\u0026#39; \\ --header \u0026#39;Content-Type: application/json\u0026#39; \\ --data-raw \u0026#39;{ \u0026#34;owned_by\u0026#34;: \u0026#34;LCM\u0026#34;, \u0026#34;cluster_ip\u0026#34;: \u0026#34;\u0026lt;nsx-alb-controller-cluster-ip\u0026gt;\u0026#34;, \u0026#34;infra_admin_username\u0026#34; : \u0026#34;username\u0026#34;, \u0026#34;infra_admin_password\u0026#34; : \u0026#34;password\u0026#34;, \u0026#34;dns_servers\u0026#34;: [\u0026#34;\u0026lt;dns-servers-ips\u0026gt;\u0026#34;], \u0026#34;ntp_servers\u0026#34;: [\u0026#34;\u0026lt;ntp-servers-ips\u0026gt;\u0026#34;] }\u0026#39; I had issues authenticating using header, and I did not want to use time to troubleshoot why. I think one of the reasons is that I need to genereate some auth token etc\u0026hellip;\nAnyway, after a successful api call. You should get this output:\n{ \u0026#34;connection_info\u0026#34; : { \u0026#34;username\u0026#34; : \u0026#34;\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34;, \u0026#34;tenant\u0026#34; : \u0026#34;admin\u0026#34;, \u0026#34;expires_at\u0026#34; : \u0026#34;2023-09-22T18:22:22.627Z\u0026#34;, \u0026#34;managed_by\u0026#34; : \u0026#34;LCM\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;DEACTIVATE_PROVIDER\u0026#34;, \u0026#34;certificate\u0026#34; : \u0026#34;-----BEGIN CERTIFICATE-----\\nMIIDRTCCAi2gAwIBAgIUCcOSYBrBybt6zHfJKojPnX/fZ8xD8azd9mWp4oksVA1vHXzbWdsY\\nw/Tdr3zTOEgEjn9mflE/aBhsahEhhaZfKtZtLO/OnvSZZtaMlHvlsHgfl8nOqhLh\\nGBJzNNwIS8sjzi8E1/y3TI3kVshoCclL9A==\\n-----END CERTIFICATE-----\\n\u0026#34;, \u0026#34;enforcement_point_address\u0026#34; : \u0026#34;172.24.3.50\u0026#34;, \u0026#34;resource_type\u0026#34; : \u0026#34;AviConnectionInfo\u0026#34; }, \u0026#34;auto_enforce\u0026#34; : true, \u0026#34;resource_type\u0026#34; : \u0026#34;EnforcementPoint\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;alb-endpoint\u0026#34;, \u0026#34;display_name\u0026#34; : \u0026#34;alb-endpoint\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/infra/sites/default/enforcement-points/alb-endpoint\u0026#34;, \u0026#34;relative_path\u0026#34; : \u0026#34;alb-endpoint\u0026#34;, \u0026#34;parent_path\u0026#34; : \u0026#34;/infra/sites/default\u0026#34;, \u0026#34;remote_path\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;unique_id\u0026#34; : \u0026#34;8ef2126e-9311-40ff-bd6d-08c51017326c\u0026#34;, \u0026#34;realization_id\u0026#34; : \u0026#34;8ef2126e-9311-40ff-bd6d-08c51017326c\u0026#34;, \u0026#34;owner_id\u0026#34; : \u0026#34;4b04712e-498d-42d0-ad90-7ab06c398c60\u0026#34;, \u0026#34;marked_for_delete\u0026#34; : false, \u0026#34;overridden\u0026#34; : false, \u0026#34;_create_time\u0026#34; : 1695385329242, \u0026#34;_create_user\u0026#34; : \u0026#34;admin\u0026#34;, \u0026#34;_last_modified_time\u0026#34; : 1695385329242, \u0026#34;_last_modified_user\u0026#34; : \u0026#34;admin\u0026#34;, \u0026#34;_system_owned\u0026#34; : false, \u0026#34;_protection\u0026#34; : \u0026#34;NOT_PROTECTED\u0026#34;, \u0026#34;_revision\u0026#34; : 0 You can also do the following API call to verify if the NSX-ALB controller has been added:\nandreasm@ubuntu02:~$ curl -s -k -u admin:password https://172.24.3.10/policy/api/v1/infra/sites/default/enforcement-points/alb-endpoint { \u0026#34;connection_info\u0026#34; : { \u0026#34;username\u0026#34; : \u0026#34;\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\u0026#34;, \u0026#34;tenant\u0026#34; : \u0026#34;admin\u0026#34;, \u0026#34;expires_at\u0026#34; : \u0026#34;2023-09-24T02:00:05.362Z\u0026#34;, \u0026#34;managed_by\u0026#34; : \u0026#34;LCM\u0026#34;, \u0026#34;status\u0026#34; : \u0026#34;DEACTIVATE_PROVIDER\u0026#34;, \u0026#34;certificate\u0026#34; : \u0026#34;-----BEGIN CERTIFICATE-----\\nMIIGy2k9O+hp6fX+iG5BGDurG8hP8A\\nKI96AUNxV39pXOBIqBr/sL3v/24DVz85ObAvIzoWnTak9ZhZyP4jUfZD/w21xdXz\\nKaTJ5ioC+M6RLRKVVJ159lrm3A==\\n-----END CERTIFICATE-----\\n\u0026#34;, \u0026#34;enforcement_point_address\u0026#34; : \u0026#34;172.24.3.50\u0026#34;, \u0026#34;resource_type\u0026#34; : \u0026#34;AviConnectionInfo\u0026#34; }, \u0026#34;auto_enforce\u0026#34; : true, \u0026#34;resource_type\u0026#34; : \u0026#34;EnforcementPoint\u0026#34;, \u0026#34;id\u0026#34; : \u0026#34;alb-endpoint\u0026#34;, \u0026#34;display_name\u0026#34; : \u0026#34;alb-endpoint\u0026#34;, \u0026#34;path\u0026#34; : \u0026#34;/infra/sites/default/enforcement-points/alb-endpoint\u0026#34;, \u0026#34;relative_path\u0026#34; : \u0026#34;alb-endpoint\u0026#34;, \u0026#34;parent_path\u0026#34; : \u0026#34;/infra/sites/default\u0026#34;, \u0026#34;remote_path\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;unique_id\u0026#34; : \u0026#34;8ef2126e-9311-40ff-bd6d-08c51017326c\u0026#34;, \u0026#34;realization_id\u0026#34; : \u0026#34;8ef2126e-9311-40ff-bd6d-08c51017326c\u0026#34;, \u0026#34;owner_id\u0026#34; : \u0026#34;4b04712e-498d-42d0-ad90-7ab06c398c60\u0026#34;, \u0026#34;marked_for_delete\u0026#34; : false, \u0026#34;overridden\u0026#34; : false, \u0026#34;_create_time\u0026#34; : 1695385329242, \u0026#34;_create_user\u0026#34; : \u0026#34;admin\u0026#34;, \u0026#34;_last_modified_time\u0026#34; : 1695499205391, \u0026#34;_last_modified_user\u0026#34; : \u0026#34;system\u0026#34;, \u0026#34;_system_owned\u0026#34; : false, \u0026#34;_protection\u0026#34; : \u0026#34;NOT_PROTECTED\u0026#34;, \u0026#34;_revision\u0026#34; : 7 The \u0026ldquo;certificate\u0026rdquo; you get back should be the current certificate on the NSX-ALB controller. More on that later.\nThats it on the NSX manager side.\nAfter you a have done the above operation and you log back into the NSX-ALB controller this welcome wizard pops up. Just click cancel on it.\nI figured out a way to disable this initial setup-wizard instead fo going through it via the UI. SSH into the NSX-ALB controller, enter into shell and enter the following commands:\n### Show the current systemconfiguration [admin:172-24-3-50]: \u0026gt; show systemconfiguration +----------------------------------+------------------------------------+ | Field | Value | +----------------------------------+------------------------------------+ | uuid | default | | dns_configuration | | | server_list[1] | 172.24.3.1 | | ntp_configuration | | | ntp_servers[1] | | | server | 172.24.3.1 | | portal_configuration | | | enable_https | True | | redirect_to_https | True | | enable_http | True | | sslkeyandcertificate_refs[1] | tkgm-cert-controller | | use_uuid_from_input | False | | sslprofile_ref | System-Standard-Portal | | enable_clickjacking_protection | True | | allow_basic_authentication | True | | password_strength_check | False | | disable_remote_cli_shell | False | | disable_swagger | False | | api_force_timeout | 24 hours | | minimum_password_length | 8 | | global_tenant_config | | | tenant_vrf | False | | se_in_provider_context | True | | tenant_access_to_provider_se | True | | email_configuration | | | smtp_type | SMTP_LOCAL_HOST | | from_email | admin@avicontroller.net | | mail_server_name | localhost | | mail_server_port | 25 | | disable_tls | False | | docker_mode | False | | ssh_ciphers[1] | aes128-ctr | | ssh_ciphers[2] | aes256-ctr | | ssh_hmacs[1] | hmac-sha2-512-etm@openssh.com | | ssh_hmacs[2] | hmac-sha2-256-etm@openssh.com | | ssh_hmacs[3] | hmac-sha2-512 | | default_license_tier | ENTERPRISE | | secure_channel_configuration | | | sslkeyandcertificate_refs[1] | System-Default-Secure-Channel-Cert | | welcome_workflow_complete | False | | fips_mode | False | | enable_cors | False | | common_criteria_mode | False | | host_key_algorithm_exclude | | | kex_algorithm_exclude | | +----------------------------------+------------------------------------+ Notice the welcome_workflow_complete is set to False. Set this to true using the following command:\n[admin:172-24-3-50]: \u0026gt; configure systemconfiguration welcome_workflow_complete 1 Or\n[admin:172-24-3-50]: \u0026gt; configure systemconfiguration [admin:172-24-3-50-configure]: \u0026gt; welcome_workflow_complete (hit henter) If you show the systemconfiguration again now it should be changed to True\u0026hellip; Logout and back in to the NSX-ALB gui and you should not be asked to do the initial setup wizard.\nCustom certificate in the NSX-ALB controller # After adding the NSX-ALB endpoint, configure NSX-ALB to use the newly created custom certificate created earlier. Head over to Administration -\u0026gt; System Settings:\nClick edit and adjust the settings accordingly, and make sure to select the custom certificate. Check the option Allow Basic Authentication and update the field SSL/TLS Certificate with the custom certificate. Click save.\nNow the pre-requisities has been done, and its time to get started with the WCP installation.\nMy NSX and NSX-ALB environment before WCP installation # Before I head over and do the actual WCP installation, I will first just take a couple of screenshots from my NSX environment as a before and after.\nNSX environment before WCP installation # I have three Tier-0s configured, whereas I will only use the stc-tier-0 (the one with most objects connected to it) in this post.\nNSX-ALB environment before WCP installation # One Virtual Service (my DNS service).\nIPAM and DNS profile (IPAM only needed for WCP)\nContent of my current IPAM profile\nMy clouds configured\nCurrent networks configured in my stc-nsx-cloud\nCurrent running Service Engines\nMy Service Engine Groups\nMy configured Network Profiles\nThen my VRF contexts, only configured for the other services I have been running.\nNow that we have all the before screenshots, lets do the vSphere with Tanzu installation and see how it goes.\nvSphere with Tanzu installation # There is nothing different in the UI when it comes to this specific feature, I select NSX as the networkl and populate the fields as normal. But I will list the steps here any way.\nI will use the new Import and Export feature as I already have done this installation several times. The first time I filled everything in manually then at the end of the wizard I clicked Export configuration and saved my config. So now I just have to import the config and go through all the fiels using next, and change something if needed, then finish.\nHere is how to choose the export, select it and a download dialog will pop up. To use it later for import extract the content and select the file called wcp-config.json\nGet Started\nImport Config\nImport, and now all fields are populated.\nThe wcp-config.json file content:\n{\u0026#34;specVersion\u0026#34;:\u0026#34;1.0\u0026#34;,\u0026#34;supervisorSpec\u0026#34;:{\u0026#34;supervisorName\u0026#34;:\u0026#34;stc-svc\u0026#34;},\u0026#34;envSpec\u0026#34;:{\u0026#34;vcenterDetails\u0026#34;:{\u0026#34;vcenterAddress\u0026#34;:\u0026#34;vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34;,\u0026#34;vcenterCluster\u0026#34;:\u0026#34;Cluster\u0026#34;}},\u0026#34;tkgsComponentSpec\u0026#34;:{\u0026#34;tkgsStoragePolicySpec\u0026#34;:{\u0026#34;masterStoragePolicy\u0026#34;:\u0026#34;vSAN Default Storage Policy\u0026#34;,\u0026#34;imageStoragePolicy\u0026#34;:\u0026#34;vSAN Default Storage Policy\u0026#34;,\u0026#34;ephemeralStoragePolicy\u0026#34;:\u0026#34;vSAN Default Storage Policy\u0026#34;},\u0026#34;tkgsMgmtNetworkSpec\u0026#34;:{\u0026#34;tkgsMgmtNetworkName\u0026#34;:\u0026#34;ls-mgmt\u0026#34;,\u0026#34;tkgsMgmtIpAssignmentMode\u0026#34;:\u0026#34;STATICRANGE\u0026#34;,\u0026#34;tkgsMgmtNetworkStartingIp\u0026#34;:\u0026#34;10.13.10.20\u0026#34;,\u0026#34;tkgsMgmtNetworkGatewayCidr\u0026#34;:\u0026#34;10.13.10.1/24\u0026#34;,\u0026#34;tkgsMgmtNetworkDnsServers\u0026#34;:[\u0026#34;172.24.3.1\u0026#34;],\u0026#34;tkgsMgmtNetworkSearchDomains\u0026#34;:[\u0026#34;cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34;],\u0026#34;tkgsMgmtNetworkNtpServers\u0026#34;:[\u0026#34;172.24.3.1\u0026#34;]},\u0026#34;tkgsNcpClusterNetworkInfo\u0026#34;:{\u0026#34;tkgsClusterDistributedSwitch\u0026#34;:\u0026#34;VDSwitch\u0026#34;,\u0026#34;tkgsNsxEdgeCluster\u0026#34;:\u0026#34;SomeEdgeCluster\u0026#34;,\u0026#34;tkgsNsxTier0Gateway\u0026#34;:\u0026#34;stc-tier-0\u0026#34;,\u0026#34;tkgsNamespaceSubnetPrefix\u0026#34;:28,\u0026#34;tkgsRoutedMode\u0026#34;:true,\u0026#34;tkgsNamespaceNetworkCidrs\u0026#34;:[\u0026#34;10.13.80.0/23\u0026#34;],\u0026#34;tkgsIngressCidrs\u0026#34;:[\u0026#34;10.13.90.0/24\u0026#34;],\u0026#34;tkgsEgressCidrs\u0026#34;:[],\u0026#34;tkgsWorkloadDnsServers\u0026#34;:[\u0026#34;172.24.3.1\u0026#34;],\u0026#34;tkgsWorkloadServiceCidr\u0026#34;:\u0026#34;10.96.0.0/23\u0026#34;},\u0026#34;apiServerDnsNames\u0026#34;:[],\u0026#34;controlPlaneSize\u0026#34;:\u0026#34;SMALL\u0026#34;}} I am deselecting NAT as I dont need that.\nNow, its just clicking finish and monitor the progress.\nOne will soon see two new Virtual Services being created, two new service engines (according to my default-service-engine group config) being deployed.\nWhen everything is ready and available the virtual services will become yellow/green (fully green after a while as it depends on how long the VS is down before it becomes available/up):\nAnd in vCenter the Workload Management progress:\nCan I reach it?\nYes!\nThis is really really nice.\nNow next chapters will go through what has been done in my NSX environment and the NSX-ALB environment.\nMy NSX and NSX-ALB environment after WCP installation # Now that everything is green and joy. Let us have a check inside and see what has been configured.\nNSX environment after WCP installation # Topology view:\nA new Tier-1 I can see, with two new segments, with some VMs in each.\nBelow I can see the new Tier-1\nIn the new Tier-1 router it has also automatically added the static routes for the VIP to the SEs:\nThese static routes needs to be either advertised by the Tier-0 using either BGP or OSPF, or manually created static routes in the network infrastructure. I am using BGP.\nThe two new segments:\nAvi-domain-c8:507\u0026hellip; This segment is where my NSX-ALB Service Engines Dataplane network is located.\nand the second segment vm-domain-c8:507.. is where the default workload network for my Supervisor Control Plane nodes is placed.\nNow, what is under Loadbalancing:\nIt has created a distributed loadbalancer where all virtual servers are kubernetes services using the services CIDR:\nThe NSX Distributed Loadbalancer is used for the ClusterIP services running inside the Supervisor, the ones that are running on the ESXi hosts. The LoadBalancer services is handled by NSX-ALB.\nandreasm@ubuntu02:~/avi_nsxt_wcp$ k get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 15h kube-system docker-registry ClusterIP 10.96.0.37 \u0026lt;none\u0026gt; 5000/TCP 15h kube-system kube-apiserver-authproxy-svc ClusterIP 10.96.0.243 \u0026lt;none\u0026gt; 8443/TCP 14h kube-system kube-apiserver-lb-svc LoadBalancer 10.96.0.201 10.13.90.1 443:32163/TCP,6443:31957/TCP 15h kube-system kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 15h kube-system snapshot-validation-service ClusterIP 10.96.0.128 \u0026lt;none\u0026gt; 443/TCP 15h ns-stc-1 cluster-1-76996f5b17a254b02e55a LoadBalancer 10.96.0.239 10.13.92.2 80:30165/TCP 12h ns-stc-1 cluster-1-control-plane-service LoadBalancer 10.96.1.157 10.13.92.1 6443:30363/TCP 13h vmware-system-appplatform-operator-system packaging-api ClusterIP 10.96.0.87 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-appplatform-operator-system vmware-system-appplatform-operator-controller-manager-service ClusterIP None \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 15h vmware-system-appplatform-operator-system vmware-system-psp-operator-k8s-cloud-operator-service ClusterIP 10.96.1.101 \u0026lt;none\u0026gt; 29002/TCP 15h vmware-system-appplatform-operator-system vmware-system-psp-operator-service ClusterIP None \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 15h vmware-system-appplatform-operator-system vmware-system-psp-operator-webhook-service ClusterIP 10.96.1.84 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-capw capi-controller-manager-metrics-service ClusterIP 10.96.1.47 \u0026lt;none\u0026gt; 9844/TCP 15h vmware-system-capw capi-kubeadm-bootstrap-controller-manager-metrics-service ClusterIP 10.96.1.67 \u0026lt;none\u0026gt; 9845/TCP 15h vmware-system-capw capi-kubeadm-bootstrap-webhook-service ClusterIP 10.96.0.170 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-capw capi-kubeadm-control-plane-controller-manager-metrics-service ClusterIP 10.96.1.48 \u0026lt;none\u0026gt; 9848/TCP 15h vmware-system-capw capi-kubeadm-control-plane-webhook-service ClusterIP 10.96.0.102 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-capw capi-webhook-service ClusterIP 10.96.1.248 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-capw capv-webhook-service ClusterIP 10.96.1.124 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-capw capw-controller-manager-metrics-service ClusterIP 10.96.1.252 \u0026lt;none\u0026gt; 9846/TCP 15h vmware-system-capw capw-webhook-service ClusterIP 10.96.0.101 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-cert-manager cert-manager ClusterIP 10.96.0.158 \u0026lt;none\u0026gt; 9402/TCP 15h vmware-system-cert-manager cert-manager-webhook ClusterIP 10.96.0.134 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-csi vmware-system-csi-webhook-service ClusterIP 10.96.1.130 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-csi vsphere-csi-controller LoadBalancer 10.96.0.108 10.13.90.2 2112:30383/TCP,2113:31530/TCP 15h vmware-system-imageregistry vmware-system-imageregistry-controller-manager-metrics-service ClusterIP 10.96.0.47 \u0026lt;none\u0026gt; 9857/TCP 14h vmware-system-imageregistry vmware-system-imageregistry-webhook-service ClusterIP 10.96.1.108 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-license-operator vmware-system-license-operator-webhook-service ClusterIP 10.96.1.51 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-netop vmware-system-netop-controller-manager-metrics-service ClusterIP 10.96.0.234 \u0026lt;none\u0026gt; 9851/TCP 15h vmware-system-nsop vmware-system-nsop-webhook-service ClusterIP 10.96.1.73 \u0026lt;none\u0026gt; 443/TCP 15h vmware-system-nsx nsx-operator ClusterIP 10.96.0.150 \u0026lt;none\u0026gt; 8093/TCP 15h vmware-system-pinniped pinniped-concierge-api ClusterIP 10.96.1.204 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-pinniped pinniped-supervisor ClusterIP 10.96.1.139 \u0026lt;none\u0026gt; 12001/TCP 14h vmware-system-pinniped pinniped-supervisor-api ClusterIP 10.96.1.94 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-tkg tanzu-addons-manager-webhook-service ClusterIP 10.96.1.18 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-tkg tanzu-featuregates-webhook-service ClusterIP 10.96.0.160 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-tkg tkgs-plugin-service ClusterIP 10.96.1.31 \u0026lt;none\u0026gt; 8099/TCP 14h vmware-system-tkg tkr-conversion-webhook-service ClusterIP 10.96.0.249 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-tkg tkr-resolver-cluster-webhook-service ClusterIP 10.96.1.146 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-tkg vmware-system-tkg-controller-manager-metrics-service ClusterIP 10.96.1.147 \u0026lt;none\u0026gt; 9847/TCP 14h vmware-system-tkg vmware-system-tkg-state-metrics-service ClusterIP 10.96.1.174 \u0026lt;none\u0026gt; 8443/TCP 14h vmware-system-tkg vmware-system-tkg-webhook-service ClusterIP 10.96.0.167 \u0026lt;none\u0026gt; 443/TCP 14h vmware-system-vmop vmware-system-vmop-controller-manager-metrics-service ClusterIP 10.96.0.71 \u0026lt;none\u0026gt; 9848/TCP 15h vmware-system-vmop vmware-system-vmop-web-console-validator ClusterIP 10.96.0.41 \u0026lt;none\u0026gt; 80/TCP 15h vmware-system-vmop vmware-system-vmop-webhook-service ClusterIP 10.96.0.182 \u0026lt;none\u0026gt; 443/TCP 15h andreasm@ubuntu02:~/avi_nsxt_wcp$ k get nodes NAME STATUS ROLES AGE VERSION 4234784dcf1b9d8d15d541fab8855b55 Ready control-plane,master 15h v1.26.4+vmware.wcp.0 4234e1920ede3cad62bcd3ce8bd2f2dc Ready control-plane,master 15h v1.26.4+vmware.wcp.0 4234f25d5bdc9796ce1e247a4190bb58 Ready control-plane,master 15h v1.26.4+vmware.wcp.0 esx01.cpod-nsxam-stc.az-stc.cloud-garage.net Ready agent 15h v1.26.4-sph-79b2bd9 esx02.cpod-nsxam-stc.az-stc.cloud-garage.net Ready agent 15h v1.26.4-sph-79b2bd9 esx03.cpod-nsxam-stc.az-stc.cloud-garage.net Ready agent 15h v1.26.4-sph-79b2bd9 esx04.cpod-nsxam-stc.az-stc.cloud-garage.net Ready agent 15h v1.26.4-sph-79b2bd9 And lastly it has also created a DHCP server for the Service Engines Dataplane interfaces.\nNSX-ALB environment after WCP installation # Two new Virtual Services, one for the the Supervisor Kubernetes API and the other for the CSI controller (monitoring?)\nThe content of my IPAM profile:\nIt has added a new usable network there.\nIt has added a new Data Network in my NSX cloud\nTwo new Service Engines\nDataplane network for these two new SEs\nOne new Service Engine Group\nTwo new Network Profiles\n(notice the ingress cidr network profile (VIP) is placed in the global vrf)\nOne new VRF context\nNow I can consume my newly provisioned Supervisor cluster through NSX-ALB - In the next chapters I have provisioned a workload cluster in a different network, different from the default workload network of the Supervisor. Then I will provision some L4 services (serviceType loadBalancer) and Ingress (L7) inside this cluster. Lets see what happens\nCreating vSphere Namespaces in other networks - override supervisor workload network # I went ahead and created a new vSphere Namespace with these settings:\nImmediately after, it also created some new objects in NSX like a new Tier-1, segments etc similar to what it does before vSphere 8 U2. Then in NSX-ALB new network profiles and VRF context, creating the network profile for the SEs in the new VRF t1-domain-xxxxxx-xxxxx-xxx-xxx-x-ns-stc-1.\nThe ingress cidr (VIP) network profile vcf-ako-net-domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-1 was created using the global vrf. If you remember I deselected the option in my IPAM profile Allocate ip in VRF, so it will use the global vrf here also (as in the initial deployment of the Supervisor cluster).\nNow I can deploy my workload cluster in the newly created vSphere namespace.\nandreasm@ubuntu02:~/avi_nsxt_wcp$ k apply -f cluster-1-default.yaml cluster.cluster.x-k8s.io/cluster-1 created In NSX-ALB shortly after a new VS is created\nThe already exisiting Service Engines has been reconfigured adding a second dataplane interface, using same subnet, but in two different VRF contexts. So it should work just fine.\nNow I just need to wait for the nodes to be provisioned.\nIn the meantime I checked the T1 created for this workload cluster and it has been so kind to create the static routes for me there also:\nVirtual service is now green, so the control plane vm is up and running.\nCluster ready, and I can test some services from it.\nandreasm@ubuntu02:~/avi_nsxt_wcp$ k get nodes NAME STATUS ROLES AGE VERSION cluster-1-f82lv-fdvw8 Ready control-plane 12m v1.26.5+vmware.2-fips.1 cluster-1-node-pool-01-tb4tw-555756bd56-76qv6 Ready \u0026lt;none\u0026gt; 8m39s v1.26.5+vmware.2-fips.1 cluster-1-node-pool-01-tb4tw-555756bd56-klgcs Ready \u0026lt;none\u0026gt; 8m39s v1.26.5+vmware.2-fips.1 L4 services inside the workload clusters # This test is fairly straightforward. I have deployed my test application Yelb. This creates a web-frontend and exposes it via serviceType loadBalancer. As soon as I deploy it NSX-ALB will create the virtual service using the same VIP range or Ingress range defined in the vSphere Namespace:\nandreasm@ubuntu02:~/examples$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-56d97cc8c-f42fr 1/1 Running 0 7m2s yelb-appserver-65855b7ffd-p74r8 1/1 Running 0 7m2s yelb-db-6f78dc6f8f-qbj2v 1/1 Running 0 7m2s yelb-ui-5c5b8d8887-bbwxl 1/1 Running 0 79s andreasm@ubuntu02:~/examples$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-server ClusterIP 20.10.173.143 \u0026lt;none\u0026gt; 6379/TCP 7m10s yelb-appserver ClusterIP 20.10.215.161 \u0026lt;none\u0026gt; 4567/TCP 7m9s yelb-db ClusterIP 20.10.208.242 \u0026lt;none\u0026gt; 5432/TCP 7m9s yelb-ui LoadBalancer 20.10.120.42 10.13.92.2 80:30917/TCP 87s This works fine, now my diagram can look like this:\nNext step is to deploy an Ingress\nL7 services inside the workload clusters # If I check my newly deployed workload cluster, there is no AKO pod running.\nandreasm@ubuntu02:~/avi_nsxt_wcp$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-4fzbm 2/2 Running 0 7m15s kube-system antrea-agent-v94js 2/2 Running 0 7m16s kube-system antrea-agent-w6fqr 2/2 Running 0 8m20s kube-system antrea-controller-797ffdc4df-2p4km 1/1 Running 0 8m20s kube-system coredns-794978f977-4pjvl 1/1 Running 0 7m8s kube-system coredns-794978f977-xz76z 1/1 Running 0 10m kube-system docker-registry-cluster-1-f82lv-fdvw8 1/1 Running 0 11m kube-system docker-registry-cluster-1-node-pool-01-tb4tw-555756bd56-76qv6 1/1 Running 0 5m59s kube-system docker-registry-cluster-1-node-pool-01-tb4tw-555756bd56-klgcs 1/1 Running 0 5m58s kube-system etcd-cluster-1-f82lv-fdvw8 1/1 Running 0 11m kube-system kube-apiserver-cluster-1-f82lv-fdvw8 1/1 Running 0 11m kube-system kube-controller-manager-cluster-1-f82lv-fdvw8 1/1 Running 0 11m kube-system kube-proxy-2mch7 1/1 Running 0 7m16s kube-system kube-proxy-dd2hm 1/1 Running 0 7m15s kube-system kube-proxy-pmc2c 1/1 Running 0 11m kube-system kube-scheduler-cluster-1-f82lv-fdvw8 1/1 Running 0 11m kube-system metrics-server-6cdbbbf775-kqbfm 1/1 Running 0 8m15s kube-system snapshot-controller-59d996bd4c-m7dqv 1/1 Running 0 8m23s secretgen-controller secretgen-controller-b68787489-js8dj 1/1 Running 0 8m5s tkg-system kapp-controller-b4dfc4659-kggg7 2/2 Running 0 9m tkg-system tanzu-capabilities-controller-manager-7c8dc68b84-7mv9v 1/1 Running 0 7m38s vmware-system-antrea register-placeholder-9n4kk 0/1 Completed 0 8m20s vmware-system-auth guest-cluster-auth-svc-rxt9v 1/1 Running 0 7m18s vmware-system-cloud-provider guest-cluster-cloud-provider-844cdc6ffc-54957 1/1 Running 0 8m25s vmware-system-csi vsphere-csi-controller-7db59f4569-lxzkc 7/7 Running 0 8m23s vmware-system-csi vsphere-csi-node-2pmlj 3/3 Running 3 (6m2s ago) 7m15s vmware-system-csi vsphere-csi-node-9kr82 3/3 Running 4 (6m10s ago) 8m23s vmware-system-csi vsphere-csi-node-9pxc8 3/3 Running 3 (6m2s ago) 7m16s So what will happen if I try to deploy an Ingress?\nNothing, as there is no IngressClass available.\nandreasm@ubuntu02:~/examples$ k get ingressclasses.networking.k8s.io No resources found So this means we still need to deploy AKO to be able to use Ingress. I have a post of how that is done here where NSX is providing the IP addresses for the Ingress, but I expect now it will be NSX-ALB providing the IP addresses using the ingress cidr configured in the namespace. So the only difference is NSX-ALB is providing it instead of NSX.\nThis concludes this post.\nCredits # A big thanks goes to Tom Schwaller, Container Networking TPM @VMware, (Twitter/X handle @tom_schwaller), for getting me started on this post, and for providing useful insights on the requirements for getting this to work.\n","date":"23 September 2023","externalUrl":null,"permalink":"/2023/09/23/vsphere-with-tanzu-8-u2-using-nsx-and-nsx-advanced-loadbalancer/","section":"Posts","summary":"A quick test on the new feature in vSphere 8 U2 with Tanzu using NSX and NSX Advanced Loadbalancer for Kubernetes API endpoint","title":"vSphere with Tanzu 8 U2 using NSX AND NSX Advanced Loadbalancer","type":"posts"},{"content":"","date":"19 September 2023","externalUrl":null,"permalink":"/tags/kubernetes-api-endpoint/","section":"Tags","summary":"","title":"Kubernetes-Api-Endpoint","type":"tags"},{"content":"","date":"19 September 2023","externalUrl":null,"permalink":"/tags/load-balancing/","section":"Tags","summary":"","title":"Load-Balancing","type":"tags"},{"content":"","date":"19 September 2023","externalUrl":null,"permalink":"/tags/tkgs/","section":"Tags","summary":"","title":"TKGs","type":"tags"},{"content":" vSphere with Tanzu and HAProxy # Using HAProxy is an option if in need of a an easy and quick way to deploy vSphere with Tanzu. There is no redundancy built into this approach as the HAProxy appliance only consist of one appliance. A support contract is directly with HAProxy not VMware. I would probably recommend this approach for poc, smaller environments, test environments etc. In production environments I would highly recommend NSX Advanced Loadbalancer for many many reasons like resiliency, scalbility and a whole bunch of enterprise features. See my post covering vSphere with Tanzu and NSX-ALB here and here\nBefore I begin this post I will just quickly go through the two different ways HAProxy can be deployed to support a vSphere with Tanzu deployment. The two options are called Default and Frontend. The difference being how the HAProxy network is configured. Lets start with the Default option: The Default option basically means you will deploy the HAProxy appliance using only 2 network interfaces. One network interface is being used for management and the second one will be a combined interface for both VIP(frontend) and data-network(workload network).\nI dont like this approach as it does not give me the option to have a separation between the workload network and the frontend network, meaning there is no distinction networking/firewall wise when restricting traffic from any to the VIP of the kubernetes api. For more information on this topology have a look here in the VMware official documentation.\nSo I will opt for the second option, Frontend, when using HAProxy in vSphere with Tanzu.\nThis option will deploy the HAProxy appliance with three network interfaces. A separate interface for management, workload and frontend (VIP/Kubernetes API). When users hit the VIP or the frontend network assigned it is a separate network interface on the HAProxy appliance, and will forward the traffic (using routes internally in the HAProxy) to forward the requests to the correct kubernetes api in the workload network. It also supports any additional workload networks which I will show later. For more information on the Frontend topology head over here\nThats it for the introduction, lets do the actual implementation of this.\nInstall HAProxy # First thing first, prepare the three network in vCenter using either VDS portgroups or NSX backed overlay segments. 1 portgroup for the management interface, one for the default workload network and one for the HAProxy frontend network. In my setup I will be using this IP information:\nManagement network - NSX backed overlay segment: Network = 10.13.10.0/24, GW = 10.13.10.1, HAProxy Interface IP = 10.13.10.199. Routed Workload network - VLAN backed VDS portgroup: Network = 10.13.201.0/24, GW = 10.13.201.1, HAProxy Interface IP = 10.13.201.10. Routed Frontend network - VLAN backed VDS portgroup: Network = 10.13.200.0/24, GW = 10.13.200.1, HAProxy Interface IP = 10.13.200.10. Routed When the networks is ready it is time to deploy the HAProxy appliance/vm itself. To deploy the HAProxy I will use the OVA approach described here. Then it is getting hold of the HAProxy OVA to be used for this purpose which can be found here, or directly to the OVA version 0.2.0 here.\nBefore deploying the OVA prepare the additional information below:\nDNS server = 172.24.3.1 LoadBalancer IP Range = 10.13.200.32/27 - this is just a range I have defined inside the HAProxy frontend network. The Frontend network is 10.13.200.0/24 (where my HAProxy will use 10.13.200.10) and the range I will make available for HAProxy to use is 10.13.200.32/27 which gives me 30 usable addresses x.x.x.33-x.x.x.62. There is also the option to add more ranges e.g 10.13.200.64/27 and so on. To save some time I will skip directly to the actual ova deployment from my vCenter server.\nHAProxy OVA deployment # The next step below is where I select the network topology, I will here choose Frontend\nIn the step below I will select my three prepared networks accordingly.\nEnter the root password for the appliance/os itself, check Permit Root Login (useful for troubleshooting and for this demo), leave the TLS sections default blank.\nFill in hostname if wanted, DNS server(s) the appliance management ip in CIDR format 10.13.10.199/24 and the gateway 10.13.10.1.\nThen it is the workload related network settings. Workload IP 10.13.201.10/24 and gateway 10.13.201.1. The Additional Workload Networks can be ignored (it does not seem to have any effect), I will go through that later adding additional workload networks. Then it is the Frontend IP = 10.13.200.10/24. It does say optional, but its only optional if doing the Default setup with only two nics.\nFrontend gateway = 10.13.200.1. Not optional when doing the Frontend setup. Then it is the range you want HAProxy to use for VIP. I have defined a range of 30 usable address by filling in the CIDR 10.13.200.32/27 as below. This can be one of many ranges if you want. Leave the port as default, if not in need of changing it. Then I define a custom API username for the Supervisor to use against the HAProxy, in my case admin.\nThen it is the password for the API admin user. The password I have chosen is \u0026hellip;.. \u0026hellip;. ..\nThen do a quick review and make sure everything is there. Click finish and wait.\nSSH into the HAProxy appliance # When the appliance has been deployed and powered on, ssh into the HAProxy appliance using its IP address in the management network and using root as username and password defined above.\nssh root@10.13.10.199 The authenticity of host \u0026#39;10.13.10.199 (10.13.10.199)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:Asefsdfgsdfsdgsdg5kyKHX0. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.13.10.199\u0026#39; (ED25519) to the list of known hosts. (root@10.13.10.199) Password: 12:06:48 up 2 min, 0 users, load average: 0.15, 0.12, 0.05 tdnf update info not available yet! root@haproxy [ ~ ]# The reason I want to log in here is because I need the CA.CRT generated so I can tell the Supervisor to use that later on, but also to do some information dump on certain config files that will be good to know about.\nThe most important part, the certificate. When enabling the Supervisor later I will need the certificate. Grab this by using this command:\nroot@haproxy [ ~ ]# cat /etc/haproxy/ca.crt -----BEGIN CERTIFICATE----- MIIDJ6HD6qCAn3KyQ0KNTJiqT7HgwRvFgZoHcG9vWXjvZy2 ..................................... qUOOIbrvQStRTqJl/DjEKi0FyR3lJ1OTOHAbD4YJltSixLkVHRqEH/PY0CbwUNib wEkCcibnNLMVLvqvXmyvj0x/Hg== -----END CERTIFICATE----- Then there is a couple of config files that is relevant to know about, it would be interesting to have a look at them now before enabling the supervisor and another look after it has been enabled. So lets have a look now.\nI will just run all of them in the same window:\n########## haproxy.cfg ################ root@haproxy [ ~ ]# cat /etc/haproxy/haproxy.cfg global log stdout format raw local0 info chroot /var/lib/haproxy stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy master-worker # Setting maxconn in the global section is what successfully sets the ulimit on the # host, otherwise we run out of file descriptors (defaulted at 4096). maxconn 60000 # This property indicates the number of maximum number of reloads a worker # will survive before being forcefully killed. This number is required to control # the rate at which processes can scale due to the number of reloads outscaling # the rate processes are reaped when all of their connections have been cleaned up. # This number was derived by taking the average virtual memory consumption for a # single HA Proxy process under load, ~28MB, and allocating HA Proxy 3GB out of 4GB # of the total virtual memory space. mworker-max-reloads 100 # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from: # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults mode tcp log global option tcplog option dontlognull option tcp-smart-accept timeout check 5s timeout connect 9s timeout client 10s timeout queue 5m timeout server 10s # tunnel timeout needs to be set at a lowish value to deal with the frequent # reloads invoked by dataplaneapi at scale. With a higher value set, established # connections will hang around and prevent haproxy from killing all off older processes # because those old processes won\u0026#39;t terminate those established connections unless # it is told to do so. Having these processes linger for too long can eventually # starve the system of resources as the spawn rate of processes exceeds the death rate. timeout tunnel 5m timeout client-fin 10s # Stats are disabled by default because enabling them on a non-local IP address # would result in allocating a port that could result in a conflict with one # of the binds programmed at runtime. # # To enable stats, uncomment the following section and replace SYSTEM_IP_ADDR # with the IP address of the HAProxy host. #frontend stats # mode http # bind SYSTEM_IP_ADDR:8404 # stats enable # stats uri /stats # stats refresh 500ms # stats hide-version # stats show-legends userlist controller user admin password $dfsdfsdf/mdsfsdfb$6/xsdfsdfsdf379dfsdfMOJ/W1 ##################### haproxy.cfg - no entries yet for any virtual services ############################### ##################### any-ip-routes ########################### root@haproxy [ ~ ]# cat /etc/vmware/anyip-routes.cfg # # Configuration file that contains a line-delimited list of CIDR values # that define the network ranges used to bind the load balancer\u0026#39;s frontends # to virtual IP addresses. # # * Lines beginning with a comment character, #, are ignored # * This file is used by the anyip-routes service # 10.13.200.32/27 ################### any-ip-routes ######################### ################## route-tables ####################### root@haproxy [ ~ ]# cat /etc/vmware/route-tables.cfg # # Configuration file that contains a line-delimited list of values used to # create route tables on which default gateways are defined. This enables # the use of IP policy to ensure traffic to interfaces that do not use the # default gateway is routed correctly. # # * Lines beginning with a comment character, #, are ignored # * This file is used by the route-tables service # # Each line that contains a value must adhere to the following, # comma-separated format: # # \u0026lt;TableID\u0026gt;,\u0026lt;TableName\u0026gt;,\u0026lt;MACAddress\u0026gt;,\u0026lt;NetworkCIDR\u0026gt;,\u0026lt;Gateway4\u0026gt; # # The fields in the above format are as follows: # # * TableID The route table ID. This value should be an integer between # 2-250. Please see /etc/iproute2/rt_tables for a list of the # route table IDs currently in use, including reserved IDs. # # * TableName The name of the route table. This value will be appended # to a constant prefix, used to identify route tables managed # by the route-tables service. # # * MACAddress The MAC address of the interface connected to the network # specified by NetworkCIDR # # * NetworkCIDR The CIDR of the network to which the interface by MACAddress # is connected # # * Gateway4 The IPv4 address of the gateway used by the network specified # by NetworkCIDR # # For example, the following lines are valid values: # # 2,frontend,00:00:00:ab:cd:ef,192.168.1.0/24,192.168.1.1 # 3,workload,00:00:00:12:34:56,192.168.2.0/24,192.168.2.1 # 2,workload,00:50:56:b4:ca:fd,10.13.201.10/24,10.13.201.1 2,workload,00:50:56:b4:ca:fd,10.13.201.10/24 3,frontend,00:50:56:b4:ef:d1,10.13.200.10/24,10.13.200.1 3,frontend,00:50:56:b4:ef:d1,10.13.200.10/24 #################################### route-tables #################### ########### iproute2 tables ################## root@haproxy [ ~ ]# cat /etc/iproute2/rt_tables # # reserved values # 255\tlocal 254\tmain 253\tdefault 0\tunspec # # local # #1\tinr.ruhep 2\trtctl_workload 2\trtctl_workload 3\trtctl_frontend 3\trtctl_frontend ################ iproute2 tables ################## One can also view the current configuration of the HAProxy appliance under vApp Settings in vCenter like this:\nThis concludes the deployment of the HAProxy appliance, next step is to enable the Supervisor using this HAProxy appliance for the L4 VIPs.\nEnable Supervisor in vSphere using HAProxy # Head over to workload management in vCenter and click get started\nSit back and wait\u0026hellip;. and wait\u0026hellip;..\nAnd after some waiting it should be green and ready and the Supervisor Control Plane IP should be an IP address from the LoadBalancer range:\nNow I just want to check the HAProxy config (/etc/haproxy/haproxy.cfg) to see if there has been any update there:\nhaproxy.cfg -\u0026gt; frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc mode tcp bind 10.13.200.34:443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:nginx bind 10.13.200.34:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:kube-apiserver log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc option tcplog use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver if { dst_port 6443 } use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx if { dst_port 443 } frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller mode tcp bind 10.13.200.33:2112 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:ctlr bind 10.13.200.33:2113 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:syncer log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller option tcplog use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer if { dst_port 2113 } use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr if { dst_port 2112 } backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:6443 10.13.201.20:6443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:6443 10.13.201.21:6443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:6443 10.13.201.22:6443 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:443 10.13.201.20:443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:443 10.13.201.21:443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:443 10.13.201.22:443 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2112 10.13.201.20:2112 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2112 10.13.201.21:2112 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2112 10.13.201.22:2112 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2113 10.13.201.20:2113 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2113 10.13.201.21:2113 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2113 10.13.201.22:2113 check weight 100 verify none So it seems. All the virtual services for the Supervisor Cluster entries has been added. I should now be able to reach the Kubernetes API endpoint and log-in using the loadbalancer/external ip provided by HAProxy 10.13.200.34\nvsphere-kubectl login --server=\u0026#34;10.13.200.34\u0026#34; --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-domain.net KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below Password: Logged in successfully. You have access to the following contexts: 10.13.200.34 If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator. To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` Now I can just go ahead and deploy my first workload cluster and get started doing some Kubernetes\u0026rsquo;ing using the default Workload Network defined. Then it will be like the diagram below:\nBut instead I want to create a new vSphere Namespace in a new subnet/workload network and deploy a workload cluster there. Lets see how this works out.\nAdd additional workload network # Assuming I have certain requirement that some Workload Clusters needs to be in separate IP subnets, such as exhausted existing default workload network, ip separation, security zones etc. I need to have the ability to define and add additional workload networks.\nI already have a new workload network defined. Its configured using NSX overlay segment has the following CIDR: 10.13.21.0/24. The only requirement seems to be needed for HAProxy to use this new network I just need to make sure the Workload network can reach the additional networks I add. So routing between the defautl workload network and any additional workload networks added needs to be in place. No other configuration is needed on the HAProxy side. The diagram below tries to illustrate a second workload network:\nWhen a new workload cluster is provisioned in the new workload network the VIP/LoadBalancer range will be the same, but instead HAProxy needs to route the traffic from its Workload network to the new workload network. Its not HAProxy that is responsible for the actual routing, that is something that needs to be in place in the infrastructure. HAProxy receives the traffic on the Frontend/LoadBalancer network interface, forwards it to the workload interface which then uses its configured gateway to forward the traffic to the new workload network where there is a router in between that knows about those network and can forward it to the correct destination and back again.\nLets try to deploy a workload cluster in the new workload network above, creating a new vSphere Namespace with this network.\nClick ADD and populate the necessary fields\nCreate the namespace using and select the new network:\nNow, deploy a cluster in this namespace:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: cluster-2 namespace: ns-stc-wld-1 spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] pods: cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.26.5---vmware.2-fips.1-tkg.1 controlPlane: replicas: 1 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: machineDeployments: - class: node-pool name: node-pool-01 replicas: 2 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu variables: - name: vmClass value: best-effort-small - name: storageClass value: vsan-default-storage-policy kubectl apply -f cluster-2.yaml cluster.cluster.x-k8s.io/cluster-2 created And here the cluster has been deployed:\nThis looks very promising, as the first indicator of something not working is it only deploys the first control plane and stops. Let me log into the cluster and check the Kubernetes API access also.\nandreasm@ubuntu02:~$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cluster-2-8tq8q-mvj5d Ready control-plane 15m v1.26.5+vmware.2-fips.1 10.13.21.20 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 cluster-2-node-pool-01-jqdms-b8fd8b5bb-7mjkh Ready \u0026lt;none\u0026gt; 11m v1.26.5+vmware.2-fips.1 10.13.21.22 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 cluster-2-node-pool-01-jqdms-b8fd8b5bb-cxjqn Ready \u0026lt;none\u0026gt; 11m v1.26.5+vmware.2-fips.1 10.13.21.21 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 The nodes are using IP address in the new workload network (10.13.21.0/24) and below we can see the loadbalanced kubernetes api for this workload cluster has been assigned the ip 10.13.200.35 from the HAProxy loadbalancer range.\nAnd a quick look at the haproxy.cfg file again:\nfrontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service mode tcp bind 10.13.200.35:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-10.13.200.35:apiserver log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service option tcplog use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver if { dst_port 6443 } The entry for the new Cluster-2 has been added.\nRestore from a deleted/crashed/killed HAProxy appliance # This exercise will involve deleting my existing HAProxy appliance. In the meantime I will loose access to my Supervisor cluster and any other workload cluster that has been deployed. I will have to re-deploy the HAProxy again, following the exact same deployment procedure as described above and update my Supervisor so it can reconfigure it back as it was before I deleted the old HAProxy deployment. So lets start by deleting the existing HAProxy appliance.\nRecent task\nNow that is gone.\nLets check my access to the Supervisor API:\nandreasm@ubuntu02:~$ k get pods -A Unable to connect to the server: dial tcp 10.13.200.34:6443: connect: no route to host Not good. So lets redeploy the HAProxy\nIts deployed Power it on, can I reach the Supervisor API?\nandreasm@ubuntu02:~$ k get pods -A The connection to the server 10.13.200.34:6443 was refused - did you specify the right host or port? No luck yet.\nLog in with SSH and check the HAProxy config:\naandreasm@linuxvm02:~/.ssh$ ssh root@10.13.10.199 The authenticity of host \u0026#39;10.13.10.199 (10.13.10.199)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:BJGSoo5icUWW7+s2FIKBQV+bg33ZOhk10s9+LFoQgXs. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;10.13.10.199\u0026#39; (ED25519) to the list of known hosts. (root@10.13.10.199) Password: 08:09:36 up 0 min, 0 users, load average: 0.09, 0.03, 0.01 tdnf update info not available yet! root@haproxy [ ~ ]# root@haproxy [ ~ ]# cat /etc/haproxy/haproxy.cfg global log stdout format raw local0 info chroot /var/lib/haproxy stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy master-worker # Setting maxconn in the global section is what successfully sets the ulimit on the # host, otherwise we run out of file descriptors (defaulted at 4096). maxconn 60000 # This property indicates the number of maximum number of reloads a worker # will survive before being forcefully killed. This number is required to control # the rate at which processes can scale due to the number of reloads outscaling # the rate processes are reaped when all of their connections have been cleaned up. # This number was derived by taking the average virtual memory consumption for a # single HA Proxy process under load, ~28MB, and allocating HA Proxy 3GB out of 4GB # of the total virtual memory space. mworker-max-reloads 100 # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from: # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults mode tcp log global option tcplog option dontlognull option tcp-smart-accept timeout check 5s timeout connect 9s timeout client 10s timeout queue 5m timeout server 10s # tunnel timeout needs to be set at a lowish value to deal with the frequent # reloads invoked by dataplaneapi at scale. With a higher value set, established # connections will hang around and prevent haproxy from killing all off older processes # because those old processes won\u0026#39;t terminate those established connections unless # it is told to do so. Having these processes linger for too long can eventually # starve the system of resources as the spawn rate of processes exceeds the death rate. timeout tunnel 5m timeout client-fin 10s # Stats are disabled by default because enabling them on a non-local IP address # would result in allocating a port that could result in a conflict with one # of the binds programmed at runtime. # # To enable stats, uncomment the following section and replace SYSTEM_IP_ADDR # with the IP address of the HAProxy host. #frontend stats # mode http # bind SYSTEM_IP_ADDR:8404 # stats enable # stats uri /stats # stats refresh 500ms # stats hide-version # stats show-legends # ###No entries..... Certainly a new appliance, not the same authenticity fingerprint. But no entries.. Now I will head over to the Workload Management in vCenter and update the certificate under the LoadBalancer section with the new certificate from the newly deployed HAProxy.\nNote.. If I had copied out the cert and key from the old one I could have imported them back again during deployment.\nClick save. I still cant reach the my kubernetes api endpoints. The next I need to to is just restart the WCP service from the vCenter VAMI.\nScroll all the way down to find the WCP service, select it, scroll all the way up again.\nClick restart.\nAfter the service has been restarted having a look inside tha haproxy.cfg file again now:\nfrontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc mode tcp bind 10.13.200.34:443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:nginx bind 10.13.200.34:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:kube-apiserver log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc option tcplog use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx if { dst_port 443 } use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver if { dst_port 6443 } frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service mode tcp bind 10.13.200.35:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-10.13.200.35:apiserver log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service option tcplog use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver if { dst_port 6443 } frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller mode tcp bind 10.13.200.33:2112 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:ctlr bind 10.13.200.33:2113 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:syncer log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller option tcplog use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer if { dst_port 2113 } use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr if { dst_port 2112 } backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:6443 10.13.201.20:6443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:6443 10.13.201.21:6443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:6443 10.13.201.22:6443 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:443 10.13.201.20:443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:443 10.13.201.21:443 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:443 10.13.201.22:443 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-10.13.21.20:6443 10.13.21.20:6443 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2112 10.13.201.20:2112 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2112 10.13.201.21:2112 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2112 10.13.201.22:2112 check weight 100 verify none backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer mode tcp balance roundrobin option tcp-check log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2113 10.13.201.20:2113 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2113 10.13.201.21:2113 check weight 100 verify none server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2113 10.13.201.22:2113 check weight 100 verify none root@haproxy [ ~ ]# All my entries are back, and I can reach my kubernetes cluster again:\nandreasm@ubuntu02:~$ k get nodes NAME STATUS ROLES AGE VERSION 42344c0c03ff6c1cf443328504169cbe Ready control-plane,master 19h v1.25.6+vmware.wcp.2 4234cc01880600b11de8b894be0c2a30 Ready control-plane,master 19h v1.25.6+vmware.wcp.2 4234d1cd346addba50a751b3cbdfd5ed Ready control-plane,master 19h v1.25.6+vmware.wcp.2 esx01.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c esx02.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c esx03.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c esx04.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c andreasm@ubuntu02:~$ k config use-context cluster-2 Switched to context \u0026#34;cluster-2\u0026#34;. andreasm@ubuntu02:~$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cluster-2-8tq8q-mvj5d Ready control-plane 17h v1.26.5+vmware.2-fips.1 10.13.21.20 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 cluster-2-node-pool-01-jqdms-b8fd8b5bb-klqff Ready \u0026lt;none\u0026gt; 5m18s v1.26.5+vmware.2-fips.1 10.13.21.23 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 cluster-2-node-pool-01-jqdms-b8fd8b5bb-m76rf Ready \u0026lt;none\u0026gt; 5m18s v1.26.5+vmware.2-fips.1 10.13.21.22 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 All services back up again.\nThis concludes this post. Thanks for reading.\n","date":"19 September 2023","externalUrl":null,"permalink":"/2023/09/19/vsphere-with-tanzu-and-haproxy/","section":"Posts","summary":"In this post I will quickly go through how to use vSphere with Tanzu and HAProxy exposing the Kubernetes API","title":"vSphere with Tanzu and HAproxy","type":"posts"},{"content":"","date":"19 September 2023","externalUrl":null,"permalink":"/tags/vsphere-with-tanzu/","section":"Tags","summary":"","title":"VSphere-With-Tanzu","type":"tags"},{"content":"","date":"7 September 2023","externalUrl":null,"permalink":"/tags/autoscaling/","section":"Tags","summary":"","title":"Autoscaling","type":"tags"},{"content":"","date":"7 September 2023","externalUrl":null,"permalink":"/categories/autoscaling/","section":"Categories","summary":"","title":"Autoscaling","type":"categories"},{"content":" TKG autoscaler # From the official TKG documentation page:\nCluster Autoscaler is a Kubernetes program that automatically scales Kubernetes clusters depending on the demands on the workload clusters. Use Cluster Autoscaler only for workload clusters deployed by a standalone management cluster.\nOk, lets try out this then.\nEnable Cluster Autoscaler # So one of the pre-requisites is a TKG standalone management cluster. I have that already deployed and running. Then for a workload cluster to be able to use the cluster autoscaler I need to enable this by adding some parameters in the cluster deployment manifest. The following is the autoscaler relevant variables, some variables are required some are optional and only valid for use on a workload cluster deployment manifest. According to the official documentation the only supported way to enable autoscaler is when provisioning a new workload cluster.\nENABLE_AUTOSCALER: \u0026ldquo;true\u0026rdquo; #Required if you want to enable the autoscaler\nAUTOSCALER_MAX_NODES_TOTAL: \u0026ldquo;0\u0026rdquo; #Optional\nAUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026ldquo;10m\u0026rdquo; #Optional\nAUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: \u0026ldquo;10s\u0026rdquo; #Optional\nAUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: \u0026ldquo;3m\u0026rdquo; #Optional\nAUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: \u0026ldquo;10m\u0026rdquo; #Optional\nAUTOSCALER_MAX_NODE_PROVISION_TIME: \u0026ldquo;15m\u0026rdquo; #Optional\nAUTOSCALER_MIN_SIZE_0: \u0026ldquo;1\u0026rdquo; #Required (if Autoscaler is enabled as above)\nAUTOSCALER_MAX_SIZE_0: \u0026ldquo;2\u0026rdquo; #Required (if Autoscaler is enabled as above)\nAUTOSCALER_MIN_SIZE_1: \u0026ldquo;1\u0026rdquo; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nAUTOSCALER_MAX_SIZE_1: \u0026ldquo;3\u0026rdquo; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nAUTOSCALER_MIN_SIZE_2: \u0026ldquo;1\u0026rdquo; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nAUTOSCALER_MAX_SIZE_2: \u0026ldquo;4\u0026rdquo; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nEnable Autoscaler upon provisioning of a new workload cluster # Start by preparing a class-based yaml for the workload cluster. This procedure involves adding the AUTOSCALER variables (above) to the tkg bootstrap yaml (the one used to deploy the TKG management cluster). Then generate a cluster-class yaml manifest for the new workload cluster. I will make a copy of my existing TKG bootstrap yaml file, name it something relevant to autoscaling. Then in this file I will add these variables:\n#! --------------- #! Workload Cluster Specific #! ------------- ENABLE_AUTOSCALER: \u0026#34;true\u0026#34; AUTOSCALER_MAX_NODES_TOTAL: \u0026#34;0\u0026#34; AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026#34;10m\u0026#34; AUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: \u0026#34;10s\u0026#34; AUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: \u0026#34;3m\u0026#34; AUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: \u0026#34;10m\u0026#34; AUTOSCALER_MAX_NODE_PROVISION_TIME: \u0026#34;15m\u0026#34; AUTOSCALER_MIN_SIZE_0: \u0026#34;1\u0026#34; #This will be used if not using availability zones. If using az this will count as zone 1 - required AUTOSCALER_MAX_SIZE_0: \u0026#34;2\u0026#34; ##This will be used if not using availability zones. If using az this will count as zone 1 - required AUTOSCALER_MIN_SIZE_1: \u0026#34;1\u0026#34; #This will be used for availability zone 2 AUTOSCALER_MAX_SIZE_1: \u0026#34;3\u0026#34; #This will be used for availability zone 2 AUTOSCALER_MIN_SIZE_2: \u0026#34;1\u0026#34; #This will be used for availability zone 3 AUTOSCALER_MAX_SIZE_2: \u0026#34;4\u0026#34; #This will be used for availability zone 3 If not using TKG in a multi availability zone deployment, there is no need to add the lines AUTOSCALER_MIN_SIZE_1, AUTOSCALER_MAX_SIZE_1, AUTOSCALER_MIN_SIZE_2, and AUTOSCALER_MAX_SIZE_2 as these are only used for the additional zones you have configured. For a \u0026ldquo;no AZ\u0026rdquo; deployment AUTOSCALER_MIN/MAX_SIZE_1 is sufficient.\nAfter the above has been added I will do a \u0026ldquo;\u0026ndash;dry-run\u0026rdquo; to create my workload cluster class-based yaml file:\nandreasm@tkg-bootstrap:~$ tanzu cluster create tkg-cluster-3-auto --namespace tkg-ns-3 --file tkg-mgmt-bootstrap-tkg-2.3-autoscaler.yaml --dry-run \u0026gt; tkg-cluster-3-auto.yaml The above command gives the workload cluster the name tkg-cluster-3-auto in the namespace tkg-ns-3 and using the modified tkg bootstrap file containing the autocluster variables. The output is the class-based yaml I will use to create the cluster, like this (if no error during the dry-run command). In my mgmt bootstrap I have defined the autoscaler min_max settings just to reflect the capabilities in differentiating settings pr availability zone. According to the manual this should only be used in AWS, but in 2.3 multi-az is fully supported and the docs has probably not been updated yet. If I take a look at the class-based yaml:\nworkers: machineDeployments: - class: tkg-worker failureDomain: wdc-zone-2 metadata: annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;2\u0026#34; cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 strategy: type: RollingUpdate - class: tkg-worker failureDomain: wdc-zone-3 metadata: annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;3\u0026#34; cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-1 strategy: type: RollingUpdate - class: tkg-worker failureDomain: wdc-zone-3 metadata: annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;4\u0026#34; cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-2 strategy: type: RollingUpdate --- I notice that it does take into consideration my different availability zones. Perfect.\nBefore I deploy my workload cluster, I will edit the manifest to only deploy worker nodes in my AZ zone 2 due to resource constraints in my lab and to make the demo a bit better (scaling up from one worker and back again) then I will deploy the workload cluster.\nandreasm@tkg-bootstrap:~$ tanzu cluster create --file tkg-cluster-3-auto.yaml Validating configuration... cluster class based input file detected, getting tkr version from input yaml input TKR Version: v1.26.5+vmware.2-tkg.1 TKR Version v1.26.5+vmware.2-tkg.1, Kubernetes Version v1.26.5+vmware.2-tkg.1 configured Now it is all about wating\u0026hellip; After the wating period is done it is time for some testing\u0026hellip;\nEnable Autoscaler on existing/running workload cluster # I have already a TKG workload cluster up and running and I want to \u0026ldquo;post-enable\u0026rdquo; autoscaler in this cluster. This cluster has been deployed with the AUTOSCALER_ENABLE=false and below is the class based yaml manifest (no autoscaler variables):\nworkers: machineDeployments: - class: tkg-worker failureDomain: wdc-zone-2 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 replicas: 1 strategy: type: RollingUpdate The above class based yaml has been generated from my my mgmt bootstrap yaml with the AUTOSCALER settings like this:\n#! --------------- #! Workload Cluster Specific #! ------------- ENABLE_AUTOSCALER: \u0026#34;false\u0026#34; AUTOSCALER_MAX_NODES_TOTAL: \u0026#34;0\u0026#34; AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026#34;10m\u0026#34; AUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: \u0026#34;10s\u0026#34; AUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: \u0026#34;3m\u0026#34; AUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: \u0026#34;10m\u0026#34; AUTOSCALER_MAX_NODE_PROVISION_TIME: \u0026#34;15m\u0026#34; AUTOSCALER_MIN_SIZE_0: \u0026#34;1\u0026#34; AUTOSCALER_MAX_SIZE_0: \u0026#34;4\u0026#34; AUTOSCALER_MIN_SIZE_1: \u0026#34;1\u0026#34; AUTOSCALER_MAX_SIZE_1: \u0026#34;4\u0026#34; AUTOSCALER_MIN_SIZE_2: \u0026#34;1\u0026#34; AUTOSCALER_MAX_SIZE_2: \u0026#34;4\u0026#34; If I check the autoscaler status:\nandreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status Error from server (NotFound): configmaps \u0026#34;cluster-autoscaler-status\u0026#34; not found Now, this cluster is in \u0026ldquo;serious\u0026rdquo; need to have autoscaler enabled. So how do I do that? This step is most likely not officially supported. I will now go back to the tkg mgmt bootstrap yaml, enable the autoscaler. Do a dry run of the config and apply the new class-based yaml manifest. This is all done in the TKG mgmt cluster context.\nandreasm@linuxvm01:~$ tanzu cluster create tkg-cluster-3-auto --namespace tkg-ns-3 --file tkg-mgmt-bootstrap-tkg-2.3-autoscaler-wld-1-zone.yaml --dry-run \u0026gt; tkg-cluster-3-auto-az.yaml Before applying the yaml new class based manifest I will edit out the uneccessary crds, and just keep the updated settings relevant to the autoscaler, it may even be reduced further. Se my yaml below:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: annotations: osInfo: ubuntu,20.04,amd64 tkg/plan: dev labels: tkg.tanzu.vmware.com/cluster-name: tkg-cluster-3-auto name: tkg-cluster-3-auto namespace: tkg-ns-3 spec: clusterNetwork: pods: cidrBlocks: - 100.96.0.0/11 services: cidrBlocks: - 100.64.0.0/13 topology: class: tkg-vsphere-default-v1.1.0 controlPlane: metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu replicas: 1 variables: - name: cni value: antrea - name: controlPlaneCertificateRotation value: activate: true daysBefore: 90 - name: auditLogging value: enabled: false - name: podSecurityStandard value: audit: restricted deactivated: false warn: restricted - name: apiServerEndpoint value: \u0026#34;\u0026#34; - name: aviAPIServerHAProvider value: true - name: vcenter value: cloneMode: fullClone datacenter: /cPod-NSXAM-WDC datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 folder: /cPod-NSXAM-WDC/vm/TKGm network: /cPod-NSXAM-WDC/network/ls-tkg-mgmt resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources server: vcsa.FQDN storagePolicyID: \u0026#34;\u0026#34; tlsThumbprint: F8:----:7D - name: user value: sshAuthorizedKeys: - ssh-rsa BBAAB3NzaC1yc2EAAAADAQABA------QgPcxDoOhL6kdBHQY3ZRPE5LIh7RWM33SvsoIgic1OxK8LPaiGEPaOfUvP2ki7TNHLxP78bPxAfbkK7llDSmOIWrm7ukwG4DLHnyriBQahLqv1Wpx4kIRj5LM2UEBx235bVDSve== - name: controlPlane value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: worker value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: controlPlaneZoneMatchingLabels value: region: k8s-region tkg-cp: allowed - name: security value: fileIntegrityMonitoring: enabled: false imagePolicy: pullAlways: false webhook: enabled: false spec: allowTTL: 50 defaultAllow: true denyTTL: 60 retryBackoff: 500 kubeletOptions: eventQPS: 50 streamConnectionIdleTimeout: 4h0m0s systemCryptoPolicy: default version: v1.26.5+vmware.2-tkg.1 workers: machineDeployments: - class: tkg-worker failureDomain: wdc-zone-2 metadata: annotations: cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;4\u0026#34; cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 strategy: type: RollingUpdate --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: tkg-cluster-3-auto-cluster-autoscaler name: tkg-cluster-3-auto-cluster-autoscaler namespace: tkg-ns-3 spec: replicas: 1 selector: matchLabels: app: tkg-cluster-3-auto-cluster-autoscaler template: metadata: labels: app: tkg-cluster-3-auto-cluster-autoscaler spec: containers: - args: - --cloud-provider=clusterapi - --v=4 - --clusterapi-cloud-config-authoritative - --kubeconfig=/mnt/tkg-cluster-3-auto-kubeconfig/value - --node-group-auto-discovery=clusterapi:clusterName=tkg-cluster-3-auto,namespace=tkg-ns-3 - --scale-down-delay-after-add=10m - --scale-down-delay-after-delete=10s - --scale-down-delay-after-failure=3m - --scale-down-unneeded-time=10m - --max-node-provision-time=15m - --max-nodes-total=0 command: - /cluster-autoscaler image: projects.registry.vmware.com/tkg/cluster-autoscaler:v1.26.2_vmware.1 name: tkg-cluster-3-auto-cluster-autoscaler volumeMounts: - mountPath: /mnt/tkg-cluster-3-auto-kubeconfig name: tkg-cluster-3-auto-cluster-autoscaler-volume readOnly: true serviceAccountName: tkg-cluster-3-auto-autoscaler terminationGracePeriodSeconds: 10 tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master - effect: NoSchedule key: node-role.kubernetes.io/control-plane volumes: - name: tkg-cluster-3-auto-cluster-autoscaler-volume secret: secretName: tkg-cluster-3-auto-kubeconfig --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: creationTimestamp: null name: tkg-cluster-3-auto-autoscaler-workload roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler-workload subjects: - kind: ServiceAccount name: tkg-cluster-3-auto-autoscaler namespace: tkg-ns-3 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: creationTimestamp: null name: tkg-cluster-3-auto-autoscaler-management roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-autoscaler-management subjects: - kind: ServiceAccount name: tkg-cluster-3-auto-autoscaler namespace: tkg-ns-3 --- apiVersion: v1 kind: ServiceAccount metadata: name: tkg-cluster-3-auto-autoscaler namespace: tkg-ns-3 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-autoscaler-workload rules: - apiGroups: - \u0026#34;\u0026#34; resources: - persistentvolumeclaims - persistentvolumes - pods - replicationcontrollers verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - get - list - update - watch - apiGroups: - \u0026#34;\u0026#34; resources: - pods/eviction verbs: - create - apiGroups: - policy resources: - poddisruptionbudgets verbs: - list - watch - apiGroups: - storage.k8s.io resources: - csinodes - storageclasses verbs: - get - list - watch - apiGroups: - batch resources: - jobs verbs: - list - watch - apiGroups: - apps resources: - daemonsets - replicasets - statefulsets verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - events verbs: - create - patch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps verbs: - create - delete - get - update - apiGroups: - coordination.k8s.io resources: - leases verbs: - create - get - update --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: cluster-autoscaler-management rules: - apiGroups: - cluster.x-k8s.io resources: - machinedeployments - machines - machinesets verbs: - get - list - update - watch - patch - apiGroups: - cluster.x-k8s.io resources: - machinedeployments/scale - machinesets/scale verbs: - get - update - apiGroups: - infrastructure.cluster.x-k8s.io resources: - \u0026#39;*\u0026#39; verbs: - get - list And now I will apply the above yaml on my running TKG workload cluster using kubectl (done from the mgmt context):\nandreasm@linuxvm01:~$ kubectl apply -f tkg-cluster-3-enable-only-auto-az.yaml cluster.cluster.x-k8s.io/tkg-cluster-3-auto configured Warning: would violate PodSecurity \u0026#34;restricted:v1.24\u0026#34;: allowPrivilegeEscalation != false (container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) deployment.apps/tkg-cluster-3-auto-cluster-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/tkg-cluster-3-auto-autoscaler-workload created clusterrolebinding.rbac.authorization.k8s.io/tkg-cluster-3-auto-autoscaler-management created serviceaccount/tkg-cluster-3-auto-autoscaler created clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-workload unchanged clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-management unchanged Checking for autoscaler status now shows this:\nandreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status Name: cluster-autoscaler-status Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-11 10:40:02.369535271 +0000 UTC Data ==== status: ---- Cluster-autoscaler status at 2023-09-11 10:40:02.369535271 +0000 UTC: Cluster-wide: Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 ScaleUp: NoActivity (ready=2 registered=2) LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 NodeGroups: Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-s7d7t Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=1 (minSize=1, maxSize=4)) LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 ScaleUp: NoActivity (ready=1 cloudProviderTarget=1) LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 BinaryData ==== Events: \u0026lt;none\u0026gt; Thats great.\nAnother way to do it is to edit the cluster directly following this KB article. This KB article can also be used to change/modify existing autoscaler settings.\nTest the autoscaler # In the following chapters I will test the scale up and down of my worker nodes, based on load in the cluster. My initial cluster is up and running:\nNAME STATUS ROLES AGE VERSION tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 4m17s v1.26.5+vmware.2 tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 8m31s v1.26.5+vmware.2 One control-plane node and one worker node. Now I want to check the status of the cluster-scaler:\nandreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status Name: cluster-autoscaler-status Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 13:30:12.611110965 +0000 UTC Data ==== status: ---- Cluster-autoscaler status at 2023-09-08 13:30:12.611110965 +0000 UTC: Cluster-wide: Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 ScaleUp: NoActivity (ready=2 registered=2) LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC NodeGroups: Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=1 (minSize=1, maxSize=4)) LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 ScaleUp: NoActivity (ready=1 cloudProviderTarget=1) LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC BinaryData ==== Events: \u0026lt;none\u0026gt; Scale-up - amount of worker nodes (horizontally) # Now I need to generate some load and see if it will do some magic scaling in the background.\nI have deployed my Yelb app again, the only missing pod is the UI pod:\nNAME READY STATUS RESTARTS AGE redis-server-56d97cc8c-4h54n 1/1 Running 0 6m56s yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 6m55s yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 6m56s I still have my one cp node and one worker node. I will now deploy the UI pod and scale an insane amount of UI pods for the Yelb application.\nyelb-ui-5c5b8d8887-9598s 1/1 Running 0 2m35s andreasm@linuxvm01:~$ k scale deployment -n yelb yelb-ui --replicas 200 deployment.apps/yelb-ui scaled Lets check some status after this\u0026hellip; A bunch of pods in pending states, waiting for a node to be scheduled on.\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-56d97cc8c-4h54n 1/1 Running 0 21m 100.96.1.9 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 21m 100.96.1.11 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 21m 100.96.1.10 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-22v8p 1/1 Running 0 6m18s 100.96.1.53 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2587j 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2bzcg 0/1 Pending 0 3m51s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2gncl 0/1 Pending 0 3m51s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2gwp8 1/1 Running 0 3m53s 100.96.1.86 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2gz7r 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2jlvv 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2pfgp 1/1 Running 0 6m18s 100.96.1.36 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2prwf 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2vr4f 0/1 Pending 0 3m53s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2w2t8 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2x6b7 1/1 Running 0 6m18s 100.96.1.34 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2x726 1/1 Running 0 9m40s 100.96.1.23 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-452bx 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-452dd 1/1 Running 0 6m17s 100.96.1.69 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-45nmz 0/1 Pending 0 3m48s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-4kj69 1/1 Running 0 3m53s 100.96.1.109 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-4svbf 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-4t6dm 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-4zlhw 0/1 Pending 0 3m51s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-55qzm 1/1 Running 0 9m40s 100.96.1.15 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-5fts4 1/1 Running 0 6m18s 100.96.1.55 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The autoscaler status:\nandreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status Name: cluster-autoscaler-status Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 14:01:43.794315378 +0000 UTC Data ==== status: ---- Cluster-autoscaler status at 2023-09-08 14:01:43.794315378 +0000 UTC: Cluster-wide: Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 ScaleUp: InProgress (ready=2 registered=2) LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 LastTransitionTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-08 14:01:30.091765978 +0000 UTC m=+3235.032975159 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC NodeGroups: Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=4)) LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 ScaleUp: InProgress (ready=1 cloudProviderTarget=2) LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 LastTransitionTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-08 14:01:30.091765978 +0000 UTC m=+3235.032975159 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC BinaryData ==== Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScaledUpGroup 12s cluster-autoscaler Scale-up: setting group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size to 2 instead of 1 (max: 4) Normal ScaledUpGroup 11s cluster-autoscaler Scale-up: group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size set to 2 instead of 1 (max: 4) O yes, it has triggered a scale up. And in vCenter a new worker node is in the process:\nNAME STATUS ROLES AGE VERSION tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 55m v1.26.5+vmware.2 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc NotReady \u0026lt;none\u0026gt; 10s v1.26.5+vmware.2 tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 59m v1.26.5+vmware.2 Lets check the pods status when the new node has been provisioned and ready..\nThe node is now ready:\nNAME STATUS ROLES AGE VERSION tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 56m v1.26.5+vmware.2 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc Ready \u0026lt;none\u0026gt; 101s v1.26.5+vmware.2 tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 60m v1.26.5+vmware.2 All my 200 UI pods are now scheduled and running across two worker nodes:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-56d97cc8c-4h54n 1/1 Running 0 30m 100.96.1.9 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 30m 100.96.1.11 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 30m 100.96.1.10 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-22v8p 1/1 Running 0 15m 100.96.1.53 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2587j 1/1 Running 0 12m 100.96.2.82 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2bzcg 1/1 Running 0 12m 100.96.2.9 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2gncl 1/1 Running 0 12m 100.96.2.28 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2gwp8 1/1 Running 0 12m 100.96.1.86 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2gz7r 1/1 Running 0 12m 100.96.2.38 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2jlvv 1/1 Running 0 12m 100.96.2.58 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2pfgp 1/1 Running 0 15m 100.96.1.36 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2prwf 1/1 Running 0 12m 100.96.2.48 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2vr4f 1/1 Running 0 12m 100.96.2.77 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2w2t8 1/1 Running 0 12m 100.96.2.63 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2x6b7 1/1 Running 0 15m 100.96.1.34 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-2x726 1/1 Running 0 18m 100.96.1.23 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-452bx 1/1 Running 0 12m 100.96.2.67 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-452dd 1/1 Running 0 15m 100.96.1.69 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-5c5b8d8887-45nmz 1/1 Running 0 12m 100.96.2.100 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Scale-down - remove un-needed worker nodes # Now that I have seen that the autoscaler is indeed scaling the amount worker nodes automatically, I will like to test whether it is also being capable of scaling down, removing unneccessary worker nodes as the load is not there any more. To test this I will just scale down the amount of UI pods in the Yelb application:\nandreasm@linuxvm01:~$ k scale deployment -n yelb yelb-ui --replicas 2 deployment.apps/yelb-ui scaled andreasm@linuxvm01:~$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-56d97cc8c-4h54n 1/1 Running 0 32m yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 32m yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 32m yelb-ui-5c5b8d8887-22v8p 1/1 Terminating 0 17m yelb-ui-5c5b8d8887-2587j 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2bzcg 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2gncl 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2gwp8 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2gz7r 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2jlvv 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2pfgp 1/1 Terminating 0 17m yelb-ui-5c5b8d8887-2prwf 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2vr4f 1/1 Terminating 0 14m yelb-ui-5c5b8d8887-2w2t8 1/1 Terminating 0 14m When all the unnecessary pods are gone, I need to monitor the removal of the worker nodes. It may take some minutes\nThe Yelb application is back to \u0026ldquo;normal\u0026rdquo;\nNAME READY STATUS RESTARTS AGE redis-server-56d97cc8c-4h54n 1/1 Running 0 33m yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 33m yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 33m yelb-ui-5c5b8d8887-dxlth 1/1 Running 0 21m yelb-ui-5c5b8d8887-gv829 1/1 Running 0 21m Checking the autoscaler status now, it has identified a candidate to scale down. But as I have sat this AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026ldquo;10m\u0026rdquo; I will need to wait 10 minutes after LastTransitionTime \u0026hellip;\nName: cluster-autoscaler-status Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 14:19:46.985695728 +0000 UTC Data ==== status: ---- Cluster-autoscaler status at 2023-09-08 14:19:46.985695728 +0000 UTC: Cluster-wide: Health: Healthy (ready=3 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=3 longUnregistered=0) LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 ScaleUp: NoActivity (ready=3 registered=3) LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 ScaleDown: CandidatesPresent (candidates=1) LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 LastTransitionTime: 2023-09-08 14:18:26.989571984 +0000 UTC m=+4251.930781291 NodeGroups: Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=4)) LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 ScaleUp: NoActivity (ready=2 cloudProviderTarget=2) LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 ScaleDown: CandidatesPresent (candidates=1) LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 LastTransitionTime: 2023-09-08 14:18:26.989571984 +0000 UTC m=+4251.930781291 BinaryData ==== Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScaledUpGroup 18m cluster-autoscaler Scale-up: setting group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size to 2 instead of 1 (max: 4) Normal ScaledUpGroup 18m cluster-autoscaler Scale-up: group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size set to 2 instead of 1 (max: 4) After the 10 minutes:\nNAME STATUS ROLES AGE VERSION tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 77m v1.26.5+vmware.2 tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 81m v1.26.5+vmware.2 Back to two nodes again, and the VM has been deleted from vCenter.\nThe autoscaler status:\nName: cluster-autoscaler-status Namespace: kube-system Labels: \u0026lt;none\u0026gt; Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 14:29:32.692769073 +0000 UTC Data ==== status: ---- Cluster-autoscaler status at 2023-09-08 14:29:32.692769073 +0000 UTC: Cluster-wide: Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 ScaleUp: NoActivity (ready=2 registered=2) LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 LastTransitionTime: 2023-09-08 14:28:46.471388976 +0000 UTC m=+4871.412598145 NodeGroups: Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=1 (minSize=1, maxSize=4)) LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 ScaleUp: NoActivity (ready=1 cloudProviderTarget=1) LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 ScaleDown: NoCandidates (candidates=0) LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 LastTransitionTime: 2023-09-08 14:28:46.471388976 +0000 UTC m=+4871.412598145 BinaryData ==== Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScaledUpGroup 27m cluster-autoscaler Scale-up: setting group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size to 2 instead of 1 (max: 4) Normal ScaledUpGroup 27m cluster-autoscaler Scale-up: group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size set to 2 instead of 1 (max: 4) Normal ScaleDownEmpty 61s cluster-autoscaler Scale-down: removing empty node \u0026#34;tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc\u0026#34; Normal ScaleDownEmpty 55s cluster-autoscaler Scale-down: empty node tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc removed This works really well. Quite straight forward to enable and a really nice feature to have. And this also concludes this post.\n","date":"7 September 2023","externalUrl":null,"permalink":"/2023/09/07/tkg-autoscaler/","section":"Posts","summary":"In this post I will go through the TKG Autoscaler, how to configure it and how it works.","title":"TKG Autoscaler","type":"posts"},{"content":"","date":"6 September 2023","externalUrl":null,"permalink":"/tags/lifecycle-management/","section":"Tags","summary":"","title":"Lifecycle-Management","type":"tags"},{"content":"","date":"6 September 2023","externalUrl":null,"permalink":"/categories/lifecycle-management/","section":"Categories","summary":"","title":"Lifecycle-Management","type":"categories"},{"content":"","date":"6 September 2023","externalUrl":null,"permalink":"/tags/management/","section":"Tags","summary":"","title":"Management","type":"tags"},{"content":"","date":"6 September 2023","externalUrl":null,"permalink":"/categories/management/","section":"Categories","summary":"","title":"Management","type":"categories"},{"content":" TMC Self-Managed version 1.0.1 # Not that long ago I published an article where I went through how to deploy TMC-SM in my lab, the post can be found here. That post were based on the first release of TMC local, version 1.0. Now version 1.0.1 is out and I figured I wanted to create a post how I upgrade my current TMC local installation to the latest version 1.0.1. And who knows, maybe this will be a short and snappy post from me for a change \u0026#x1f604;\nWhats new in TMC-SM 1.0.1 # Taken from the official documentaion page here where you can find more details, like information about issues that have been resolved.\nTanzu Mission Control Self-Managed now supports deployment to and lifecycle management of the following Tanzu Kubernetes Grid clusters:\nCluster type Environment TKG 2.2.x (Kubernetes 1.25.x) vSphere 8.0 and vSphere 7.0 TKG 2.1.x (Kubernetes 1.24.x) vSphere 8.0 and vSphere 7.0 TKG 1.6.x (Kubernetes 1.23.x) vSphere 7.0 Tanzu Kubernetes Grid Service clusters running in vSphere with Tanzu (Kubernetes 1.24.x and 1.23.x) vSphere 8.0 Update 0 or Update 2 vSphere 7.0 latest update New Features and Improvements\nAdded lifecycle management support for vSphere 8\nYou can now manage Tanzu Kubernetes Grid Service clusters running in vSphere with Tanzu 8u1b. Tanzu Mission Control Self-Managed allows you to register your vSphere with Tanzu Supervisor to perform lifecycle management operations on your Tanzu Kubernetes Grid service clusters.\nAdded Terraform provider support for Tanzu Mission Control Self-Managed\nTanzu Mission Control Self-Managed can now be managed and automated using Hashicorp Terraform platform.\nThe Tanzu Mission Control provider v1.2.1 in Terraform implements support for managing your fleet of Kubernetes clusters by connecting with Tanzu Mission Control Self-Managed.\nYou can use the Tanzu Mission Control provider for Terraform to:\nConnect to Tanzu Mission Control Self-Managed.\nAttach conformant Kubernetes clusters.\nManage the lifecycle of workload clusters.\nManage cluster security using policies - access, image registry, security, network, custom, namespace quota.\nUpgrade TMC-SM to 1.0.1 # I am using the steps describing how to upgrade TMC-SM in this chapter from the official TMC documentation page here. Before executing the actual upgrade process there are some necessary steps that needs to be done first. I will go through them here in their own little chapters/sections below. I will reuse the same bootstrap machine and container registry I used in this post in all the steps described.\nDownload the latest packages # First I need to download the latest packages from the VMware Customer Connect portal here. The file I will be downloading is this:\nThis file will be landing on my laptop where I will copy it over to my bootstrap machine as soon as it is downloaded.\nandreasm:~/Downloads/TMC$ scp bundle-1.0.1.tar andreasm@10.101.10.99:/home/andreasm/tmc-sm andreasm@10.101.10.99\u0026#39;s password: bundle-1.0.1.tar 15% 735MB 5.3MB/s 13:05 ETA Extract and push images to registry # From my bootstrap machine I need to extract the newly downloaded bundle-1.0.1.tar file, and put it in a new folder:\nandreasm@linuxvm01:~/tmc-sm$ mkdir tmc-sm-1.0.1 andreasm@linuxvm01:~/tmc-sm$ tar -xf bundle-1.0.1.tar -C ./tmc-sm-1.0.1/ Then I will push them to my registry, the same registry and project used in the first installation of TMC-SM.\nandreasm@linuxvm01:~/tmc-sm/tmc-sm-1.0.1$ ./tmc-sm push-images harbor --project registry.some-domain.net/tmcproject --username username --password password After some waiting, the below should be the output if everything went successfully.\nINFO[0171] Pushing PackageRepository uri=registry.some-domain.net/tmc-project/package-repository Image Staging Complete. Next Steps: Setup Kubeconfig (if not already done) to point to cluster: export KUBECONFIG={YOUR_KUBECONFIG} Create \u0026#39;tmc-local\u0026#39; namespace: kubectl create namespace tmc-local Download Tanzu CLI from Customer Connect (If not already installed) Update TMC Self Managed Package Repository: Run: tanzu package repository add tanzu-mission-control-packages --url \u0026#34;registry.some-domain.net/tmc-project/package-repository:1.0.1\u0026#34; --namespace tmc-local Create a values based on the TMC Self Managed Package Schema: View the Values Schema: tanzu package available get \u0026#34;tmc.tanzu.vmware.com/1.0.1\u0026#34; --namespace tmc-local --values-schema Create a Values file named values.yaml matching the schema Install the TMC Self Managed Package: Run: tanzu package install tanzu-mission-control -p tmc.tanzu.vmware.com --version \u0026#34;1.0.1\u0026#34; --values-file values.yaml --namespace tmc-local I should also have a file called pushed-package-repository.json in my tmc-sm-1.0.1 folder:\nandreasm@linuxvm01:~/tmc-sm/tmc-sm-1.0.1$ ls agent-images dependencies packages pushed-package-repository.json tmc-sm The content of this file:\nandreasm@linuxvm01:~/tmc-sm/tmc-sm-1.0.1$ cat pushed-package-repository.json {\u0026#34;repositoryImage\u0026#34;:\u0026#34;registry.some-domain.net/tmc-project/package-repository\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;1.0.1\u0026#34;} This information is needed in the next step.\nUpdate tanzu package repository # This step will update the already installed tmc-sm package repository to contain version 1.0.1. Make sure to be logged into the correct context, the kubernetes cluster where the TMC installation is running before doing the below.\n#Using the information above from the pushed-package-repository file, execute the following command: tanzu package repository update tanzu-mission-control-packages --url \u0026#34;registry.some-domain.net/tmc-project/package-repository:1.0.1\u0026#34; --namespace tmc-local Waiting for package repository to be updated 11:10:20AM: Waiting for package repository reconciliation for \u0026#39;tanzu-mission-control-packages\u0026#39; 11:10:25AM: Waiting for generation 2 to be observed 11:10:29AM: Fetching | apiVersion: vendir.k14s.io/v1alpha1 | directories: | - contents: | - imgpkgBundle: | image: registry.some-domain.net/tmc-project/package-repository@sha256:89e53c26a9184580c2778a3bf08c45392e1d09773f0e8d1c22052dfb | tag: 1.0.1 | path: . | path: \u0026#34;0\u0026#34; | kind: LockConfig | 11:10:29AM: Fetch succeeded 11:10:30AM: Template succeeded 11:10:30AM: Deploy started (3s ago) 11:10:33AM: Deploying | Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; | Changes | Namespace Name Kind Age Op Op st. Wait to Rs Ri | tmc-local contour.bitnami.com.12.1.0 Package 51d delete - - ok - | ^ contour.bitnami.com.12.2.6 Package - create ??? - - - | ^ kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 Package 51d delete - - ok - | ^ kafka-topic-controller.tmc.tanzu.vmware.com.0.0.22 Package - create ??? - - - | ^ kafka.bitnami.com.22.1.3 Package 51d delete - - ok - | ^ kafka.bitnami.com.23.0.7 Package - create ??? - - - | ^ minio.bitnami.com.12.6.12 Package - create ??? - - - | ^ minio.bitnami.com.12.6.4 Package 51d delete - - ok - | ^ monitoring.tmc.tanzu.vmware.com.0.0.13 Package 51d delete - - ok - | ^ monitoring.tmc.tanzu.vmware.com.0.0.14 Package - create ??? - - - | ^ pinniped.bitnami.com.1.2.1 Package 51d delete - - ok - | ^ pinniped.bitnami.com.1.2.8 Package - create ??? - - - | ^ postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 Package 51d delete - - ok - | ^ postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.47 Package - create ??? - - - | ^ s3-access-operator.tmc.tanzu.vmware.com.0.1.22 Package 51d delete - - ok - | ^ s3-access-operator.tmc.tanzu.vmware.com.0.1.24 Package - create ??? - - - | ^ tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 Package 51d delete - - ok - | ^ tmc-local-postgres.tmc.tanzu.vmware.com.0.0.67 Package - create ??? - - - | ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 Package 51d delete - - ok - | ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.21880 Package - create ??? - - - | ^ tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 Package 51d delete - - ok - | ^ tmc-local-stack.tmc.tanzu.vmware.com.0.0.21880 Package - create ??? - - - | ^ tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 Package 51d delete - - ok - | ^ tmc-local-support.tmc.tanzu.vmware.com.0.0.21880 Package - create ??? - - - | ^ tmc.tanzu.vmware.com.1.0.0 Package 51d delete - - ok - | ^ tmc.tanzu.vmware.com.1.0.1 Package - create ??? - - - | Op: 13 create, 13 delete, 0 update, 0 noop, 0 exists | Wait to: 0 reconcile, 0 delete, 26 noop | 11:10:32AM: ---- applying 26 changes [0/26 done] ---- | 11:10:32AM: delete package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/tmc.tanzu.vmware.com.1.0.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: delete package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/minio.bitnami.com.12.6.12 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/pinniped.bitnami.com.1.2.8 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.67 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/contour.bitnami.com.12.2.6 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/monitoring.tmc.tanzu.vmware.com.0.0.14 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.47 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/s3-access-operator.tmc.tanzu.vmware.com.0.1.24 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:32AM: create package/tmc-local-support.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: create package/kafka.bitnami.com.23.0.7 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ---- waiting on 26 changes [0/26 done] ---- | 11:10:33AM: ok: noop package/kafka.bitnami.com.23.0.7 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc.tanzu.vmware.com.1.0.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/contour.bitnami.com.12.2.6 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/minio.bitnami.com.12.6.12 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/pinniped.bitnami.com.1.2.8 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.67 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/monitoring.tmc.tanzu.vmware.com.0.0.14 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.47 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/s3-access-operator.tmc.tanzu.vmware.com.0.1.24 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc-local-support.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ok: noop package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 11:10:33AM: ---- applying complete [26/26 done] ---- | 11:10:33AM: ---- waiting complete [26/26 done] ---- | Succeeded 11:10:33AM: Deploy succeeded If everything went well, lets check the package version:\nandreasm@linuxvm01:~/tanzu package repository list --namespace tmc-local NAME SOURCE STATUS tanzu-mission-control-packages (imgpkg) registry.some-domain.net/tmc-project/package-repository:1.0.1 Reconcile succeeded After the steps above, we are now ready to start the actual upgrade of the TMC-SM deployment.\nUpgrade TMC-SM deployment # To upgrade TMC execute the below command, where values.yaml is the value.yaml file I used in the previous installation:\nandreasm@linuxvm01:~/tanzu package installed update tanzu-mission-control -p tmc.tanzu.vmware.com --version \u0026#34;1.0.1\u0026#34; --values-file values.yaml --namespace tmc-local Now some output:\n11:19:14AM: Pausing reconciliation for package installation \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; 11:19:15AM: Updating secret \u0026#39;tanzu-mission-control-tmc-local-values\u0026#39; 11:19:15AM: Creating overlay secrets 11:19:15AM: Updating package install for \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; 11:19:15AM: Resuming reconciliation for package installation \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; 11:19:15AM: Waiting for PackageInstall reconciliation for \u0026#39;tanzu-mission-control\u0026#39; 11:19:15AM: Waiting for generation 9 to be observed 11:19:15AM: ReconcileFailed: kapp: Error: waiting on reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Finished unsuccessfully (Reconcile failed: (message: Expected to find at least one version, but did not (details: all=1 -\u0026gt; after-prereleases-filter=1 -\u0026gt; after-kapp-controller-version-check=1 -\u0026gt; after-constraints-filter=0))) 11:19:15AM: Error tailing app: Reconciling app: ReconcileFailed: kapp: Error: waiting on reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Finished unsuccessfully (Reconcile failed: (message: Expected to find at least one version, but did not (details: all=1 -\u0026gt; after-prereleases-filter=1 -\u0026gt; after-kapp-controller-version-check=1 -\u0026gt; after-constraints-filter=0))) 11:19:16AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:19:46AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:20:17AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:20:47AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:21:17AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:21:48AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:22:19AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:22:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:23:19AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:23:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:24:19AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:24:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:25:20AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:25:50AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:26:20AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:26:51AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:27:22AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 11:27:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: ReconcileSucceeded I did experience some error issues like the ones above Error tailing app: Reconciling app: ReconcileFailed: kapp: Error: waiting on reconcile, (Reconcile failed: (message: Expected to find at least one version, but did not\nI monitored the progress with this command:\nkubectl get pkgi -n tmc-local Which first gave me this:\nNAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE contour contour.bitnami.com 12.2.6 Reconciling 51d kafka kafka.bitnami.com 22.1.3 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d kafka-topic-controller kafka-topic-controller.tmc.tanzu.vmware.com 0.0.21 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d minio minio.bitnami.com 12.6.4 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d pinniped pinniped.bitnami.com 1.2.1 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d postgres tmc-local-postgres.tmc.tanzu.vmware.com 0.0.46 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d postgres-endpoint-controller postgres-endpoint-controller.tmc.tanzu.vmware.com 0.1.43 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d s3-access-operator s3-access-operator.tmc.tanzu.vmware.com 0.1.22 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d tanzu-mission-control tmc.tanzu.vmware.com 1.0.1 Reconciling 51d tmc-local-monitoring monitoring.tmc.tanzu.vmware.com 0.0.13 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d tmc-local-stack tmc-local-stack.tmc.tanzu.vmware.com 0.0.17161 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d tmc-local-stack-secrets tmc-local-stack-secrets.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d tmc-local-support tmc-local-support.tmc.tanzu.vmware.com 0.0.17161 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d But after a little while I tried it again and now it looked much better:\nNAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE contour contour.bitnami.com 12.2.6 Reconcile succeeded 51d kafka kafka.bitnami.com 23.0.7 Reconcile succeeded 51d kafka-topic-controller kafka-topic-controller.tmc.tanzu.vmware.com 0.0.22 Reconcile succeeded 51d minio minio.bitnami.com 12.6.12 Reconcile succeeded 51d pinniped pinniped.bitnami.com 1.2.8 Reconcile succeeded 51d postgres tmc-local-postgres.tmc.tanzu.vmware.com 0.0.67 Reconcile succeeded 51d postgres-endpoint-controller postgres-endpoint-controller.tmc.tanzu.vmware.com 0.1.47 Reconcile succeeded 51d s3-access-operator s3-access-operator.tmc.tanzu.vmware.com 0.1.24 Reconcile succeeded 51d tanzu-mission-control tmc.tanzu.vmware.com 1.0.1 Reconciling 51d tmc-local-monitoring monitoring.tmc.tanzu.vmware.com 0.0.14 Reconciling 51d tmc-local-stack tmc-local-stack.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d tmc-local-stack-secrets tmc-local-stack-secrets.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d tmc-local-support tmc-local-support.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d And if I look at the pods for the deployment:\nNAME READY STATUS RESTARTS AGE account-manager-server-dd4cb648-mhwsr 1/1 Running 2 (8m35s ago) 51d account-manager-server-dd4cb648-s6n52 1/1 Running 2 (8m34s ago) 51d agent-gateway-server-ffbd987f9-79p4v 1/1 Running 0 7m48s agent-gateway-server-ffbd987f9-7ggg4 1/1 Running 0 7m48s alertmanager-tmc-local-monitoring-tmc-local-0 2/2 Running 0 51d api-gateway-server-6ccff88f7c-5wjm7 1/1 Running 0 7m48s api-gateway-server-6ccff88f7c-c6srd 1/1 Running 0 7m48s audit-service-consumer-6665d4968-mq5k7 1/1 Running 0 7m52s audit-service-consumer-6665d4968-nbmcp 1/1 Running 0 7m52s audit-service-server-58f8cb48b-cwjd2 1/1 Running 0 7m51s audit-service-server-58f8cb48b-drgjq 1/1 Running 0 7m51s auth-manager-server-777cff744d-9whfl 1/1 Running 1 (51d ago) 51d auth-manager-server-777cff744d-hbqrl 1/1 Running 1 (51d ago) 51d auth-manager-server-777cff744d-xxq4w 1/1 Running 2 (51d ago) 51d authentication-server-555cd5b896-k7lb7 1/1 Running 0 51d authentication-server-555cd5b896-nvhtm 1/1 Running 0 51d cluster-agent-service-server-596cdb5968-5dnjz 1/1 Running 0 51d cluster-agent-service-server-596cdb5968-p2629 1/1 Running 0 51d cluster-config-server-7b5c95f48b-rsvv8 1/1 Running 2 (51d ago) 51d cluster-config-server-7b5c95f48b-t89bm 1/1 Running 2 (51d ago) 51d cluster-object-service-server-844fc87799-9jjcp 1/1 Running 0 51d cluster-object-service-server-844fc87799-mkvvv 1/1 Running 0 51d cluster-reaper-server-68b94fdcc6-l4nk4 1/1 Running 0 51d cluster-secret-server-6cdc68c88c-2ntj2 1/1 Running 1 (51d ago) 51d cluster-secret-server-6cdc68c88c-7vld7 1/1 Running 1 (51d ago) 51d cluster-service-server-76d9cc4845-mbmxj 1/1 Running 0 7m51s cluster-service-server-76d9cc4845-tl6dg 1/1 Running 0 7m51s cluster-sync-egest-5946d85c48-5zzfh 1/1 Running 0 51d cluster-sync-egest-5946d85c48-jstjl 1/1 Running 0 51d cluster-sync-ingest-b8b4b4f7b-b7t2t 1/1 Running 0 51d cluster-sync-ingest-b8b4b4f7b-vjrr7 1/1 Running 0 51d contour-contour-certgen-9nhxm 0/1 Completed 0 13m contour-contour-f99f8c554-hhl45 1/1 Running 0 13m contour-envoy-dkgmp 2/2 Running 0 12m contour-envoy-knpcp 2/2 Running 0 13m contour-envoy-mdw4l 2/2 Running 0 11m contour-envoy-w4wl6 2/2 Running 0 10m dataprotection-server-7bd8f57c9c-2b6vc 1/1 Running 0 7m51s dataprotection-server-7bd8f57c9c-bxzt2 1/1 Running 0 7m51s events-service-consumer-75d7bfbc4f-dkdgt 1/1 Running 0 51d events-service-consumer-75d7bfbc4f-hmc4c 1/1 Running 0 51d events-service-server-57cb555cc6-6tc27 1/1 Running 0 51d events-service-server-57cb555cc6-7jp92 1/1 Running 0 51d fanout-service-server-5d854fdcb9-shmsm 1/1 Running 0 51d fanout-service-server-5d854fdcb9-z4wb9 1/1 Running 0 51d feature-flag-service-server-58cb8b8967-bw8nw 1/1 Running 0 7m49s inspection-server-84fbb9f554-8kjll 2/2 Running 0 51d inspection-server-84fbb9f554-b4kwq 2/2 Running 0 51d intent-server-79db6f6cc8-5bb64 1/1 Running 0 51d intent-server-79db6f6cc8-wq46l 1/1 Running 0 51d kafka-0 1/1 Running 0 9m37s kafka-exporter-f665b6bc5-g6bfg 1/1 Running 4 (8m13s ago) 9m40s kafka-topic-controller-7745b56c4c-jxfbv 1/1 Running 0 9m57s landing-service-server-86987d87b9-rgxtj 1/1 Running 0 7m49s minio-676cfff6d6-pk5m4 1/1 Running 0 8m50s minio-provisioning-4wx4j 0/1 Completed 0 8m52s onboarding-service-server-7dfd944785-6p4qs 1/1 Running 0 7m49s onboarding-service-server-7dfd944785-fknhq 1/1 Running 0 7m49s package-deployment-server-5446696ff4-l4phd 1/1 Running 0 51d package-deployment-server-5446696ff4-w8sl4 1/1 Running 0 51d pinniped-supervisor-f44756bc7-bwtz6 1/1 Running 0 10m policy-engine-server-6455f7db8f-748mk 1/1 Running 0 51d policy-engine-server-6455f7db8f-pnpr8 1/1 Running 0 51d policy-insights-server-6cc68b7d7f-5w9c6 1/1 Running 2 (51d ago) 51d policy-sync-service-server-8687654cc9-q98bm 1/1 Running 0 7m49s policy-view-service-server-7659f84d-qxdkc 1/1 Running 0 51d policy-view-service-server-7659f84d-v95w4 1/1 Running 0 51d postgres-endpoint-controller-99987dc75-s2xzv 1/1 Running 0 9m15s postgres-postgresql-0 2/2 Running 0 9m32s prometheus-server-tmc-local-monitoring-tmc-local-0 2/2 Running 0 6m provisioner-service-server-85fb5dc6bc-7n7jh 1/1 Running 0 51d provisioner-service-server-85fb5dc6bc-lw8pm 1/1 Running 0 51d resource-manager-server-5d69d9fd88-5q97d 1/1 Running 0 7m52s resource-manager-server-5d69d9fd88-fw75m 1/1 Running 0 7m52s s3-access-operator-7ddb9d9695-l5nx4 1/1 Running 0 9m8s schema-service-schema-server-7cc9696fc5-mmv5t 1/1 Running 0 7m51s telemetry-event-service-consumer-699db98fc7-kfpht 1/1 Running 0 51d telemetry-event-service-consumer-699db98fc7-pg2xg 1/1 Running 0 51d tenancy-service-server-6db748f79-pqfzx 1/1 Running 0 7m48s ui-server-75ccd455b8-55tkg 1/1 Running 0 7m50s ui-server-75ccd455b8-nzw6f 1/1 Running 0 7m50s wcm-server-6b4f9c6-c6944 1/1 Running 0 7m51s wcm-server-6b4f9c6-wbdm9 1/1 Running 0 7m51s It looks good, all pods in a running or completed state. Now the logging into the UI:\nThats it. This concludes this post of how to upgrade TMC Self-Managed\n","date":"6 September 2023","externalUrl":null,"permalink":"/2023/09/06/tmc-self-managed-upgrade-to-1.0.1/","section":"Posts","summary":"In this post I will go through how to upgrade TMC Self-Managed from version 1.0.0 to version 1.0.1","title":"TMC Self-Managed Upgrade to 1.0.1","type":"posts"},{"content":"","date":"29 August 2023","externalUrl":null,"permalink":"/tags/availability/","section":"Tags","summary":"","title":"Availability","type":"tags"},{"content":"","date":"29 August 2023","externalUrl":null,"permalink":"/tags/tanzu-kubernetes-grid/","section":"Tags","summary":"","title":"Tanzu-Kubernetes-Grid","type":"tags"},{"content":"","date":"29 August 2023","externalUrl":null,"permalink":"/categories/tkg/","section":"Categories","summary":"","title":"TKG","type":"categories"},{"content":" Tanzu Kubernetes Grid 2.3 and availability zones # TKG 2.3 brings support for multiple availability zones (AZs) in the stable feature set. So I wanted to explore this possibility and how to configure it. I will go through the configuratuons steps needed before deployment of a new TKG management cluster and TKG workload cluster using different availability zones. This post\u0026rsquo;s primary focus is the multi availability zone feature, so I will not go into details in general TKG configurations such networking, loadbalancing as I already have a post covering a \u0026ldquo;standard\u0026rdquo; installation of TKG.\nI will start by deploying the TKG management worker nodes on two of my three vSphere clusters (Cluster-2 and 3) and control-plane nodes in my vSphere Cluster-1, just to illustrate that with TKG 2.3 I can control where the respective type of nodes will be placed. Then I will deploy a TKG workload cluster (tkg-cluster-1) using the same zone-placement as the TKG management cluster. Both the TKG management cluster deployment and first workload cluster will be using vSphere clusters as availability zones. It will end up looking like this: When I have deployed my tkg-cluster-1 (workload cluster) I will apply another zone-config using vSphere DRS host-groups and provision a second TKG workload cluster (tkg-cluster-2-hostgroups) using a zone config configured to use vSphere DRS host-groups where I will define DRS rules on Cluster-3 dividing the four hosts into two zones, something like this:\nThis post will be using a vSphere as the TKG infrastructure provider. The vSphere environment consists of 1 vCenter server, 3 vSphere clusters with 4 hosts in each ( a total of 12 ESXi hosts equally distributed across 3 vSphere clusters). All vSphere clusters are providing their own vSAN datastore local to their vSphere cluster. There is no stretched vSAN nor any datastore replication going on. NSX is the underlaying network infrastructure and NSX-ALB for all loadbalancing needs. To get started there is some steps that needs to be done in vCenter, a prepared linux jumphost/bootstrap client with necessary cli tools. So lets start with the preparations.\nPreparations # This section will cover all the needed preparations to get TKG 2.3 up and running in multiple availability zones. First out is the Linux jumphost, then vCenter configurations before doing the deployment. For more details on all requirements I dont cover in this post, head over to the offical documentation here\nLinux jumphost with necessary Tanzu CLI tools # The Linux jumphost needs to be configured with the following specifications:\nA Linux, Windows, or macOS operating system running on a physical or virtual machine that has the following hardware: At least 8 GB of RAM. VMware recommends 16 GB of RAM. A disk with 50 GB of available storage. 2 or 4 2-core CPUs. Docker installed and running. When that is sorted, log into the jumphost and start by grabbing the Tanzu CLI. This has become very easy compared to earlier. My Linux jumphost is running Ubuntu, so I just need to add the repository for the Tanzu CLI like this:\nsudo apt update sudo apt install -y ca-certificates curl gpg sudo mkdir -p /etc/apt/keyrings curl -fsSL https://packages.vmware.com/tools/keys/VMWARE-PACKAGING-GPG-RSA-KEY.pub | sudo gpg --dearmor -o /etc/apt/keyrings/tanzu-archive-keyring.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/tanzu-archive-keyring.gpg] https://storage.googleapis.com/tanzu-cli-os-packages/apt tanzu-cli-jessie main\u0026#34; | sudo tee /etc/apt/sources.list.d/tanzu.list sudo apt update sudo apt install -y tanzu-cli If not using Ubuntu, or you prefer another method of installation, read here for more options.\nThen I need to install the necessary Tanzu CLI plugins like this:\nandreasm@tkg-bootstrap:~$ tanzu plugin group get vmware-tkg/default:v2.3.0 # to list them [i] Reading plugin inventory for \u0026#34;projects.registry.vmware.com/tanzu_cli/plugins/plugin-inventory:latest\u0026#34;, this will take a few seconds. Plugins in Group: vmware-tkg/default:v2.3.0 NAME TARGET VERSION isolated-cluster global v0.30.1 management-cluster kubernetes v0.30.1 package kubernetes v0.30.1 pinniped-auth global v0.30.1 secret kubernetes v0.30.1 telemetry kubernetes v0.30.1 andreasm@tkg-bootstrap:~/.config$ tanzu plugin install --group vmware-tkg/default:v2.3.0 # to install them [i] The tanzu cli essential plugins have not been installed and are being installed now. The install may take a few seconds. [i] Installing plugin \u0026#39;isolated-cluster:v0.30.1\u0026#39; with target \u0026#39;global\u0026#39; [i] Installing plugin \u0026#39;management-cluster:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; [i] Installing plugin \u0026#39;package:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; [i] Installing plugin \u0026#39;pinniped-auth:v0.30.1\u0026#39; with target \u0026#39;global\u0026#39; [i] Installing plugin \u0026#39;secret:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; [i] Installing plugin \u0026#39;telemetry:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; [ok] successfully installed all plugins from group \u0026#39;vmware-tkg/default:v2.3.0\u0026#39; Then I need the Kubernetes CLI, \u0026ldquo;kubectl cli v1.26.5 for Linux\u0026rdquo; for TKG 2.3 which can be found here. After I have downloaded it, I copy it over to the Linux jumphost, extract it and place the binary kubectl in the folder /usr/local/bin so its in my path.\nTo verify the CLI tools and plugins are in place, I will run these commands:\n# Verify Tanzu CLI version: andreasm@tkg-bootstrap:~/.config$ tanzu version version: v1.0.0 buildDate: 2023-08-08 sha: 006d0429 # Verify Tanzu CLI plugins: andreasm@tkg-bootstrap:~/.config$ tanzu plugin list Standalone Plugins NAME DESCRIPTION TARGET VERSION STATUS isolated-cluster Prepopulating images/bundle for internet-restricted environments global v0.30.1 installed pinniped-auth Pinniped authentication operations (usually not directly invoked) global v0.30.1 installed telemetry configure cluster-wide settings for vmware tanzu telemetry global v1.1.0 installed management-cluster Kubernetes management cluster operations kubernetes v0.30.1 installed package Tanzu package management kubernetes v0.30.1 installed secret Tanzu secret management kubernetes v0.30.1 installed telemetry configure cluster-wide settings for vmware tanzu telemetry kubernetes v0.30.1 installed # verify kubectl version - look for \u0026#34;Client Version\u0026#34; andreasm@tkg-bootstrap:~/.config$ kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;26\u0026#34;, GitVersion:\u0026#34;v1.26.5+vmware.2\u0026#34;, GitCommit:\u0026#34;83112f368344a8ff6d13b89f120d5e646cd3bf19\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-06-26T06:47:19Z\u0026#34;, GoVersion:\u0026#34;go1.19.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Kustomize Version: v4.5.7 Next up is some preparations that needs to be done in vCenter.\nvSphere preparations # As vSphere is the platform where I deploy TKG there are a couple of things that needs to be done to prepare for the deployment of TKG in general but also how the different availability zones will be configured. Apart from the necessary functions such as networking and storage to support a TKG deployment.\nvCenter TKG Kubernetes OVA template # A small but important part is the OVA template to be used for the controlplane/worker nodes. I need to upload the right Kubernetes OVA template, it needs to be version 1.26.5, where I am using the latest Ubuntu 2004 Kubernetes v1.26.5 OVA. This I have downloaded from [here](the Tanzu Kubernetes Grid downloads page), uploaded it to my vCenter server, then converted it to a template, not changing anything on it, name etc.\nvSphere/vCenter availability zones # With TKG 2.3 running on vSphere there is two ways to define the availability zones. There is the option to use vSphere clusters as availability zones or vSphere DRS host-groups. This gives flexibility and the possibility to define the availability zones according to how the underlaying vSphere environment has been configured. We can potentially have many availability zones for TKG to consume, some using host-groups, some using vSphere clusters and even different vCenter servers. It all depends on the needs and where it makes sense. Regardless of using vSphere clusters or DRS host-groups we need to define define a region and a zone for TKG to use. In vCenter we need to create tag associated with a category. The category can be whatever you want to call it, they just need to be reflected correctly when defining the vSphereFailureDomain later on. You may end up with several vSphere tag-categories as this depend on the environment and how you want to use the availability zones. These tag and categories are defined a bit different between vSphere clusters and DRS host-groups. When using vSphere clusters the region is defined on the Datacenter object and the zone is the actual vsphere cluster. When using DRS host-groups the vSphere cluster is defined as the the regions and the host-group as the zone.\nvSphere DRS Host-Groups # The option to use host-groups (DRS objects) is to create \u0026ldquo;logical\u0026rdquo; zones based on host-groups/vm-groups affinity rules to place the TKG nodes in their respective host-groups inside same vSphere Cluster. This done by creating the host-groups, place the corresponding esxi hosts in the respective host-group and use vCenter Tags \u0026amp; Custom Attributes specifying tags on these objects respectively. This can be a good use case if the vSphere hosts are in the same vSphere cluster but spread across several racks. That means I can create a host-group pr rack, and define these host-groups as my availability zones for TKG to place the nodes accordingly. Lets pretend I have 12 ESXi hosts, equally divided and placed in their own rack. I can then create 3 host-groups called rack-1, rack-2 and rack-3.\nvSphere Clusters # Using the vSphere clusters option we define the vCenter Datacenter object as the region and the vSphere clusters as the zones. We define that easily by using the vCenter Tags \u0026amp; Custom Attributes specifying tags on these objects respectively. We tag the specific vSphere Datacenter to become a region and we tag the vSphere clusters to be a specific zone. In mye lab I have vSphere hosts in three different vSphere clusters. With that I have defined my vCenter Server\u0026rsquo;s only Datacenter object to be a region, and all my three vSphere clusters as three different zones within that one region. In short that means if I have only one Datacenter object in my vCenter that is a region. In this Datecenter object I have my three vSphere host clusters which will be three different zones for TKG to be aware of for potential placement of the TKG nodes.\nFor more information on multiple availability zones head over to the offical docs here.\nNext up is how configured the AZs in vCenter using vSphere clusters and DRS host-groups\nvCenter Tags - using vSphere cluster and datacenter # As I am using vSphere Clusters as my zones and vSphere Datacenter it is very straight forward. The first thing that needs to be done is to create two categories under Tags \u0026amp; Custom Attributes here:\nThe two categories is the region and zone. These two categories are created like this.\nRegion category:\nThen the Zone category:\nThe categories can also be created using a cli tool called govc like this:\nandreasm@tkg-bootstrap:~$ govc tags.category.create -t Datacenter k8s-region urn:vmomi:InventoryServiceCategory:a0248c5d-7050-4891-9635-1b5cbcb89f29:GLOBAL andreasm@tkg-bootstrap:~$ govc tags.category.create -t ClusterComputeResource k8s-zone urn:vmomi:InventoryServiceCategory:1d13b59d-1d2c-433a-b3ac-4f6528254f98:GLOBAL I should now see the categories like this in my vCenter UI:\nNow when I have created the categories, I need to create the tags using the newly created categories respectively.\nThe k8s-region category is used on the vCenter/vSphere Datacenter object. I will create a tag using the category k8s-region with some kind of meaningful name for the Datacenter object, and then attach this tag to the Datacenter object.\nCreate Datacenter Tag:\nThen attach it to the Datacenter object:\nOr using govc to attach/assign the tag:\nandreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-region wdc-region /cPod-NSXAM-WDC # There is no output after execution of this command... Next up is the tags using the k8s-zone category. I am creating three tags for this as I have three vSphere clusters I want to use a three different Availability Zones. The tags are created the same as before only using the category k8s-zone instead.\nI will end up with three tags called wdc-zone-1,wdc-zone-2, and wdc-zone-3.\nAnd here they are:\nNow I need to attach them to my vSphere clusters respectively, Cluster-1 = wdc-zone-1, Cluster-2 = wdc-zone-2 and Cluster-3 = wdc-zone-3.\nAgain, the creation of the tags and attaching them can be done using govc:\n# Creating the tags using the correct category andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone wdc-zone-1 andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone wdc-zone-2 andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone wdc-zone-3 # Attaching the tags to the respective clusters andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone wdc-zone-1 /cPod-NSXAM-WDC/host/Cluster-1 andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone wdc-zone-2 /cPod-NSXAM-WDC/host/Cluster-2 andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone wdc-zone-3 /cPod-NSXAM-WDC/host/Cluster-3 vCenter Tags - using vSphere DRS host-groups # If using vSphere DRS host-groups this how how we can confgure vCenter using DRS host-groups as availability zones for TKG. In this section I will \u0026ldquo;simulate\u0026rdquo; that my vSphere Cluster-3 with 4 ESXi hosts is equally divided into two \u0026ldquo;racks\u0026rdquo; (2 ESXi hosts in each host-group). So I will create two host-groups, to reflect two availability zones inside Cluster-3.\nFirst I need to create two host-groups where I add my corresponding ESXi hosts. Group-1 will contain ESXi-9 and ESXi-12 and Group-2 will contain ESXi-11 and ESXi-12. From the vCenter UI:\nClick add: I am naming the host-groups rack-1 and rack-2 respectively, adding two hosts in each group.\nThe two host-groups:\nWhen the host-groups have been created and defined with the esxi host membership, I need to define two DRS VM-groups. From the same place as I created the host-groups, I click add and create a VM group instead.\nI need to add a \u0026ldquo;dummy\u0026rdquo; vm to be allowed to save and create the group. This is only when creating the group via the vCenter UI.\nBoth vm groups created:\nTo create these groups with cli using govc:\n# Create Host Groups andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-1 -host esx01 esx02 [31-08-23 08:49:30] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-2 -host esx03 esx04 [31-08-23 08:48:30] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK # Create VM groups andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-1-vm-group -vm [31-08-23 08:52:00] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-2-vm-group -vm [31-08-23 08:52:04] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK Now I need to create affinity rules restricting the corresponding vm-group to only reside in the correct host-group.\nGroup-1 rule\nand group 2 rule\nNow its just creating the corresponding tag categories k8s-region and k8s-zone and the respective tags pr cluster and host-groups that have been created. Lets start by creating the categories, first from the vCenter UI then later using cli with govc. Note that these DRS host-groups are not the objects being tagged as the actual zones later on, they are just the logical boundary used in vCenter for vm placement. The ESXi hosts themselves will be the ones that are tagged with the zone tag, where the ESXi hosts are part of a host-group with a VM affinity rule.\nCategory k8s-region:\nI already have the k8s-region category from earlier, I just need to update it to also allow cluster.\nCategory k8s-zone:\nI already have the k8s-zone category from earlier, I just need to update it to also allow Host.\nThen I need to create the tag using the correct category, starting with the region tag. Create tag called room1 (in lack of own fantasy)\nThen the two tags pr zone/host-group:\nRack1\nRack2\nNow I need to attach the above tags to the correct objects in vCenter. The region tag will be used on the Cluster-3 object, the k8s-zone tags will be used on the ESXi host objects. The region room1 tag:\nThen the zone tag rack1 and rack2\nRack1\nRack2\nNow I have tagged the region and the zones, and should now have 2 availability zones for TKG to use.\nTo configure the categories, tags and attachment from cli using govc:\n# Creating the categories if not already created, if already created run the tags.category.update\randreasm@tkg-bootstrap:~$ govc tags.category.create -t ClusterComputeResource k8s-region\randreasm@tkg-bootstrap:~$ govc tags.category.create -t HostSystem k8s-zone\r# Create the region tag\randreasm@tkg-bootstrap:~$ govc tags.create -c k8s-region room1\r# Create the zone tag\randreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone rack1\randreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone rack2\r# Attach the region tag to vSphere Cluster-3\randreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-region room1 /cPod-NSXAM-WDC/host/Cluster-3\r# Attach the zone tag to the ESXi hosts\randreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack1 /cPod-NSXAM-WDC/host/Cluster-3/esxi-01.fqdn\randreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack1 /cPod-NSXAM-WDC/host/Cluster-3/esxi-02.fqdn\randreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack2 /cPod-NSXAM-WDC/host/Cluster-3/esxi-03.fqdn\randreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack2 /cPod-NSXAM-WDC/host/Cluster-3/esxi-04.fqdn Now that the necessary tags and categories have been created and assigned in vCenter, I can continue to prepare the necessary configs for TKG to use them.\nIf the categories are already in place from previous installations, and you want to add these AZs also - Create new categories as the CSI installation will fail complaining on this error: \u0026ldquo;plugin registration failed with err: rpc error: code = Internal desc = failed to retrieve topology information for Node: \u0026ldquo;\u0026rdquo;. Error: \u0026ldquo;failed to fetch topology information for the nodeVM \u0026quot;\u0026quot;. Error: duplicate values detected for category k8s-zone as \u0026quot;rack1\u0026quot; and \u0026quot;wdc-zone-3\u0026quot;\u0026rdquo;, restarting registration container.\u0026rdquo;\nThese new categories must be updated accordingly in the multi-az.yaml file and tkg-workload cluster manifest before deployment. It can also make sense to have different categories to distinguish the different environments better.\nTKG - Management Cluster # Before I can deploy a TKG management cluster I need to prepare a bootstrap yaml file and a multi-zone file so it knows about how the cluster should be configured and the availability zones. For TKG to use the tags created in vCenter we need to define these as Kubernetes FailureDomain and Deployment-Zone objects. This is done by creating a separate yaml file describing this. In this multi-zone file I need to define the region, zone and topology. The category and zone tags created in vCenter and the ones I have used in this post is only to keep it simple. We can have several categories depending on the environment you deploy it on. For more information on this head over here. Here it is also possible to define different networks and storage. A short explanation of the two CRDs in the example below: vSphereFailureDomain is where you provide the necessary information about the region/zones defined in vCenter such as the tags pr region/zone aka Datacenter/Clusters, networks and datastore. The vSphereDeploymentZone is used for placement constraints, using the vSphereFailureDomains and makes it possible mapping them using labels like I am doing below. A bit more on that later when I come to the actual deployment.\nTKG multi-az config file - using vCenter DRS host-groups # Below is the yaml file I have prepared to deploy my TKG Management cluster when using vCenter DRS host-groups as availability zones. Comments inline:\n--- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereFailureDomain metadata: name: rack1 # A name for this specifc zone, does not have to be the same as the tag used in vCenter spec: region: name: room1 # The specific tag created and assigned to the Datacenter object in vCenter type: ComputeCluster tagCategory: k8s-region # The specific tag category created earlier in vCenter zone: name: rack1 # The specific tag created and assigned to the cluster object in vCenter type: HostGroup tagCategory: k8s-zone # The specific tag category created earlier in vCenter topology: datacenter: /cPod-NSXAM-WDC # Specifies which Datacenter in vCenter computeCluster: Cluster-3 # Specifices which Cluster in vCenter hosts: vmGroupName: rack-1-vm-group # The vm group name created earlier in vCenter hostGroupName: rack-1 # The host group name created earlier in vCenter networks: - /cPod-NSXAM-WDC/network/ls-tkg-mgmt # Specify the network the nodes shall use in this region/cluster datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 # Specify the datastore the nodes shall use in this region/cluster --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereFailureDomain metadata: name: rack2 # A name for this specifc zone, does not have to be the same as the tag used in vCenter spec: region: name: room1 # The specific tag created and assigned to the Datacenter object in vCenter type: ComputeCluster tagCategory: k8s-region # The specific tag category created earlier in vCenter zone: name: rack2 # The specific tag created and assigned to the cluster object in vCenter type: HostGroup tagCategory: k8s-zone # The specific tag category created earlier in vCenter topology: datacenter: /cPod-NSXAM-WDC # Specifies which Datacenter in vCenter computeCluster: Cluster-3 # Specifices which Cluster in vCenter hosts: vmGroupName: rack-2-vm-group # The vm group name created earlier in vCenter hostGroupName: rack-2 # The host group name created earlier in vCenter networks: - /cPod-NSXAM-WDC/network/ls-tkg-mgmt # Specify the network the nodes shall use in this region/cluster datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 # Specify the datastore the nodes shall use in this region/cluster --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereDeploymentZone metadata: name: rack1 # Give the deploymentzone a name labels: region: room1 # For controlplane placement tkg-cp: allowed # For controlplane placement spec: server: vcsa.fqdn failureDomain: rack1 # Calls on the vSphereFailureDomain defined above placementConstraint: resourcePool: /cPod-NSXAM-WDC/host/Cluster-3/Resources folder: /cPod-NSXAM-WDC/vm/TKGm --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereDeploymentZone metadata: name: rack2 # Give the deploymentzone a name labels: region: room1 tkg-cp: allowed spec: server: vcsa.fqdn failureDomain: rack2 # Calls on the vSphereFailureDomain defined above placementConstraint: resourcePool: /cPod-NSXAM-WDC/host/Cluster-3/Resources folder: /cPod-NSXAM-WDC/vm/TKGm --- TKG multi-az config file - using vSphere cluster and datacenter # Below is the yaml file I have prepared to deploy my TKG Management cluster with when using vSphere clusters as availability zones. Comments inline:\n--- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereFailureDomain metadata: name: wdc-zone-1 # A name for this specifc zone, does not have to be the same as the tag used in vCenter spec: region: name: wdc-region # The specific tag created and assigned to the Datacenter object in vCenter type: Datacenter tagCategory: k8s-region # The specific tag category created earlier in vCenter zone: name: wdc-zone-1 # The specific tag created and assigned to the cluster object in vCenter type: ComputeCluster tagCategory: k8s-zone # The specific tag category created earlier in vCenter topology: datacenter: /cPod-NSXAM-WDC # Specifies which Datacenter in vCenter computeCluster: Cluster-1 # Specifices which Cluster in vCenter networks: - /cPod-NSXAM-WDC/network/ls-tkg-mgmt # Specify the network the nodes shall use in this region/cluster datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 # Specify the datastore the nodes shall use in this region/cluster --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereFailureDomain metadata: name: wdc-zone-2 spec: region: name: wdc-region type: Datacenter tagCategory: k8s-region zone: name: wdc-zone-2 type: ComputeCluster tagCategory: k8s-zone topology: datacenter: /cPod-NSXAM-WDC computeCluster: Cluster-2 networks: - /cPod-NSXAM-WDC/network/ls-tkg-mgmt datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-02 --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereFailureDomain metadata: name: wdc-zone-3 spec: region: name: wdc-region type: Datacenter tagCategory: k8s-region zone: name: wdc-zone-3 type: ComputeCluster tagCategory: k8s-zone topology: datacenter: /cPod-NSXAM-WDC computeCluster: Cluster-3 networks: - /cPod-NSXAM-WDC/network/ls-tkg-mgmt datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereDeploymentZone metadata: name: wdc-zone-1 # A name for the DeploymentZone. Does not have to be the same as above labels: region: wdc-region # A specific label to be used for placement restriction, allowing flexibility of node placement. tkg-cp: allowed # A specific label to be used for placement restriction, allowing flexibility of node placement. spec: server: vcsa.fqdn # Specifies the vCenter IP or FQDN failureDomain: wdc-zone-1 # Calls on the respective vSphereFailureDomain defined above placementConstraint: resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources # Specify which ResourcePool or Cluster directly folder: /cPod-NSXAM-WDC/vm/TKGm # Specify which folder in vCenter to use for node placement --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereDeploymentZone metadata: name: wdc-zone-2 labels: region: wdc-region tkg-cp: allowed worker: allowed spec: server: vcsa.fqdn failureDomain: wdc-zone-2 placementConstraint: resourcePool: /cPod-NSXAM-WDC/host/Cluster-2/Resources folder: /cPod-NSXAM-WDC/vm/TKGm --- apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 kind: VSphereDeploymentZone metadata: name: wdc-zone-3 labels: region: wdc-region tkg-cp: allowed worker: allowed spec: server: vcsa.fqdn failureDomain: wdc-zone-3 placementConstraint: resourcePool: /cPod-NSXAM-WDC/host/Cluster-3/Resources folder: /cPod-NSXAM-WDC/vm/TKGm The multi-az yaml file above is now ready to be used deploying my TKG mgmt cluster in a multi-az environment using vSphere Clusters as the Availability Zones. I have added a label in my multi-az configuration under the vSphereDeploymentZones: tkg-cp: allowed. This custom label is used for the placement of the TKG controlplane nodes. I only want the worker nodes to be placed on AZ-2 and AZ-3 (wdc-zone-2 and wdc-zone-3) where AZ-1 or wdc-zone-1 is only for Control Plane node placement. This is one usecase for the vSphereDeploymentZone, placement constraints. Using these labels to specify the control-plane placement. The worker nodes placement for both TKG management cluster and workload cluster is defined in the bootstrap yaml or the cluster-class manifest for workload cluster.\nTKG bootstrap yaml - common for both vSphere cluster and DRS host-groups # In additon to the regular settings that is needed in the bootstrap yaml file I need to add these lines to take into consideration the Availability zones.\n#! --------------------------------------------------------------------- #! Multi-AZ configuration #! --------------------------------------------------------------------- USE_TOPOLOGY_CATEGORIES: \u0026#34;true\u0026#34; VSPHERE_REGION: k8s-region VSPHERE_ZONE: k8s-zone VSPHERE_AZ_0: wdc-zone-2 # Here I am defining the zone placement for the workers that ends with md-0 VSPHERE_AZ_1: wdc-zone-3 # Here I am defining the zone placement for the workers that ends with md-1 VSPHERE_AZ_2: wdc-zone-3 # Here I am defining the zone placement for the workers that ends with md-2 VSPHERE_AZ_CONTROL_PLANE_MATCHING_LABELS: \u0026#34;region=wdc-region,tkg-cp=allowed\u0026#34; #This defines and uses the vSphereDeploymentsZone labels I have added to instruct the control-plane node placement Note! The Zone names under vSphere_AZ_0-2 needs to reflect the correct zone tag/label used in your corresponding multi-az.yaml file pr vSphereDeploymentZone. The same goes for the VSPHERE_AZ_CONTROL_PLANE_MATCHING_LABELS: the values needs to reflect the labels used/added.\nNow my full bootstrap.yaml below:\n#! --------------- #! Basic config #! ------------- CLUSTER_NAME: tkg-wdc-az-mgmt CLUSTER_PLAN: prod INFRASTRUCTURE_PROVIDER: vsphere ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; CLUSTER_CIDR: 100.96.0.0/11 SERVICE_CIDR: 100.64.0.0/13 TKG_IP_FAMILY: ipv4 DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; #! --------------- #! vSphere config #! ------------- VSPHERE_DATACENTER: /cPod-NSXAM-WDC VSPHERE_DATASTORE: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 VSPHERE_FOLDER: /cPod-NSXAM-WDC/vm/TKGm VSPHERE_INSECURE: \u0026#34;false\u0026#34; VSPHERE_NETWORK: /cPod-NSXAM-WDC/network/ls-tkg-mgmt VSPHERE_CONTROL_PLANE_ENDPOINT: \u0026#34;\u0026#34; VSPHERE_PASSWORD: \u0026#34;password\u0026#34; VSPHERE_RESOURCE_POOL: /cPod-NSXAM-WDC/host/Cluster-1/Resources VSPHERE_SERVER: vcsa.fqdn VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa VSPHERE_TLS_THUMBPRINT: F:::::::::E VSPHERE_USERNAME: andreasm@vsphereSSOdomain.net #! --------------------------------------------------------------------- #! Multi-AZ configuration #! --------------------------------------------------------------------- USE_TOPOLOGY_CATEGORIES: \u0026#34;true\u0026#34; VSPHERE_REGION: k8s-region VSPHERE_ZONE: k8s-zone VSPHERE_AZ_0: wdc-zone-2 VSPHERE_AZ_1: wdc-zone-3 VSPHERE_AZ_2: wdc-zone-3 VSPHERE_AZ_CONTROL_PLANE_MATCHING_LABELS: \u0026#34;region=wdc-region,tkg-cp=allowed\u0026#34; AZ_FILE_PATH: /home/andreasm/tanzu-v-2.3/multi-az/multi-az.yaml #! --------------- #! Node config #! ------------- OS_ARCH: amd64 OS_NAME: ubuntu OS_VERSION: \u0026#34;20.04\u0026#34; VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; #CONTROL_PLANE_MACHINE_COUNT: 3 #WORKER_MACHINE_COUNT: 3 #! --------------- #! Avi config #! ------------- AVI_CA_DATA_B64: BASE64ENC AVI_CLOUD_NAME: wdc-1-nsx AVI_CONTROL_PLANE_HA_PROVIDER: \u0026#34;true\u0026#34; AVI_CONTROLLER: 172.21.101.50 # Network used to place workload clusters\u0026#39; endpoint VIPs AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 AVI_CONTROL_PLANE_NETWORK_CIDR: 10.101.114.0/24 # Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) AVI_DATA_NETWORK: vip-tkg-wld-l7 AVI_DATA_NETWORK_CIDR: 10.101.115.0/24 # Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.101.113.0/24 AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 # Network used to place management clusters\u0026#39; endpoint VIPs AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.101.112.0/24 AVI_NSXT_T1LR: Tier-1 AVI_CONTROLLER_VERSION: 22.1.2 AVI_ENABLE: \u0026#34;true\u0026#34; AVI_LABELS: \u0026#34;\u0026#34; AVI_PASSWORD: \u0026#34;password\u0026#34; AVI_SERVICE_ENGINE_GROUP: nsx-se-generic-group AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: nsx-se-generic-group AVI_USERNAME: admin AVI_DISABLE_STATIC_ROUTE_SYNC: true AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true AVI_INGRESS_SHARD_VS_SIZE: SMALL AVI_INGRESS_SERVICE_TYPE: NodePortLocal #! --------------- #! Proxy config #! ------------- TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; #! --------------------------------------------------------------------- #! Antrea CNI configuration #! --------------------------------------------------------------------- ANTREA_NODEPORTLOCAL: true ANTREA_PROXY: true ANTREA_ENDPOINTSLICE: true ANTREA_POLICY: true ANTREA_TRACEFLOW: true ANTREA_NETWORKPOLICY_STATS: false ANTREA_EGRESS: true ANTREA_IPAM: false ANTREA_FLOWEXPORTER: false ANTREA_SERVICE_EXTERNALIP: false ANTREA_MULTICAST: false #! --------------------------------------------------------------------- #! Machine Health Check configuration #! --------------------------------------------------------------------- ENABLE_MHC: \u0026#34;true\u0026#34; ENABLE_MHC_CONTROL_PLANE: true ENABLE_MHC_WORKER_NODE: true MHC_UNKNOWN_STATUS_TIMEOUT: 5m MHC_FALSE_STATUS_TIMEOUT: 12m One last thing to do before heading over to the actual deployment is to add the following to my current Linux jumphost session:\nandreasm@tkg-bootstrap:~$ export SKIP_MULTI_AZ_VERIFY=\u0026#34;true\u0026#34; This is needed as there is no mgmt cluster yet, and there is no way for anything that\u0026rsquo;s not there to verify anything \u0026#x1f63a;\nTKG Deployment # Now that I have done all the needed preparations it is time to do the actual deployment and see if my Availability Zones are being used as I wanted. When the TKG management cluster has been deployed I should end up with all the Control Plane (3) nodes distributed across all my 3 AZs. Then the worker nodes should only be placed in the AZ-2 and AZ-3.\nTKG Mgmt Cluster deployment with multi-availability-zones # From my Linux jumphost where I have all the CLI tools in place I am now ready to execute the following command to deploy the mgmt cluster. 3 Control Plane Nodes and 3 Worker Nodes.\nandreasm@tkg-bootstrap:~$ tanzu mc create -f my-tkg-mgmt-bootstrap.yaml --az-file my-multi-az-file.yaml Validating the pre-requisites... vSphere 8 with Tanzu Detected. You have connected to a vSphere 8 with Tanzu environment that includes an integrated Tanzu Kubernetes Grid Service which turns a vSphere cluster into a platform for running Kubernetes workloads in dedicated resource pools. Configuring Tanzu Kubernetes Grid Service is done through the vSphere HTML5 Client. Tanzu Kubernetes Grid Service is the preferred way to consume Tanzu Kubernetes Grid in vSphere 8 environments. Alternatively you may deploy a non-integrated Tanzu Kubernetes Grid instance on vSphere 8. Deploying TKG management cluster on vSphere 8 ... Identity Provider not configured. Some authentication features won\u0026#39;t work. Using default value for CONTROL_PLANE_MACHINE_COUNT = 3. Reason: CONTROL_PLANE_MACHINE_COUNT variable is not set Using default value for WORKER_MACHINE_COUNT = 3. Reason: WORKER_MACHINE_COUNT variable is not set Setting up management cluster... Validating configuration... Using infrastructure provider vsphere:v1.7.0 Generating cluster configuration... Setting up bootstrapper... Sit back and enjoy while the kind cluster is being deployed locally and hopefully provisioned in your vCenter server..\nWhen you see the below: Start creating management cluster something should start to happen in the vCenter server.\nManagement cluster config file has been generated and stored at: \u0026#39;/home/andreasm/.config/tanzu/tkg/clusterconfigs/tkg-wdc-az-mgmt.yaml\u0026#39; Start creating management cluster... And by just clicking on the respective TKG VM so far I can see that they are respecting my zone placement.\nThat is very well. Now just wait for the two last control plane nodes also.\nYou can now access the management cluster tkg-wdc-az-mgmt by running \u0026#39;kubectl config use-context tkg-wdc-az-mgmt-admin@tkg-wdc-az-mgmt\u0026#39; Management cluster created! You can now create your first workload cluster by running the following: tanzu cluster create [name] -f [file] Some addons might be getting installed! Check their status by running the following: kubectl get apps -A Exciting, lets have a look at the control plane nodes placement:\nThey have been distributed across my three cluster as wanted. Perfect.\nNow next step is to deploy a workload cluster to achieve the same placement constraints there.\nAdding or adjusting the vSphereFailureDomains and vSphereDeploymentZone # In my TKG management cluster deployment above I have used the vSphereFailureDomains and vSphereDeploymentZones for vSphere clusters as my availability zones. If I want to have workload clusters deployed in other availability zones, different zones or even new zones I can add these to the management cluster. In the example below I will add the availability zones configured to use vCenter DRS host-groups using the vsphere-zones yaml config here.\nTo check which zones are available for the management cluster:\n# vSphereFailureDomains andreasm@tkg-bootstrap:~$ k get vspherefailuredomains.infrastructure.cluster.x-k8s.io -A NAME AGE wdc-zone-1 24h wdc-zone-2 24h wdc-zone-3 24h # vSphereDeploymentZones andreasm@tkg-bootstrap:~$ k get vspheredeploymentzones.infrastructure.cluster.x-k8s.io -A NAME AGE wdc-zone-1 24h wdc-zone-2 24h wdc-zone-3 24h # I have defined both FailureDomains and DeploymentsZone with the same name Now, let me add the DRS host-groups zones.\nandreasm@tkg-bootstrap:~$ kubectl apply -f multi-az-host-groups.yaml #The file containing host-groups definition vspherefailuredomain.infrastructure.cluster.x-k8s.io/rack1 created vspherefailuredomain.infrastructure.cluster.x-k8s.io/rack2 created vspheredeploymentzone.infrastructure.cluster.x-k8s.io/rack1 created vspheredeploymentzone.infrastructure.cluster.x-k8s.io/rack2 created # Or andreasm@tkg-bootstrap:~$ tanzu mc az set -f multi-az-host-groups.yaml # This command actually validate the settings if the export SKIP_MULTI_AZ_VERIFY=\u0026#34;true\u0026#34; is not set ofcourse Now check the failuredomains and placementzones:\nandreasm@tkg-bootstrap:~$ k get vspherefailuredomains.infrastructure.cluster.x-k8s.io -A NAME AGE rack1 13s rack2 13s wdc-zone-1 24h wdc-zone-2 24h wdc-zone-3 24h andreasm@tkg-bootstrap:~$ k get vspheredeploymentzones.infrastructure.cluster.x-k8s.io -A NAME AGE rack1 2m44s rack2 2m44s wdc-zone-1 24h wdc-zone-2 24h wdc-zone-3 24h andreasm@tkg-bootstrap:~$ tanzu mc available-zone list -a AZNAME ZONENAME ZONETYPE REGIONNAME REGIONTYPE DATASTORE NETWORK OWNERCLUSTER STATUS rack1 rack1 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt not ready rack2 rack2 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt not ready wdc-zone-1 wdc-zone-1 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready wdc-zone-2 wdc-zone-2 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-02 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready wdc-zone-3 wdc-zone-3 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready I had the variable export SKIP_MULTI_AZ_VERIFY=\u0026ldquo;true\u0026rdquo; set so it did not validate my settings and just applied it. Therefore I have the two new zones/AZs in a not ready state. Deleting them, updated the config so it was correct. Sat the export SKIP_MULTI_AZ_VERIFY=\u0026ldquo;false\u0026rdquo;. Reapplied using the mc set command it came out ready:\nandreasm@tkg-bootstrap:~$ tanzu mc az set -f multi-az-host-groups.yaml andreasm@tkg-bootstrap:~$ tanzu mc available-zone list -a AZNAME ZONENAME ZONETYPE REGIONNAME REGIONTYPE DATASTORE NETWORK OWNERCLUSTER STATUS rack1 rack1 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready rack2 rack2 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready wdc-zone-1 wdc-zone-1 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready wdc-zone-2 wdc-zone-2 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-02 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready wdc-zone-3 wdc-zone-3 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready TKG Workload Cluster deployment with multi-availability-zones - using vSphere clusters as AZs # At this stage I will use the values already provided earlier, like the multi-az config, network, vCenter folder placements and so on. If I like I could have added a specific multi-az file for the workload cluster, changed the network settings, folder etc. But I am just using the config already in place from the management cluster for now.\nWe have already done the hard work. So the workload cluster deployment is now more or less walk in the park. To generate the necessary workload-cluster yaml definition I execute the following command (this will accommodate the necessary AZs setting, so if you already have a class-based cluster yaml file from previous TKG clusters make sure to add this or just run the command below):\nandreasm@tkg-bootstrap:~$ tanzu cluster create tkg-cluster-1 --namespace tkg-ns-1 --file tkg-mgmt-bootstrap-for-wld.az.yaml --dry-run \u0026gt; workload-cluster/tkg-cluster-1.yaml # tanzu cluster create tkg-cluster-1 gives the cluster the name tkg-cluster-1 # --namespace is the namespace I have created in my management cluster to place this workload cluster in # --file points to the bootstrap.yaml file used to deploy the management cluster # --dry-run \u0026gt; generates my workload-cluster.yaml file called tkg-cluster-1.yaml under the folder workload-cluster This process convert the flat bootstrap.yaml file to a cluster-class config-file.\nThe most interesting part in this file is whether the placement constraints have been considered. Lets have a look:\n- name: controlPlane value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: worker value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: controlPlaneZoneMatchingLabels # check value: region: k8s-region # check - will place my control planes only in the zones with the correct label tkg-cp: allowed # check - will place my control planes only in the zones with the correct label - name: security value: fileIntegrityMonitoring: enabled: false imagePolicy: pullAlways: false webhook: enabled: false spec: allowTTL: 50 defaultAllow: true denyTTL: 60 retryBackoff: 500 kubeletOptions: eventQPS: 50 streamConnectionIdleTimeout: 4h0m0s systemCryptoPolicy: default version: v1.26.5+vmware.2-tkg.1 workers: machineDeployments: - class: tkg-worker failureDomain: wdc-zone-2 # check - worker in zone-2 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 replicas: 1 strategy: type: RollingUpdate - class: tkg-worker failureDomain: wdc-zone-3 # check - worker in zone-3 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-1 replicas: 1 strategy: type: RollingUpdate - class: tkg-worker failureDomain: wdc-zone-3 # check - worker in zone-3 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-2 replicas: 1 strategy: type: RollingUpdate The file looks good. Lets deploy it.\nandreasm@tkg-bootstrap:~$ tanzu cluster create --file tkg-cluster-1.yaml Validating configuration... cluster class based input file detected, getting tkr version from input yaml input TKR Version: v1.26.5+vmware.2-tkg.1 TKR Version v1.26.5+vmware.2-tkg.1, Kubernetes Version v1.26.5+vmware.2-tkg.1 configured Warning: Pinniped configuration not found; Authentication via Pinniped will not be set up in this cluster. If you wish to set up Pinniped after the cluster is created, please refer to the documentation. Skip checking VIP overlap when the VIP is empty. Cluster\u0026#39;s endpoint VIP will be allocated by NSX ALB IPAM. creating workload cluster \u0026#39;tkg-cluster-1\u0026#39;... After a couple of minutes or cups of coffee (depends on your environment):\nWorkload cluster \u0026#39;tkg-cluster-1\u0026#39; created Now lets go the same with the nodes here also, where are they placed in my vSphere environment.\nControl plane nodes placement:\nWorker nodes placemement:\nNice, just according to plan.\nTKG Workload Cluster deployment with multi-availability-zones - using vCenter host-groups as AZs # In the first workload cluster deployment above I deployed the cluster to use my availability zones configured to use vSphere clusters as AZs. Now I will deploy a second workload cluster using the zones here added after the TKG management cluster was deployed. I will just reuse the workload-cluster.yaml from the first cluster, edit the names, namespaces and zones/regions accordingly.\nLets deploy it:\nandreasm@tkg-bootstrap:~$ tanzu cluster create --file tkg-cluster-2-host-groups.yaml Validating configuration... cluster class based input file detected, getting tkr version from input yaml input TKR Version: v1.26.5+vmware.2-tkg.1 TKR Version v1.26.5+vmware.2-tkg.1, Kubernetes Version v1.26.5+vmware.2-tkg.1 configured Warning: Pinniped configuration not found; Authentication via Pinniped will not be set up in this cluster. If you wish to set up Pinniped after the cluster is created, please refer to the documentation. Skip checking VIP overlap when the VIP is empty. Clusters endpoint VIP will be allocated by NSX ALB IPAM. creating workload cluster \u0026#39;tkg-cluster-2-hostgroup\u0026#39;... waiting for cluster to be initialized... cluster control plane is still being initialized: ScalingUp waiting for cluster nodes to be available... unable to get the autoscaler deployment, maybe it is not exist waiting for addons core packages installation... Workload cluster \u0026#39;tkg-cluster-2-hostgroup\u0026#39; created This cluster should now only be deployed to Cluster 3, using the DRS host-group based Availability Zones.\nAnd here the cluster has been deployed in Cluster-3, using the AZs rack1 and rack2 (nodes tkg-cluster-2-hostgroup-xxxx)\nNext up is to deploy a test application in the workload cluster utilizing the availability zones.\nDeploy applications on workload cluster in a multi-az environment # In my scenario so far I have placed all the control plane nodes evenly distributed across all the vSphere Clusters/AZs. The worker nodes on the other hand is placed only on AZ-2 and AZ-3. Now I want to deploy an application in my workload cluster where I do a placement decision on where the different pods will be placed, according to the available zones the workload cluster is in.\nApplication/pod placement using nodeAffinity # When a TKG workload cluster or TKG management cluster has been deployed with availability zones, the nodes will get updated label information that is referencing the availability zone the worker and control-plane nodes have been deployed in. This information can be used if one want to deploy the application in a specific zone. So in this chapter I will do exactly that. Deploy an application consisting of 4 pods, define the placement of the pods using the zone information available on the nodes.\nTo find the labels for the different placements I need to have a look at the nodes. There should be some labels indicating where they reside. I will clean up the output as I am only looking for something that starts with topology and define the zones, and the worker nodes only. So the output below gives me this:\nandreasm@tkg-bootstrap:~$ k get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj Ready \u0026lt;none\u0026gt; 82m v1.26.5+vmware.2 topology.kubernetes.io/zone=wdc-zone-2 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df Ready \u0026lt;none\u0026gt; 82m v1.26.5+vmware.2 topology.kubernetes.io/zone=wdc-zone-3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h Ready \u0026lt;none\u0026gt; 82m v1.26.5+vmware.2 topology.kubernetes.io/zone=wdc-zone-3 One can also see the failuredomain placement using the following commands:\nandreasm@tkg-bootstrap:~$ kubectl get machinedeployment -n tkg-ns-1 -o=custom-columns=NAME:.metadata.name,FAILUREDOMAIN:.spec.template.spec.failureDomain NAME FAILUREDOMAIN tkg-cluster-1-md-0-b4pfl wdc-zone-2 tkg-cluster-1-md-1-vfzhk wdc-zone-3 tkg-cluster-1-md-2-rpk4z wdc-zone-3 andreasm@tkg-bootstrap:~$ kubectl get machine -n tkg-ns-1 -o=custom-columns=NAME:.metadata.name,FAILUREDOMAIN:.spec.failureDomain NAME FAILUREDOMAIN tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj wdc-zone-2 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df wdc-zone-3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h wdc-zone-3 tkg-cluster-1-znr8h-4v2tm wdc-zone-2 tkg-cluster-1-znr8h-d72g5 wdc-zone-1 tkg-cluster-1-znr8h-j6899 wdc-zone-3 Now I need to update my application deployment adding a section where I can define the placement information, this is done by using affinity.\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-X My example application has been updated with the relevant information below. With this deployment I am allowing deployment in the wdc-zone-2 and wdc-zone-3 for the yelb-ui deployment, the 3 other deployments are only allowed to be placed in wdc-zone-3.\napiVersion: v1 kind: Service metadata: name: redis-server labels: app: redis-server tier: cache namespace: yelb spec: type: ClusterIP ports: - port: 6379 selector: app: redis-server tier: cache --- apiVersion: v1 kind: Service metadata: name: yelb-db labels: app: yelb-db tier: backenddb namespace: yelb spec: type: ClusterIP ports: - port: 5432 selector: app: yelb-db tier: backenddb --- apiVersion: v1 kind: Service metadata: name: yelb-appserver labels: app: yelb-appserver tier: middletier namespace: yelb spec: type: ClusterIP ports: - port: 4567 selector: app: yelb-appserver tier: middletier --- apiVersion: v1 kind: Service metadata: name: yelb-ui labels: app: yelb-ui tier: frontend namespace: yelb spec: loadBalancerClass: ako.vmware.com/avi-lb type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 80 selector: app: yelb-ui tier: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-ui namespace: yelb spec: selector: matchLabels: app: yelb-ui replicas: 1 template: metadata: labels: app: yelb-ui tier: frontend spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-2 - wdc-zone-3 containers: - name: yelb-ui image: registry.guzware.net/yelb/yelb-ui:0.3 imagePullPolicy: Always ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-server namespace: yelb spec: selector: matchLabels: app: redis-server replicas: 1 template: metadata: labels: app: redis-server tier: cache spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-3 containers: - name: redis-server image: registry.guzware.net/yelb/redis:4.0.2 ports: - containerPort: 6379 --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-db namespace: yelb spec: selector: matchLabels: app: yelb-db replicas: 1 template: metadata: labels: app: yelb-db tier: backenddb spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-3 containers: - name: yelb-db image: registry.guzware.net/yelb/yelb-db:0.3 ports: - containerPort: 5432 --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-appserver namespace: yelb spec: selector: matchLabels: app: yelb-appserver replicas: 1 template: metadata: labels: app: yelb-appserver tier: middletier spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-3 containers: - name: yelb-appserver image: registry.guzware.net/yelb/yelb-appserver:0.3 ports: - containerPort: 4567 Now to apply it and check the outcome.\nandreasm@tkg-bootstrap:~$ k get pods -n yelb -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-5997cbfdf7-f7wgh 1/1 Running 0 10m 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-6d65cc8-xt82g 1/1 Running 0 10m 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-7d4c56597f-58zd4 1/1 Running 0 10m 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 10m 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If I compare this to the nodes below:\nNAME FAILUREDOMAIN tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj wdc-zone-2 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df wdc-zone-3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h wdc-zone-3 So far eveything looks good. All pods have been deployed in wdc-zone-3. The ui-pod is also allowed to be placed in wdc-zone-2. What happens if I scale it up with a couple of pods.\nandreasm@tkg-bootstrap:~$ k scale deployment -n yelb --replicas 5 yelb-ui deployment.apps/yelb-ui scaled NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-5997cbfdf7-f7wgh 1/1 Running 0 22m 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-6d65cc8-xt82g 1/1 Running 0 22m 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-7d4c56597f-58zd4 1/1 Running 0 22m 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-59zqs 1/1 Running 0 5m2s 100.96.1.5 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-8w48g 1/1 Running 0 5m2s 100.96.2.4 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-mprxx 1/1 Running 0 5m2s 100.96.1.4 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-n9slz 1/1 Running 0 5m2s 100.96.3.19 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 22m 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Two of the ui-pods has been placed in wdc-zone-2\nNow that I have full control of the app placement, lets test some failure scenarios.\nFailure simulations # In this chapter I will quickly simulate an outage of Zone-2 or vSphere Cluster-2 where the 1 of the TKG mgmt control plane is residing, 1 of the workload Cluster-1 is residing plus 1 worker node for both the management cluster and tkg-cluster-1:\n# TKG management cluster placement in my vSphere environment Nodes in the cluster with the \u0026#39;node.cluster.x-k8s.io/esxi-host\u0026#39; label: Node: tkg-wdc-az-mgmt-hgt2v-hb7vj | ESXi Host: esx04.cpod-nsxam-wdc #controlplane node on Zone-1 Node: tkg-wdc-az-mgmt-hgt2v-w6r64 | ESXi Host: esx03.cpod-nsxam-wdc-03 #controlplane node on Zone-3 Node: tkg-wdc-az-mgmt-hgt2v-zl5k9 | ESXi Host: esx04.cpod-nsxam-wdc-02 #controlplane node on Zone-2 Node: tkg-wdc-az-mgmt-md-0-xn6cg-79f97555c7x45h4b-6ghbg | ESXi Host: esx04.cpod-nsxam-wdc-02 #worker node on Zone 2 Node: tkg-wdc-az-mgmt-md-1-zmr4d-56ff586997xxndn8-hzs7f | ESXi Host: esx01.cpod-nsxam-wdc-03 #worker node on Zone-3 Node: tkg-wdc-az-mgmt-md-2-67dm4-64f79b7dd7x6f56s-76qhv | ESXi Host: esx02.cpod-nsxam-wdc-03 #worker node on Zone-3 # TKG workload cluster (tkg-cluster-1) placement in my vSphere environment Nodes in the cluster with the \u0026#39;node.cluster.x-k8s.io/esxi-host\u0026#39; label: Node: tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj | ESXi Host: esx03.cpod-nsxam-wdc-02 #worker node on Zone-2 Node: tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df | ESXi Host: esx02.cpod-nsxam-wdc-03 #worker node on Zone-3 Node: tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h | ESXi Host: esx03.cpod-nsxam-wdc-03 #worker node on Zone-3 Node: tkg-cluster-1-znr8h-4v2tm | ESXi Host: esx03.cpod-nsxam-wdc-02 #controlplane node on Zone-2 Node: tkg-cluster-1-znr8h-d72g5 | ESXi Host: esx04.cpod-nsxam-wdc.az-wdc #controlplane node on Zone-1 Node: tkg-cluster-1-znr8h-j6899 | ESXi Host: esx04.cpod-nsxam-wdc-03.az-wdc #controlplane node on Zone-3 The Yelb application pods placement:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-5997cbfdf7-f7wgh 1/1 Running 0 2d22h 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-6d65cc8-xt82g 1/1 Running 0 2d22h 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-7d4c56597f-58zd4 1/1 Running 0 2d22h 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-59zqs 1/1 Running 0 2d22h 100.96.1.5 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-8w48g 1/1 Running 0 2d22h 100.96.2.4 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-mprxx 1/1 Running 0 2d22h 100.96.1.4 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-n9slz 1/1 Running 0 2d22h 100.96.3.19 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 2d22h 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; What I want to achieve is an available TKG mgmt cluster control plane, TKG workload tkg-cluster-1 control plane, and the Yelb application still up and running. The simple test I will do is to just go into vCenter power off all the nodes for these TKG cluster in Zone-2/vSphere Cluster-2. I could also shutdown the whole ESXi hosts but I have other services running there depending on this cluster also.\nOne last observation to see status of the two control planes, or K8s API endpoints is from my NSX-ALB dashboard for both TKG mgmt cluster and TKG-cluster-1.\nThe TKG mgmt cluster controlplanes/k8s-api endpoint:\nBefore shutting down I have full access to both the mgmt and tkg-cluster-1 k8s api, and the yelb-app ui is accessible.\nNow, powering off the nodes:\nSome observations after power off:\nFrom NSX-ALB\nYelb app is still available:\nIs the k8s api available?\nNAME STATUS ROLES AGE VERSION tkg-wdc-az-mgmt-hgt2v-hb7vj Ready control-plane 4d23h v1.26.5+vmware.2 tkg-wdc-az-mgmt-hgt2v-w6r64 Ready control-plane 4d23h v1.26.5+vmware.2 tkg-wdc-az-mgmt-hgt2v-zl5k9 NotReady control-plane 4d23h v1.26.5+vmware.2 tkg-wdc-az-mgmt-md-0-xn6cg-79f97555c7x45h4b-6ghbg NotReady \u0026lt;none\u0026gt; 4d23h v1.26.5+vmware.2 tkg-wdc-az-mgmt-md-1-zmr4d-56ff586997xxndn8-hzs7f Ready \u0026lt;none\u0026gt; 4d23h v1.26.5+vmware.2 tkg-wdc-az-mgmt-md-2-67dm4-64f79b7dd7x6f56s-76qhv Ready \u0026lt;none\u0026gt; 4d23h v1.26.5+vmware.2 The management cluster is, though complaining on the two nodes above as not ready. (They are powered off). The workload cluster k8s api available?\nNAME STATUS ROLES AGE VERSION\rtkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2\rtkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2\rtkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2\rtkg-cluster-1-znr8h-4v2tm NotReady control-plane 4d21h v1.26.5+vmware.2\rtkg-cluster-1-znr8h-d72g5 Ready control-plane 4d21h v1.26.5+vmware.2\rtkg-cluster-1-znr8h-j6899 Ready control-plane 4d22h v1.26.5+vmware.2 It is. though a control plane node is down.\nThe Yelb pods:\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-5997cbfdf7-f7wgh 1/1 Running 0 2d22h 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-6d65cc8-xt82g 1/1 Running 0 2d22h 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-7d4c56597f-58zd4 1/1 Running 0 2d22h 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-59zqs 1/1 Running 1 (6m24s ago) 2d22h 100.96.1.5 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-8w48g 1/1 Running 0 2d22h 100.96.2.4 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-mprxx 1/1 Running 1 (6m23s ago) 2d22h 100.96.1.2 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-n9slz 1/1 Running 0 2d22h 100.96.3.19 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 2d22h 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; So it seems everything is still reacheable still after loosing Zone-2. When I did power off the nodes the first time it took around a minute before they were powered back on. I powered them down again and now it seems they are not being powered on again. Will wait a bit and see, otherwise I will try to power them on manually. After a longer while the tkg-cluster-1 nodes have now this status:\nNAME STATUS ROLES AGE VERSION tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj NotReady,SchedulingDisabled \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 tkg-cluster-1-znr8h-4v2tm NotReady,SchedulingDisabled control-plane 4d21h v1.26.5+vmware.2 tkg-cluster-1-znr8h-d72g5 Ready control-plane 4d22h v1.26.5+vmware.2 tkg-cluster-1-znr8h-j6899 Ready control-plane 4d22h v1.26.5+vmware.2 I will try to manually power the nodes back on. I did not have the chance to do that, they are now being deleted and recreated! Wow, cool\nUpdate existing TKG cluster to use new Availability Zones # For details on how to update existing cluster to use new availabilty zones follow the official documentation here\nWrapping up # This finishes the exploration of this useful feature in TKG 2.3. It is very flexible and allows for very robust design of node placement. Opens up for designs with high availability requirements. I do like very much the choice to use vSphere clusters, vCenter servers and DRS host-groups as the different objects in vCenter to use.\n","date":"29 August 2023","externalUrl":null,"permalink":"/2023/08/29/tkg-2.3-deployment-in-multiple-availability-zones/","section":"Posts","summary":"In this post I will go through how to configure and deploy TKG 2.3 in mutliple availability zones using vSphere clusters and how to reconfigure already deployed TKG cluster to new availability zones.","title":"TKG 2.3 deployment in multiple availability zones","type":"posts"},{"content":"","date":"12 July 2023","externalUrl":null,"permalink":"/tags/authentication/","section":"Tags","summary":"","title":"Authentication","type":"tags"},{"content":" TMC local or TMC-SM # TMC, Tanzu Mission Control, has always been a SaaS offering. But now it has also been released as a installable product you can deploy in your own environment. Throughout this post I will most likely refer to it as TMC SM or TMC local. TMC SM stands for Self Managed. For all official documentation and updated content head over here including the installation process.\nPre-requirements # There is always some pre-requirements to be in place. Why should it always be pre-requirements? Well there is no need create any cars if there is no roads for them to drive on, will it? Thats enough humour for today. Instead of listing a detailed list of the requirements here, head over to the official page here and get familiar with it. In this post I have already deployed a Kubernetes cluster in my vSphere with Tanzu environment, that meets the requirements. More on that later. Then I will cover the certificate requirement deploying Cert-Manager and configure a ClusterIssuer. The image registry I will not cover as I already have a registry up and running and will be using that. I will not cover the loadbalancer/Ingress installation as I am assuming the following is already in place:\nA working vSphere 8 Environment A working Tanzu with vSphere Supervisor deployment A working NSX-ALB configuration to support both L4 and L7 services (meaning AKO is installed on the cluster for TMC-SM) A working image registry with a valid signed certificate, I will be using Harbor Registry. I will be using NSX ALB in combination with Contour that is being installed with TMC-SM, I will cover the specifics in configuring NSX-ALB, more specifically AKO, to support Keycloak via Ingress. Then I will cover the installation and configuration of Keycloak as the OIDC requirement. Then I will show how I handle my DNS zone for the TMC installation. As a final note, remember that the certificate I going to use needs to be trusted by the components that will be consuming them and DNS is important. Well lets go through it step by step.\nIn this order the following steps will be done:\nAnd, according to the official documentation:\nDeploying TMC Self-Managed 1.0 on a Tanzu Kubernetes Grid (TKG) 2.0 workload cluster running in vSphere with Tanzu on vSphere version 8.x is for tech preview only. Initiate deployments only in pre-production environments or production environments where support for the integration is not required. vSphere 8u1 or later is required in order to test the tech preview integration.\nI will use vSphere 8 U1 in this post, and is by no means meant as a guideline to a production ready setup of TMC-SM.\nThe TKG cluster - where TMC will be deployed # I have used this configuration to deploy my TKG cluster, I have used the VM class guaranteed-large, it will work with 4CPU and 8GB ram on the nodes also. Oh, and by the way. This installation is done on a vSphere with Tanzu multi-zone setup:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: tmc-sm-cluster #My own name on the cluster namespace: ns-wdc-prod #My vSphere Namespace spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] #Edited by me pods: cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] #Edited by me serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.24.9+vmware.1-tkg.4 #My latest available TKR version controlPlane: replicas: 1 # only one controlplane (saving resources and time) metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: #muliple node pools are used machineDeployments: - class: node-pool name: node-pool-1 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-1 #named after my vSphere zone - class: node-pool name: node-pool-2 replicas: 2 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-2 #named after my vSphere zone - class: node-pool name: node-pool-3 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-3 #named after my vSphere zone variables: - name: vmClass value: guaranteed-large - name: storageClass value: all-vsans #my zonal storageclass - name: defaultStorageClass value: all-vsans - name: controlPlaneVolumes value: - name: etcd capacity: storage: 10Gi mountPath: /var/lib/etcd storageClass: all-vsans - name: nodePoolVolumes value: - name: containerd capacity: storage: 50Gi mountPath: /var/lib/containerd storageClass: all-vsans - name: kubelet capacity: storage: 50Gi mountPath: /var/lib/kubelet storageClass: all-vsans As soon as the cluster is ready and deployed I will log into it and change my context using kubectl vsphere login \u0026hellip;. and apply my clusterrole policy:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: psp:privileged rules: - apiGroups: [\u0026#39;policy\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: - vmware-system-privileged --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: all:psp:privileged roleRef: kind: ClusterRole name: psp:privileged apiGroup: rbac.authorization.k8s.io subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io ClusterIssuer # To support dynamically creating/issuing certificates I will deploy and install Cert-Manager. The approach I am using to deploy Cert-Manager is to use the provided Cert-Manager Packages available in Tanzu.\nTanzu Cert-Manager Package # I will have to add the package repository where I can download and install Cert-Manager from and a namespace for the packages themselves. Before I can approach with this I need the Tanzu CLI. The official approach can be found here Download the Tanzu CLI from here\nExtract it:\ntar -zxvf tanzu-cli-bundle-linux-amd64.tar.gz Enter into the cli folder and copy or move to a folder in your paht:\nandreasm@linuxvm01:~/tanzu-cli/cli/core/v0.29.0$ cp tanzu-core-linux_amd64 /usr/local/bin/tanzu Run tanzu init and tanzu plugin sync:\ntanzu init tanzu plugin sync When that is done, go ahead dreate the namespace:\nkubectl create ns tanzu-package-repo-global Then add the the repository:\ntanzu package repository add tanzu-standard --url projects.registry.vmware.com/tkg/packages/standard/repo:v2.2.0 -n tanzu-package-repo-global Then installing the Cert-Manager package:\ntanzu package install cert-manager --package cert-manager.tanzu.vmware.com --version 1.7.2+vmware.3-tkg.3 -n tanzu-package-repo-global CA Issuer # Now it is time to configure Cert-Manager with a CA certifcate so it can act as a CA ClusterIssuer. To do that lets start by creating a CA certificate.\nCreate the certificate, without passphrase:\nandreasm@linuxvm01:~/tmc-sm$ openssl req -nodes -x509 -sha256 -days 1825 -newkey rsa:2048 -keyout rootCA.key -out rootCA.crt Generating a RSA private key ..........................................................................+++++ .+++++ writing new private key to \u0026#39;rootCA.key\u0026#39; ----- You are about to be asked to enter information that will be incorporated into your certificate request. What you are about to enter is what is called a Distinguished Name or a DN. There are quite a few fields but you can leave some blank For some fields there will be a default value, If you enter \u0026#39;.\u0026#39;, the field will be left blank. ----- Country Name (2 letter code) [AU]:US State or Province Name (full name) [Some-State]:punxsutawney Locality Name (eg, city) []:Groundhog Organization Name (eg, company) [Internet Widgits Pty Ltd]:Day Organizational Unit Name (eg, section) []:SameDay Common Name (e.g. server FQDN or YOUR name) []:tmc.pretty-awesome-domain.net Email Address []: This should give me two files:\n1407 Jul 12 14:21 rootCA.crt 1704 Jul 12 14:19 rootCA.key Then I will go ahead and create a secret for Cert-Manager using these two above files in Base64 format:\nandreasm@linuxvm01:~/tmc-sm$ cat rootCA.crt | base64 -w0 LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ0ekNDQXN1Z0F3SUJBZ0lVSFgyak5rbysvdnNlcjc0dGpxS2R3U1ZMQlhVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd2dZQXhDekFKQmdOVkJBWVRBbFZUTVJVd0V3WURWUVFJREF4d2RXNTRjM1YwWVhkdVpYa3hFakFRQmdOVgpCQWNNQ1VkeWIzVnVaR2h2WnpFTU1Bb0dBMVVFQ2d3RFJHRjVNUkF3RGdZRFZRUUxEQWRUWVcxbFJHRjVNU1l3CkpBWURWUVFEREIxMGJX andreasm@linuxvm01:~/tmc-sm$ cat rootCA.key | base64 -w0 LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2Z0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktnd2dnU2tBZ0VBQW9JQkFRREFSR2RCSWwreUVUbUsKOGI0N2l4NUNJTDlXNVh2dkZFY0Q3KzZMbkxxQ3ZVTWdyNWxhNGFjUU8vZUsxUFdIV0YvWk9UN0ZyWUY0QVpmYgpFbzB5ejFxL3pGT3AzQS9sMVNqN3lUeHY5WmxYRU9DbWI4dGdQVm9Ld3drUHFiQ0RtNVZ5Ri9HaGUvMDFsbXl6CnEyMlpGM0M4 Put the above content into my secret.yaml file below\napiVersion: v1 kind: Secret metadata: name: ca-key-pair namespace: cert-manager data: tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQvekNDQ.... tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2UUlCQUR... Then apply it:\nandreasm@linuxvm01:~/tmc-sm$ k apply -f secret.yaml secret/ca-key-pair configured Now create the ClusterIssuer yaml definition:\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: ca-issuer spec: ca: secretName: ca-key-pair This points to the secret created in the previous step. And apply it:\nandreasm@linuxvm01:~/tmc-sm$ k apply -f secret-key-pair.yaml clusterissuer.cert-manager.io/ca-issuer configured Now check the status of the clusterissuer. It can take a couple of seconds. If it does not go to a Ready state, check the logs of the cert-manager pod.\nandreasm@linuxvm01:~/tmc-sm$ k get clusterissuers.cert-manager.io NAME READY AGE ca-issuer True 20s Now, we have a ClusterIssuer we can use to provide us with self-signed certificates.\nDNS-Zone # In my environment I am using dnsmasq as my backend DNS server for all my clients, servers etc to handle dns records and zones. So in my dnsmasq config I will need to create a \u0026ldquo;forward\u0026rdquo; zone for my specific tmc.pretty-awesome-domain.net which will forward all requests to the DNS service I have configured in Avi. Here is the dnsmasq.conf:\nserver=/.tmc.pretty-awesome-domain.net/10.101.211.9 The IP 10.101.211.9 is my NSX ALB DNS VS. Now in my NSX ALB DNS service I need to create an entry that points to tmc.pretty-awesome-domain.net where the IP is the Contour IP. In the later stage of this post we need to define a value yaml file. In there we can specify a certain IP the Contour service should get. This IP is being used by the NSX ALB dns to forward all the wildcard requests to the tmc.pretty-awesome-domain.net. To configure that in NSX ALB:\nEdit the the DNS VS, add a static DNS record, point to the ip of the Contour service (not there yet, but will come when we start deploying TMC-SM). Also remeber to check Enable wild-card match:\nSo what is going on now. I have configured my NSX ALB DNS servide to be responsible for a domain called pretty-awesome-domain.net by adding this domain to my DNS Profile template which the NSX ALB Cloud is configured with. Each time a Kubernetes service requests a DNS record in this domain NSX ALB will create this entry with correct fqdn/IP mapping. Then I have also created a static entry for the subdomain tmc.pretty-awesome-domain.net in the NSX ALB provider which will forward all wildcard requests to the Contour service which holds these actual records:\n\u0026lt;my-tmc-dns-zone\u0026gt; alertmanager.\u0026lt;my-tmc-dns-zone\u0026gt; auth.\u0026lt;my-tmc-dns-zone\u0026gt; blob.\u0026lt;my-tmc-dns-zone\u0026gt; console.s3.\u0026lt;my-tmc-dns-zone\u0026gt; gts-rest.\u0026lt;my-tmc-dns-zone\u0026gt; gts.\u0026lt;my-tmc-dns-zone\u0026gt; landing.\u0026lt;my-tmc-dns-zone\u0026gt; pinniped-supervisor.\u0026lt;my-tmc-dns-zone\u0026gt; prometheus.\u0026lt;my-tmc-dns-zone\u0026gt; s3.\u0026lt;my-tmc-dns-zone\u0026gt; tmc-local.s3.\u0026lt;my-tmc-dns-zone\u0026gt; So I dont have to manually create these dns records, they will just happily be handed over to the Contour ingress records. This is how my DNS lookups look like: Keycloak - OIDC/ID provider - using AKO as Ingress controller # One of the requirements for TMC local is also an OIDC provider. My colleague Alex gave me the tip to test out Keycloak as it also work as a standalone provider, without any backend ldap service. So this section will be divided into two sub-sections, one section covers the actual installation of Keycloak using Helm, and the other section covers the Keycloak authentication settings that is required for TMC local.\nKeycloak installation # I am using Helm to install Keycloak in my cluster. That means we need Helm installed, the Helm repository that contains the Keycloak charts. I will be using the Bitnami repo for this purpose. So first add the Bitnami repo:\nandreasm@linuxvm01:~$ helm repo add bitnami https://charts.bitnami.com/bitnami \u0026#34;bitnami\u0026#34; has been added to your repositories Then do a Helm search repo to see if it has been added (look for a long list of bitnami/xxxx):\nandreasm@linuxvm01:~$ helm search repo NAME CHART VERSION\tAPP VERSION DESCRIPTION bitnami/airflow 14.3.1 2.6.3 Apache Airflow is a tool to express and execute... bitnami/apache 9.6.4 2.4.57 Apache HTTP Server is an open-source HTTP serve... bitnami/apisix 2.0.3 3.3.0 Apache APISIX is high-performance, real-time AP... bitnami/appsmith 0.3.9 1.9.25 Appsmith is an open source platform for buildin... bitnami/argo-cd 4.7.14 2.7.7 Argo CD is a continuous delivery tool for Kuber... bitnami/argo-workflows 5.3.6 3.4.8 Argo Workflows is meant to orchestrate Kubernet... bitnami/aspnet-core 4.3.3 7.0.9 ASP.NET Core is an open-source framework for we... bitnami/cassandra 10.4.3 4.1.2 Apache Cassandra is an open source distributed ... bitnami/cert-manager 0.11.5 1.12.2 cert-manager is a Kubernetes add-on to automate... bitnami/clickhouse 3.5.4 23.6.2 ClickHouse is an open-source column-oriented OL... bitnami/common 2.6.0 2.6.0 A Library Helm Chart for grouping common logic ... bitnami/concourse 2.2.3 7.9.1 Concourse is an automation system written in Go... bitnami/consul 10.12.4 1.16.0 HashiCorp Consul is a tool for discovering and ... bitnami/contour 12.1.1 1.25.0 Contour is an open source Kubernetes ingress co... bitnami/contour-operator 4.2.1 1.24.0 DEPRECATED The Contour Operator extends the Kub... bitnami/dataplatform-bp2 12.0.5 1.0.1 DEPRECATED This Helm chart can be used for the ... bitnami/discourse 10.3.4 3.0.4 Discourse is an open source discussion platform... bitnami/dokuwiki 14.1.4 20230404.1.0 DokuWiki is a standards-compliant wiki optimize... bitnami/drupal 14.1.5 10.0.9 Drupal is one of the most versatile open source... bitnami/ejbca 7.1.3 7.11.0 EJBCA is an enterprise class PKI Certificate Au... bitnami/elasticsearch 19.10.3 8.8.2 Elasticsearch is a distributed search and analy... bitnami/etcd 9.0.4 3.5.9 etcd is a distributed key-value store designed ... bitnami/external-dns 6.20.4 0.13.4 ExternalDNS is a Kubernetes addon that configur... bitnami/flink 0.3.3 1.17.1 Apache Flink is a framework and distributed pro... bitnami/fluent-bit 0.4.6 2.1.6 Fluent Bit is a Fast and Lightweight Log Proces... bitnami/fluentd 5.8.5 1.16.1 Fluentd collects events from various data sourc... bitnami/flux 0.3.5 0.36.1 Flux is a tool for keeping Kubernetes clusters ... bitnami/geode 1.1.8 1.15.1 DEPRECATED Apache Geode is a data management pl... bitnami/ghost 19.3.23 5.54.0 Ghost is an open source publishing platform des... bitnami/gitea 0.3.5 1.19.4 Gitea is a lightweight code hosting solution. W... bitnami/grafana 9.0.1 10.0.1 Grafana is an open source metric analytics and ... bitnami/grafana-loki 2.10.0 2.8.2 Grafana Loki is a horizontally scalable, highly... bitnami/grafana-mimir 0.5.4 2.9.0 Grafana Mimir is an open source, horizontally s... bitnami/grafana-operator 3.0.2 5.1.0 Grafana Operator is a Kubernetes operator that ... bitnami/grafana-tempo 2.3.4 2.1.1 Grafana Tempo is a distributed tracing system t... bitnami/haproxy 0.8.4 2.8.1 HAProxy is a TCP proxy and a HTTP reverse proxy... bitnami/haproxy-intel 0.2.11 2.7.1 DEPRECATED HAProxy for Intel is a high-performa... bitnami/harbor 16.7.0 2.8.2 Harbor is an open source trusted cloud-native r... bitnami/influxdb 5.7.1 2.7.1 InfluxDB(TM) is an open source time-series data... bitnami/jaeger 1.2.6 1.47.0 Jaeger is a distributed tracing system. It is u... bitnami/jasperreports 15.1.3 8.2.0 JasperReports Server is a stand-alone and embed... bitnami/jenkins 12.2.4 2.401.2 Jenkins is an open source Continuous Integratio... bitnami/joomla 14.1.5 4.3.3 Joomla! is an award winning open source CMS pla... bitnami/jupyterhub 4.1.6 4.0.1 JupyterHub brings the power of notebooks to gro... bitnami/kafka 23.0.2 3.5.0 Apache Kafka is a distributed streaming platfor... bitnami/keycloak 15.1.6 21.1.2 Keycloak is a high performance Java-based ident... And in the list above we can see the bitnami/keycloak charts. So far so good. Now grab the default keycloak chart values file:\nhelm show values bitnami/keycloak \u0026gt; keycloak-values.yaml This should provide you with a file called keycloak-values.yaml. We need to do some basic changes in here. My values file below is snippets from the full values file where I have edited with comments on what I have changed:\n## Keycloak authentication parameters ## ref: https://github.com/bitnami/containers/tree/main/bitnami/keycloak#admin-credentials ## auth: ## @param auth.adminUser Keycloak administrator user ## adminUser: admin # I have changed the user to admin ## @param auth.adminPassword Keycloak administrator password for the new user ## adminPassword: \u0026#34;PASSWORD\u0026#34; # I have entered my password here ## @param auth.existingSecret Existing secret containing Keycloak admin password ## existingSecret: \u0026#34;\u0026#34; ## @param auth.passwordSecretKey Key where the Keycloak admin password is being stored inside the existing secret. ## passwordSecretKey: \u0026#34;\u0026#34; ... ## @param production Run Keycloak in production mode. TLS configuration is required except when using proxy=edge. ## production: false ## @param proxy reverse Proxy mode edge, reencrypt, passthrough or none ## ref: https://www.keycloak.org/server/reverseproxy ## proxy: edge # I am using AKO to terminate the SSL cert at the Service Engine side. So set this to edge ## @param httpRelativePath Set the path relative to \u0026#39;/\u0026#39; for serving resources. Useful if you are migrating from older version which were using \u0026#39;/auth/\u0026#39; ## ref: https://www.keycloak.org/migration/migrating-to-quarkus#_default_context_path_changed ## ... postgresql: enabled: true auth: postgresPassword: \u0026#34;PASSWORD\u0026#34; # I have added my own password here username: bn_keycloak password: \u0026#34;PASSWORD\u0026#34; # I have added my own password here database: bitnami_keycloak existingSecret: \u0026#34;\u0026#34; architecture: standalone In short, the places I have done changes is adjusting the adminUser, password for the adminUser. Then I changed the proxy setting to edge, and adjusted the PostgreSQL password as I dont want to use the auto-generated passwords.\nThen I can deploy Keycloak with this value yaml file:\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k create ns keycloak andreasm@linuxvm01:~/tmc-sm/keycloak$ helm upgrade -i -n keycloak keycloak bitnami/keycloak -f keycloak-values.yaml Release \u0026#34;keycloak\u0026#34; has been upgraded. Happy Helming! NAME: keycloak LAST DEPLOYED: Wed Jul 12 21:34:32 2023 NAMESPACE: keycloak STATUS: deployed REVISION: 4 TEST SUITE: None NOTES: CHART NAME: keycloak CHART VERSION: 15.1.6 APP VERSION: 21.1.2 ** Please be patient while the chart is being deployed ** Keycloak can be accessed through the following DNS name from within your cluster: keycloak.keycloak.svc.cluster.local (port 80) To access Keycloak from outside the cluster execute the following commands: 1. Get the Keycloak URL by running these commands: export HTTP_SERVICE_PORT=$(kubectl get --namespace keycloak -o jsonpath=\u0026#34;{.spec.ports[?(@.name==\u0026#39;http\u0026#39;)].port}\u0026#34; services keycloak) kubectl port-forward --namespace keycloak svc/keycloak ${HTTP_SERVICE_PORT}:${HTTP_SERVICE_PORT} \u0026amp; echo \u0026#34;http://127.0.0.1:${HTTP_SERVICE_PORT}/\u0026#34; 2. Access Keycloak using the obtained URL. 3. Access the Administration Console using the following credentials: echo Username: admin echo Password: $(kubectl get secret --namespace keycloak keycloak -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 -d) I am using the helm command upgrade -i, which means if it is not installed it will, if it is installed it will upgrade the existing installation with the content in the values yaml file.\nKeeping the values.yaml as default as possible it will not create any serviceType loadBalancer or Ingress. That is something I would like to handle my self after the actual Keycloak deployment is up and running. More on that later.\nAny pods running:\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k get pods -n keycloak NAME READY STATUS RESTARTS AGE keycloak-0 0/1 Running 0 14s keycloak-postgresql-0 1/1 Running 0 11h Almost. Give it a couple of seconds more and it should be ready.\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k get pods -n keycloak NAME READY STATUS RESTARTS AGE keycloak-0 1/1 Running 0 2m43s keycloak-postgresql-0 1/1 Running 0 11h The Keycloak is running. Then I need to expose it with a serviceType loadBalancer or Ingress. I have opted to use Ingress as I feel it is much easier to managed the certificates in NSX-ALB and also let the NSX-ALB SEs handle the TLS termination, instead of in the pod itself. So now I need to confige the Ingress for the ClusterIP service that is automatically created by the Helm chart above. Lets check the service:\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k get svc -n keycloak NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE keycloak ClusterIP 20.10.61.222 \u0026lt;none\u0026gt; 80/TCP 31h keycloak-headless ClusterIP None \u0026lt;none\u0026gt; 80/TCP 31h keycloak-postgresql ClusterIP 20.10.8.129 \u0026lt;none\u0026gt; 5432/TCP 31h keycloak-postgresql-hl ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 31h The one I am interested in is the keycloak ClusterIP service. Next step is to configure the Ingress for this service. I will post the yaml I am using for this Ingress, and explain a bit more below. This step assumes Avi is installed and configured, and AKO has been deployed and ready to provision Ingress requests. For details on how to install AKO in TKG read here and here.\nJust a quick comment before we go through the Ingress, what I want to achieve is an Ingress that is handling the client requests and TLS termination at the \u0026ldquo;loadbalancer\u0026rdquo; side. Traffic from the \u0026ldquo;loadbalancer\u0026rdquo; (the Avi SEs) to the Keycloak pod is pure http, no SSL. I trust my infra between the SEs and Keycloak pods.\nThe Ingress for Keycloak:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: keycloak namespace: keycloak annotations: cert-manager.io/cluster-issuer: ca-issuer cert-manager.io/common-name: keycloak.tmc.pretty-awesome-domain.net # ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; spec: ingressClassName: avi-lb rules: - host: keycloak.tmc.pretty-awesome-domain.net http: paths: - path: / pathType: Prefix backend: service: name: keycloak port: number: 80 tls: - hosts: - keycloak.tmc.pretty-awesome-domain.net secretName: keycloak-ingress-secret In the above yaml I am creating the Ingress to expose my Keycloak instance externally. I am also kindly asking my ca-issuer to issue a fresh new certificate for this Ingress to use. This is done by adding the annotation cert-manager.io/cluster-issuer: ca-issuer which would be sufficient enough in other scenarios, but I also needed to add this section:\ntls: - hosts: - keycloak.tmc.pretty-awesome-domain.net secretName: keycloak-ingress-secret Now I just need to apply it:\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k apply -f keycloak-ingress.yaml ingress.networking.k8s.io/keycloak created Now, what is created on the NSX-ALB side: There is my Ingress for Keycloak. Lets check the certificate it is using:\nIt is using my new freshly created certificate. I will go ahead and open the ui of Keycloak in my browser: Whats this? The certificate is the correct one\u0026hellip; Remember that I am using Cert-Manager to issue self-signed certificates? I need to trust the root of the CA in my client to make this certificate trusted. Depending on your client\u0026rsquo;s operating system I will not go through how this is done. But I have now added my rootCA.crt certificate created earlier (the same rootCA.crt I generated for my ClusterIssuer) as a trusted root certificate in my client. Let me try again now.\nNow it is looking much better \u0026#x1f604;\nLets try to log in: Using the username and password provided in the value yaml file. Seems to be something wrong here.. My login is just \u0026ldquo;looping\u0026rdquo; somehow.. Lets check the Keycloak pod logs :\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k logs -n keycloak keycloak-0 keycloak 21:34:34.96 keycloak 21:34:34.97 Welcome to the Bitnami keycloak container keycloak 21:34:34.97 Subscribe to project updates by watching https://github.com/bitnami/containers keycloak 21:34:34.97 Submit issues and feature requests at https://github.com/bitnami/containers/issues keycloak 21:34:34.97 keycloak 21:34:34.97 INFO ==\u0026gt; ** Starting keycloak setup ** keycloak 21:34:34.98 INFO ==\u0026gt; Validating settings in KEYCLOAK_* env vars... keycloak 21:34:35.00 INFO ==\u0026gt; Trying to connect to PostgreSQL server keycloak-postgresql... keycloak 21:34:35.01 INFO ==\u0026gt; Found PostgreSQL server listening at keycloak-postgresql:5432 keycloak 21:34:35.02 INFO ==\u0026gt; Configuring database settings keycloak 21:34:35.05 INFO ==\u0026gt; Enabling statistics keycloak 21:34:35.06 INFO ==\u0026gt; Configuring http settings keycloak 21:34:35.08 INFO ==\u0026gt; Configuring hostname settings keycloak 21:34:35.09 INFO ==\u0026gt; Configuring cache count keycloak 21:34:35.10 INFO ==\u0026gt; Configuring log level keycloak 21:34:35.11 INFO ==\u0026gt; Configuring proxy keycloak 21:34:35.12 INFO ==\u0026gt; ** keycloak setup finished! ** keycloak 21:34:35.14 INFO ==\u0026gt; ** Starting keycloak ** Appending additional Java properties to JAVA_OPTS: -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local Updating the configuration and installing your custom providers, if any. Please wait. 2023-07-12 21:34:38,622 WARN [org.keycloak.services] (build-6) KC-SERVICES0047: metrics (org.jboss.aerogear.keycloak.metrics.MetricsEndpointFactory) is implementing the internal SPI realm-restapi-extension. This SPI is internal and may change without notice 2023-07-12 21:34:39,163 WARN [org.keycloak.services] (build-6) KC-SERVICES0047: metrics-listener (org.jboss.aerogear.keycloak.metrics.MetricsEventListenerFactory) is implementing the internal SPI eventsListener. This SPI is internal and may change without notice 2023-07-12 21:34:51,024 INFO [io.quarkus.deployment.QuarkusAugmentor] (main) Quarkus augmentation completed in 14046ms 2023-07-12 21:34:52,578 INFO [org.keycloak.quarkus.runtime.hostname.DefaultHostnameProvider] (main) Hostname settings: Base URL: \u0026lt;unset\u0026gt;, Hostname: \u0026lt;request\u0026gt;, Strict HTTPS: false, Path: \u0026lt;request\u0026gt;, Strict BackChannel: false, Admin URL: \u0026lt;unset\u0026gt;, Admin: \u0026lt;request\u0026gt;, Port: -1, Proxied: true 2023-07-12 21:34:54,013 WARN [io.quarkus.agroal.runtime.DataSources] (main) Datasource \u0026lt;default\u0026gt; enables XA but transaction recovery is not enabled. Please enable transaction recovery by setting quarkus.transaction-manager.enable-recovery=true, otherwise data may be lost if the application is terminated abruptly 2023-07-12 21:34:54,756 INFO [org.infinispan.SERVER] (keycloak-cache-init) ISPN005054: Native IOUring transport not available, using NIO instead: io.netty.incubator.channel.uring.IOUring 2023-07-12 21:34:54,961 WARN [org.infinispan.CONFIG] (keycloak-cache-init) ISPN000569: Unable to persist Infinispan internal caches as no global state enabled 2023-07-12 21:34:54,987 WARN [io.quarkus.vertx.http.runtime.VertxHttpRecorder] (main) The X-Forwarded-* and Forwarded headers will be considered when determining the proxy address. This configuration can cause a security issue as clients can forge requests and send a forwarded header that is not overwritten by the proxy. Please consider use one of these headers just to forward the proxy address in requests. 2023-07-12 21:34:54,990 WARN [org.infinispan.PERSISTENCE] (keycloak-cache-init) ISPN000554: jboss-marshalling is deprecated and planned for removal 2023-07-12 21:34:55,005 INFO [org.infinispan.CONTAINER] (keycloak-cache-init) ISPN000556: Starting user marshaller \u0026#39;org.infinispan.jboss.marshalling.core.JBossUserMarshaller\u0026#39; 2023-07-12 21:34:55,450 INFO [org.infinispan.CLUSTER] (keycloak-cache-init) ISPN000078: Starting JGroups channel `ISPN` 2023-07-12 21:34:55,455 INFO [org.jgroups.JChannel] (keycloak-cache-init) local_addr: 148671ea-e4a4-4b1f-9ead-78c598924c94, name: keycloak-0-45065 2023-07-12 21:34:55,466 INFO [org.jgroups.protocols.FD_SOCK2] (keycloak-cache-init) server listening on *.57800 2023-07-12 21:34:57,471 INFO [org.jgroups.protocols.pbcast.GMS] (keycloak-cache-init) keycloak-0-45065: no members discovered after 2002 ms: creating cluster as coordinator 2023-07-12 21:34:57,480 INFO [org.infinispan.CLUSTER] (keycloak-cache-init) ISPN000094: Received new cluster view for channel ISPN: [keycloak-0-45065|0] (1) [keycloak-0-45065] 2023-07-12 21:34:57,486 INFO [org.infinispan.CLUSTER] (keycloak-cache-init) ISPN000079: Channel `ISPN` local address is `keycloak-0-45065`, physical addresses are `[20.20.2.68:7800]` 2023-07-12 21:34:57,953 INFO [org.keycloak.connections.infinispan.DefaultInfinispanConnectionProviderFactory] (main) Node name: keycloak-0-45065, Site name: null 2023-07-12 21:34:57,962 INFO [org.keycloak.broker.provider.AbstractIdentityProviderMapper] (main) Registering class org.keycloak.broker.provider.mappersync.ConfigSyncEventListener 2023-07-12 21:34:59,149 INFO [io.quarkus] (main) Keycloak 21.1.2 on JVM (powered by Quarkus 2.13.8.Final) started in 7.949s. Listening on: http://0.0.0.0:8080 2023-07-12 21:34:59,150 INFO [io.quarkus] (main) Profile dev activated. 2023-07-12 21:34:59,150 INFO [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, jdbc-h2, jdbc-mariadb, jdbc-mssql, jdbc-mysql, jdbc-oracle, jdbc-postgresql, keycloak, logging-gelf, micrometer, narayana-jta, reactive-routes, resteasy, resteasy-jackson, smallrye-context-propagation, smallrye-health, vertx] 2023-07-12 21:34:59,160 ERROR [org.keycloak.services] (main) KC-SERVICES0010: Failed to add user \u0026#39;admin\u0026#39; to realm \u0026#39;master\u0026#39;: user with username exists 2023-07-12 21:34:59,161 WARN [org.keycloak.quarkus.runtime.KeycloakMain] (main) Running the server in development mode. DO NOT use this configuration in production. 2023-07-12 22:04:22,511 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:04:27,809 WARN [org.keycloak.events] (executor-thread-6) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:04:33,287 WARN [org.keycloak.events] (executor-thread-3) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:04:44,105 WARN [org.keycloak.events] (executor-thread-7) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:04:55,303 WARN [org.keycloak.events] (executor-thread-5) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:00,707 WARN [org.keycloak.events] (executor-thread-6) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:06,861 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:12,484 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:18,351 WARN [org.keycloak.events] (executor-thread-6) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:28,509 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:37,438 WARN [org.keycloak.events] (executor-thread-7) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:42,742 WARN [org.keycloak.events] (executor-thread-5) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:47,750 WARN [org.keycloak.events] (executor-thread-5) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:53,019 WARN [org.keycloak.events] (executor-thread-3) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 2023-07-12 22:05:58,020 WARN [org.keycloak.events] (executor-thread-3) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret Hmm, error=invalid_token\u0026hellip; type=REFRESH_TOKEN_ERROR\u0026hellip; Well after some investigating, after some Sherlock Holmsing, I managed to figure out what caused this. I need to deselect a setting in my Avi Application profile selected default for this Ingress. So first I need to create an Application Profile, with most of the setting, but unselect the HTTP-only Cookies. So head over to the NSX-ALB gui, create a new application profile: Click create, select under Type: HTTP: Then scroll down under Security and make these selections: Give it a name at the top and click save at the bottom right corner:\nNow we need to tell our Ingress to use this Application profile. To be able to do that I need to use an AKO crd called HostRule. So I will go ahead and create a yaml using this HostRule crd like this:\napiVersion: ako.vmware.com/v1alpha1 kind: HostRule metadata: name: keycloak-host-rule namespace: keycloak spec: virtualhost: fqdn: keycloak.tmc.pretty-awesome-domain.net # mandatory fqdnType: Exact enableVirtualHost: true tls: # optional sslKeyCertificate: name: keycloak-ingress-secret type: secret termination: edge applicationProfile: keycloak-http The TLS section is optional, but I have decided to keep it in regardless. The important piece is the applicationProfile where I enter the name of my newly created application profile above. Save it and apply:\nandreasm@linuxvm01:~/tmc-sm/keycloak$ k apply -f keycloak-hostrule.yaml hostrule.ako.vmware.com/keycloak-host-rule created Now, has my application profile changed in my Keycloak Ingress? It has.. So far so good. Will I be able to log in to Keycloak now then?\nSo it seems. Wow, cool. Now lets head over to the section where I configure Keycloak settings to support TMC local authentication.\nKeycloak authentication settings for TMC local # One of the recommendations from Keycloak is to create a new realm. So when logged in, head over to the top left corner where you have a dropdown menu: Click Create Realm:\nGive it a name and click CREATE. Select the newly created realm in the top left corners drop-down menu: The first thing I will create is a new Client. Click on Clients in the left menu and click on Create client: Fill in the below information, according to your environment: Click save at the bottom:\nLater on we will need the Client ID and Client Secret, these can be found here: Next head over to the Client scopes section on the left side click Create client scope: Make the following selection as below:\nClick save.\nFind the newly create Client scope called groups and click on its name. From there click on the tab Mappers and click the blue button Add mapper and select From predefined mappers. In the list below select the newly created Client scope named *groups\u0026quot; and add it. Head back to Clients menu again, select your tmc-sm application. In there click on the tab Client scopes and click Add client scope and select the groups mapper. It will be the only available in the list to select from. After it has been added, it shoul be in the list below.\nNext head over to the left menu and click Realm roles, In there click on Create role give it the name tmc:admin and save. Nothing more to be done with this role.\nNow head over to Users in the left menu, and click Add user Here it is important to add an email-address and select Email-verified. Otherwise we will get an error status when trying to log in to TMC later. Click create.\nAfter the user has been created select the Credentials tab and click on Set password Set Temporary to OFF\nNext up and final steps is to create a group and and my user to this group and add the role mapping tmc:admin to the group: Now Keycloak has been configured to work with TMC. Next step is to prepare the packages for TMC local.\nInstalling TMC local # The actual Installation of TMC local involves a couple of steps. First its the packages, the source files for the application TMC, they need to be downloaded and uploaded to a registry. A defined value file, the cli tools tanzu and tmc-sm.\nDownload and upload the TMC packages # To begin the actuall installation of TMC local we need to download the needed packages from my.vmware.com here\nMove the downloaded tmc-self-managed-1.0.0.tar file to your jumphost, where you also have access to a registry. Create a folder called sourcefiles. Then extract the the tmc-self-managed-1.0.0.tar with the following command enter the dir where files have been extracted. Inside this folder there is a cli called tmc-sm you will use to upload the images to your registry.\n# create dir andreasm@linuxvm01:~/tmc-sm$ mkdir sourcefiles # extract the downloaded tmc tar file from my.vmware.com andreasm@linuxvm01:~/tmc-sm$ tar -xf tmc-self-managed-1.0.0.tar -C ./tanzumc # cd into the folder sourcefiles andreasm@linuxvm01:~/tmc-sm$ cd sourcefiles # upload the images to your registry andreasm@linuxvm01:~/tmc-sm$ tmc-sm push-images harbor --project registry.some-domain.net/project --username \u0026lt;USERNAME\u0026gt; --password \u0026lt;PASSWORD\u0026gt; # if using special characters in password use \u0026#39;passw@rd\u0026#39; (single quote) before and after Have a cup of coffee and wait for the images to be uploaded to the registry.\nAdd package repository using the tanzu cli # # create a new namespace for the tmc-local installation andreasm@linuxvm01:~/tmc-sm/sourcefiles$ k create ns tmc-local namespace/tmc-local created # add the package repo for tmc-local andreasm@linuxvm01:~/tmc-sm/sourcefiles$ tanzu package repository add tanzu-mission-control-packages --url \u0026#34;registry.some-domain.net/project/package-repository:1.0.0\u0026#34; --namespace tmc-local Waiting for package repository to be added 7:22:48AM: Waiting for package repository reconciliation for \u0026#39;tanzu-mission-control-packages\u0026#39; 7:22:48AM: Fetch started (5s ago) 7:22:53AM: Fetching | apiVersion: vendir.k14s.io/v1alpha1 | directories: | - contents: | - imgpkgBundle: | image: registry.some-domain.net/project/package-repository@sha256:3e19259be2der8d05a342d23dsd3f902c34ffvac4b3c4e61830e27cf0245159e | tag: 1.0.0 | path: . | path: \u0026#34;0\u0026#34; | kind: LockConfig | 7:22:53AM: Fetch succeeded 7:22:54AM: Template succeeded 7:22:54AM: Deploy started (2s ago) 7:22:56AM: Deploying | Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; | Changes | Namespace Name Kind Age Op Op st. Wait to Rs Ri | tmc-local contour.bitnami.com PackageMetadata - create ??? - - - | ^ contour.bitnami.com.12.1.0 Package - create ??? - - - | ^ kafka-topic-controller.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 Package - create ??? - - - | ^ kafka.bitnami.com PackageMetadata - create ??? - - - | ^ kafka.bitnami.com.22.1.3 Package - create ??? - - - | ^ minio.bitnami.com PackageMetadata - create ??? - - - | ^ minio.bitnami.com.12.6.4 Package - create ??? - - - | ^ monitoring.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ monitoring.tmc.tanzu.vmware.com.0.0.13 Package - create ??? - - - | ^ pinniped.bitnami.com PackageMetadata - create ??? - - - | ^ pinniped.bitnami.com.1.2.1 Package - create ??? - - - | ^ postgres-endpoint-controller.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 Package - create ??? - - - | ^ s3-access-operator.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ s3-access-operator.tmc.tanzu.vmware.com.0.1.22 Package - create ??? - - - | ^ tmc-local-postgres.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 Package - create ??? - - - | ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 Package - create ??? - - - | ^ tmc-local-stack.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 Package - create ??? - - - | ^ tmc-local-support.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 Package - create ??? - - - | ^ tmc.tanzu.vmware.com PackageMetadata - create ??? - - - | ^ tmc.tanzu.vmware.com.1.0.0 Package - create ??? - - - | Op: 26 create, 0 delete, 0 update, 0 noop, 0 exists | Wait to: 0 reconcile, 0 delete, 26 noop | 7:22:55AM: ---- applying 26 changes [0/26 done] ---- | 7:22:55AM: create packagemetadata/postgres-endpoint-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/s3-access-operator.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/tmc-local-postgres.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/tmc-local-stack-secrets.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/tmc-local-stack.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/tmc-local-support.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/contour.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/kafka-topic-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/monitoring.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/kafka.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/minio.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create packagemetadata/pinniped.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:55AM: create package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: create package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ---- waiting on 26 changes [0/26 done] ---- | 7:22:56AM: ok: noop package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/tmc-local-support.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/kafka.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/contour.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/kafka-topic-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/monitoring.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/postgres-endpoint-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/s3-access-operator.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/pinniped.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/minio.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/tmc-local-postgres.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/tmc-local-stack-secrets.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop packagemetadata/tmc-local-stack.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ok: noop package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:22:56AM: ---- applying complete [26/26 done] ---- | 7:22:56AM: ---- waiting complete [26/26 done] ---- | Succeeded 7:22:56AM: Deploy succeeded Check the status of the package repository added:\nandreasm@linuxvm01:~/tmc-sm$ k get packagerepositories.packaging.carvel.dev -n tmc-local NAME AGE DESCRIPTION tanzu-mission-control-packages 31s Reconcile succeeded Install the TMC-SM package # Before one can execute the package installation, there is a values-yaml file that needs to be created and edited according to your environment. So I will start with the values-yaml file. Create a file called something like tmc-values.yaml and open with your favourite editor. Below is the content I am using, reflecting the setting in my environment:\nharborProject: registry.some-domain.net/project # I am using Harbor registry, pointing it to my url/project dnsZone: tmc.pretty-awesome-domain.net.net # my tmc DNS zone clusterIssuer: ca-issuer # the clusterissuer created earlier postgres: userPassword: password # my own password maxConnections: 300 minio: username: root password: password # my own password contourEnvoy: serviceType: LoadBalancer # serviceAnnotations: # needed only when specifying load balancer controller specific config like preferred IP # ako.vmware.com/load-balancer-ip: \u0026#34;10.12.2.17\u0026#34; # when using an auto-assigned IP instead of a preferred IP, please use the following key instead of the serviceAnnotations above loadBalancerClass: ako.vmware.com/avi-lb # I am using this class as I want NSX ALB to provide me the L4 IP for the Contour Ingress being deployed. oidc: issuerType: pinniped issuerURL: https://keycloak.tmc.pretty-awesome-domain.net/realms/tmc-sm # url for my keycloak instance and realm tmc-sm clientID: tmc-sm-application # Id of the client created in keycloak earlier clientSecret: bcwefg3rgrg444ffHH44HHtTTQTnYN # the secret for the client trustedCAs: local-ca.pem: | # this is rootCA.crt, created under ClusterIssuer using openssl -----BEGIN CERTIFICATE----- -----END CERTIFICATE----- When the value yaml file has been edited, its time to spin off the installation of TMC-SM.\nExecute the following command:\nandreasm@linuxvm01:~/tmc-sm$ tanzu package install tanzu-mission-control -p tmc.tanzu.vmware.com --version \u0026#34;1.0.0\u0026#34; --values-file tmc-values.yaml --namespace tmc-local Then you will get a long list of outputs:\n7:38:02AM: Creating service account \u0026#39;tanzu-mission-control-tmc-local-sa\u0026#39; 7:38:02AM: Creating cluster admin role \u0026#39;tanzu-mission-control-tmc-local-cluster-role\u0026#39; 7:38:02AM: Creating cluster role binding \u0026#39;tanzu-mission-control-tmc-local-cluster-rolebinding\u0026#39; 7:38:02AM: Creating secret \u0026#39;tanzu-mission-control-tmc-local-values\u0026#39; 7:38:02AM: Creating overlay secrets 7:38:02AM: Creating package install resource 7:38:02AM: Waiting for PackageInstall reconciliation for \u0026#39;tanzu-mission-control\u0026#39; 7:38:03AM: Fetch started (4s ago) 7:38:07AM: Fetching | apiVersion: vendir.k14s.io/v1alpha1 | directories: | - contents: | - imgpkgBundle: | image: registry.some-domain.net/project/package-repository@sha256:30ca40e2d5bb63ab5b3ace796c87b5358e85b8fe129d4d145d1bac5633a81cca | path: . | path: \u0026#34;0\u0026#34; | kind: LockConfig | 7:38:07AM: Fetch succeeded 7:38:07AM: Template succeeded 7:38:07AM: Deploy started (2s ago) 7:38:09AM: Deploying | Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; (nodes: tmc-sm-cluster-node-pool-3-ctgxg-5f76bd48d8-hzh7h, 4+) | Changes | Namespace Name Kind Age Op Op st. Wait to Rs Ri | (cluster) tmc-install-cluster-admin-role ClusterRole - create - reconcile - - | ^ tmc-install-cluster-admin-role-binding ClusterRoleBinding - create - reconcile - - | tmc-local contour PackageInstall - create - reconcile - - | ^ contour-values-ver-1 Secret - create - reconcile - - | ^ kafka PackageInstall - create - reconcile - - | ^ kafka-topic-controller PackageInstall - create - reconcile - - | ^ kafka-topic-controller-values-ver-1 Secret - create - reconcile - - | ^ kafka-values-ver-1 Secret - create - reconcile - - | ^ minio PackageInstall - create - reconcile - - | ^ minio-values-ver-1 Secret - create - reconcile - - | ^ monitoring-values-ver-1 Secret - create - reconcile - - | ^ pinniped PackageInstall - create - reconcile - - | ^ pinniped-values-ver-1 Secret - create - reconcile - - | ^ postgres PackageInstall - create - reconcile - - | ^ postgres-endpoint-controller PackageInstall - create - reconcile - - | ^ postgres-endpoint-controller-values-ver-1 Secret - create - reconcile - - | ^ postgres-values-ver-1 Secret - create - reconcile - - | ^ s3-access-operator PackageInstall - create - reconcile - - | ^ s3-access-operator-values-ver-1 Secret - create - reconcile - - | ^ tmc-install-sa ServiceAccount - create - reconcile - - | ^ tmc-local-monitoring PackageInstall - create - reconcile - - | ^ tmc-local-stack PackageInstall - create - reconcile - - | ^ tmc-local-stack-secrets PackageInstall - create - reconcile - - | ^ tmc-local-stack-values-ver-1 Secret - create - reconcile - - | ^ tmc-local-support PackageInstall - create - reconcile - - | ^ tmc-local-support-values-ver-1 Secret - create - reconcile - - | Op: 26 create, 0 delete, 0 update, 0 noop, 0 exists | Wait to: 26 reconcile, 0 delete, 0 noop | 7:38:07AM: ---- applying 13 changes [0/26 done] ---- | 7:38:08AM: create secret/pinniped-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/minio-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create serviceaccount/tmc-install-sa (v1) namespace: tmc-local | 7:38:08AM: create secret/kafka-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/contour-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/monitoring-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/postgres-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: create clusterrole/tmc-install-cluster-admin-role (rbac.authorization.k8s.io/v1) cluster | 7:38:08AM: ---- waiting on 13 changes [0/26 done] ---- | 7:38:08AM: ok: reconcile serviceaccount/tmc-install-sa (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/pinniped-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile clusterrole/tmc-install-cluster-admin-role (rbac.authorization.k8s.io/v1) cluster | 7:38:08AM: ok: reconcile secret/contour-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/kafka-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/minio-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/monitoring-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/postgres-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ok: reconcile secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local | 7:38:08AM: ---- applying 1 changes [13/26 done] ---- | 7:38:08AM: create clusterrolebinding/tmc-install-cluster-admin-role-binding (rbac.authorization.k8s.io/v1) cluster | 7:38:08AM: ---- waiting on 1 changes [13/26 done] ---- | 7:38:08AM: ok: reconcile clusterrolebinding/tmc-install-cluster-admin-role-binding (rbac.authorization.k8s.io/v1) cluster | 7:38:08AM: ---- applying 2 changes [14/26 done] ---- | 7:38:08AM: create packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:08AM: create packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:08AM: ---- waiting on 2 changes [14/26 done] ---- | 7:38:08AM: ongoing: reconcile packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:08AM: ^ Waiting for generation 1 to be observed | 7:38:08AM: ongoing: reconcile packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:08AM: ^ Waiting for generation 1 to be observed | 7:38:09AM: ongoing: reconcile packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:09AM: ^ Reconciling | 7:38:09AM: ongoing: reconcile packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:09AM: ^ Reconciling | 7:38:14AM: ok: reconcile packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:14AM: ---- waiting on 1 changes [15/26 done] ---- | 7:38:43AM: ok: reconcile packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:43AM: ---- applying 2 changes [16/26 done] ---- | 7:38:43AM: create packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:43AM: create packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:43AM: ---- waiting on 2 changes [16/26 done] ---- | 7:38:43AM: ongoing: reconcile packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:43AM: ^ Waiting for generation 1 to be observed | 7:38:43AM: ongoing: reconcile packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:43AM: ^ Waiting for generation 1 to be observed | 7:38:44AM: ongoing: reconcile packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:44AM: ^ Reconciling | 7:38:44AM: ongoing: reconcile packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:44AM: ^ Reconciling | 7:38:51AM: ok: reconcile packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: ---- applying 4 changes [18/26 done] ---- | 7:38:51AM: create packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: create packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: create packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: create packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: ---- waiting on 5 changes [17/26 done] ---- | 7:38:51AM: ongoing: reconcile packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: ^ Waiting for generation 1 to be observed | 7:38:51AM: ongoing: reconcile packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: ^ Waiting for generation 1 to be observed | 7:38:51AM: ongoing: reconcile packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: ^ Waiting for generation 1 to be observed | 7:38:51AM: ongoing: reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:51AM: ^ Waiting for generation 1 to be observed | 7:38:52AM: ongoing: reconcile packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:52AM: ^ Reconciling | 7:38:52AM: ongoing: reconcile packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:52AM: ^ Reconciling | 7:38:52AM: ongoing: reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:52AM: ^ Reconciling | 7:38:52AM: ongoing: reconcile packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:38:52AM: ^ Reconciling You can monitor the progress using this command:\nandreasm@linuxvm01:~$ k get pods -n tmc-local -w NAME READY STATUS RESTARTS AGE contour-contour-67b48bff88-fqvwk 1/1 Running 0 107s contour-contour-certgen-kt6hk 0/1 Completed 0 108s contour-envoy-9r4nm 2/2 Running 0 107s contour-envoy-gzkdf 2/2 Running 0 107s contour-envoy-hr8lj 2/2 Running 0 108s contour-envoy-m95qh 2/2 Running 0 107s kafka-0 0/1 ContainerCreating 0 66s kafka-exporter-6b4c74b596-k4crf 0/1 CrashLoopBackOff 3 (18s ago) 66s kafka-topic-controller-7bc498856b-sj5jw 1/1 Running 0 66s minio-7dbcffd86-w4rv9 1/1 Running 0 54s minio-provisioning-tsb6q 0/1 Completed 0 54s pinniped-supervisor-55c575555-shzjh 1/1 Running 0 74s postgres-endpoint-controller-5c784cd44d-gfg55 1/1 Running 0 23s postgres-postgresql-0 2/2 Running 0 57s s3-access-operator-68b6485c9b-jdbww 0/1 ContainerCreating 0 15s s3-access-operator-68b6485c9b-jdbww 1/1 Running 0 16s kafka-0 0/1 Running 0 72s There will be stages where several of the pods enters CrashLoopBackOff, Error, etc. Just give it time. If the package reconciliation fails. There is time to do some troubleshooting. And most likely it is DNS, certificate or the OIDC configuration. Check the progress on the package reconciliation:\nandreasm@linuxvm01:~$ k get pkgi -n tmc-local NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE contour contour.bitnami.com 12.1.0 Reconcile succeeded 7m20s kafka kafka.bitnami.com 22.1.3 Reconcile succeeded 6m37s kafka-topic-controller kafka-topic-controller.tmc.tanzu.vmware.com 0.0.21 Reconcile succeeded 6m37s minio minio.bitnami.com 12.6.4 Reconcile succeeded 6m37s pinniped pinniped.bitnami.com 1.2.1 Reconcile succeeded 6m45s postgres tmc-local-postgres.tmc.tanzu.vmware.com 0.0.46 Reconcile succeeded 6m37s postgres-endpoint-controller postgres-endpoint-controller.tmc.tanzu.vmware.com 0.1.43 Reconcile succeeded 5m58s s3-access-operator s3-access-operator.tmc.tanzu.vmware.com 0.1.22 Reconcile succeeded 5m46s tanzu-mission-control tmc.tanzu.vmware.com 1.0.0 Reconciling 7m26s tmc-local-stack tmc-local-stack.tmc.tanzu.vmware.com 0.0.17161 Reconciling 5m5s tmc-local-stack-secrets tmc-local-stack-secrets.tmc.tanzu.vmware.com 0.0.17161 Reconcile succeeded 7m20s tmc-local-support tmc-local-support.tmc.tanzu.vmware.com 0.0.17161 Reconcile succeeded 6m45s In the meantime, also check some of the required dns records such as tmc.pretty-awesome-domain.net and pinniped-supervisor.tmc.pretty-awesome-domain.net if they can be resolved:\nandreasm@linuxvm01:~$ ping pinniped-supervisor.tmc.pretty-awesome-domain.net If this error:\nping: pinniped-supervisor.tmc.pretty-awesome-domain.net: Temporary failure in name resolution I need to troubleshoot my dns-zone.\nIf I get this:\nandreasm@linuxvm01:~$ ping tmc.pretty-awesome-domain.net PING tmc.pretty-awesome-domain.net (10.101.210.12) 56(84) bytes of data. 64 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=13 ttl=61 time=7.31 ms 64 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=14 ttl=61 time=6.47 ms andreasm@linuxvm01:~$ ping pinniped-supervisor.tmc.pretty-awesome-domain.net PING pinniped-supervisor.tmc.pretty-awesome-domain.net (10.101.210.12) 56(84) bytes of data. 64 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=1 ttl=61 time=3.81 ms 64 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=2 ttl=61 time=9.28 ms I am good \u0026#x1f604;\nAfter waiting a while, the package installation process finished, either 100% successfully or with errors. In my environment it fails on step 25/26 on the tmc-local-monitoring. This turns out to be the alertmanager. I have a section below that explains how this can be solved.\nHere is the pod that is failing:\nandreasm@linuxvm01:~$ k get pods -n tmc-local NAME READY STATUS RESTARTS AGE account-manager-server-84b4758ccd-5zx7n 1/1 Running 0 14m account-manager-server-84b4758ccd-zfqlj 1/1 Running 0 14m agent-gateway-server-bf4f6c67-mvq2m 1/1 Running 1 (14m ago) 14m agent-gateway-server-bf4f6c67-zlj9d 1/1 Running 1 (14m ago) 14m alertmanager-tmc-local-monitoring-tmc-local-0 1/2 CrashLoopBackOff 7 (46s ago) 12m api-gateway-server-679b8478f9-57ss5 1/1 Running 1 (14m ago) 14m api-gateway-server-679b8478f9-t6j9s 1/1 Running 1 (14m ago) 14m audit-service-consumer-7bbdd4f55f-bjc5x 1/1 Running 0 14m But its not bad considering all the services and pods being deployed by TMC, one failed out of MANY:\nandreasm@linuxvm01:~$ k get pods -n tmc-local NAME READY STATUS RESTARTS AGE account-manager-server-84b4758ccd-5zx7n 1/1 Running 0 14m account-manager-server-84b4758ccd-zfqlj 1/1 Running 0 14m agent-gateway-server-bf4f6c67-mvq2m 1/1 Running 1 (14m ago) 14m agent-gateway-server-bf4f6c67-zlj9d 1/1 Running 1 (14m ago) 14m alertmanager-tmc-local-monitoring-tmc-local-0 1/2 CrashLoopBackOff 7 (46s ago) 12m api-gateway-server-679b8478f9-57ss5 1/1 Running 1 (14m ago) 14m api-gateway-server-679b8478f9-t6j9s 1/1 Running 1 (14m ago) 14m audit-service-consumer-7bbdd4f55f-bjc5x 1/1 Running 0 14m audit-service-consumer-7bbdd4f55f-h6h8c 1/1 Running 0 14m audit-service-server-898c98dc5-97s8l 1/1 Running 0 14m audit-service-server-898c98dc5-qvc9k 1/1 Running 0 14m auth-manager-server-79d7567986-7699w 1/1 Running 0 14m auth-manager-server-79d7567986-bbrg8 1/1 Running 0 14m auth-manager-server-79d7567986-tbdww 1/1 Running 0 14m authentication-server-695fd77f46-8p67m 1/1 Running 0 14m authentication-server-695fd77f46-ttd4l 1/1 Running 0 14m cluster-agent-service-server-599cf966f4-4ndkl 1/1 Running 0 14m cluster-agent-service-server-599cf966f4-h4g9l 1/1 Running 0 14m cluster-config-server-7c5f5f8dc6-99prt 1/1 Running 1 (13m ago) 14m cluster-config-server-7c5f5f8dc6-z4rvg 1/1 Running 0 14m cluster-object-service-server-7bc8f7c45c-fw97r 1/1 Running 0 14m cluster-object-service-server-7bc8f7c45c-k8bwc 1/1 Running 0 14m cluster-reaper-server-5f94f8dd6b-k2pxd 1/1 Running 0 14m cluster-secret-server-9fc44564f-g5lv5 1/1 Running 1 (14m ago) 14m cluster-secret-server-9fc44564f-vnbck 1/1 Running 0 14m cluster-service-server-6f7c657d7-ls9t7 1/1 Running 0 14m cluster-service-server-6f7c657d7-xvz7z 1/1 Running 0 14m cluster-sync-egest-f96d9b6bb-947c2 1/1 Running 0 14m cluster-sync-egest-f96d9b6bb-q22sg 1/1 Running 0 14m cluster-sync-ingest-798c88467d-c2pgj 1/1 Running 0 14m cluster-sync-ingest-798c88467d-pc2z7 1/1 Running 0 14m contour-contour-certgen-gdnns 0/1 Completed 0 17m contour-contour-ffddc764f-k25pb 1/1 Running 0 17m contour-envoy-4ptk4 2/2 Running 0 17m contour-envoy-66v8r 2/2 Running 0 17m contour-envoy-6shc8 2/2 Running 0 17m contour-envoy-br4nk 2/2 Running 0 17m dataprotection-server-58c6c9bd8d-dplbs 1/1 Running 0 14m dataprotection-server-58c6c9bd8d-hp2nz 1/1 Running 0 14m events-service-consumer-76bd756879-49bpb 1/1 Running 0 14m events-service-consumer-76bd756879-jnlkw 1/1 Running 0 14m events-service-server-694648bcc8-rjg27 1/1 Running 0 14m events-service-server-694648bcc8-trtm2 1/1 Running 0 14m fanout-service-server-7c6d9559b7-g7mvg 1/1 Running 0 14m fanout-service-server-7c6d9559b7-nhcjc 1/1 Running 0 14m feature-flag-service-server-855756576c-zltgh 1/1 Running 0 14m inspection-server-695b778b48-29s8q 2/2 Running 0 14m inspection-server-695b778b48-7hzf4 2/2 Running 0 14m intent-server-566dd98b76-dhcrx 1/1 Running 0 14m intent-server-566dd98b76-pjdpb 1/1 Running 0 14m kafka-0 1/1 Running 0 16m kafka-exporter-745d578567-5vhgq 1/1 Running 4 (15m ago) 16m kafka-topic-controller-5cf4d8c559-lxpcb 1/1 Running 0 15m landing-service-server-7ddd9774f-szx8v 1/1 Running 0 14m minio-764b688f5f-p7lrx 1/1 Running 0 16m minio-provisioning-5vsqs 0/1 Completed 1 16m onboarding-service-server-5ff888758f-bnzp5 1/1 Running 0 14m onboarding-service-server-5ff888758f-fq9dg 1/1 Running 0 14m package-deployment-server-79dd4b896d-9rv8z 1/1 Running 0 14m package-deployment-server-79dd4b896d-txq2x 1/1 Running 0 14m pinniped-supervisor-677578c495-jqbq4 1/1 Running 0 16m policy-engine-server-6bcbddf747-jks25 1/1 Running 0 14m policy-engine-server-6bcbddf747-vhxlm 1/1 Running 0 14m policy-insights-server-6878c9c8f-64ggn 1/1 Running 0 14m policy-sync-service-server-7699f47d65-scl5f 1/1 Running 0 14m policy-view-service-server-86bb698454-bvclh 1/1 Running 0 14m policy-view-service-server-86bb698454-zpkg9 1/1 Running 0 14m postgres-endpoint-controller-9d4fc9489-kgdf4 1/1 Running 0 15m postgres-postgresql-0 2/2 Running 0 16m prometheus-server-tmc-local-monitoring-tmc-local-0 2/2 Running 0 12m provisioner-service-server-84c4f9dc8f-khv2b 1/1 Running 0 14m provisioner-service-server-84c4f9dc8f-xl6gr 1/1 Running 0 14m resource-manager-server-8567f7cbbc-pl2fz 1/1 Running 0 14m resource-manager-server-8567f7cbbc-pqkxp 1/1 Running 0 14m s3-access-operator-7f4d77647b-xnnb2 1/1 Running 0 15m schema-service-schema-server-85cb7c7796-prjq7 1/1 Running 0 14m telemetry-event-service-consumer-7d6f8cc4b7-ffjcd 1/1 Running 0 14m telemetry-event-service-consumer-7d6f8cc4b7-thf44 1/1 Running 0 14m tenancy-service-server-57898676cd-9lpjl 1/1 Running 0 14m ui-server-6994bc9cd6-gtm6r 1/1 Running 0 14m ui-server-6994bc9cd6-xzxbv 1/1 Running 0 14m wcm-server-5c95c8d587-7sc9l 1/1 Running 1 (13m ago) 14m wcm-server-5c95c8d587-r2kbf 1/1 Running 1 (12m ago) 14m Troubleshooting the Alertmanager pod # If your package installation stops at 25/26, and the alertmanager pod is in a crasloopbackoff state: And if you check the logs of the alertmanager container it will throw you this error.\nk logs -n tmc-local alertmanager-tmc-local-monitoring-tmc-local-0 -c alertmanager ts=2023-07-13T14:16:30.239Z caller=main.go:231 level=info msg=\u0026#34;Starting Alertmanager\u0026#34; version=\u0026#34;(version=0.24.0, branch=HEAD, revision=f484b17fa3c583ed1b2c8bbcec20ba1db2aa5f11)\u0026#34; ts=2023-07-13T14:16:30.239Z caller=main.go:232 level=info build_context=\u0026#34;(go=go1.17.8, user=root@265f14f5c6fc, date=20220325-09:31:33)\u0026#34; ts=2023-07-13T14:16:30.240Z caller=cluster.go:178 level=warn component=cluster err=\u0026#34;couldn\u0026#39;t deduce an advertise address: no private IP found, explicit advertise addr not provided\u0026#34; ts=2023-07-13T14:16:30.241Z caller=main.go:263 level=error msg=\u0026#34;unable to initialize gossip mesh\u0026#34; err=\u0026#34;create memberlist: Failed to get final advertise address: No private IP address found, and explicit IP not provided\u0026#34; After some searching around, a workaround is to add the below values to the stateful set (see comments below):\nspec: containers: - args: - --volume-dir=/etc/alertmanager - --webhook-url=http://127.0.0.1:9093/-/reload image: registry.domain.net/project/package-repository@sha256:9125ebac75af1eb247de0982ce6d56bc7049a1f384f97c77a7af28de010f20a7 imagePullPolicy: IfNotPresent name: configmap-reloader resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/alertmanager/config name: config-volume readOnly: true - args: - --config.file=/etc/alertmanager/config/alertmanager.yaml - --cluster.advertise-address=$(POD_IP):9093 # added from here env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP # To here But setting this directly on the statefulset will be overwritten by the package conciliation.\nSo we need to apply this config using ytt overlay. Create a new yaml file, call it something like alertmanager-overlay.yaml. Below is my ytt config to achieve this:\napiVersion: v1 kind: Secret metadata: name: alertmanager-overlay-secret namespace: tmc-local stringData: patch.yaml: | #@ load(\u0026#34;@ytt:overlay\u0026#34;, \u0026#34;overlay\u0026#34;) #@overlay/match by=overlay.subset({\u0026#34;kind\u0026#34;:\u0026#34;StatefulSet\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;alertmanager-tmc-local-monitoring-tmc-local\u0026#34;}}) --- spec: template: spec: containers: #@overlay/replace - args: - --volume-dir=/etc/alertmanager - --webhook-url=http://127.0.0.1:9093/-/reload image: registry.domain.net/project/package-repository@sha256:9125ebac75af1eb247de0982ce6d56bc7049a1f384f97c77a7af28de010f20a7 imagePullPolicy: IfNotPresent name: configmap-reloader resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/alertmanager/config name: config-volume readOnly: true - args: - --config.file=/etc/alertmanager/config/alertmanager.yaml - --cluster.advertise-address=$(POD_IP):9093 env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP image: registry.domain.net/project/package-repository@sha256:74d46d5614791496104479bbf81c041515c5f8c17d9e9fcf1b33fa36e677156f imagePullPolicy: IfNotPresent name: alertmanager ports: - containerPort: 9093 name: alertmanager protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: /#/status port: 9093 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 30 resources: limits: cpu: 300m memory: 100Mi requests: cpu: 100m memory: 70Mi terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /etc/alertmanager/config name: config-volume readOnly: true - mountPath: /data name: data --- apiVersion: v1 kind: Secret metadata: name: tmc-overlay-override namespace: tmc-local stringData: patch-alertmanager.yaml: | #@ load(\u0026#34;@ytt:overlay\u0026#34;, \u0026#34;overlay\u0026#34;) #@overlay/match by=overlay.subset({\u0026#34;kind\u0026#34;:\u0026#34;PackageInstall\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;tmc-local-monitoring\u0026#34;}}) --- metadata: annotations: #@overlay/match missing_ok=True ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: alertmanager-overlay-secret This was the only way I managed to get the configs applied correctly. It can probably be done a different way, but it works.\nApply the above yaml:\nandreasm@linuxvm01:~/tmc-sm/errors$ k apply -f alertmanager-overlay.yaml secret/alertmanager-overlay-secret configured secret/tmc-overlay-override configured Then I need to annotate the package:\nkubectl annotate packageinstalls tanzu-mission-control -n tmc-local ext.packaging.carvel.dev/ytt-paths-from-secret-name.0=tmc-overlay-override Pause and unpause the reconciliation (if it is already in a reconciliation state its not always necessary to pause and unpause). But to kick it off immediately, run the commands below.\nandreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tmc-local-monitoring --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: true}}\u0026#39; andreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tmc-local-monitoring --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: false}}\u0026#39; packageinstall.packaging.carvel.dev/tmc-local-monitoring patched One can also kick the reconcile by pointing to the package tanzu-mission-control:\nandreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tanzu-mission-control --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: true}}\u0026#39; packageinstall.packaging.carvel.dev/tanzu-mission-control patched andreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tanzu-mission-control --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: false}}\u0026#39; packageinstall.packaging.carvel.dev/tanzu-mission-control patched The end result should give us this in our alertmanager statefulset:\nandreasm@linuxvm01:~/tmc-sm/errors$ k get statefulsets.apps -n tmc-local alertmanager-tmc-local-monitoring-tmc-local -oyaml #snippet - args: - --config.file=/etc/alertmanager/config/alertmanager.yaml - --cluster.advertise-address=$(POD_IP):9093 env: - name: POD_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.podIP #snippet And the alertmanager pod should start:\nandreasm@linuxvm01:~/tmc-sm/errors$ k get pod -n tmc-local alertmanager-tmc-local-monitoring-tmc-local-0 NAME READY STATUS RESTARTS AGE alertmanager-tmc-local-monitoring-tmc-local-0 2/2 Running 0 10m If its still in CrashLoopBackOff just delete the pod and it should go into a running state. If not, describe the alermananger statefulset for any additional errors, maybe a typo in the ytt overlay yaml\u0026hellip;\nOne can also do this operation while the installation is waiting on the package tmc-local-monitoring re-conciliation. So the package installation will be successful after all.\nInstall the TMC-SM package - continued # What about the services created, httpproxies and Ingress?\nGet the Services:\nandreasm@linuxvm01:~$ k get svc -n tmc-local NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE account-manager-grpc ClusterIP 20.10.134.215 \u0026lt;none\u0026gt; 443/TCP 18m account-manager-service ClusterIP 20.10.6.142 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m agent-gateway-service ClusterIP 20.10.111.64 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m alertmanager-tmc-local-monitoring-tmc-local ClusterIP 20.10.113.103 \u0026lt;none\u0026gt; 9093/TCP 15m api-gateway-service ClusterIP 20.10.241.28 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m audit-service-consumer ClusterIP 20.10.183.29 \u0026lt;none\u0026gt; 7777/TCP 18m audit-service-grpc ClusterIP 20.10.94.221 \u0026lt;none\u0026gt; 443/TCP 18m audit-service-rest ClusterIP 20.10.118.27 \u0026lt;none\u0026gt; 443/TCP 18m audit-service-service ClusterIP 20.10.193.140 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m auth-manager-server ClusterIP 20.10.86.230 \u0026lt;none\u0026gt; 443/TCP 18m auth-manager-service ClusterIP 20.10.136.164 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m authentication-grpc ClusterIP 20.10.32.80 \u0026lt;none\u0026gt; 443/TCP 18m authentication-service ClusterIP 20.10.69.22 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m cluster-agent-service-grpc ClusterIP 20.10.55.122 \u0026lt;none\u0026gt; 443/TCP 18m cluster-agent-service-installer ClusterIP 20.10.185.105 \u0026lt;none\u0026gt; 80/TCP 18m cluster-agent-service-service ClusterIP 20.10.129.243 \u0026lt;none\u0026gt; 443/TCP,80/TCP,7777/TCP 18m cluster-config-service ClusterIP 20.10.237.148 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m cluster-object-service-grpc ClusterIP 20.10.221.128 \u0026lt;none\u0026gt; 443/TCP 18m cluster-object-service-service ClusterIP 20.10.238.0 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m cluster-reaper-grpc ClusterIP 20.10.224.97 \u0026lt;none\u0026gt; 443/TCP 18m cluster-reaper-service ClusterIP 20.10.65.179 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m cluster-secret-service ClusterIP 20.10.17.122 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m cluster-service-grpc ClusterIP 20.10.152.204 \u0026lt;none\u0026gt; 443/TCP 18m cluster-service-rest ClusterIP 20.10.141.159 \u0026lt;none\u0026gt; 443/TCP 18m cluster-service-service ClusterIP 20.10.40.169 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m cluster-sync-egest ClusterIP 20.10.47.77 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m cluster-sync-egest-grpc ClusterIP 20.10.219.9 \u0026lt;none\u0026gt; 443/TCP 18m cluster-sync-ingest ClusterIP 20.10.223.205 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m cluster-sync-ingest-grpc ClusterIP 20.10.196.7 \u0026lt;none\u0026gt; 443/TCP 18m contour ClusterIP 20.10.5.59 \u0026lt;none\u0026gt; 8001/TCP 21m contour-envoy LoadBalancer 20.10.72.121 10.101.210.12 80:31964/TCP,443:31350/TCP 21m contour-envoy-metrics ClusterIP None \u0026lt;none\u0026gt; 8002/TCP 21m dataprotection-grpc ClusterIP 20.10.47.233 \u0026lt;none\u0026gt; 443/TCP 18m dataprotection-service ClusterIP 20.10.73.15 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m events-service-consumer ClusterIP 20.10.38.207 \u0026lt;none\u0026gt; 7777/TCP 18m events-service-grpc ClusterIP 20.10.65.181 \u0026lt;none\u0026gt; 443/TCP 18m events-service-service ClusterIP 20.10.34.169 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m fanout-service-grpc ClusterIP 20.10.77.108 \u0026lt;none\u0026gt; 443/TCP 18m fanout-service-service ClusterIP 20.10.141.34 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m feature-flag-service-grpc ClusterIP 20.10.171.161 \u0026lt;none\u0026gt; 443/TCP 18m feature-flag-service-service ClusterIP 20.10.112.195 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m inspection-grpc ClusterIP 20.10.20.119 \u0026lt;none\u0026gt; 443/TCP 18m inspection-service ClusterIP 20.10.85.86 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m intent-grpc ClusterIP 20.10.213.53 \u0026lt;none\u0026gt; 443/TCP 18m intent-service ClusterIP 20.10.19.196 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m kafka ClusterIP 20.10.135.162 \u0026lt;none\u0026gt; 9092/TCP 20m kafka-headless ClusterIP None \u0026lt;none\u0026gt; 9092/TCP,9094/TCP,9093/TCP 20m kafka-metrics ClusterIP 20.10.175.161 \u0026lt;none\u0026gt; 9308/TCP 20m landing-service-metrics ClusterIP None \u0026lt;none\u0026gt; 7777/TCP 18m landing-service-rest ClusterIP 20.10.37.157 \u0026lt;none\u0026gt; 443/TCP 18m landing-service-server ClusterIP 20.10.28.110 \u0026lt;none\u0026gt; 443/TCP 18m minio ClusterIP 20.10.234.32 \u0026lt;none\u0026gt; 9000/TCP,9001/TCP 20m onboarding-service-metrics ClusterIP None \u0026lt;none\u0026gt; 7777/TCP 18m onboarding-service-rest ClusterIP 20.10.66.85 \u0026lt;none\u0026gt; 443/TCP 18m package-deployment-service ClusterIP 20.10.40.90 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m pinniped-supervisor ClusterIP 20.10.138.177 \u0026lt;none\u0026gt; 443/TCP 20m pinniped-supervisor-api ClusterIP 20.10.218.242 \u0026lt;none\u0026gt; 443/TCP 20m policy-engine-grpc ClusterIP 20.10.114.38 \u0026lt;none\u0026gt; 443/TCP 18m policy-engine-service ClusterIP 20.10.85.191 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m policy-insights-grpc ClusterIP 20.10.95.196 \u0026lt;none\u0026gt; 443/TCP 18m policy-insights-service ClusterIP 20.10.119.38 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m policy-sync-service-service ClusterIP 20.10.32.72 \u0026lt;none\u0026gt; 7777/TCP 18m policy-view-service-grpc ClusterIP 20.10.4.163 \u0026lt;none\u0026gt; 443/TCP 18m policy-view-service-service ClusterIP 20.10.41.172 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m postgres-endpoint-controller ClusterIP 20.10.3.234 \u0026lt;none\u0026gt; 9876/TCP 18m postgres-postgresql ClusterIP 20.10.10.197 \u0026lt;none\u0026gt; 5432/TCP 20m postgres-postgresql-hl ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 20m postgres-postgresql-metrics ClusterIP 20.10.79.247 \u0026lt;none\u0026gt; 9187/TCP 20m prometheus-server-tmc-local-monitoring-tmc-local ClusterIP 20.10.152.45 \u0026lt;none\u0026gt; 9090/TCP 15m provisioner-service-grpc ClusterIP 20.10.138.198 \u0026lt;none\u0026gt; 443/TCP 18m provisioner-service-service ClusterIP 20.10.96.47 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m resource-manager-grpc ClusterIP 20.10.143.168 \u0026lt;none\u0026gt; 443/TCP 18m resource-manager-service ClusterIP 20.10.238.70 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m s3-access-operator ClusterIP 20.10.172.230 \u0026lt;none\u0026gt; 443/TCP,8080/TCP 19m schema-service-grpc ClusterIP 20.10.237.93 \u0026lt;none\u0026gt; 443/TCP 18m schema-service-service ClusterIP 20.10.99.167 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m telemetry-event-service-consumer ClusterIP 20.10.196.48 \u0026lt;none\u0026gt; 7777/TCP 18m tenancy-service-metrics-headless ClusterIP None \u0026lt;none\u0026gt; 7777/TCP 18m tenancy-service-tenancy-service ClusterIP 20.10.80.23 \u0026lt;none\u0026gt; 443/TCP 18m tenancy-service-tenancy-service-rest ClusterIP 20.10.200.153 \u0026lt;none\u0026gt; 443/TCP 18m ui-server ClusterIP 20.10.233.160 \u0026lt;none\u0026gt; 8443/TCP,7777/TCP 18m wcm-grpc ClusterIP 20.10.19.188 \u0026lt;none\u0026gt; 443/TCP 18m wcm-service ClusterIP 20.10.175.206 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m Get the Ingresses:\nandreasm@linuxvm01:~$ k get ingress -n tmc-local NAME CLASS HOSTS ADDRESS PORTS AGE alertmanager-tmc-local-monitoring-tmc-local-ingress tmc-local alertmanager.tmc.pretty-awesome-domain.net 10.101.210.12 80 16m landing-service-ingress-global tmc-local landing.tmc.pretty-awesome-domain.net 10.101.210.12 80, 443 19m minio tmc-local console.s3.tmc.pretty-awesome-domain.net 10.101.210.12 80 20m minio-api tmc-local s3.tmc.pretty-awesome-domain.net 10.101.210.12 80, 443 20m prometheus-server-tmc-local-monitoring-tmc-local-ingress tmc-local prometheus.tmc.pretty-awesome-domain.net 10.101.210.12 80 16m Ah, there is my dns records \u0026#x1f604;\nGet the HTTPProxies\nandreasm@linuxvm01:~$ k get httpproxies -n tmc-local NAME FQDN TLS SECRET STATUS STATUS DESCRIPTION auth-manager-server auth.tmc.pretty-awesome-domain.net server-tls valid Valid HTTPProxy minio-api-proxy s3.tmc.pretty-awesome-domain.net minio-tls valid Valid HTTPProxy minio-bucket-proxy tmc-local.s3.tmc.pretty-awesome-domain.net minio-tls valid Valid HTTPProxy minio-console-proxy console.s3.tmc.pretty-awesome-domain.net minio-tls valid Valid HTTPProxy pinniped-supervisor pinniped-supervisor.tmc.pretty-awesome-domain.net valid Valid HTTPProxy stack-http-proxy tmc.pretty-awesome-domain.net stack-tls valid Valid HTTPProxy tenancy-service-http-proxy gts.tmc.pretty-awesome-domain.net valid Valid HTTPProxy tenancy-service-http-proxy-rest gts-rest.tmc.pretty-awesome-domain.net valid Valid HTTPProxy Ah.. More DNS records.\nUnfortunately my tmc-sm deployement gave me this error in the end, which can be solved afterwards or during the install process following the section on Alertmanager above:\n| 8:32:35AM: ---- waiting on 1 changes [25/26 done] ---- | 8:32:36AM: ongoing: reconcile packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 8:32:36AM: ^ Reconciling 8:33:30AM: Deploy failed | kapp: Error: Timed out waiting after 15m0s for resources: [packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local] | Deploying: Error (see .status.usefulErrorMessage for details) 8:33:30AM: Error tailing app: Reconciling app: Deploy failed 8:33:30AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: ReconcileFailed Error: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling: kapp: Error: Timed out waiting after 15m0s for resources: [packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local]. Reconcile failed: Error (see .status.usefulErrorMessage for details) Except the Alertmanager pod which can be fixed, it is kind a success. Remember also the note in the official documentation:\nDeploying TMC Self-Managed 1.0 on a Tanzu Kubernetes Grid (TKG) 2.0 workload cluster running in vSphere with Tanzu on vSphere version 8.x is for tech preview only. Initiate deployments only in pre-production environments or production environments where support for the integration is not required. vSphere 8u1 or later is required in order to test the tech preview integration.\nNow its time to log in to the TMC-SM UI..\nUninstall tmc-sm packages # To uninstall, after a failed deployement or other reasons. Issue this command:\nandreasm@linuxvm01:~/tmc-sm$ tanzu package installed delete tanzu-mission-control -n tmc-local Delete package install \u0026#39;tanzu-mission-control\u0026#39; from namespace \u0026#39;tmc-local\u0026#39; Continue? [yN]: y 7:55:19AM: Deleting package install \u0026#39;tanzu-mission-control\u0026#39; from namespace \u0026#39;tmc-local\u0026#39; 7:55:19AM: Waiting for deletion of package install \u0026#39;tanzu-mission-control\u0026#39; from namespace \u0026#39;tmc-local\u0026#39; 7:55:19AM: Waiting for generation 2 to be observed 7:55:19AM: Delete started (2s ago) 7:55:21AM: Deleting | Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; (nodes: tmc-sm-cluster-node-pool-3-ctgxg-5f76bd48d8-hzh7h, 4+) | Changes | Namespace Name Kind Age Op Op st. Wait to Rs Ri | (cluster) tmc-install-cluster-admin-role ClusterRole 17m delete - delete ok - | ^ tmc-install-cluster-admin-role-binding ClusterRoleBinding 17m delete - delete ok - | tmc-local contour PackageInstall 17m delete - delete ok - | ^ contour-values-ver-1 Secret 17m delete - delete ok - | ^ kafka PackageInstall 16m delete - delete ok - | ^ kafka-topic-controller PackageInstall 16m delete - delete ok - | ^ kafka-topic-controller-values-ver-1 Secret 17m delete - delete ok - | ^ kafka-values-ver-1 Secret 17m delete - delete ok - | ^ minio PackageInstall 16m delete - delete ok - | ^ minio-values-ver-1 Secret 17m delete - delete ok - | ^ monitoring-values-ver-1 Secret 17m delete - delete ok - | ^ pinniped PackageInstall 16m delete - delete ok - | ^ pinniped-values-ver-1 Secret 17m delete - delete ok - | ^ postgres PackageInstall 16m delete - delete ok - | ^ postgres-endpoint-controller PackageInstall 15m delete - delete ok - | ^ postgres-endpoint-controller-values-ver-1 Secret 17m delete - delete ok - | ^ postgres-values-ver-1 Secret 17m delete - delete ok - | ^ s3-access-operator PackageInstall 15m delete - delete ok - | ^ s3-access-operator-values-ver-1 Secret 17m delete - delete ok - | ^ tmc-install-sa ServiceAccount 17m delete - delete ok - | ^ tmc-local-monitoring PackageInstall 4m delete - delete ongoing Reconciling | ^ tmc-local-stack PackageInstall 14m delete - delete fail Reconcile failed: (message: Error | (see .status.usefulErrorMessage for | details)) | ^ tmc-local-stack-secrets PackageInstall 17m delete - delete ok - | ^ tmc-local-stack-values-ver-1 Secret 17m delete - delete ok - | ^ tmc-local-support PackageInstall 16m delete - delete ok - | ^ tmc-local-support-values-ver-1 Secret 17m delete - delete ok - | Op: 0 create, 26 delete, 0 update, 0 noop, 0 exists | Wait to: 0 reconcile, 26 delete, 0 noop | 7:55:19AM: ---- applying 23 changes [0/26 done] ---- | 7:55:19AM: delete secret/monitoring-values-ver-1 (v1) namespace: tmc-local | 7:55:19AM: delete secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local | 7:55:19AM: delete secret/contour-values-ver-1 (v1) namespace: tmc-local | 7:55:19AM: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:19AM: delete secret/kafka-values-ver-1 (v1) namespace: tmc-local | 7:55:19AM: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:19AM: delete secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local | 7:55:19AM: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: delete packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete secret/postgres-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: delete secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: delete secret/minio-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete secret/pinniped-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: delete packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ---- waiting on 23 changes [0/26 done] ---- | 7:55:20AM: ok: delete secret/monitoring-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ok: delete secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ok: delete secret/contour-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ongoing: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ongoing: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ok: delete secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ok: delete secret/kafka-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ongoing: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ongoing: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ok: delete secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ongoing: delete packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ok: delete secret/postgres-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ongoing: delete packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ok: delete secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ok: delete secret/minio-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ongoing: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ongoing: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ok: delete secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ok: delete secret/pinniped-values-ver-1 (v1) namespace: tmc-local | 7:55:20AM: ongoing: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ongoing: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ongoing: delete packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ongoing: delete packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:55:20AM: ---- waiting on 12 changes [11/26 done] ---- | 7:55:27AM: ok: delete packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:27AM: ---- waiting on 11 changes [12/26 done] ---- | 7:55:28AM: ok: delete packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:28AM: ---- waiting on 10 changes [13/26 done] ---- | 7:55:59AM: ok: delete packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:55:59AM: ---- waiting on 9 changes [14/26 done] ---- | 7:56:03AM: ok: delete packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:03AM: ---- waiting on 8 changes [15/26 done] ---- | 7:56:20AM: ongoing: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:20AM: ongoing: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:56:37AM: ok: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:37AM: ---- waiting on 7 changes [16/26 done] ---- | 7:56:38AM: ok: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:38AM: ---- waiting on 6 changes [17/26 done] ---- | 7:56:40AM: ok: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:40AM: ---- waiting on 5 changes [18/26 done] ---- | 7:56:43AM: ok: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:43AM: ---- waiting on 4 changes [19/26 done] ---- | 7:56:48AM: ok: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:48AM: ---- waiting on 3 changes [20/26 done] ---- | 7:56:54AM: ok: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:56:54AM: ---- waiting on 2 changes [21/26 done] ---- | 7:57:21AM: ongoing: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:57:21AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:57:21AM: ongoing: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:57:21AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete | 7:57:40AM: ok: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local | 7:57:40AM: ---- waiting on 1 changes [22/26 done] ---- 7:57:43AM: App \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; deleted 7:57:44AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: DeletionSucceeded 7:57:44AM: Deleting \u0026#39;Secret\u0026#39;: tanzu-mission-control-tmc-local-values 7:57:44AM: Deleting \u0026#39;ServiceAccount\u0026#39;: tanzu-mission-control-tmc-local-sa 7:57:44AM: Deleting \u0026#39;ClusterRole\u0026#39;: tanzu-mission-control-tmc-local-cluster-role 7:57:44AM: Deleting \u0026#39;ClusterRoleBinding\u0026#39;: tanzu-mission-control-tmc-local-cluster-rolebinding There may be reasons you need to remove the namespace tmc-local also has it contains a lot of configmaps, secret and pvc volumes. So if you want to completeley and easily remove everything TMC-SM related, delete the namespace. From the official documentation:\nTo remove Tanzu Mission Control Self-Managed and its artifacts from you cluster, use the tanzu cli.\nBack up any data that you do not want to lose.\nRun the following commands:\ntanzu package installed delete tanzu-mission-control --namespace tmc-local\rtanzu package repository delete tanzu-mission-control-packages --namespace tmc-local If necessary, delete residual resources.\nThe above commands clean up most of the resources that were created by the tanzu-mission-control Tanzu package. However, there are some resources that you have to remove manually. The resources include: - persistent volumes - internal TLS certificates - configmaps\nAlternatively, you can delete the tmc-local namespace. When you delete the tmc-local namespace, the persistent volume claims associated with the namespace are deleted. Make sure you have already backed up any data that you don’t want to lose.\nFirst time login TMC-SM # If everything above went after plan (not for me, just a minor issue), I should now be able to login to my TMC-SM console.\nUsing my regular client from a web-browser enter https://tmc.pretty-awesome-domain.net\nAnd \u0026#x1f941; I am logged into TMC-SM.\nI will end this post here. Will create a second post on working with TMC-SM. Thanks for reading.\nCredits where credit\u0026rsquo;s due # In this post its necessary to give credits again for making this post. This time it goes to my manager Antonio and colleague Jose that helped out with the initial configs, then my colleague Alex that helped out with the Keycloak authentication related settings.\n","date":"12 July 2023","externalUrl":null,"permalink":"/2023/07/12/installing-tmc-local-on-vsphere-8-with-tanzu-using-keycloak-as-oidc-provider/","section":"Posts","summary":"In this post I will be going through how to deploy the newly released TMC local in a lab environment to start playing around with it. I will also cover how I have deployed Keycloak to serve my needs for TMC-SM.","title":"Installing TMC local on vSphere 8 with Tanzu using Keycloak as OIDC provider","type":"posts"},{"content":"","date":"12 July 2023","externalUrl":null,"permalink":"/tags/keycloak/","section":"Tags","summary":"","title":"Keycloak","type":"tags"},{"content":"","date":"12 July 2023","externalUrl":null,"permalink":"/categories/lcm/","section":"Categories","summary":"","title":"LCM","type":"categories"},{"content":"","date":"12 July 2023","externalUrl":null,"permalink":"/tags/tmc-local/","section":"Tags","summary":"","title":"Tmc-Local","type":"tags"},{"content":"","date":"12 July 2023","externalUrl":null,"permalink":"/tags/tmc-sm/","section":"Tags","summary":"","title":"Tmc-Sm","type":"tags"},{"content":" Some context\u0026hellip; # I have written a couple of post previously on the topic Antrea Policies, this time I will try to put it more into a context, how we can use and create Antrea Policies in different scenarios and with some \u0026ldquo;frameworks\u0026rdquo; from different perspectives in the organization.\nWhat if, and how, can we deliver a already secured Kubernetes cluster, like an out of the box experience, with policies applied that meets certain guidelines for what is allowed and not allowed in the organization for certain Kubernetes clusters. Whether they are manually provisioned, or provisined on demand. So in this post a will try to be a bit specific on how to achieve this, with a simulated requirement as context, will get back to this context a further down. The following products will be used in this post: vSphere with Tanzu and TKG workload clusters, Antrea as the CNI, Tanzu Mission Control and VMware NSX.\nAs usual, for more details on the above mentioned product head over to the below links\nAntrea for detailed and updated documentation. vSphere with Tanzu for detailed and updated documentation. Tanzu Mission Control for detailed and updated documentation. VMware NSX for detailed and updated documentation. Different layers of security, different personas, different enforcement points # This post will mostly be focusing in on the Kubernetes perspective, using specifically Antrea Network policies to restrict traffic inside the Kubernetes cluster. A Kubernetes cluster is just one infrastructure component in the organization, but contains many moving parts with applications and services inside. Even inside a Kubernetes cluster there can be different classifications for what should be allowed and not. Therefore a Kubernetes cluster is also in need to be to be secured with a set of tools and policies to satisfy the security policy guidelines in the organization. A Kubernetes cluster is another layer in the infrastructure that needs to be controlled. In a typical datacenter we have several security mechanisms in place like AV agents, physical firewall, virtual firewall, NSX distributed firewall. All these play an important role in the different layers of the datacenter/organization. Assuming the Kubernetes worker nodes are running as virtual machines on VMware vSphere the below illustration describes two layers of security using NSX distributed firewall securing the VM workers, and Antrea Network Policies securing pods, services inside the Kubernetes cluster.\nWith the illustration above in mind it is fully possible to create a very strict environment with no unwanted lateral movement. Meaning only the strict necessary firewall openings inside the kubernetes cluster between pods, namespaces and services, but also between workers in same subnet and across several several Kubernetes clusters. But the above two layers, VMs in vSphere protected by the NSX distributed firewall and apps running Kubernetes clusters and protected by Antrea Network policies, are often managed by different personas in the organization. We have the vSphere admins, Network admins, Security Admins, App Operators and App Developers. Security is crucial in a modern datacenter, so, again, the correct tools needs to be in place for the organization\u0026rsquo;s security-framework to be implemented all the way down the \u0026ldquo;stack\u0026rdquo; to be compliant. Very often there is a decided theoretical security framework/design in place, but that plan is not always so straightforward to implement.\nGoing back to Kubernetes again and Antrea Network policies. Antrea feature several static (and optional custom) Tiers where different types of network policies can be applied. As all the Antrea Network policies are evaluated \u0026ldquo;top-down\u0026rdquo; it is very handy to be able to place some strict rules very early in the \u0026ldquo;chain\u0026rdquo; of firewall policies to ensure the organization\u0026rsquo;s security compliance is met. Being able to place these rules at the top prohibits the creation of rules further down that contradicts these top rules, they will not be evaluated. Then there is room to create a framework that gives some sense of \u0026ldquo;flexibility\u0026rdquo; to support the environment\u0026rsquo;s workload according to the type of classification (prod, dev, test, dmz, trust, untrust). Other policies can be applied to further restrict movement before hitting a default block rule that takes care of anything that is not specified earlier in the \u0026ldquo;chain\u0026rdquo; of policies. The illustration below is an example of whom and where these personas can take charge and apply their needed policies.\nThen the next illustration is the default Static Tiers that comes with Antrea. These Tiers makes it easier to categorize the different policies in a Kubernetes cluster, but also provides a great way to delegate responsibility/permissions by using RBAC to control access to the Tiers. This means we can have some admins to apply policies in specific Tiers, and no one else can overwrite these.\nNow, how can the different personas make sure their policies are applied? This is what I will go through next.\nManaging and making sure the required Antrea Policies are applied # Lets start out by bringing some light on the simulated requirement I mentioned above. Customer Andreas have some strict security guidelines they need to follow to ensure compliance before anyone can do anything in the Kubernetes platforms. To be compliant according to the strict security guidelines the following must be in place:\nAll Kubernetes workload clusters are considered isolated and not allowed to reach nothing more than themselves, including pods and services (all nodes in the same cluster) Only necessary backend functions such as DNS/NTP are allowed. Certain management tools need access to the clusters All non-system namespaces should be considered \u0026ldquo;untrusted\u0026rdquo; and isolated by default. RBAC needs to be in place to ensure no tampering on applied security policies. The above diagram is what customer Andreas needs to have in place. Lets go ahead and apply them. In the next sub-chapters I will show how to apply and manage the policies in three different ways to acheive this. I assume the NSX personas has done their part and applied the correct distributed firewall rules isolating the worker nodes.\nApplying Antrea policies with kubectl # This process involves logging into a newly provisioned Kubernetes cluster (TKG cluster in my environment) that someone has provisioned, could be the vSphere admin persona, or via a self-service. Then the security admin will be using kubectl to log in and apply some yaml definitions to acheive the above requirements. This operation will typically be the security admin responsibilities. The definitions the security admin is applying will all be configured in the static Tier \u0026ldquo;securityops\u0026rdquo; with different priorities. Here is the demo-environment I will be using in the following chapters: The first requirement is a \u0026ldquo;no-trust\u0026rdquo; in any non-system namespaces, where I want to achieve full isolation between namespace. No communication from one namespace to another. In the Antrea homepage there are several examples, and I will use one of the examples that suits my need perfectly. It looks like this:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: strict-ns-isolation-except-system-ns spec: priority: 9 tier: securityops appliedTo: - namespaceSelector: # Selects all non-system Namespaces in the cluster matchExpressions: - {key: kubernetes.io/metadata.name, operator: NotIn, values: [avi-system,default,kube-node-lease,kube-public,kube-system,secretgen-controller,tanzu-continuousdelivery-resources,tanzu-fluxcd-packageinstalls,tanzu-kustomize-controller,tanzu-source-controller,tkg-system,vmware-system-auth,vmware-system-cloud-provider,vmware-system-csi,vmware-system-tkg,vmware-system-tmc]} ingress: - action: Pass from: - namespaces: match: Self # Skip ACNP evaluation for traffic from Pods in the same Namespace name: PassFromSameNS - action: Drop from: - namespaceSelector: {} # Drop from Pods from all other Namespaces name: DropFromAllOtherNS egress: - action: Pass to: - namespaces: match: Self # Skip ACNP evaluation for traffic to Pods in the same Namespace name: PassToSameNS - action: Drop to: - namespaceSelector: {} # Drop to Pods from all other Namespaces name: DropToAllOtherNS The only modifications I have done is adding all my system-namespaces. Then I will apply it.\n# Verifying no policies in place: andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp No resources found andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f acnp-ns-isolation-except-system-ns.yaml clusternetworkpolicy.crd.antrea.io/strict-ns-isolation-except-system-ns created andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE strict-ns-isolation-except-system-ns securityops 9 0 0 15s Notice the 0 under Desired Nodes and Current Nodes. The reason is that this cluster is completely new, and there is no workload in any non-system namespaces yet. Here are the current namespaces:\nandreasm@linuxvm01:~/antrea/policies/groups$ k get ns NAME STATUS AGE default Active 28d kube-node-lease Active 28d kube-public Active 28d kube-system Active 28d secretgen-controller Active 28d tkg-system Active 28d vmware-system-auth Active 28d vmware-system-cloud-provider Active 28d vmware-system-csi Active 28d vmware-system-tkg Active 28d Now if I apply a couple of namespaces and deploy some workload in them:\nandreasm@linuxvm01:~/antrea/policies/groups$ k apply -f dev-app.yaml -f dev-app2.yaml namespace/dev-app created deployment.apps/ubuntu-20-04 created namespace/dev-app2 created deployment.apps/ubuntu-dev-app2 created How does the policy look like now?\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE strict-ns-isolation-except-system-ns securityops 9 1 1 6s # Why only one andreasm@linuxvm01:~/antrea/policies/groups$ k get pods -n dev-app -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-548545fc87-t2lg2 1/1 Running 0 82s 20.20.3.216 three-zone-cluster-1-node-pool-3-6r8c2-6c8d48656c-wntwc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; andreasm@linuxvm01:~/antrea/policies/groups$ k get pods -n dev-app2 -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-dev-app2-564f46785c-g8vb6 1/1 Running 0 86s 20.20.3.215 three-zone-cluster-1-node-pool-3-6r8c2-6c8d48656c-wntwc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Both workloads ended up on same node\u0026hellip;\nSo far so good. Now I need to verify if it is actually enforcing anything. From one of the dev-app pods I will execute into bash and try ping another pod in one for the system-namespaces, a pod in the the other dev-app namespace and try to a dns lookup.\nandreasm@linuxvm01:~/antrea/policies/groups$ k exec -it -n dev-app ubuntu-20-04-548545fc87-t2lg2 bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.1.7 PING 20.20.1.7 (20.20.1.7) 56(84) bytes of data. ^C --- 20.20.1.7 ping statistics --- 170 packets transmitted, 0 received, 100% packet loss, time 173033ms The ping above was from my dev-app pod to the coredns pod in kube-system. Ping to the other dev-app pod in the other dev-app namespace.\nroot@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.3.215 PING 20.20.3.215 (20.20.3.215) 56(84) bytes of data. ^C --- 20.20.3.215 ping statistics --- 9 packets transmitted, 0 received, 100% packet loss, time 8181ms Is also blocked.\nNow DNS lookup:\nroot@ubuntu-20-04-548545fc87-t2lg2:/# ping google.com ping: google.com: Temporary failure in name resolution #So much empty DNS was also one of the requirements, so I will have to fix this also. I mean, the security admin will have to fix this otherwise going to lunch will not be such a great place to be\u0026hellip;\nAs the security admin have applied the above policy in the securityops tier with a priority of 9 he need to open up for DNS with policies in a higher tier or within same tier with a lower priority number (lower equals higher priority).\nThis is the policy he needs to apply:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: allow-all-egress-dns-service spec: priority: 8 tier: securityops appliedTo: - namespaceSelector: {} # matchLabels: # k8s-app: kube-dns egress: - action: Allow toServices: - name: kube-dns namespace: kube-system name: \u0026#34;allowdnsegress-service\u0026#34; A simple one, and the requirement is satisfied. Company Andreas allowed necessay functions such as DNS.. This policy will allow any namespace to reach the kube-dns service.\nThe rule applied:\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE allow-all-egress-dns-service securityops 8 4 4 2m24s strict-ns-isolation-except-system-ns securityops 9 1 1 24m What about DNS lookup now:\nroot@ubuntu-20-04-548545fc87-t2lg2:/# ping google.com PING google.com (172.217.12.110) 56(84) bytes of data. 64 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=1 ttl=105 time=33.0 ms 64 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=2 ttl=104 time=29.8 ms 64 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=3 ttl=105 time=30.2 ms 64 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=4 ttl=104 time=30.3 ms 64 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=5 ttl=105 time=30.4 ms ^C --- google.com ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4003ms rtt min/avg/max/mdev = 29.763/30.733/32.966/1.138 ms Works.\nThats one more requirement met. Now one of the requirements was also to restrict access to services in other kubernetes clusters. Even though we trust that the NSX admins have created these isolation rules for us we need to make sure we are not allowed from the current kubernetes cluster also.\nSo to acheive this the security admin needs to create ClusterGroup containing the CIDR for its own worker nodes. Then apply a policy using the ClusterGroup. Here is the ClusterGroup definition (containing the cidr for the worker nodes):\napiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: name: tz-cluster-1-node-cidr spec: # ipBlocks cannot be set along with podSelector, namespaceSelector or serviceReference. ipBlocks: - cidr: 10.101.82.32/27 And I also need to define another ClusterGroup for all the RFC1918 subnets I need to block (this will include the cidr above):\napiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: name: tz-cluster-1-drop-cidr spec: # ipBlocks cannot be set along with podSelector, namespaceSelector or serviceReference. ipBlocks: - cidr: 10.0.0.0/8 - cidr: 172.16.0.0/12 - cidr: 192.168.0.0/16 Apply them:\nandreasm@linuxvm01:~/antrea/policies/groups$ k apply -f tz-cluster-1-group-node-cidr.yaml clustergroup.crd.antrea.io/tz-cluster-1-node-cidr created andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f tz-cluster-1-drop-cidr.yaml clustergroup.crd.antrea.io/tz-cluster-1-drop-cidr created andreasm@linuxvm01:~/antrea/policies/groups$ k get clustergroup NAME AGE tz-cluster-1-drop-cidr 6s tz-cluster-1-node-cidr 5s And the policy to deny anything except the own kubernetes worker nodes:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-drop-except-own-cluster-node-cidr spec: priority: 8 tier: securityops appliedTo: - namespaceSelector: # Selects all non-system Namespaces in the cluster matchExpressions: - {key: kubernetes.io/metadata.name, operator: NotIn, values: [avi-system,default,kube-node-lease,kube-public,kube-system,secretgen-controller,tanzu-continuousdelivery-resources,tanzu-fluxcd-packageinstalls,tanzu-kustomize-controller,tanzu-source-controller,tkg-system,vmware-system-auth,vmware-system-cloud-provider,vmware-system-csi,vmware-system-tkg,vmware-system-tmc]} egress: - action: Allow to: - group: \u0026#34;tz-cluster-1-node-cidr\u0026#34; - action: Drop to: - group: \u0026#34;tz-cluster-1-drop-cidr\u0026#34; Applied:\nandreasm@linuxvm01:~/antrea/policies/groups$ k apply -f tz-cluster-1-drop-anything-but-own-nodes.yaml clusternetworkpolicy.crd.antrea.io/acnp-drop-except-own-cluster-node-cidr created andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-except-own-cluster-node-cidr securityops 8 1 1 3m39s allow-all-egress-dns-service securityops 8 4 4 28m strict-ns-isolation-except-system-ns securityops 9 1 1 50m From the dev-app pod again I will verify if I am allowed to SSH to a worker node in \u0026ldquo;own\u0026rdquo; Kubernetes cluster, and another Linux machine not in the ClusterGroup cidr I have applied.\nroot@ubuntu-20-04-548545fc87-t2lg2:/# ssh vmware-system-user@10.101.82.34 #A worker node in the current k8s cluster vmware-system-user@10.101.82.34\u0026#39;s password: #This is allowed What about other machines outside the cidr: root@ubuntu-20-04-548545fc87-t2lg2:/# ssh 10.101.10.99 ssh: connect to host 10.101.10.99 port 22: Connection timed out That is very close to achieving this requirement also, but I should be allowed to reach pods inside same namespace regardless of which node they reside on. Here are my dev-app namespace with pods on all three nodes:\nandreasm@linuxvm01:~/antrea/policies/groups$ k get pods -n dev-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-548545fc87-75nsm 1/1 Running 0 116s 20.20.2.35 three-zone-cluster-1-node-pool-2-kbzvq-6846d5cc5b-6hdmj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ubuntu-20-04-548545fc87-hhnv2 1/1 Running 0 116s 20.20.1.14 three-zone-cluster-1-node-pool-1-dgcpq-656c75f4f4-nsr2r \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ubuntu-20-04-548545fc87-t2lg2 1/1 Running 0 66m 20.20.3.216 three-zone-cluster-1-node-pool-3-6r8c2-6c8d48656c-wntwc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.1.14 PING 20.20.1.14 (20.20.1.14) 56(84) bytes of data. 64 bytes from 20.20.1.14: icmp_seq=1 ttl=62 time=20.6 ms 64 bytes from 20.20.1.14: icmp_seq=2 ttl=62 time=2.87 ms ^C --- 20.20.1.14 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1002ms rtt min/avg/max/mdev = 2.869/11.735/20.601/8.866 ms root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.2.35 PING 20.20.2.35 (20.20.2.35) 56(84) bytes of data. 64 bytes from 20.20.2.35: icmp_seq=1 ttl=62 time=3.49 ms 64 bytes from 20.20.2.35: icmp_seq=2 ttl=62 time=2.09 ms 64 bytes from 20.20.2.35: icmp_seq=3 ttl=62 time=1.00 ms ^C --- 20.20.2.35 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 2003ms rtt min/avg/max/mdev = 1.000/2.194/3.494/1.020 ms From the Antrea UI, lets do some tests there also:\nNote that I have not created any default-block-all-else-rule. There is always room for improvement, and this was just an excercise to show what is possible, not an final answer on how things should be done. Some of the policies can be even more granular like specifying only ports/protocol/FQDN etc..\nSo just to summarize what I have done:\nThese are the applied rules:\nNAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-except-own-cluster-node-cidr securityops 8 3 3 23h allow-all-egress-dns-service securityops 8 4 4 23h strict-ns-isolation-except-system-ns securityops 9 3 3 23h The first rule is allowing only traffic to the nodes in its own cluster - matches this requirement \u0026ldquo;All Kubernetes workload clusters are considered isolated and not allowed to reach nothing more than themselves, including pods and services (all nodes in the same cluster)\u0026rdquo;\nThe second rule is allowing all namespaces to access the kube-dns service in the kube-system namespace - matches this requirement \u0026ldquo;Only necessary backend functions such as DNS/NTP are allowed\u0026rdquo;\nThe third rule is dropping all traffic between namespaces, except the \u0026ldquo;system\u0026rdquo;-namespaces I have defined. But it allows intra communication inside each namespace - matches this requirement \u0026ldquo;All non-system namespaces should be considered \u0026ldquo;untrusted\u0026rdquo; and isolated by default\u0026rdquo;\nThen I have not done anything with RBAC yet, will come later in this post. And the requirement: \u0026ldquo;Certain management tools need access to the clusters\u0026rdquo; I can assume the NSX admins have covered, as I am not blocking any ingress traffic to the \u0026ldquo;system\u0026rdquo;-namespaces, same is true for egress from the system-namespaces. But it could be, if adjusted to allow the necessary traffic from these namespaces to the certain management tools.\nApplying Antrea policies using TMC - Tanzu Mission Control # This section will not create any new scenario, it will re-use all the policies created and applied in the above section. The biggest difference is how the policies are being applied.\nNot that I dont think any security admin dont want to log in to a Kubernetes cluster and apply these security policies, but it can be a bit tedious each time a new cluster is applied. What wouldnt be better then if we can auto-deploy them each time a new cluster is being deployed? Like an out-of-the-box experience? Yes, excactly. If we already have defined a policy scope for the different Kubernetes cluster in our environment, we could just apply the correct policies to each cluster respectively each time they are provisioned. This saves a lot of time. We can be sure each time a new cluster is provisioned, it is being applied with the correct set of policies. With the abiltiy to auto apply these required policies on creation or directly after creation will make provisioning out-of-the-box compliant clusters a joy.\nNow this sounds interesting, how can I do that?\n\u0026hellip;.Into the door comes TMC\u0026hellip;. Hello Tanzu Mission Control, short TMC. With TMC we can administer Tanzu with vSphere in addition to a lot of other Kubernetes platforms. From the TMC official docs :\nVMware Tanzu Mission Control™ is a centralized management platform for consistently operating and securing your Kubernetes infrastructure and modern applications across multiple teams and clouds.\nAvailable through VMware Cloud™ services, Tanzu Mission Control provides operators with a single control point to give developers the independence they need to drive business forward, while ensuring consistent management and operations across environments for increased security and governance.\nTanzu Mission Control provides instances of the service in regions around the world, including Australia, Canada, India, Ireland, Japan, and USA. For a list of the regions in which the Tanzu Mission Control is hosted, go to the Cloud Management Services Availability page at https://www.vmware.com/global-infrastructure.html and select VMware Tanzu Mission Control.\nUse Tanzu Mission Control to manage your entire Kubernetes footprint, regardless of where your clusters reside.\nLets cut to the chase and make my cluster compliant with the above rules.\nPreparing TMC # In my TMC dashboard I need two thing in place:\nA Git repository where I host my yamls, specifically my Antrea policy yamls. A configured Kustomization using the above Git repo Git repository # I will create a dedicated Git repo called tmc-cd-repo, and a folder structure. Here is my Github repo for this purpose: Now push the yamls to this repo\u0026rsquo;s subfolder antrea-baseline-policies:\nandreasm:~/github_repos/tmc-cd-repo (main)$ git add . andreasm:~/github_repos/tmc-cd-repo (main)$ git commit -s -m \u0026#34;ready-to-lockdown\u0026#34; [main 4ab93a7] ready-to-lockdown 4 files changed, 53 insertions(+) create mode 100644 antrea/antrea-baseline-policies/acnp-allow-egress-all-coredns-service.yaml create mode 100644 antrea/antrea-baseline-policies/tz-cluster-1-drop-anything-but-own-nodes.yaml create mode 100644 antrea/antrea-baseline-policies/tz-cluster-1-drop-cidr.yaml create mode 100644 antrea/antrea-baseline-policies/tz-cluster-1-group-node-cidr.yaml andreasm:~/github_repos/tmc-cd-repo (main)$ git push Enumerating objects: 11, done. Counting objects: 100% (11/11), done. Delta compression using up to 16 threads Compressing objects: 100% (7/7), done. Writing objects: 100% (8/8), 1.43 KiB | 733.00 KiB/s, done. Total 8 (delta 1), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (1/1), done. To github.com:andreasm80/tmc-cd-repo.git 5c9ba04..4ab93a7 main -\u0026gt; main andreasm:~/github_repos/tmc-cd-repo (main)$ And here they are:\nTMC Kustomization # Now in my TMC dashboard configure Git repo:\nI can choose to add the Git repo per cluster that is managed by TMC or in a cluster group. I will go with adding the Git repo on my cluster called three-zone-cluster-1 for the moment. The benefit with adding it at the group is that it can be shared across multiple clusters. In TMC click Clusters and find your already managed and added cluster then click on it to \u0026ldquo;enter it\u0026rdquo;.\nIn your cluster group click on the tab Add-ons Then find Git repositories and Add Git Repository Fill in the needed fields. Make sure to expand advanced settings to update the branch to your branch or main branch. Can also adjust the sync intervall to higher or smaller. Default is 5, I have sat mine to 1. The repository url points to the actual repository, no subfolders. This is because in the Kustomization later we can have multiple pointing to the respective subfolder which can then be unique pr cluster etc. Make sure you also choose \u0026ldquo;no credentials needed\u0026rdquo; under Repository Credentials if using a public Git repo as I am. After save you should see a green status: Now, we need to add a Kustomization. This can also be done in either a group or pr cluster. I will start with adding it directly to my specific cluster. In TMC click Cluster and select your cluster. Click Add-ons, Under Continuous Delivery click Installed Kustomizations. Add Kustomization.\nBefore I add my Kustomization, I have made sure I have deleted all the policies and groups in my test-cluster three-zone-cluster-1:\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp No resources found andreasm@linuxvm01:~/antrea/policies/groups$ k get clustergroups No resources found Then I will continue and add the Kustomization:\nMake sure to point to the correct subfolder in the Git repo. I have enabled the Prune option so I everything deployed via Kustomization will be deleted in my cluster if I decide to remove the Kustomization.\nClick add.\nClick refresh in the top right corner, and it should be green. Lets check the policies and groups in the cluster itself..\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-except-own-cluster-node-cidr securityops 8 3 3 70s allow-all-egress-dns-service securityops 8 4 4 70s strict-ns-isolation-except-system-ns securityops 9 3 3 70s andreasm@linuxvm01:~/antrea/policies/groups$ k get clustergroups NAME AGE tz-cluster-1-drop-cidr 73s tz-cluster-1-node-cidr 73s The Antrea Policies have been applied.\nDeploy TKC cluster from TMC - auto apply security policies # The above section enabled Kustomization on a already managed TKC cluster in TMC. In this section I will apply a TKC cluster from TMC and let the Antrea policies be automatically be applied.\nIn TMC I will create two Cluster Groups, one called andreas-dev-clusters and one called andreas-prod-clusters. After I have added the two cluster groups I will configure Add-ons. Same as in previous section, adding the the Git reop but this time I will point to the different subfolders I created in my Git repo. I have created two different sub-folders in my Git repo called: tmc-cd-repo/antrea/antrea-baseline-policies/dev-clusters and tmc-cd-repo/antrea/antrea-baseline-policies/prod-clusters. The reason I have done that is because I want the option to apply different Antrea policies for certain clusters, different environments different needs.\nBefore adding the Git repo on the two new Cluster groups in TMC I need to enable continuous delivere by clicking on this blue button. The Git repo has been added two both my new cluster groups. Now I just need to add the Kustomization pointing to my new Git repo subfolders dev-clusters and prod-clusters.\nNow the preparations have been done in TMC, it is time to deploy the two TKC clusters from TMC and see if my policies are automatically applied. One \u0026ldquo;prod-cluster\u0026rdquo; and one \u0026ldquo;dev-cluster\u0026rdquo;.\nLets start with the \u0026ldquo;prod-cluster\u0026rdquo; Creating the dev-cluster The clusters are ready: Let us check the sync status of my Kustomizations. Prod-Cluster Group: Dev-Cluster Group: Still applied.\nLets have a look inside the two TKC cluster using kubectl. Prod-Cluster-2:\nandreasm@linuxvm01:~/antrea/policies/groups$ k config current-context prod-cluster-2 andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE allow-all-egress-dns-service securityops 8 2 2 35m prod-clusters-acnp-drop-except-own-cluster-node-cidr securityops 8 0 0 35m prod-clusters-strict-ns-isolation-except-system-ns securityops 9 0 0 35m Dev-Cluster-2:\nandreasm@linuxvm01:~/antrea/policies/groups$ k config current-context dev-cluster-2 andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE dev-clusters-strict-ns-isolation-except-system-ns securityops 9 0 0 45s dev-clusters-acnp-drop-except-own-cluster-node-cidr securityops 8 0 0 45s dev-clusters-allow-all-egress-dns-service securityops 8 2 2 45s Thats it then, if I need to change the policies I can just edit policies, git add, commit and push and they will be applied to all clusters in the group. By enabling this feature in TMC its just all about adding or attaching your clusters in the respective group in TMC and they will automatically get all the needed yamls applied. Applying Antrea policies with NSX # With NSX one can also manage the native Antrea policies inside each TKC cluster (or any other Kubernetes cluster Antrea supports for that matter). I have written about this here. NSX can also create security policies \u0026ldquo;outside\u0026rdquo; the TKC cluster by using the inventory information it gets from Antrea and enforce them in the NSX Distributed firewall, a short section on this below.\nApplying Antrea native policies from the NSX manager # So in this section I will quickly go through using the same \u0026ldquo;framework\u0026rdquo; as above using NSX as the \u0026ldquo;management-plane\u0026rdquo;. Just a reminder, we have these three policies:\nNAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-except-own-cluster-node-cidr securityops 8 3 3 23h allow-all-egress-dns-service securityops 8 4 4 23h strict-ns-isolation-except-system-ns securityops 9 3 3 23h The first rule is allowing only traffic to the nodes in its own cluster - matches this requirement \u0026ldquo;All Kubernetes workload clusters are considered isolated and not allowed to reach nothing more than themselves, including pods and services (all nodes in the same cluster)\u0026rdquo;\nThe second rule is allowing all namespaces to access the kube-dns service in the kube-system namespace - matches this requirement \u0026ldquo;Only necessary backend functions such as DNS/NTP are allowed\u0026rdquo;\nThe third rule is dropping all traffic between namespaces, except the \u0026ldquo;system\u0026rdquo;-namespaces I have defined. But it allows intra communication inside each namespace - matches this requirement \u0026ldquo;All non-system namespaces should be considered \u0026ldquo;untrusted\u0026rdquo; and isolated by default\u0026rdquo;\nIn NSX I will need to create some Security Groups, then use these groups in a Security Policy. So I will start by creating the Security Group for the concerning kube-dns service:\nOne can either define the service kube-dns:\nOr the pods that is responsible for the DNS service (CoreDNS:\nThis depends on how we define the policy in NSX. I have gone with the pod selection group.\nAS the requirement supports all services to access DNS, I dont have to create a security group for the source. Then the policy will look like this in NSX:\nNotice also that I have placed the policy in the Infrastructrue Tier in NSX.\nThis is how it looks like in the Kubernetes clusters:\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 933e463e-c061-4e80-80b3-eff3402e41a9 -oyaml apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/display-name: k8s-core-dns creationTimestamp: \u0026#34;2023-06-27T11:15:30Z\u0026#34; generation: 11 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 933e463e-c061-4e80-80b3-eff3402e41a9 resourceVersion: \u0026#34;2248486\u0026#34; uid: a5d7378d-ede0-4f8c-848b-413c10ce5602 spec: egress: - action: Allow appliedTo: - podSelector: {} enableLogging: false name: \u0026#34;2025\u0026#34; ports: - port: 53 protocol: TCP - port: 53 protocol: UDP to: - group: c7e96b35-1961-4659-8a62-688a0e98fe63 priority: 1.0000000177635693 tier: nsx-category-infrastructure status: currentNodesRealized: 4 desiredNodesRealized: 4 observedGeneration: 11 phase: Realized andreasm@linuxvm01:~/antrea/policies/groups$ k get tiers NAME PRIORITY AGE application 250 6d1h baseline 253 6d1h emergency 50 6d1h networkops 150 6d1h nsx-category-application 4 6d nsx-category-emergency 1 6d nsx-category-environment 3 6d nsx-category-ethernet 0 6d nsx-category-infrastructure 2 6d platform 200 6d1h securityops 100 6d1h For the next policy, allowing only node in same cluster, I will need to create two groups with \u0026ldquo;ip-blocks\u0026rdquo; containing all RFC1918 in one group and the actual node range in the second: The policy in NSX will then look like this: This is how it looks like in the Kubernetes clusters:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/display-name: dev-cluster-1-intra creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; generation: 2 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 17dbadce-06cf-4d1e-9747-3e888f0f58e0 resourceVersion: \u0026#34;2257468\u0026#34; uid: 73814a58-2da8-44c2-ba85-2522865430d1 spec: egress: - action: Allow appliedTo: - podSelector: {} enableLogging: false name: \u0026#34;2027\u0026#34; to: - group: 2051f64c-8c65-46a2-8397-61c926c8c4ce - action: Drop appliedTo: - podSelector: {} enableLogging: false name: \u0026#34;2028\u0026#34; to: - group: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 priority: 1.000000017763571 tier: nsx-category-infrastructure status: currentNodesRealized: 4 desiredNodesRealized: 4 observedGeneration: 2 phase: Realized Where the groups contain this:\napiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: 2051f64c-8c65-46a2-8397-61c926c8c4ce creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 2051f64c-8c65-46a2-8397-61c926c8c4ce resourceVersion: \u0026#34;2257281\u0026#34; uid: 18009c1b-c44f-4c75-a9f2-8a30e2415859 spec: childGroups: - 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 ccp-adapter.antrea.tanzu.vmware.com/parent: 2051f64c-8c65-46a2-8397-61c926c8c4ce creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 resourceVersion: \u0026#34;2257278\u0026#34; uid: b1d4a59b-0557-4f6c-a08c-7b76af6bca8c spec: ipBlocks: - cidr: 10.101.84.32/27 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 resourceVersion: \u0026#34;2257282\u0026#34; uid: 6782589e-8488-47df-a750-04432c3c2f18 spec: childGroups: - 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 ccp-adapter.antrea.tanzu.vmware.com/parent: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 resourceVersion: \u0026#34;2257277\u0026#34; uid: fd2a1c32-1cf8-4ca8-8dad-f5420f57e55c spec: ipBlocks: - cidr: 192.168.0.0/16 - cidr: 10.0.0.0/8 - cidr: 172.16.0.0/12 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed Now the last rule is blocking all non-system namespaces to any other namespace than themselves.\nFirst I need to create a Security Group with the namespace as sole member, then a Security Group with the criteria not-equals. Group for the namespace: Negated Security Group, selecting all pods which does not have the same label as any pods in the namespace \u0026ldquo;dev-app\u0026rdquo;. Then the Security Policy looks like this: This is how it looks like in the Kubernetes clusters:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/display-name: dev-cluster-strict-ns-islolation creationTimestamp: \u0026#34;2023-07-03T13:00:40Z\u0026#34; generation: 3 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: cfbe3754-c365-4697-b124-5fbaddd87b57 resourceVersion: \u0026#34;2267847\u0026#34; uid: 47949441-a69b-47e5-ae9b-1d5760d5c195 spec: egress: - action: Allow appliedTo: - group: beed7011-4fc7-49e6-b7ed-d521095eb293 enableLogging: false name: \u0026#34;2029\u0026#34; to: - group: beed7011-4fc7-49e6-b7ed-d521095eb293 - action: Drop appliedTo: - group: beed7011-4fc7-49e6-b7ed-d521095eb293 enableLogging: false name: \u0026#34;2030\u0026#34; to: - group: f240efd5-3a95-49d3-9252-058cc80bc0c0 priority: 1.0000000177635728 tier: nsx-category-infrastructure status: currentNodesRealized: 3 desiredNodesRealized: 3 observedGeneration: 3 phase: Realized Where the cluster groups look like this:\nandreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup beed7011-4fc7-49e6-b7ed-d521095eb293 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: beed7011-4fc7-49e6-b7ed-d521095eb293 creationTimestamp: \u0026#34;2023-07-03T13:00:40Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: beed7011-4fc7-49e6-b7ed-d521095eb293 resourceVersion: \u0026#34;2266125\u0026#34; uid: 7bf8d0f4-d719-47d5-98a9-5fba3b5da7b9 spec: childGroups: - beed7011-4fc7-49e6-b7ed-d521095eb293-0 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T13:00:41Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup beed7011-4fc7-49e6-b7ed-d521095eb293-0 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: beed7011-4fc7-49e6-b7ed-d521095eb293-0 ccp-adapter.antrea.tanzu.vmware.com/parent: beed7011-4fc7-49e6-b7ed-d521095eb293 creationTimestamp: \u0026#34;2023-07-03T13:00:40Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: beed7011-4fc7-49e6-b7ed-d521095eb293-0 resourceVersion: \u0026#34;2266123\u0026#34; uid: 4b393674-981a-488c-a2e2-d794f0b0a312 spec: namespaceSelector: matchExpressions: - key: kubernetes.io/metadata.name operator: In values: - dev-app status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T13:00:41Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup f240efd5-3a95-49d3-9252-058cc80bc0c0 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: f240efd5-3a95-49d3-9252-058cc80bc0c0 creationTimestamp: \u0026#34;2023-07-03T13:06:59Z\u0026#34; generation: 1 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: f240efd5-3a95-49d3-9252-058cc80bc0c0 resourceVersion: \u0026#34;2267842\u0026#34; uid: cacd1386-a434-4c42-8739-6813dd1d475b spec: childGroups: - f240efd5-3a95-49d3-9252-058cc80bc0c0-0 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T13:07:00Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup f240efd5-3a95-49d3-9252-058cc80bc0c0-0 -oyaml apiVersion: crd.antrea.io/v1alpha3 kind: ClusterGroup metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg ccp-adapter.antrea.tanzu.vmware.com/display-name: f240efd5-3a95-49d3-9252-058cc80bc0c0-0 ccp-adapter.antrea.tanzu.vmware.com/parent: f240efd5-3a95-49d3-9252-058cc80bc0c0 creationTimestamp: \u0026#34;2023-07-03T13:06:59Z\u0026#34; generation: 5 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: f240efd5-3a95-49d3-9252-058cc80bc0c0-0 resourceVersion: \u0026#34;2269597\u0026#34; uid: bd7f4526-2be9-4a4e-860e-0bb85ea30516 spec: podSelector: matchExpressions: - key: app operator: NotIn values: - ubuntu-20-04 status: conditions: - lastTransitionTime: \u0026#34;2023-07-03T13:07:00Z\u0026#34; status: \u0026#34;True\u0026#34; type: GroupMembersComputed With all three policies applied, they look like this in the TKC cluster:\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 17dbadce-06cf-4d1e-9747-3e888f0f58e0 nsx-category-infrastructure 1.000000017763571 4 4 18h 933e463e-c061-4e80-80b3-eff3402e41a9 nsx-category-infrastructure 1.0000000177635702 4 4 18h cfbe3754-c365-4697-b124-5fbaddd87b57 nsx-category-infrastructure 1.0000000177635728 3 3 17h By using NSX managing the Antrea policies there is also a very easy way to verify if the policies are working or not by using the Traffic Analysis tool in NSX: This tools will also inform you of any policies applied by using kubectl inside the cluster, in other words it can also show you policies not created or applied from the NSX manager.\nI have applied a Antrea Policy directly in the TKC cluster using kubectl called block-ns-app3-app4.\nandreasm@linuxvm01:~/antrea/policies/groups$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 17dbadce-06cf-4d1e-9747-3e888f0f58e0 nsx-category-infrastructure 1.000000017763571 4 4 20h 933e463e-c061-4e80-80b3-eff3402e41a9 nsx-category-infrastructure 1.0000000177635702 4 4 20h block-ns-app3-app4 #this securityops 4 1 1 3s cfbe3754-c365-4697-b124-5fbaddd87b57 nsx-category-infrastructure 1.0000000177635728 3 3 20h If I do a traceroute from within NSX from a pod in ns Dev-App3 to a pod in ns Dev-App4 and hit this rule, the NSX manager will show me this: Its clearly doing its job and blocking the traffic, but which rule is it? Click on EgressMetric, copy the rule id and paste it in the search field in NSX: Applying Kubernetes related policies using inventory information from Antrea # As mentioned above, NSX can also utilize the information from TKC cluster (or any Kubernetes cluster that uses Antrea) to enforce them in the Distributed firewall. The information NSX is currently:\nKubernetes Cluster - Used to create security group containing Kubernetes clusters by name, not used alone but in combination with the below -\u0026gt; Kubernetes Namespace - Used to create security group containing Kubernetes clusters namespace by name or tag, not used alone but in combination from a Kubernetes cluster defined above. Kubernetes Service - Used to create security group containing Kubernetes Services by name or tag, not used alone but in combination with any of the above -\u0026gt; Kubernetes Ingress - Used to create security group containing Kubernetes Ingresses by name or tag, not used alone but in combination with any of the above Kubernetes Cluster or Kubernetes Namespace. Antrea Egress - Used to create security group containing Antrea Egress IP in use by name or tag, not used alone but in combination with only Kubernetes Cluster. Antrea IP Pool - Used to create security group containing Antrea Egress IP Pool by name or tag, not used alone but in combination with only Kubernetes Cluster. Kubernetes Node - Used to create security group containing Kubernetes Node IPs or POD CIDRs by node IP address or POD CIDR, not used alone but in combination with only Kubernetes Cluster. Kubernetes Gateway - Used to create security group containing Kubernetes Gateways by name or tag, not used alone but in combination with only Kubernetes Cluster. An example of a Security group in NSX using the contexts above, Kubernetes Cluster with name dev-cluster and Kubernetes Node IP address: Now, if I want to create a NSX firewall policy isolating two Kubernetes clusters from each other using the constructs above:\nI will simply create two security groups like the one above, selection the two different cluster in each group. Then the policy will be like this:\nNow if I do a traceflow from any node in dev-cluster-1 to any node in dev-cluster-2 it will dropped.\nThe Firewall Rule ID is: With this approach, its very easy to isolate complete clusters from each other with some really simple rules. We could even create a negated rule, saying you are allowed to reach any workers from same cluster but nothing else with one blocking rule (using a negated selection where source is dev-cluster-1 and destination is also dev-cluster-1: The policy: This is just one rule blocing everything except its own Kubernetes nodes.\nRBAC - making sure no one can overwrite/override existing rules. # How to manage RBAC, or Tier Entitlement with Antrea I have already covered here\nOutro\u0026hellip; # I have in this post shown three different ways to manage and apply Antrea Network policies. Three different approaches, the first approach was all manual, the second automatic but the policies still needs to be defined. The last one with the NSX manager a bit different approach as not all the Antrea Network policy features are available and some policies have to be defined different. But, the NSX manager can also be used to automate some of the policies by just adding the clusters to existing policies. Then they will be applied at once.\nThe Antrea policies used and how they are defined in this post is by all means not the final answer or best practice. They were just used as simple examples to have something to \u0026ldquo;work with\u0026rdquo; during this post. As I have mentioned, one could utilise the different tiers to delegate administration of the policies to the right set of responsibilities (security admins, vSphere operators, Dev-ops etc). If the target is zero-trust also inside your TKC clusters, this can be achieved by utilizing the tiers and place a drop-all-else rule dead last in the Antrea policy chain (baseline tier e.g).\n","date":"21 June 2023","externalUrl":null,"permalink":"/2023/06/21/securing-kubernetes-clusters-with-antrea-network-policies/","section":"Posts","summary":"In this post I will go through how to utilize Antrea Network policies with Tanzu Mission Control and a little bit NSX. So jump in and hopefully get some ideas how what we can do with Antrea Network Policies and how to use them.","title":"Securing Kubernetes clusters with Antrea Network Policies","type":"posts"},{"content":"","date":"21 June 2023","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"21 June 2023","externalUrl":null,"permalink":"/categories/tanzu-mission-control/","section":"Categories","summary":"","title":"Tanzu Mission Control","type":"categories"},{"content":"","date":"21 June 2023","externalUrl":null,"permalink":"/tags/tmc/","section":"Tags","summary":"","title":"Tmc","type":"tags"},{"content":"","date":"21 June 2023","externalUrl":null,"permalink":"/categories/tmc/","section":"Categories","summary":"","title":"TMC","type":"categories"},{"content":" Antrea in vSphere with Tanzu # Antrea is the default CNI being used in TKG 2.0 clusters. TKG 2.0 clusters are the workload clusters you deploy with the Supervisor deployed in vSphere 8. Antrea comes in to flavours, we have the open source edition of Antrea which can be found here and then we have the Antrea Advanced (\u0026ldquo;downstream\u0026rdquo;) version which is being used in vSphere with Tanzu. This version is also needed when we want to integrate Antrea with NSX-T for policy management. The Antrea Advanced can be found in your VMware customer connect portal here. Both version of Antrea has a very broad support Kubernetes platforms it can be used in. Antrea can be used for Windows worker nodes, Photon, Ubuntu, ARM, x86, VMware TKG, OpenShift, Rancher, AKS, EKS. the list is long see more info here. This post will be focusing on the Antrea Advanced edition and its features like (read more here):\nCentral management of Antrea Security Policies with NSX Central troubleshooting with TraceFlow with NSX FQDN/L7 Security policies RBAC Tiered policies Flow Exporter Egress (Source NAT IP selection of PODs egressing) Managing Antrea settings and Feature Gates in TKG 2 clusters # When you deploy a TKG 2 cluster on vSphere with Tanzu and you dont specify a CNI Antrea will be de default CNI. Depending on the TKG version you are on a set of default Antrea features are enabled or disabled. You can easily check which features are enabled after a cluster has been provisioned by issuing the below command: If you know already before you deploy a cluster that a specific feature should be enabled or disabled this can also be handled during bring-up of the cluster so it should come with the settings you want. More on that later.\nlinux-vm:~/from_ubuntu_vm/tkgs/tkgs-stc-cpod$ k get configmaps -n kube-system antrea-config -oyaml apiVersion: v1 data: antrea-agent.conf: | featureGates: AntreaProxy: true EndpointSlice: true Traceflow: true NodePortLocal: true AntreaPolicy: true FlowExporter: false NetworkPolicyStats: false Egress: true AntreaIPAM: false Multicast: false Multicluster: false SecondaryNetwork: false ServiceExternalIP: false TrafficControl: false trafficEncapMode: encap noSNAT: false tunnelType: geneve trafficEncryptionMode: none enableBridgingMode: false disableTXChecksumOffload: false wireGuard: port: 51820 egress: exceptCIDRs: [] serviceCIDR: 20.10.0.0/16 nodePortLocal: enable: true portRange: 61000-62000 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 multicast: {} antreaProxy: proxyAll: false nodePortAddresses: [] skipServices: [] proxyLoadBalancerIPs: false multicluster: {} antrea-cni.conflist: | { \u0026#34;cniVersion\u0026#34;:\u0026#34;0.3.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;antrea\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;antrea\u0026#34;, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34; } } , { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true} } , { \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} } ] } antrea-controller.conf: | featureGates: Traceflow: true AntreaPolicy: true NetworkPolicyStats: false Multicast: false Egress: true AntreaIPAM: false ServiceExternalIP: false tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 nodeIPAM: null kind: ConfigMap metadata: annotations: kapp.k14s.io/identity: v1;kube-system//ConfigMap/antrea-config;v1 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;antrea-agent.conf\u0026#34;:\u0026#34;featureGates:\\n AntreaProxy: true\\n EndpointSlice: true\\n Traceflow: true\\n NodePortLocal: true\\n AntreaPolicy: true\\n FlowExporter: false\\n NetworkPolicyStats: false\\n Egress: true\\n AntreaIPAM: false\\n Multicast: false\\n Multicluster: false\\n SecondaryNetwork: false\\n ServiceExternalIP: false\\n TrafficControl: false\\ntrafficEncapMode: encap\\nnoSNAT: false\\ntunnelType: geneve\\ntrafficEncryptionMode: none\\nenableBridgingMode: false\\ndisableTXChecksumOffload: false\\nwireGuard:\\n port: 51820\\negress:\\n exceptCIDRs: []\\nserviceCIDR: 20.10.0.0/16\\nnodePortLocal:\\n enable: true\\n portRange: 61000-62000\\ntlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384\\nmulticast: {}\\nantreaProxy:\\n proxyAll: false\\n nodePortAddresses: []\\n skipServices: []\\n proxyLoadBalancerIPs: false\\nmulticluster: {}\\n\u0026#34;,\u0026#34;antrea-cni.conflist\u0026#34;:\u0026#34;{\\n \\\u0026#34;cniVersion\\\u0026#34;:\\\u0026#34;0.3.0\\\u0026#34;,\\n \\\u0026#34;name\\\u0026#34;: \\\u0026#34;antrea\\\u0026#34;,\\n \\\u0026#34;plugins\\\u0026#34;: [\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;antrea\\\u0026#34;,\\n \\\u0026#34;ipam\\\u0026#34;: {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;host-local\\\u0026#34;\\n }\\n }\\n ,\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;portmap\\\u0026#34;,\\n \\\u0026#34;capabilities\\\u0026#34;: {\\\u0026#34;portMappings\\\u0026#34;: true}\\n }\\n ,\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;bandwidth\\\u0026#34;,\\n \\\u0026#34;capabilities\\\u0026#34;: {\\\u0026#34;bandwidth\\\u0026#34;: true}\\n }\\n ]\\n}\\n\u0026#34;,\u0026#34;antrea-controller.conf\u0026#34;:\u0026#34;featureGates:\\n Traceflow: true\\n AntreaPolicy: true\\n NetworkPolicyStats: false\\n Multicast: false\\n Egress: true\\n AntreaIPAM: false\\n ServiceExternalIP: false\\ntlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384\\nnodeIPAM: null\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685607245932804320\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.c39c4aca919097e50452c3432329dd40\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;antrea-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}}\u0026#39; kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 creationTimestamp: \u0026#34;2023-06-01T08:14:14Z\u0026#34; labels: app: antrea kapp.k14s.io/app: \u0026#34;1685607245932804320\u0026#34; kapp.k14s.io/association: v1.c39c4aca919097e50452c3432329dd40 name: antrea-config namespace: kube-system resourceVersion: \u0026#34;948\u0026#34; uid: fd18fd20-a82b-4df5-bb1a-686463b86f27 If you want to enable or disable any of these features its a matter of applying an AntreaConfig using the included AntreaConfig CRD in TKG 2.0.\nOne can apply this AntreaConfig on an already provisioned TKG 2.0 cluster or apply before the cluster is provisioned so it will get the features enabled or disabled at creation. Below is an example of AntreaConfig:\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: three-zone-cluster-2-antrea-package namespace: ns-three-zone-1 spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: true Egress: true #This needs to be enabled (an example) NodePortLocal: true AntreaTraceflow: true NetworkPolicyStats: true This example is applied either before or after provisioning of the TKG 2.0 cluster. Just make sure the config has been applied to the correct NS, the same NS as the cluster is deployed in and the name of the config needs to start like this CLUSTER-NAME-antrea-package. In other words the name needs to start with the clustername of the TKG 2.0 cluster and end with -antrea-package.\nIf it is being done after the cluster has provisioned we need to make sure the already running Antrea pods (agents and controller) are restarted so they can read the new configmap.\nIf you need to check which version of Antrea is included in your TKR version (and other components for that sake) just run the following command:\nlinuxvm01:~/three-zones$ k get tkr v1.24.9---vmware.1-tkg.4 -o yaml apiVersion: run.tanzu.vmware.com/v1alpha3 kind: TanzuKubernetesRelease metadata: creationTimestamp: \u0026#34;2023-06-01T07:35:28Z\u0026#34; finalizers: - tanzukubernetesrelease.run.tanzu.vmware.com generation: 2 labels: os-arch: amd64 os-name: photon os-type: linux os-version: \u0026#34;3.0\u0026#34; v1: \u0026#34;\u0026#34; v1.24: \u0026#34;\u0026#34; v1.24.9: \u0026#34;\u0026#34; v1.24.9---vmware: \u0026#34;\u0026#34; v1.24.9---vmware.1: \u0026#34;\u0026#34; v1.24.9---vmware.1-tkg: \u0026#34;\u0026#34; v1.24.9---vmware.1-tkg.4: \u0026#34;\u0026#34; name: v1.24.9---vmware.1-tkg.4 ownerReferences: - apiVersion: vmoperator.vmware.com/v1alpha1 kind: VirtualMachineImage name: ob-21552850-ubuntu-2004-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 uid: 92d3d6af-53f8-4f9a-b262-f70dd33ad19b - apiVersion: vmoperator.vmware.com/v1alpha1 kind: VirtualMachineImage name: ob-21554409-photon-3-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 uid: 6a0aa87a-63e3-475d-a52d-e63589f454e9 resourceVersion: \u0026#34;12111\u0026#34; uid: 54db049e-fdf0-45a2-b4d1-46fa90a22b44 spec: bootstrapPackages: - name: antrea.tanzu.vmware.com.1.7.2+vmware.1-tkg.1-advanced - name: vsphere-pv-csi.tanzu.vmware.com.2.6.1+vmware.1-tkg.1 - name: vsphere-cpi.tanzu.vmware.com.1.24.3+vmware.1-tkg.1 - name: kapp-controller.tanzu.vmware.com.0.41.5+vmware.1-tkg.1 - name: guest-cluster-auth-service.tanzu.vmware.com.1.1.0+tkg.1 - name: metrics-server.tanzu.vmware.com.0.6.2+vmware.1-tkg.1 - name: secretgen-controller.tanzu.vmware.com.0.11.2+vmware.1-tkg.1 - name: pinniped.tanzu.vmware.com.0.12.1+vmware.3-tkg.3 - name: capabilities.tanzu.vmware.com.0.28.0+vmware.2 - name: calico.tanzu.vmware.com.3.24.1+vmware.1-tkg.1 kubernetes: coredns: imageTag: v1.8.6_vmware.15 etcd: imageTag: v3.5.6_vmware.3 imageRepository: localhost:5000/vmware.io pause: imageTag: \u0026#34;3.7\u0026#34; version: v1.24.9+vmware.1 osImages: - name: ob-21552850-ubuntu-2004-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 - name: ob-21554409-photon-3-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 version: v1.24.9+vmware.1-tkg.4 status: conditions: - lastTransitionTime: \u0026#34;2023-06-01T07:35:28Z\u0026#34; status: \u0026#34;True\u0026#34; type: Ready - lastTransitionTime: \u0026#34;2023-06-01T07:35:28Z\u0026#34; status: \u0026#34;True\u0026#34; type: Compatible So enabling and disabling Antrea Feature Gates is quite simple. To summarize, the feature gates that can be adjusted is these (as of TKR 1.24.9):\nspec: antrea: config: defaultMTU: \u0026#34;\u0026#34; disableUdpTunnelOffload: false featureGates: AntreaPolicy: true AntreaProxy: true AntreaTraceflow: true Egress: true EndpointSlice: true FlowExporter: false NetworkPolicyStats: false NodePortLocal: true noSNAT: false tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 trafficEncapMode: encap Getting the Antrea config \u0026ldquo;templates\u0026rdquo; for a specific TKR version # Usually with new TKR versions, a new version of Antrea is shipped. And with a new version of Antrea is shipped it most liley containt new and exciting features. So if you want to see which feature gates are being available in your latest and greatest TKR, run these commands from the Supervisor context:\n# to get all the Antrea configs andreasm@ubuntu02:~/avi_nsxt_wcp$ k get antreaconfigs.cni.tanzu.vmware.com -A NAMESPACE NAME TRAFFICENCAPMODE DEFAULTMTU ANTREAPROXY ANTREAPOLICY SECRETREF ns-stc-1 cluster-1-antrea-package encap true true cluster-1-antrea-data-values vmware-system-tkg v1.23.15---vmware.1-tkg.4 encap true true vmware-system-tkg v1.23.15---vmware.1-tkg.4-routable noEncap true true vmware-system-tkg v1.23.8---vmware.2-tkg.2-zshippable encap true true vmware-system-tkg v1.23.8---vmware.2-tkg.2-zshippable-routable noEncap true true vmware-system-tkg v1.24.9---vmware.1-tkg.4 encap true true vmware-system-tkg v1.24.9---vmware.1-tkg.4-routable noEncap true true vmware-system-tkg v1.25.7---vmware.3-fips.1-tkg.1 encap true true vmware-system-tkg v1.25.7---vmware.3-fips.1-tkg.1-routable noEncap true true vmware-system-tkg v1.26.5---vmware.2-fips.1-tkg.1 encap true true vmware-system-tkg v1.26.5---vmware.2-fips.1-tkg.1-routable noEncap true true # Get the content of a specific Antrea config andreasm@ubuntu02:~/avi_nsxt_wcp$ k get antreaconfigs.cni.tanzu.vmware.com -n vmware-system-tkg v1.26.5---vmware.2-fips.1-tkg.1 -oyaml apiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: annotations: tkg.tanzu.vmware.com/template-config: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-09-24T17:49:37Z\u0026#34; generation: 1 name: v1.26.5---vmware.2-fips.1-tkg.1 namespace: vmware-system-tkg resourceVersion: \u0026#34;19483\u0026#34; uid: 8cdaa6ec-4059-4d35-a0d4-63711831edc8 spec: antrea: config: antreaProxy: proxyLoadBalancerIPs: true defaultMTU: \u0026#34;\u0026#34; disableTXChecksumOffload: false disableUdpTunnelOffload: false dnsServerOverride: \u0026#34;\u0026#34; enableBridgingMode: false enableUsageReporting: false featureGates: AntreaIPAM: false AntreaPolicy: true AntreaProxy: true AntreaTraceflow: true Egress: true EndpointSlice: true FlowExporter: false Multicast: false Multicluster: false NetworkPolicyStats: true NodePortLocal: true SecondaryNetwork: false ServiceExternalIP: false TopologyAwareHints: false TrafficControl: false flowExporter: activeFlowTimeout: 60s collectorAddress: flow-aggregator/flow-aggregator:4739:tls noSNAT: false tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 trafficEncapMode: encap tunnelCsum: false tunnelPort: 0 With the above you can always get the latest config coming with the specific TKR release and use it as a template for your TKC cluster.\nIntegrating Antrea with NSX-T # To enable the NSX-T Antrea integration there is a couple of steps that needs to be prepared. All the steps can be followed here. I have decided to create a script that automates all these steps. So if you dont want to go through all these steps manually by following the link above you can use this script instead and just enter the necesarry information as prompted, and have the pre-requisities in place before excecuting. Copy and paste the below script into a .sh file on your Linux jumpiest and make it executable with chmod +x.\n#!/bin/bash # Echo information echo \u0026#34;This script has some dependencies... make sure they are met before continuing. Otherwise click ctrl+c now 1. This script is adjusted for vSphere with Tanzu TKG clusters using Tanzu CLI 2. Have downloaded the antrea-interworking*.zip 3. This script is located in the root of where you have downloaded the zip file above 4. curl is installed 5. Need connectivity to the NSX manager 6. kubectl is installed 7. vsphere with tanzu cli is installed 8. That you are in the correct context of the cluster you want to integrate to NSX 9. If not in the correct context the script will put you in the correct context anyway 10. A big smile and good mood\u0026#34; # Prompt the user to press a key to continue echo \u0026#34;Press any key to continue...\u0026#34; read -n 1 -s # Continue with the script echo \u0026#34;Continuing...\u0026#34; # Prompt for name read -p \u0026#34;Enter the name of the tkg cluster - will be used for certificates and name in NSX: \u0026#34; name # Prompt for NSX_MGR read -p \u0026#34;Enter NSX Manager ip or FQDN: \u0026#34; nsx_mgr # Prompt for NSX_ADMIN read -p \u0026#34;Enter NSX admin username: \u0026#34; nsx_admin # Prompt for NSX_PASS read -p \u0026#34;Enter NSX Password: \u0026#34; nsx_pass # Prompt for Supervisor Endpoint IP or FQDN read -p \u0026#34;Enter Supervisor API IP or FQDN: \u0026#34; svc_api_ip # Prompt for vSphere Username read -p \u0026#34;Enter vSphere Username: \u0026#34; vsphere_username # Prompt for Tanzu Kubernetes Cluster Namespace read -p \u0026#34;Enter Tanzu Kubernetes Cluster Namespace: \u0026#34; tanzu_cluster_namespace # Prompt for Tanzu Kubernetes Cluster Name read -p \u0026#34;Enter Tanzu Kubernetes Cluster Name: \u0026#34; tanzu_cluster_name # Login to vSphere using kubectl kubectl vsphere login --server=\u0026#34;$svc_api_ip\u0026#34; --insecure-skip-tls-verify --vsphere-username=\u0026#34;$vsphere_username\u0026#34; --tanzu-kubernetes-cluster-namespace=\u0026#34;$tanzu_cluster_namespace\u0026#34; --tanzu-kubernetes-cluster-name=\u0026#34;$tanzu_cluster_name\u0026#34; key_name=\u0026#34;${name}-private.key\u0026#34; csr_output=\u0026#34;${name}.csr\u0026#34; crt_output=\u0026#34;${name}.crt\u0026#34; openssl genrsa -out \u0026#34;$key_name\u0026#34; 2048 openssl req -new -key \u0026#34;$key_name\u0026#34; -out \u0026#34;$csr_output\u0026#34; -subj \u0026#34;/C=US/ST=CA/L=Palo Alto/O=VMware/OU=Antrea Cluster/CN=$name\u0026#34; openssl x509 -req -days 3650 -sha256 -in \u0026#34;$csr_output\u0026#34; -signkey \u0026#34;$key_name\u0026#34; -out \u0026#34;$crt_output\u0026#34; # Convert the certificate file to a one-liner with line breaks crt_contents=$(awk \u0026#39;{printf \u0026#34;%s\\\\n\u0026#34;, $0}\u0026#39; \u0026#34;$crt_output\u0026#34;) # Replace the certificate and name in the curl body curl_body=\u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$name\u0026#34;\u0026#39;\u0026#34;, \u0026#34;node_id\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$name\u0026#34;\u0026#39;\u0026#34;, \u0026#34;roles_for_paths\u0026#34;: [ { \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;roles\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;enterprise_admin\u0026#34; } ] } ], \u0026#34;role\u0026#34;: \u0026#34;enterprise_admin\u0026#34;, \u0026#34;is_protected\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;certificate_pem\u0026#34; : \u0026#34;\u0026#39;\u0026#34;$crt_contents\u0026#34;\u0026#39;\u0026#34; }\u0026#39; # Make the curl request with the updated body # curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;$curl_body\u0026#34; https://example.com/api/endpoint curl -ku \u0026#34;$nsx_admin\u0026#34;:\u0026#34;$nsx_pass\u0026#34; -X POST https://\u0026#34;$nsx_mgr\u0026#34;/api/v1/trust-management/principal-identities/with-certificate -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;$curl_body\u0026#34; # Check if a subfolder starting with \u0026#34;antrea-interworking\u0026#34; exists if ls -d antrea-interworking* \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;Subfolder starting with \u0026#39;antrea-interworking\u0026#39; exists. Skipping extraction.\u0026#34; else # Extract the zip file starting with \u0026#34;antrea-interworking\u0026#34; unzip \u0026#34;antrea-interworking\u0026#34;*.zip fi # Create a new folder with the name antrea-interworking-\u0026#34;from-name\u0026#34; new_folder=\u0026#34;antrea-interworking-$name\u0026#34; mkdir \u0026#34;$new_folder\u0026#34; # Copy all YAML files from the antrea-interworking subfolder to the new folder cp antrea-interworking*/{*.yaml,*.yml} \u0026#34;$new_folder/\u0026#34; # Replace the field after \u0026#34;image: vmware.io/antrea/interworking\u0026#34; with \u0026#34;image: projects.registry.vmware.com/antreainterworking/interworking-debian\u0026#34; in interworking.yaml sed -i \u0026#39;s|image: vmware.io/antrea/interworking|image: projects.registry.vmware.com/antreainterworking/interworking-debian|\u0026#39; \u0026#34;$new_folder/interworking.yaml\u0026#34; # Replace the field after \u0026#34;image: vmware.io/antrea/interworking\u0026#34; with \u0026#34;image: projects.registry.vmware.com/antreainterworking/interworking-debian\u0026#34; in deregisterjob.yaml sed -i \u0026#39;s|image: vmware.io/antrea/interworking|image: projects.registry.vmware.com/antreainterworking/interworking-debian|\u0026#39; \u0026#34;$new_folder/deregisterjob.yaml\u0026#34; # Edit the bootstrap.yaml file in the new folder sed -i \u0026#39;s|clusterName:.*|clusterName: \u0026#39;\u0026#34;$name\u0026#34;\u0026#39;|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; sed -i \u0026#39;s|NSXManagers:.*|NSXManagers: [\u0026#34;\u0026#39;\u0026#34;$nsx_mgr\u0026#34;\u0026#39;\u0026#34;]|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; tls_crt_base64=$(base64 -w 0 \u0026#34;$crt_output\u0026#34;) sed -i \u0026#39;s|tls.crt:.*|tls.crt: \u0026#39;\u0026#34;$tls_crt_base64\u0026#34;\u0026#39;|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; tls_key_base64=$(base64 -w 0 \u0026#34;$key_name\u0026#34;) sed -i \u0026#39;s|tls.key:.*|tls.key: \u0026#39;\u0026#34;$tls_key_base64\u0026#34;\u0026#39;|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; # Interactive prompt to select Kubernetes context kubectl config get-contexts read -p \u0026#34;Enter the name of the Kubernetes context: \u0026#34; kubectl_context kubectl config use-context \u0026#34;$kubectl_context\u0026#34; # Apply the bootstrap-config.yaml and interworking.yaml files from the new folder kubectl apply -f \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; -f \u0026#34;$new_folder/interworking.yaml\u0026#34; # Run the last command to verify that something is happening kubectl get pods -o wide -n vmware-system-antrea echo \u0026#34;As it was written each time we ssh\u0026#39;ed into a Suse Linux back in the good old days - Have a lot of fun\u0026#34; As soon as the script has been processed through it should not take long until you have your TKG cluster in the NSX manager:\nThats it for the NSX-T integration, as soon as that have been done its time to look into what we can do with this integration in the following chapters\nAntrea Security Policies # Antrea has two sets of security policies, Antrea Network Policies (ANP) and Antrea Cluster Network Policies (ACNP). The difference between these two is that ANP is applied on a Kubernetes Namespace and ACNP is cluster-wide. Both belongs to Antrea Native Policies. Both ANP and ACNP can work together with Kubernetes Network Policies.\nThere are many benefits of using Antrea Native Policies in combination or not in combination with Kubernetes Network Policies.\nSome of the benefits of using Antrea Native Policies:\nCan be tiered Select both ingress and egress Support the following actions: allow, drop, reject and pass Support FQDN filtering in egress (to) with actions allow, drop and reject Tiered policies # The benefit of having tiered policies is very useful when for example we have different parts of the organization are responsible for security at different levels/scopes in the platform. Antrea can have policies placed in different tiers where the tiers are evaluated in a given order. If we want some rules to be very early in the policy evaluation and enforced as soon as possible we can place rule in a tier that is considered first, then within the same tier the rules or policies are also being enforced in the order of a given priority, a number. The rule with the lowest number (higher priority) will be evaluated first and then when all rules in a tier has been processed it will go to the next tier. Antrea comes with a set of static tiers already defined. These tier can be shown by running the command:\nlinuxvm01:~$ k get tiers NAME PRIORITY AGE application 250 3h11m baseline 253 3h11m emergency 50 3h11m networkops 150 3h11m platform 200 3h11m securityops 100 3h11m Below will show a diagram of how they look, notice also where the Kubernets network policies will be placed:\nThere is also the option to add custom tiers using the following CRD (taken from the offical Antrea docs here:\napiVersion: crd.antrea.io/v1alpha1 kind: Tier metadata: name: mytier spec: priority: 10 description: \u0026#34;my custom tier\u0026#34; When doing the Antrea NSX integration some additional tiers are added automatically (they start with nsx*):\nlinuxvm01:~$ k get tiers NAME PRIORITY AGE application 250 3h11m baseline 253 3h11m emergency 50 3h11m networkops 150 3h11m nsx-category-application 4 87m nsx-category-emergency 1 87m nsx-category-environment 3 87m nsx-category-ethernet 0 87m nsx-category-infrastructure 2 87m platform 200 3h11m securityops 100 3h11m I can quickly show two examples where I create one rule as a \u0026ldquo;security-admin\u0026rdquo;, where this security admin has to follow the company\u0026rsquo;s compliance policy to block access to a certain FQDN. This must be enforced all over. So I need to create this policy in the securityops tier. I could have defined it in the emergency tier also but in this tier it makes more sense to have rules applied that are disabled/not-enforced/idle in case of an emergency and we need a way to quickly enable it and override rules later down the hierarchy. So securityops it is:\nLets apply this one:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-drop-yelb spec: priority: 1 tier: securityops appliedTo: - podSelector: matchLabels: app: ubuntu-20-04 egress: - action: Drop to: - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; ports: - protocol: TCP port: 80 - action: Allow #Allow the rest To check if it is applied and in use (notice under desired nodes and current nodes):\nlinuxvm01:~/antrea/policies$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-drop-yelb securityops 1 1 1 5m33s Now from a test pod I will try to curl the blocked fqdn and another one not in any block rule:\nroot@ubuntu-20-04-548545fc87-kkzbh:/# curl yelb-ui.yelb.cloudburst.somecooldomain.net curl: (6) Could not resolve host: yelb-ui.yelb.cloudburst.somecooldomain.net # Curling a FQDN that is allowed: root@ubuntu-20-04-548545fc87-kkzbh:/# curl allowed-yelb.yelb-2.carefor.some-dns.net \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; That works as expected. Now what happens then if another use with access to the Kubernetes cluster decide to create a rule later down in the hierarchy, lets go with the application tier, to create an allow rule for this FQDN that is currently being dropped? Lets see what happens\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-allow-yelb spec: priority: 1 tier: application appliedTo: - podSelector: matchLabels: app: ubuntu-20-04 egress: - action: Allow to: - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; ports: - protocol: TCP port: 80 - action: Allow #Allow the rest I will apply this above rule and then try to curl the same fqdn which is supposed to be dropped.\nlinuxvm01:~/antrea/policies$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-allow-yelb application 1 1 1 4s acnp-drop-yelb securityops 1 1 1 5h1m From my test pod again:\nkubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ubuntu-20-04-548545fc87-kkzbh:/# curl yelb-ui.yelb.cloudburst.somecooldomain.net curl: (6) Could not resolve host: yelb-ui.yelb.cloudburst.somecooldomain.net root@ubuntu-20-04-548545fc87-kkzbh:/# curl allowed-yelb.yelb-2.carefor.some-dns.net \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; That was expected. It is still being dropped by the first rule placed in the securityops tier. So far so good. But what if this user also has access to the tier where the first rule is applied? Well, then they can override it. That is why I we can now go to the next chapter.\nAntrea RBAC # Antrea comes with a couple of CRDs that allow us to configure granular user permissions on the different objects, like the Policy Tiers. So to restrict \u0026ldquo;normal\u0026rdquo; users from applying and/or delete security polices created in the higher priority Tiers we need to apply some rolebindings, or to be exact ClusterRoleBindings. Let us see how we can achieve that.\nIn my lab environment I have defined two users, my own admin user (andreasm) that is part of the ClusterRole/cluster-admin and a second user (User1) that is part of the the ClusterRole/view. The ClusterRole View has only read access, not to all objects in the cluster but many. To see what run the following command:\nlinuxvm01:~/antrea/policies$ k get clusterrole view -oyaml aggregationRule: clusterRoleSelectors: - matchLabels: rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2023-06-04T09:37:44Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; name: view resourceVersion: \u0026#34;1052\u0026#34; uid: c4784a81-4451-42af-9134-e141ccf8bc50 rules: - apiGroups: - crd.antrea.io resources: - clustergroups verbs: - get - list - watch - apiGroups: - crd.antrea.io resources: - clusternetworkpolicies - networkpolicies verbs: - get - list - watch - apiGroups: - crd.antrea.io resources: - traceflows verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - configmaps - endpoints - persistentvolumeclaims - persistentvolumeclaims/status - pods - replicationcontrollers - replicationcontrollers/scale - serviceaccounts - services - services/status verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - bindings - events - limitranges - namespaces/status - pods/log - pods/status - replicationcontrollers/status - resourcequotas - resourcequotas/status verbs: - get - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - namespaces verbs: - get - list - watch - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - get - list - watch - apiGroups: - apps resources: - controllerrevisions - daemonsets - daemonsets/status - deployments - deployments/scale - deployments/status - replicasets - replicasets/scale - replicasets/status - statefulsets - statefulsets/scale - statefulsets/status verbs: - get - list - watch - apiGroups: - autoscaling resources: - horizontalpodautoscalers - horizontalpodautoscalers/status verbs: - get - list - watch - apiGroups: - batch resources: - cronjobs - cronjobs/status - jobs - jobs/status verbs: - get - list - watch - apiGroups: - extensions resources: - daemonsets - daemonsets/status - deployments - deployments/scale - deployments/status - ingresses - ingresses/status - networkpolicies - replicasets - replicasets/scale - replicasets/status - replicationcontrollers/scale verbs: - get - list - watch - apiGroups: - policy resources: - poddisruptionbudgets - poddisruptionbudgets/status verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses - ingresses/status - networkpolicies verbs: - get - list - watch - apiGroups: - metrics.k8s.io resources: - pods - nodes verbs: - get - list - watch - apiGroups: - policy resourceNames: - vmware-system-privileged resources: - podsecuritypolicies verbs: - use On the other hand my own admin user has access to everything, get, list, create, patch, update, delete - the whole shabang. What I would like to demonstrate now is that user1 is a regular user and should only be allowed to create security policies in the Tier application while all other Tiers is restricted to the admins that have the responsibility to create policies there. User1 should also not be allowed to create any custom Tiers.\nSo the first thing I need to create is an Antrea TierEntitlement and TierEntitlementBinding like this:\napiVersion: crd.antrea.tanzu.vmware.com/v1alpha1 kind: TierEntitlement metadata: name: secops-edit spec: tiers: # Accept list of Tier names. Tier may or may not exist yet. - emergency - securityops - networkops - platform - baseline permission: edit --- apiVersion: crd.antrea.tanzu.vmware.com/v1alpha1 kind: TierEntitlementBinding metadata: name: secops-bind spec: subjects: # List of users to grant this entitlement to - kind: User name: sso:andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net apiGroup: rbac.authorization.k8s.io # - kind: Group # name: security-admins # apiGroup: rbac.authorization.k8s.io # - kind: ServiceAccount # name: network-admins # namespace: kube-system tierEntitlement: secops-edit # Reference to the TierEntitlement Now, notice that I am listing the Tiers that should only be available for the users, groups, or ServiceAccounts in the TierEntitlementBinding (I am only using Kind: User in this example). This means that all unlisted tiers should be allowed for other users to place security policies in.\nNow apply it:\nlinuxvm01:~/antrea/policies$ k apply -f tierentitlement.yaml tierentitlement.crd.antrea.tanzu.vmware.com/secops-edit created tierentitlementbinding.crd.antrea.tanzu.vmware.com/secops-bind created Next up is to add my User1 to the Antrea CRD \u0026ldquo;tiers\u0026rdquo; to be allowed to list and get the tiers:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: tier-placement rules: - apiGroups: [\u0026#34;crd.antrea.io\u0026#34;] resources: [\u0026#34;tiers\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tier-bind subjects: - kind: User name: sso:user1@cpod-nsxam-stc.az-stc.cloud-garage.net # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: tier-placement apiGroup: rbac.authorization.k8s.io If you want some user to also add/create/delete custom Tiers this can be allowed by adding: \u0026quot;create\u0026quot;,\u0026quot;patch\u0026quot;,\u0026quot;update\u0026quot;,\u0026quot;delete\u0026quot;\nNow apply the above yaml:\nlinuxvm01:~/antrea/policies$ k apply -f antrea-crd-tier-list.yaml clusterrole.rbac.authorization.k8s.io/tier-placement created clusterrolebinding.rbac.authorization.k8s.io/tier-bind created I will now log in with the User1 and try to apply this network policy:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: override-rule-allow-yelb spec: priority: 1 tier: securityops appliedTo: - podSelector: matchLabels: app: ubuntu-20-04 egress: - action: Allow to: - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; ports: - protocol: TCP port: 80 - action: Allow As User1:\nlinuxvm01:~/antrea/policies$ k apply -f fqdn-rule-secops-tier.test.yaml Error from server (Forbidden): error when creating \u0026#34;fqdn-rule-secops-tier.test.yaml\u0026#34;: clusternetworkpolicies.crd.antrea.io is forbidden: User \u0026#34;sso:user1@cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34; cannot create resource \u0026#34;clusternetworkpolicies\u0026#34; in API group \u0026#34;crd.antrea.io\u0026#34; at the cluster scope First bump in the road.. This user is not allowed to create any security policies at all.\nSo I need to use my admin user and apply this ClusterRoleBinding:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: clusternetworkpolicies-edit rules: - apiGroups: [\u0026#34;crd.antrea.io\u0026#34;] resources: [\u0026#34;clusternetworkpolicies\u0026#34;] verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: clusternetworkpolicies-bind subjects: - kind: User name: sso:user1@cpod-nsxam-stc.az-stc.cloud-garage.net # Name is case sensitive apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: clusternetworkpolicies-edit apiGroup: rbac.authorization.k8s.io Now the user1 has access to create policies\u0026hellip; Lets try again:\nlinuxvm01:~/antrea/policies$ k apply -f fqdn-rule-secops-tier.test.yaml Error from server: error when creating \u0026#34;fqdn-rule-secops-tier.test.yaml\u0026#34;: admission webhook \u0026#34;acnpvalidator.antrea.io\u0026#34; denied the request: user not authorized to access Tier securityops There it is, I am not allowed to place any security policies in the tier securityops. That is what I wanted to achieve, so thats good. What if user1 tries to apply a policy in the application tier? Lets see:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: override-attempt-failed-allow-yelb spec: priority: 1 tier: application appliedTo: - podSelector: matchLabels: app: ubuntu-20-04 egress: - action: Allow to: - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; ports: - protocol: TCP port: 80 - action: Allow linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-baseline-tier.test.yaml clusternetworkpolicy.crd.antrea.io/override-attempt-failed-allow-yelb created linuxvm01:~/antrea/policies$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-allow-yelb application 1 1 1 147m acnp-drop-yelb securityops 1 1 1 18h override-attempt-failed-allow-yelb application 1 1 1 11s That worked, even though the above rule is trying to allow access to yelb it will not allow it due to the Drop rule in the securityops Tier. So how much the User1 tries to get this access it will be blocked.\nThese users\u0026hellip;.\nWhat if user1 tries to apply the same policy without stating any Tier in in the policy? Lets see:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: override-attempt-failed-allow-yelb spec: priority: 1 appliedTo: - podSelector: matchLabels: app: ubuntu-20-04 egress: - action: Allow to: - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; ports: - protocol: TCP port: 80 - action: Allow linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-no-tier.yaml clusternetworkpolicy.crd.antrea.io/override-attempt-failed-allow-yelb created linuxvm01:~/antrea/policies$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE acnp-allow-yelb application 1 1 1 151m acnp-drop-yelb securityops 1 1 1 18h override-attempt-failed-allow-yelb application 1 1 1 10s The rule will be placed in the application Tier, even though the user has permission to create clusternetworkpolicies\u0026hellip;\nWith this the network or security admins have full control of the network policies before and after the application Tier (ref the Tier diagram above).\nThis example has only shown how to do this on Cluster level, one can also add more granular permission on Namespace level.\nSo far I have gone over how to manage the Antrea FeatureGates in TKG, how to configure the Antrea-NSX integration, Antrea Policies in general and how to manage RBAC. In the the two next chapters I will cover two different ways how we can apply the Antrea Policies. Lets get into it\nHow to manage the Antrea Native Policies # As mentioned previously Antrea Native Policies can be applied from inside the Kubernetes cluster using yaml manifests, but there is also another way to manage them using the NSX manager. As not mentioned previously this opens up for a whole new way of managing security policies. Centrally managed across multiple clusters wherever located, easier adoption of roles and responsibilities. If NSX is already in place, chances are that NSX security policies are already in place and being managed by the network or security admins. Now they can continue doing that but also take into consideration pod network security across the different TKG/Kubernetes clusters.\nAntrea Security policies from the NSX manager # After you have connected your TKG clusters to the NSX manager (as shown earlier in this post) you will see the status of these connections in the NSX manager under System -\u0026gt; Fabric -\u0026gt; Nodes:\nThe status indicator is also a benefit of this integration as it will show you the status of Antrea Controller, and the components responsible for the Antrea-NSX integration.\nUnder inventory we can get all the relevant info from the TKG clusters:\nWhere in the screenshot above stc-tkg-cluster 1 and 2 are my TKG Antrea clusters. I can get all kinds of information like namespaces, pods, labels, ip addresses, names, services. This informaton is relevant as I can use them in my policy creation, but it also gives me status on whether pods, services are up.\nAntrea Cluster Network Policies - Applied from the NSX manager # With the NSX manager we can create and manage the Antrea Native Policies from the NSX graphical user interface instead of CLI. Using NSX security groups and labels make it also much more fun, but also very easy to maintain know what we do as we can see the policies.\nLets create some policies from the NSX manager microsegmenting my demo application Yelb. This is my demo application, it consists of four pods, and a service called yelb-ui where the webpage is exposed. I know the different parts of the application (e.g pods) are using labels so I will use them. First let us list them from cli and then get them from the NSX manager.\nlinuxvm01:~/antrea/policies$ k get pods -n yelb --show-labels NAME READY STATUS RESTARTS AGE LABELS redis-server-69846b4888-5m757 1/1 Running 0 22h app=redis-server,pod-template-hash=69846b4888,tier=cache yelb-appserver-857c5c76d5-4cgbq 1/1 Running 0 22h app=yelb-appserver,pod-template-hash=857c5c76d5,tier=middletier yelb-db-6bd4fc5d9b-92rkf 1/1 Running 0 22h app=yelb-db,pod-template-hash=6bd4fc5d9b,tier=backenddb yelb-ui-6df49457d6-4bktw 1/1 Running 0 20h app=yelb-ui,pod-template-hash=6df49457d6,tier=frontend Ok, there I have the labels. Fine, just for the sake of it I will find the same labels in the NSX manager also:\nNow I need to create some security groups in NSX using these labels.\nFirst group is called acnp-yelb-frontend-ui and are using these membership criterias: (I am also adding the namespace criteria, to exclude any other applications using the same labels in other namespaces).\nNow hurry back to the security group and check whether there are any members\u0026hellip;. Disappointment. Just empty:\nFear not, let us quickly create a policy with this group:\nCreate a new policy and set Antrea Container Clusters in the applied to field:\nThe actual rule:\nThe rule above allows my AVI Service Engines to reach the web-port on my yelb-ui pod on port 80 (http) as they are the loadbalancer for my application.\nAny members in the group now? Yes \u0026#x1f603;\nNow go ahead and create similar groups and rules (except the ports) for the other pods using their respective label.\nEnd result:\nDo they work? Let us find that out a bit later as I need something to put in my TraceFlow chapter \u0026#x1f604;\nThe rules I have added above was just for the application in the namespace Yelb. If I wanted to extend this ruleset to also include the same application from other clusters its just adding the Kubernetes cluster in the Applied field like this: NSX Distributed Firewall - Kubernetes objects Policies # In additon to managing the Antrea Native Policies from the NSX manager as above, in the recent NSX release additional features have been added to support security policies enforced in the NSX Distributed Firewall to also cover these components:\nWith this we can create security policies in NSX using the distributed firewall to cover the above components using security groups. With this feature its no longer necessary to investigate to get the information about the above components as they are already reported into the NSX manager. Let is do an example of how such a rule can be created and work.\nI will create a security policy based on this feature where I will use Kubernetes Service in my example. I will create a security group as above, but this time I will do some different selections. First grab the labels from the service, I will use the yelb-ui service in my example:\nlinuxvm01:~/antrea/policies$ k get svc -n yelb --show-labels NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE LABELS redis-server ClusterIP 20.10.102.8 \u0026lt;none\u0026gt; 6379/TCP 23h app=redis-server,tier=cache yelb-appserver ClusterIP 20.10.247.130 \u0026lt;none\u0026gt; 4567/TCP 23h app=yelb-appserver,tier=middletier yelb-db ClusterIP 20.10.44.17 \u0026lt;none\u0026gt; 5432/TCP 23h app=yelb-db,tier=backenddb yelb-ui LoadBalancer 20.10.194.179 10.13.210.10 80:30912/TCP 21h app=yelb-ui,tier=frontend I can either decide to use app=yelb-ui or tier=frontend. Now that I have my labels I will create my security group like this:\nI used the name of the service itself and the name of the namespace. This gives me this member: Which is right\u0026hellip;\nNow create a security policy using this group where source is another group where I have defined a VM running in the same NSX environment. I have also created a any group which contains just 0.0.0.0/0. Remember that this policy is enforced in the DFW, so there must be something that is running in NSX for this to work, which in my environments is not only the the TKG cluster, but also the Avi Service Engines which acts as LoadBalancer and Ingress for my exposed services. This is kind of important to think of, as the Avi Service Engines communicates with the TKG cluster nodes using NodePortLocal in the default portrange 61000-62000 (if not changed in the Antrea configmap).\nLets see if the below rule works then:\nI will adjust it to Action Drop:\nTest Yelb ui access from my linux vm via curl and my physical laptop\u0026rsquo;s browser, and the results are in:\nubuntu02:~$ curl yelb-ui.yelb.carefor.some-dns.net curl: (28) Failed to connect to yelb-ui.yelb.carefor.some-dns.net port 80: Connection timed out From my physical laptop browser:\nThis will be dropped even though I still have these rules in place from earlier (remember): Now, what about the Avi Service Engines?\nIf we just look at the rules above, the NSX Kubernetes Service rule and the Antrea Policies rules we are doing the firewall enforcing at two different levels. When creating policies with the Antrea Native Policies, like the one just above, we are applying and enforcing inside the Kubernetes cluster, with the NSX Kubernetes Service rule we are applying and enforcing on the DFW layer. So the Avi Service Engines will first need a policy that is allowing them to communicate to the TKG worker nodes on specific ports/protocol, in my exampe above with Yelb it is port 61002 and TCP. We can see that by looking in Avi UI:\nRegardless of the Avi SE\u0026rsquo;s are using the same DFW as the worker nodes, we need to create this policy for the SE to reach the worker nodes to allow this connection. These policies can either be very \u0026ldquo;lazy\u0026rdquo; allowing the SEs on everyting TCP with a range of 61000-62000 to the worker nodes or can be made very granual pr service. The Avi SEs are automatically being grouped in NSX security groups if using Avi with NSX Cloud, explore that.\nIf we are not allowing the SEs this traffic, we will se this in the Avi UI:\nWhy is that though, I dont have a default block-all rule in my NSX environment\u0026hellip; Well this is because of a set of default rules being created by NCP from TKG. Have a look at this rule:\nWhat is the membership in the group used in this Drop rule? That is all my TKG nodes including the Supervisor Control Plane nodes (the workload interface).\nNow in the Antrea Policies, we need to allow the IP addresses the SEs are using to reach the yelb-ui, as its not the actual client-ip that is being used, it is the SEs dataplane network.\nThe above diagram tries to explain the traffic flow and how it will be enforced. First the user want to access the VIP of the Yelb UI service. This is allowed by the NSX Firewall saying, yes Port 80 on IP 10.13.210.10 is OK to pass. As this VIP is realized by the Avi SEs, and are on NSX this rule will be enforced by the NSX firewall. Then the Avi SEs will forward (loadbalance) the traffic to the worker node(s) using NodePortLocal ranges between 61000-62000(default) where the worker nodes are also on the same NSX DFW, so we need to allow the SEs to forward this traffic. When all above is allowed, we will get \u0026ldquo;into\u0026rdquo; the actual TKG (Kubernetes) cluster and need to negiotiate the Antrea Native Policies that have been applied. These rules remember are allowing the SE dataplane IPs to reach the pod yelb-ui on port 80. And thats it.\nJust before we end up this chapter and head over to the next, let us quickly see how the policies created from the NSX manager look like inside the TKG cluster:\nlinuxvm01:~/antrea/policies$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 823fca6f-88ee-4032-8150-ac8cf22f1c93 nsx-category-infrastructure 1.000000017763571 3 3 23h 9ae2599a-3bd3-4413-849e-06f53f467559 nsx-category-application 1.0000000532916369 2 2 24h The policies will be placed according to the NSX tiers from the UI:\nIf I describe one of the policies I will get the actual yaml manifest:\nlinuxvm01:~/antrea/policies$ k get acnp 9ae2599a-3bd3-4413-849e-06f53f467559 -oyaml apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: annotations: ccp-adapter.antrea.tanzu.vmware.com/display-name: Yelb-Zero-Trust creationTimestamp: \u0026#34;2023-06-05T12:12:14Z\u0026#34; generation: 6 labels: ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter name: 9ae2599a-3bd3-4413-849e-06f53f467559 resourceVersion: \u0026#34;404591\u0026#34; uid: 6477e785-fde4-46ba-b0a1-5ff5f784db8c spec: ingress: - action: Allow appliedTo: - group: 6f39fadf-04e8-4f49-be77-da0d4005ff37 enableLogging: false from: - ipBlock: cidr: 10.13.11.101/32 - ipBlock: cidr: 10.13.11.100/32 name: \u0026#34;4084\u0026#34; ports: - port: 80 protocol: TCP - action: Allow appliedTo: - group: 31cf5eab-8bcd-4305-b72d-f1a44843fd8e enableLogging: false from: - group: 6f39fadf-04e8-4f49-be77-da0d4005ff37 name: \u0026#34;4085\u0026#34; ports: - port: 4567 protocol: TCP - action: Allow appliedTo: - group: 672f4d75-c83b-4fa1-b0ab-ae414c2e8e8c enableLogging: false from: - group: 31cf5eab-8bcd-4305-b72d-f1a44843fd8e name: \u0026#34;4087\u0026#34; ports: - port: 5432 protocol: TCP - action: Allow appliedTo: - group: 52c3548b-4758-427f-bcde-b25d36613de6 enableLogging: false from: - group: 31cf5eab-8bcd-4305-b72d-f1a44843fd8e name: \u0026#34;4088\u0026#34; ports: - port: 6379 protocol: TCP - action: Drop appliedTo: - group: d250b7d7-3041-4f7f-8fdf-c7360eee9615 enableLogging: false from: - group: d250b7d7-3041-4f7f-8fdf-c7360eee9615 name: \u0026#34;4089\u0026#34; priority: 1.0000000532916369 tier: nsx-category-application status: currentNodesRealized: 2 desiredNodesRealized: 2 observedGeneration: 6 phase: Realized Antrea Security policies from kubernetes api # I have already covered this topic in another post here. Head over and have look, also its worth reading the official documentation page from Antrea here as it contains examples and is updated on new features.\nOne thing I would like to use this chapter for though is trying to apply a policy on the NSX added Tiers when doing the integration (explained above). Remember the Tiers?\nlinuxvm01:~/antrea/policies$ k get tiers NAME PRIORITY AGE application 250 2d2h baseline 253 2d2h emergency 50 2d2h networkops 150 2d2h nsx-category-application 4 2d nsx-category-emergency 1 2d nsx-category-environment 3 2d nsx-category-ethernet 0 2d nsx-category-infrastructure 2 2d platform 200 2d2h securityops 100 2d2h These nsx* tiers are coming from the NSX manager, but can I as a cluster-owner/editor place rules in here by default? If you look at the PRIORITY of these, they are pretty low.\nLet us apply the same rule as used earlier in this post, by just editing in the tier placement:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: acnp-nsx-tier-from-kubectl spec: priority: 1 tier: nsx-category-environment appliedTo: - podSelector: matchLabels: app: ubuntu-20-04 egress: - action: Allow to: - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; ports: - protocol: TCP port: 80 - action: Allow linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-nsx-tier.yaml Error from server: error when creating \u0026#34;fqdn-rule-nsx-tier.yaml\u0026#34;: admission webhook \u0026#34;acnpvalidator.antrea.io\u0026#34; denied the request: user not authorized to access Tier nsx-category-environment Even though I am the cluster-owner/admin/superuser I am not allowed to place any rules in these nsx tiers. So this just gives us further control and mechanisms to support both NSX created Antrea policies and Antrea policies from kubectl. This allows for a good control of security enforcement by roles in the organization.\nAntrea Dashboard # As the Octant dashboard is no more, Antrea now has its own dashboard. Its very easy to deploy. Let me quickly go through it. Read more about it here\n# Add the helm charts helm repo add antrea https://charts.antrea.io helm repo update Install it:\nhelm install antrea-ui antrea/antrea-ui --namespace kube-system linuxvm01:~/antrea/policies$ helm repo add antrea https://charts.antrea.io \u0026#34;antrea\u0026#34; has been added to your repositories linuxvm01:~/antrea/policies$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;ako\u0026#34; chart repository ...Successfully got an update from the \u0026#34;antrea\u0026#34; chart repository ...Successfully got an update from the \u0026#34;bitnami\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ linuxvm01:~/antrea/policies$ helm install antrea-ui antrea/antrea-ui --namespace kube-system NAME: antrea-ui LAST DEPLOYED: Tue Jun 6 12:56:21 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The Antrea UI has been successfully installed You are using version 0.1.1 To access the UI, forward a local port to the antrea-ui service, and connect to that port locally with your browser: $ kubectl -n kube-system port-forward service/antrea-ui 3000:3000 After running the command above, access \u0026#34;http://localhost:3000\u0026#34; in your browser.For the Antrea documentation, please visit https://antrea.io This will spin up a new pod, and a clusterip service.\nlinuxvm01:~/antrea/policies$ k get pods -n kube-system NAME READY STATUS RESTARTS AGE antrea-agent-9rvqc 2/2 Running 0 2d16h antrea-agent-m7rg7 2/2 Running 0 2d16h antrea-agent-wvpp8 2/2 Running 0 2d16h antrea-controller-6d56b6d664-vlmh2 1/1 Running 0 2d16h antrea-ui-9c89486f4-msw6m 2/2 Running 0 62s linuxvm01:~/antrea/policies$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE antrea ClusterIP 20.10.96.45 \u0026lt;none\u0026gt; 443/TCP 2d16h antrea-ui ClusterIP 20.10.228.144 \u0026lt;none\u0026gt; 3000/TCP 95s Now instead of exposing the service as nodeport, I am just creating a serviceType loadBalancer for it like this:\napiVersion: v1 kind: Service metadata: name: antrea-dashboard-ui labels: app: antrea-ui namespace: kube-system spec: loadBalancerClass: ako.vmware.com/avi-lb type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 3000 selector: app: antrea-ui Apply it:\nlinuxvm01:~/antrea$ k apply -f antrea-dashboard-lb-yaml service/antrea-dashboard-ui created linuxvm01:~/antrea$ k get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE antrea ClusterIP 20.10.96.45 \u0026lt;none\u0026gt; 443/TCP 2d16h antrea-dashboard-ui LoadBalancer 20.10.76.243 10.13.210.12 80:31334/TCP 7s antrea-ui ClusterIP 20.10.228.144 \u0026lt;none\u0026gt; 3000/TCP 8m47s Now access it through my browser:\nDefault password is admin\nGood overview:\nThe option to do Traceflow:\nOops, dropped by a NetworkPolicy\u0026hellip; Where does that come from \u0026#x1f914; \u0026hellip; More on this later.\nAntrea Network Monitoring # Being able to know what\u0026rsquo;s going on is crucial when planning security policies, but also to know if the policies are working and being enforced. With that information available we can know if we are compliant with the policies apllied. Without any network flow information we are kind of in the blind. Luckily Antrea is fully capable of report full flow information, and export it. To be able to export the flow information we need to enable the FeatureGate FlowExporter:\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: stc-tkg-cluster-1-antrea-package namespace: ns-stc-1 spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: true #This needs to be enabled Egress: true NodePortLocal: true AntreaTraceflow: true NetworkPolicyStats: true Flow-Exporter - IPFIX # From the offical Antrea documentation:\nAntrea is a Kubernetes network plugin that provides network connectivity and security features for Pod workloads. Considering the scale and dynamism of Kubernetes workloads in a cluster, Network Flow Visibility helps in the management and configuration of Kubernetes resources such as Network Policy, Services, Pods etc., and thereby provides opportunities to enhance the performance and security aspects of Pod workloads.\nFor visualizing the network flows, Antrea monitors the flows in Linux conntrack module. These flows are converted to flow records, and then flow records are post-processed before they are sent to the configured external flow collector. High-level design is given below:\nFrom the Antrea official documentation again:\nFlow Exporter\nIn Antrea, the basic building block for the Network Flow Visibility is the Flow Exporter. Flow Exporter operates within Antrea Agent; it builds and maintains a connection store by polling and dumping flows from conntrack module periodically. Connections from the connection store are exported to the Flow Aggregator Service using the IPFIX protocol, and for this purpose we use the IPFIX exporter process from the go-ipfix library.\nRead more Network Flow Visibility in Antrea here.\nTraceflow # When troubleshooting network issues or even firewall rules (is my traffic being blocked or allowed?) it is very handy to have the option to do Traceflow. Antrea supports Traceflow. To be able to use Traceflow, the AntreaTraceFlow FeatureGate needs to be enabled if not already.\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: stc-tkg-cluster-1-antrea-package namespace: ns-stc-1 spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: true Egress: true NodePortLocal: true AntreaTraceflow: true #This needs to be enabled NetworkPolicyStats: true Now that it is enabled, how can we perform Traceflow?\nWe can do Traceflow using kubectl, Antrea UI or even from the NSX manager if using the NSX/Antrea integration.\nTraceflow in Antrea supports the following:\nSource: pod, protocol (TCP/UDP/ICMP) and port numbers Destination: pod, service, ip, protocol (TCP/UDP/ICMP) and port numbers One time Traceflow or live Now to get back to my Antrea policies created earlier I want to test if they are actually being in use and enforced. So let me do a Traceflow form my famous Yelb-ui pod and see if it can reach the application pod on its allowed port. Remember that the UI pod needed to communicate with the appserver pod on TCP 4567 and that I created a rule that only allows this, all else is blocked.\nIf I want to do Traceflow from kubectl, this is an example to test if port 4567 is allowed from ui pod to appserver pod:\napiVersion: crd.antrea.io/v1alpha1 kind: Traceflow metadata: name: tf-test spec: source: namespace: yelb pod: yelb-ui-6df49457d6-m5clv destination: namespace: yelb pod: yelb-appserver-857c5c76d5-4cd86 # destination can also be an IP address (\u0026#39;ip\u0026#39; field) or a Service name (\u0026#39;service\u0026#39; field); the 3 choices are mutually exclusive. packet: ipHeader: # If ipHeader/ipv6Header is not set, the default value is IPv4+ICMP. protocol: 6 # Protocol here can be 6 (TCP), 17 (UDP) or 1 (ICMP), default value is 1 (ICMP) transportHeader: tcp: srcPort: 0 # Source port needs to be set when Protocol is TCP/UDP. dstPort: 4567 # Destination port needs to be set when Protocol is TCP/UDP. flags: 2 # Construct a SYN packet: 2 is also the default value when the flags field is omitted. Now apply it and get the output:\nlinuxvm01:~/antrea/policies$ k apply -f traceflow.yaml traceflow.crd.antrea.io/tf-test created linuxvm01:~/antrea/policies$ k get traceflows.crd.antrea.io -n yelb tf-test -oyaml apiVersion: crd.antrea.io/v1alpha1 kind: Traceflow metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;crd.antrea.io/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Traceflow\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;tf-test\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;destination\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-appserver-857c5c76d5-4cd86\u0026#34;},\u0026#34;packet\u0026#34;:{\u0026#34;ipHeader\u0026#34;:{\u0026#34;protocol\u0026#34;:6},\u0026#34;transportHeader\u0026#34;:{\u0026#34;tcp\u0026#34;:{\u0026#34;dstPort\u0026#34;:4567,\u0026#34;flags\u0026#34;:2,\u0026#34;srcPort\u0026#34;:0}}},\u0026#34;source\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-ui-6df49457d6-m5clv\u0026#34;}}} creationTimestamp: \u0026#34;2023-06-07T12:47:14Z\u0026#34; generation: 1 name: tf-test resourceVersion: \u0026#34;904386\u0026#34; uid: c550596b-ed43-4bab-a6f1-d23e90d35f84 spec: destination: namespace: yelb pod: yelb-appserver-857c5c76d5-4cd86 packet: ipHeader: protocol: 6 transportHeader: tcp: dstPort: 4567 flags: 2 srcPort: 0 source: namespace: yelb pod: yelb-ui-6df49457d6-m5clv status: phase: Succeeded results: - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-5r8gj observations: - action: Received component: Forwarding - action: Forwarded component: NetworkPolicy componentInfo: IngressRule networkPolicy: AntreaClusterNetworkPolicy:9ae2599a-3bd3-4413-849e-06f53f467559 - action: Delivered component: Forwarding componentInfo: Output timestamp: 1686142036 - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-bpx7s observations: - action: Forwarded component: SpoofGuard - action: Forwarded component: Forwarding componentInfo: Output tunnelDstIP: 10.13.82.39 timestamp: 1686142036 startTime: \u0026#34;2023-06-07T12:47:14Z\u0026#34; That was a success. - action: Forwarded\nNow I want to run it again but with another port. So I change the above yaml to use port 4568 (which should not be allowed):\nlinuxvm01:~/antrea/policies$ k get traceflows.crd.antrea.io -n yelb tf-test -oyaml apiVersion: crd.antrea.io/v1alpha1 kind: Traceflow metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;crd.antrea.io/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Traceflow\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;tf-test\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;destination\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-appserver-857c5c76d5-4cd86\u0026#34;},\u0026#34;packet\u0026#34;:{\u0026#34;ipHeader\u0026#34;:{\u0026#34;protocol\u0026#34;:6},\u0026#34;transportHeader\u0026#34;:{\u0026#34;tcp\u0026#34;:{\u0026#34;dstPort\u0026#34;:4568,\u0026#34;flags\u0026#34;:2,\u0026#34;srcPort\u0026#34;:0}}},\u0026#34;source\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-ui-6df49457d6-m5clv\u0026#34;}}} creationTimestamp: \u0026#34;2023-06-07T12:53:59Z\u0026#34; generation: 1 name: tf-test resourceVersion: \u0026#34;905571\u0026#34; uid: d76ec419-3272-4595-98a5-72a49adce9d3 spec: destination: namespace: yelb pod: yelb-appserver-857c5c76d5-4cd86 packet: ipHeader: protocol: 6 transportHeader: tcp: dstPort: 4568 flags: 2 srcPort: 0 source: namespace: yelb pod: yelb-ui-6df49457d6-m5clv status: phase: Succeeded results: - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-bpx7s observations: - action: Forwarded component: SpoofGuard - action: Forwarded component: Forwarding componentInfo: Output tunnelDstIP: 10.13.82.39 timestamp: 1686142441 - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-5r8gj observations: - action: Received component: Forwarding - action: Dropped component: NetworkPolicy componentInfo: IngressMetric networkPolicy: AntreaClusterNetworkPolicy:9ae2599a-3bd3-4413-849e-06f53f467559 timestamp: 1686142441 startTime: \u0026#34;2023-06-07T12:53:59Z\u0026#34; That was also a success, as it was dropped by design: - action: Dropped\nIts great being able to do this from kubectl, if one quickly need to check this before starting to look somewhere else and create a support ticket \u0026#x1f603; or one dont have access to other tools like the Antrea UI or even the NSX manager, speaking of NSX manager. Let us do the exact same trace from the NSX manager gui:\nHead over Plan\u0026amp;Troubleshoot -\u0026gt; Traffic Analysis:\nResults:\nNow I change it to another port again and test it again:\nDropped again.\nThe same procedure can also be done from the Antrea UI as shown above, now with a port that is allowed:\nTo read more on Traceflow in Antrea, head over here.\nTheia # Now that we have know it\u0026rsquo;s possible to export all flows using IPFIX, I thought it would be interesting to just showcase how the flow information can be presented with a solution called Theia. From the official docs:\nTheia is a network observability and analytics platform for Kubernetes. It is built on top of Antrea, and consumes network flows exported by Antrea to provide fine-grained visibility into the communication and NetworkPolicies among Pods and Services in a Kubernetes cluster.\nTo install Theia I have followed the instructions from here which is also a greate place to read more about Theia.\nTheia is installed using Helm, start by adding the charts, do an update and deploy:\nlinuxvm01:~/antrea$ helm repo add antrea https://charts.antrea.io \u0026#34;antrea\u0026#34; already exists with the same configuration, skipping linuxvm01:~/antrea$ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \u0026#34;antrea\u0026#34; chart repository Update Complete. ⎈Happy Helming!⎈ Make sure that FlowExporter has been enabled, if not apply an AntreaConfig that enables it:\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: stc-tkg-cluster-1-antrea-package namespace: ns-stc-1 spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: true #Enable this! Egress: true NodePortLocal: true AntreaTraceflow: true NetworkPolicyStats: true After the config has been enabled, delete the Antrea agents and controller so these will read the new configMap:\nlinuxvm01:~/antrea/theia$ k delete pod -n kube-system -l app=antrea\rpod \u0026#34;antrea-agent-58nn2\u0026#34; deleted\rpod \u0026#34;antrea-agent-cnq9p\u0026#34; deleted\rpod \u0026#34;antrea-agent-sx6vr\u0026#34; deleted\rpod \u0026#34;antrea-controller-6d56b6d664-km64t\u0026#34; deleted After the Helm charts have been added, I start by installing the Flow Aggregator\nhelm install flow-aggregator antrea/flow-aggregator --set clickHouse.enable=true,recordContents.podLabels=true -n flow-aggregator --create-namespace As usual with Helm charts, if there is any specific settings you would like to change get the helm chart values for your specific charts first and refer to them by using -f values.yaml..\nlinuxvm01:~/antrea/theia$ helm show values antrea/flow-aggregator \u0026gt; flow-agg-values.yaml I dont have any specifics I need to change for this one, so I will just deploy using the defaults:\nlinuxvm01:~/antrea/theia$ helm install flow-aggregator antrea/flow-aggregator --set clickHouse.enable=true,recordContents.podLabels=true -n flow-aggregator --create-namespace NAME: flow-aggregator LAST DEPLOYED: Tue Jun 6 21:28:49 2023 NAMESPACE: flow-aggregator STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The Antrea Flow Aggregator has been successfully installed You are using version 1.12.0 For the Antrea documentation, please visit https://antrea.io Now what has happened in my TKG cluster:\nlinuxvm01:~/antrea/theia$ k get pods -n flow-aggregator NAME READY STATUS RESTARTS AGE flow-aggregator-5b4c69885f-mklm5 1/1 Running 1 (10s ago) 22s linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator NAME READY STATUS RESTARTS AGE flow-aggregator-5b4c69885f-mklm5 1/1 Running 1 (13s ago) 25s linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator NAME READY STATUS RESTARTS AGE flow-aggregator-5b4c69885f-mklm5 0/1 Error 1 (14s ago) 26s linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator NAME READY STATUS RESTARTS AGE flow-aggregator-5b4c69885f-mklm5 0/1 CrashLoopBackOff 3 (50s ago) 60s Well, that did\u0026rsquo;nt go so well\u0026hellip;\nThe issue is that Flow Aggregator is looking for a service that is not created yet and will just fail until this is deployed. This is our next step.\nlinuxvm01:~/antrea/theia$ helm install theia antrea/theia --set sparkOperator.enable=true,theiaManager.enable=true -n flow-visibility --create-namespace NAME: theia LAST DEPLOYED: Tue Jun 6 22:02:37 2023 NAMESPACE: flow-visibility STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Theia has been successfully installed You are using version 0.6.0 For the Antrea documentation, please visit https://antrea.io What has been created now:\nlinuxvm01:~/antrea/theia$ k get pods -n flow-visibility\rNAME READY STATUS RESTARTS AGE\rchi-clickhouse-clickhouse-0-0-0 2/2 Running 0 8m52s\rgrafana-684d8948b-c6wzn 1/1 Running 0 8m56s\rtheia-manager-5d8d6b86b7-cbxrz 1/1 Running 0 8m56s\rtheia-spark-operator-54d9ddd544-nqhqd 1/1 Running 0 8m56s\rzookeeper-0 1/1 Running 0 8m56s Now flow-aggreator should also be in a runing state, if not just delete the pod and it should get back on its feet.\nlinuxvm01:~/antrea/theia$ k get pods -n flow-aggregator\rNAME READY STATUS RESTARTS AGE\rflow-aggregator-5b4c69885f-xhdkx 1/1 Running 0 5m2s So, now its all about getting access to the Grafana dashboard. I will just expose this with serviceType loadBalancer as it \u0026ldquo;out-of-the-box\u0026rdquo; is only exposed with NodePort:\nlinuxvm01:~/antrea/theia$ k get svc -n flow-visibility\rNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rchi-clickhouse-clickhouse-0-0 ClusterIP None \u0026lt;none\u0026gt; 8123/TCP,9000/TCP,9009/TCP 8m43s\rclickhouse-clickhouse ClusterIP 20.10.136.211 \u0026lt;none\u0026gt; 8123/TCP,9000/TCP 10m\rgrafana NodePort 20.10.172.165 \u0026lt;none\u0026gt; 3000:30096/TCP 10m\rtheia-manager ClusterIP 20.10.156.217 \u0026lt;none\u0026gt; 11347/TCP 10m\rzookeeper ClusterIP 20.10.219.137 \u0026lt;none\u0026gt; 2181/TCP,7000/TCP 10m\rzookeepers ClusterIP None \u0026lt;none\u0026gt; 2888/TCP,3888/TCP 10m So let us create a LoadBalancer service for this:\napiVersion: v1 kind: Service metadata: name: theia-dashboard-ui labels: app: grafana namespace: flow-visibility spec: loadBalancerClass: ako.vmware.com/avi-lb type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 3000 selector: app: grafana linuxvm01:~/antrea/theia$ k get svc -n flow-visibility NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) grafana NodePort 20.10.172.165 \u0026lt;none\u0026gt; 3000:30096/TCP 15m theia-dashboard-ui LoadBalancer 20.10.24.174 10.13.210.13 80:32075/TCP 13s Lets try to access it through the browser:\nGreat. Theia comes with a couple of predefined dashobards that is interesting to start out with. So let me list some of the screenshots from the predefined dashboards below:\nThe homepage:\nList of dashboards:\nFlow_Records_Dashboard:\nNetwork_Topology_Dashboard:\nNetwork Policy Recommendation # From the official docs:\nTheia NetworkPolicy Recommendation recommends the NetworkPolicy configuration to secure Kubernetes network and applications. It analyzes the network flows collected by Grafana Flow Collector to generate Kubernetes NetworkPolicies or Antrea NetworkPolicies. This feature assists cluster administrators and app developers in securing their applications according to Zero Trust principles.\nI like the sound of that. Let us try it out.\nThe first I need to install inst the Theia CLI, this can be found and the instructions from here\nTheia CLI\ncurl -Lo ./theia \u0026#34;https://github.com/antrea-io/theia/releases/download/v0.6.0/theia-$(uname)-x86_64\u0026#34; chmod +x ./theia mv ./theia /usr/local/bin/theia theia help linuxvm01:~/antrea/theia$ curl -Lo ./theia \u0026#34;https://github.com/antrea-io/theia/releases/download/v0.6.0/theia-$(uname)-x86_64\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 37.9M 100 37.9M 0 0 11.6M 0 0:00:03 0:00:03 --:--:-- 17.2M linuxvm01:~/antrea/theia$ chmod +x ./theia linuxvm01:~/antrea/theia$ sudo cp theia /usr/local/bin/theia linuxvm01:~/antrea/theia$ theia help theia is the command line tool for Theia which provides access to Theia network flow visibility capabilities Usage: theia [command] Available Commands: clickhouse Commands of Theia ClickHouse feature completion Generate the autocompletion script for the specified shell help Help about any command policy-recommendation Commands of Theia policy recommendation feature supportbundle Generate support bundle throughput-anomaly-detection Commands of Theia throughput anomaly detection feature version Show Theia CLI version Flags: -h, --help help for theia -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified -v, --verbose int set verbose level Use \u0026#34;theia [command] --help\u0026#34; for more information about a command. These are the following commands when running the policy-recommendation option:\nandreasm@linuxvm01:~/antrea/theia$ theia policy-recommendation --help Command group of Theia policy recommendation feature. Must specify a subcommand like run, status or retrieve. Usage: theia policy-recommendation [flags] theia policy-recommendation [command] Aliases: policy-recommendation, pr Available Commands: delete Delete a policy recommendation job list List all policy recommendation jobs retrieve Get the recommendation result of a policy recommendation job run Run a new policy recommendation job status Check the status of a policy recommendation job Use \u0026#34;theia policy-recommendation [command] --help\u0026#34; for more information about a command. These are the options for the run command:\nlinuxvm01:~/antrea/theia$ theia policy-recommendation run --help\rRun a new policy recommendation job.\rMust finish the deployment of Theia first\rUsage:\rtheia policy-recommendation run [flags]\rExamples:\rRun a policy recommendation job with default configuration\r$ theia policy-recommendation run\rRun an initial policy recommendation job with policy type anp-deny-applied and limit on last 10k flow records\r$ theia policy-recommendation run --type initial --policy-type anp-deny-applied --limit 10000\rRun an initial policy recommendation job with policy type anp-deny-applied and limit on flow records from 2022-01-01 00:00:00 to 2022-01-31 23:59:59.\r$ theia policy-recommendation run --type initial --policy-type anp-deny-applied --start-time \u0026#39;2022-01-01 00:00:00\u0026#39; --end-time \u0026#39;2022-01-31 23:59:59\u0026#39;\rRun a policy recommendation job with default configuration but doesn\u0026#39;t recommend toServices ANPs\r$ theia policy-recommendation run --to-services=false\rFlags:\r--driver-core-request string Specify the CPU request for the driver Pod. Values conform to the Kubernetes resource quantity convention.\rExample values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;)\r--driver-memory string Specify the memory request for the driver Pod. Values conform to the Kubernetes resource quantity convention.\rExample values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;)\r-e, --end-time string The end time of the flow records considered for the policy recommendation.\rFormat is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the end time of flow records by default.\r--exclude-labels Enable this option will exclude automatically generated Pod labels including \u0026#39;pod-template-hash\u0026#39;,\r\u0026#39;controller-revision-hash\u0026#39;, \u0026#39;pod-template-generation\u0026#39; during policy recommendation. (default true)\r--executor-core-request string Specify the CPU request for the executor Pod. Values conform to the Kubernetes resource quantity convention.\rExample values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;)\r--executor-instances int32 Specify the number of executors for the Spark application. Example values include 1, 2, 8, etc. (default 1)\r--executor-memory string Specify the memory request for the executor Pod. Values conform to the Kubernetes resource quantity convention.\rExample values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;)\r-f, --file string The file path where you want to save the result. It can only be used when wait is enabled.\r-h, --help help for run\r-l, --limit int The limit on the number of flow records read from the database. 0 means no limit.\r-n, --ns-allow-list string List of default allow Namespaces.\rIf no Namespaces provided, Traffic inside Antrea CNI related Namespaces: [\u0026#39;kube-system\u0026#39;, \u0026#39;flow-aggregator\u0026#39;,\r\u0026#39;flow-visibility\u0026#39;] will be allowed by default.\r-p, --policy-type string Types of generated NetworkPolicy.\rCurrently we have 3 generated NetworkPolicy types:\ranp-deny-applied: Recommending allow ANP/ACNP policies, with default deny rules only on Pods which have an allow rule applied.\ranp-deny-all: Recommending allow ANP/ACNP policies, with default deny rules for whole cluster.\rk8s-np: Recommending allow K8s NetworkPolicies. (default \u0026#34;anp-deny-applied\u0026#34;)\r-s, --start-time string The start time of the flow records considered for the policy recommendation.\rFormat is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the start time of flow records by default.\r--to-services Use the toServices feature in ANP and recommendation toServices rules for Pod-to-Service flows,\ronly works when option is anp-deny-applied or anp-deny-all. (default true)\r-t, --type string {initial|subsequent} Indicates this recommendation is an initial recommendion or a subsequent recommendation job. (default \u0026#34;initial\u0026#34;)\r--wait Enable this option will hold and wait the whole policy recommendation job finishes.\rGlobal Flags:\r-k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified\r--use-cluster-ip Enable this option will use ClusterIP instead of port forwarding when connecting to the Theia\rManager Service. It can only be used when running in cluster.\r-v, --verbose int set verbose level I will just run the following theia policy-recommendation run \u0026ndash;type initial \u0026ndash;policy-type anp-deny-applied \u0026ndash;limit 10000 to generate some output.\nlinuxvm01:~/antrea/theia$ theia policy-recommendation run --type initial --policy-type anp-deny-applied --limit 10000 Successfully created policy recommendation job with name pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 Lets check the status:\n# First I will list all the runs to get the name linuxvm01:~/antrea/theia$ theia policy-recommendation list CreationTime CompletionTime Name Status 2023-06-08 07:50:28 N/A pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 SCHEDULED # Then I will check the status on the specific run linuxvm01:~/antrea/theia$ theia policy-recommendation status pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 Status of this policy recommendation job is SCHEDULED Seems like I have to wait some, time to grab a coffee.\nJust poured my coffee, wanted to check again:\nlinuxvm01:~/antrea/theia$ theia policy-recommendation status pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 Status of this policy recommendation job is RUNNING: 0/1 (0%) stages completed Alright, it is running.\nNow time to drink the coffee.\nLets check in on it again:\nlinuxvm01:~/antrea/theia$ theia policy-recommendation status pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 Status of this policy recommendation job is COMPLETED Oh yes, now I am excited which policies it recommends:\nlinuxvm01:~/antrea/theia$ theia policy-recommendation retrieve pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: recommend-reject-acnp-9np4b spec: appliedTo: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: yelb podSelector: matchLabels: app: traffic-generator egress: - action: Reject to: - podSelector: {} ingress: - action: Reject from: - podSelector: {} priority: 5 tier: Baseline --- apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: recommend-reject-acnp-ega4b spec: appliedTo: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: avi-system podSelector: matchLabels: app.kubernetes.io/instance: ako-1685884771 app.kubernetes.io/name: ako statefulset.kubernetes.io/pod-name: ako-0 egress: - action: Reject to: - podSelector: {} ingress: - action: Reject from: - podSelector: {} priority: 5 tier: Baseline --- apiVersion: crd.antrea.io/v1alpha1 kind: NetworkPolicy metadata: name: recommend-allow-anp-nl6re namespace: yelb spec: appliedTo: - podSelector: matchLabels: app: traffic-generator egress: - action: Allow ports: - port: 80 protocol: TCP to: - ipBlock: cidr: 10.13.210.10/32 ingress: [] priority: 5 tier: Application --- apiVersion: crd.antrea.io/v1alpha1 kind: NetworkPolicy metadata: name: recommend-allow-anp-2ifjo namespace: avi-system spec: appliedTo: - podSelector: matchLabels: app.kubernetes.io/instance: ako-1685884771 app.kubernetes.io/name: ako statefulset.kubernetes.io/pod-name: ako-0 egress: - action: Allow ports: - port: 443 protocol: TCP to: - ipBlock: cidr: 172.24.3.50/32 ingress: [] priority: 5 tier: Application --- apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: recommend-allow-acnp-kube-system-kaoh6 spec: appliedTo: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system egress: - action: Allow to: - podSelector: {} ingress: - action: Allow from: - podSelector: {} priority: 5 tier: Platform --- apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: recommend-allow-acnp-flow-aggregator-dnvhc spec: appliedTo: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: flow-aggregator egress: - action: Allow to: - podSelector: {} ingress: - action: Allow from: - podSelector: {} priority: 5 tier: Platform --- apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: recommend-allow-acnp-flow-visibility-sqjwf spec: appliedTo: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: flow-visibility egress: - action: Allow to: - podSelector: {} ingress: - action: Allow from: - podSelector: {} priority: 5 tier: Platform --- apiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: recommend-reject-acnp-hmjt8 spec: appliedTo: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: yelb-2 podSelector: matchLabels: app: yelb-ui tier: frontend egress: - action: Reject to: - podSelector: {} ingress: - action: Reject from: - podSelector: {} priority: 5 tier: Baseline Ok, well. I appreciate the output, but I would need to do some modifications to it before I would apply it. As my lab is not generating that much traffic, it does not create all the flows needed to generate a better recommendation. For it to generate better recommendations, the flows also needs to be there. My traffic-generator is not doing a good job to achieve this. I will need to generate some more activity for the recommendation engine to get enough flows to consider.\nThroughput Anomaly Detection # From the offical docs:\nFrom Theia v0.5, Theia supports Throughput Anomaly Detection. Throughput Anomaly Detection (TAD) is a technique for understanding and reporting the throughput abnormalities in the network traffic. It analyzes the network flows collected by Grafana Flow Collector to report anomalies in the network. TAD uses three algorithms to find the anomalies in network flows such as ARIMA, EWMA, and DBSCAN. These anomaly analyses help the user to find threats if present.\nLets try it out. I already have the dependencies and Theia CLI installed.\nWhat is the different commands available:\nlinuxvm01:~/antrea/theia$ theia throughput-anomaly-detection --help Command group of Theia throughput anomaly detection feature. Must specify a subcommand like run, list, delete, status or retrieve Usage: theia throughput-anomaly-detection [flags] theia throughput-anomaly-detection [command] Aliases: throughput-anomaly-detection, tad Available Commands: delete Delete a anomaly detection job list List all anomaly detection jobs retrieve Get the result of an anomaly detection job run throughput anomaly detection using Algo status Check the status of a anomaly detection job Flags: -h, --help help for throughput-anomaly-detection --use-cluster-ip Enable this option will use ClusterIP instead of port forwarding when connecting to the Theia Manager Service. It can only be used when running in cluster. Global Flags: -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified -v, --verbose int set verbose level Use \u0026#34;theia throughput-anomaly-detection [command] --help\u0026#34; for more information about a command. linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection run --help throughput anomaly detection using algorithms, currently supported algorithms are EWMA, ARIMA and DBSCAN Usage: theia throughput-anomaly-detection run [flags] Examples: Run the specific algorithm for throughput anomaly detection $ theia throughput-anomaly-detection run --algo ARIMA --start-time 2022-01-01T00:00:00 --end-time 2022-01-31T23:59:59 Run throughput anomaly detection algorithm of type ARIMA and limit on flow records from \u0026#39;2022-01-01 00:00:00\u0026#39; to \u0026#39;2022-01-31 23:59:59\u0026#39; Please note, algo is a mandatory argument\u0026#39; Flags: -a, --algo string The algorithm used by throughput anomaly detection. Currently supported Algorithms are EWMA, ARIMA and DBSCAN. --driver-core-request string Specify the CPU request for the driver Pod. Values conform to the Kubernetes resource quantity convention. Example values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;) --driver-memory string Specify the memory request for the driver Pod. Values conform to the Kubernetes resource quantity convention. Example values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;) -e, --end-time string The end time of the flow records considered for the anomaly detection. Format is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the end time of flow records by default. --executor-core-request string Specify the CPU request for the executor Pod. Values conform to the Kubernetes resource quantity convention. Example values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;) --executor-instances int32 Specify the number of executors for the Spark application. Example values include 1, 2, 8, etc. (default 1) --executor-memory string Specify the memory request for the executor Pod. Values conform to the Kubernetes resource quantity convention. Example values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;) -h, --help help for run -n, --ns-ignore-list string List of default drop Namespaces. Use this to ignore traffic from selected namespaces If no Namespaces provided, Traffic from all namespaces present in flows table will be allowed by default. -s, --start-time string The start time of the flow records considered for the anomaly detection. Format is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the start time of flow records by default. Global Flags: -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified --use-cluster-ip Enable this option will use ClusterIP instead of port forwarding when connecting to the Theia Manager Service. It can only be used when running in cluster. -v, --verbose int set verbose level I will use the example above:\nlinuxvm01:~/antrea/theia$ theia throughput-anomaly-detection run --algo ARIMA --start-time 2023-06-06T00:00:00 --end-time 2023-06-08T09:00:00 Successfully started Throughput Anomaly Detection job with name: tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection list CreationTime CompletionTime Name Status 2023-06-08 08:25:10 N/A tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 RUNNING linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection status tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 Status of this anomaly detection job is RUNNING: 0/0 (0%) stages completed Lets wait for it to finish\u0026hellip;\nlinuxvm01:~$ theia throughput-anomaly-detection list CreationTime CompletionTime Name Status 2023-06-08 08:25:10 2023-06-08 08:40:03 tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 COMPLETED Now check the output:\n# It is a long list so I am piping it to a text file linuxvm01:~$ theia throughput-anomaly-detection retrieve tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 \u0026gt; anomaly-detection-1.txt A snippet from the output:\nid sourceIP sourceTransportPort destinationIP destinationTransportPort flowStartSeconds flowEndSeconds throughput algoCalc anomaly 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T21:41:38Z 54204 65355.16485680155 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T07:09:32Z 49901 54713.50251767502 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T03:33:48Z 50000 53550.532983008845 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T00:52:48Z 59725 52206.69079880149 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T18:49:03Z 48544 53287.107990749006 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T22:06:43Z 61832 53100.99541753638 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T20:57:28Z 58295 54168.70924924757 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T07:36:38Z 47309 53688.236655529385 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T08:05:43Z 59227 52623.71668244673 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T05:22:12Z 58217 53709.42205164235 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T02:19:03Z 48508 55649.8819138477 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T23:27:28Z 53846 48125.33491950862 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T00:09:38Z 59562 52143.367660610136 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T02:44:08Z 50966 57119.323329628125 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T08:24:17Z 55553 50480.7443391562 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T00:02:38Z 44172 53694.11880964807 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T04:07:57Z 53612 49714.00885995446 true 2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T01:54:58Z 59089 51972.42465384903 true Antrea Egress # This chapter is also covered in another post I have done here with some tweaks \u0026#x1f603;\nFrom the offical docs\nEgress is a CRD API that manages external access from the Pods in a cluster. It supports specifying which egress (SNAT) IP the traffic from the selected Pods to the external network should use. When a selected Pod accesses the external network, the egress traffic will be tunneled to the Node that hosts the egress IP if it’s different from the Node that the Pod runs on and will be SNATed to the egress IP when leaving that Node.\nYou may be interested in using this capability if any of the following apply:\nA consistent IP address is desired when specific Pods connect to services outside of the cluster, for source tracing in audit logs, or for filtering by source IP in external firewall, etc. You want to force outgoing external connections to leave the cluster via certain Nodes, for security controls, or due to network topology restrictions. ","date":"1 June 2023","externalUrl":null,"permalink":"/2023/06/01/managing-antrea-in-vsphere-with-tanzu/","section":"Posts","summary":"In this post I will go through how to manage Antrea in a vSphere with Tanzu cluster using the Antrea CRD, and then explore some features and possibilites with Antrea","title":"Managing Antrea in vSphere with Tanzu","type":"posts"},{"content":"","date":"1 June 2023","externalUrl":null,"permalink":"/tags/nsx-t/","section":"Tags","summary":"","title":"Nsx-T","type":"tags"},{"content":"","date":"30 May 2023","externalUrl":null,"permalink":"/tags/multi-dc/","section":"Tags","summary":"","title":"Multi-Dc","type":"tags"},{"content":"","date":"30 May 2023","externalUrl":null,"permalink":"/tags/nsx-advanced-loadbalancer/","section":"Tags","summary":"","title":"Nsx Advanced Loadbalancer","type":"tags"},{"content":" Overview of what this post is about # In this post my goal is to use TKG (Tanzu Kubernetes Grid here) to manage and deploy workload clusters in remote datacenter\u0026rsquo;s. The reason for such needs can be many like easier lifecycle management of workload clusters in environments where we have several datacenters, with different physical locations. To lower the management overhead and simplify lifecycle management, like updates, being able to centrally manage these operations is sometimes key.\nSo in this post I have two datacenters, with vSphere clusters each being managed by their own vCenter Server. NSX is installed in both datacenters managing the network in their respective datacenter. NSX Advanced Loadbalancer (Avi) is deployed in datacenter 1 and is managing both datacenters (this post will not cover GSLB and having Avi controllers deployed in both datacenters and why this also could be a smart thing to consider). This Avi installation will be responsible for creating SE\u0026rsquo;s (Service Engines) and virtual services for the datacenter it is deployed in as well as the remote datacenter (datacenter 2). This will be the only controller/manager that is being shared across the two datacenters.\nSo trying to illustrate the above with the following diagram:\nI will not go into how connectivity between these sites are established, I just assume that relevant connectivity between the sites are in place (as that is a requirement for this to work).\nMeet the \u0026ldquo;moving parts\u0026rdquo; involved # This section will quickly go through the components being used in their different datacenters. First out is the components used in the datacenter 1 environment.\nDatacenter 1 # In datacenter 1 I have one vSphere cluster consisting of four ESXi hosts being managed by a vCenter server. This datacenter will be my managment datacenter where I will deploy my TKG management cluster. It is placed in physical location 1 with its own physical network and storage (using vSAN).\nThis is vSphere cluster in datacenter 1:\nIn datacenter 1 I have also installed NSX-T to handle all the networking needs in this datacenter. There is no stretching of networks between the datacenters, the NSX environment is only responsible for the same datacenter it is installed in as you can see below:\nIt also has a 1:1 relationship to the vCenter Server in datacenter 1:\nThis NSX environment has created the following networks to support my TKG managment cluster deployment:\nAnd from the vCenter:\nI will quickly just describe the different networks:\nls-avi-dns-se-data: This is where I place the dataplane of my SE\u0026rsquo;s used for the DNS service in Avi ls-avi-se-data: This is where I place the dataplane of my SE\u0026rsquo;s used for my other virtual services (regular application needs, or if I happen to deploy services in my TKG mgmt cluster or a workload cluster in the same datacenter.) This network will not be used in this post. ls-mgmt: is where I place the management interface of my SE\u0026rsquo;s. ls-tkg-mgmt: This will be used in this post and is where my TKG management cluster nodes will be placed. The ls-tkg-mgmt network has also been configured with DHCP on the segment in NSX:\nAnd last but not least component, the Avi controller.\nThis is the component in this post that has been configured to handle requests from both datacenters as a shared resource, whether it is regular layer-4 services like servicetype loadBalancer or layer-7 services like Ingress. As both datacenters are being managed by their own NSX-T I have configured the Avi controller to use both NSX-T environments as two different clouds:\nEach cloud depicted above is reflecting the two different datacenters and have been configured accordingly to support the network settings in each datacenter respectively.\nEach cloud is a NSX-T cloud and have its own unique configurations that matches the configuration for the respective datacenter the cloud is in. Networks, IPAM/DNS profiles, routing contexts, service engine groups. Below is some screenshots from the Avi controller:\nService engine groups in the stc-nsx-cloud:\nThe above SE groups have been configured for placement in their respective vSphere clusters, folder, naming, datastore etc.\nThe above networks have been configured to provision IP addresses using Avi IPAM to automate SE\u0026rsquo;s dataplane creation.\nThe below networks are the vip networks configured in the stc-nsx-cloud:\nThen the routing context (or VRF context) for the SE\u0026rsquo;s to reach the backend\nThe same has been done for the wdc-nsx-cloud. I will not print them here, but just show that there\u0026rsquo;s is also a wdc-cloud configured in these sections also:\nNotice the difference in IP subnets.\nThen its the IPAM DNS profiles for both clouds:\nInstead of going into too many details how to configure Avi, its all about to configure it to support the infrastructure settings in each datacenter. Then when the requests for virtual services come to the Avi controller it knows how to handle the requests and create the virtual services, the service engines and do the ip addressing correctly. Then this part will just work butter smooth.\nAn overview of the components in datacenter 1: Datacenter 2 # In datacenter 2 I have also one vSphere cluster consisting of four ESXi hosts being managed by a vCenter server. This datacenter will be my remote/edge datacenter where I will deploy my TKG workload clusters. It is placed in physical location 2 with its own physical network and storage (using vSAN).\nThis is vSphere cluster in datacenter 2:\nIn datacenter 2 I have also installed NSX-T to handle all the networking needs in this datacenter. As mentioned above, there is no stretching of networks between the datacenters, the NSX environments is only responsible for the same datacenter it is installed in as you can see below:\nIt also has a 1:1 relationship to the vCenter Server in datacenter 1:\nThis NSX environment has created the following networks to support my TKG managment cluster deployment:\nAnd from the vCenter:\nI will quickly just describe the different networks:\nls-avi-dns-se-data: This is where I place the dataplane of my SE\u0026rsquo;s used for the DNS service in Avi ls-avi-generic-se-data: This is where I place the dataplane of my SE\u0026rsquo;s used for the virtual services created when I expose services from the workload clusters. This network will be used in this post. ls-mgmt: is where I place the management interface of my SE\u0026rsquo;s. ls-tkg-wdc-wlc-1: This will be used as placement for my TKG workload cluster nodes in this datacenter. The ls-tkg-wdc-wlc-1 network has also been configured with DHCP on the segment in NSX:\nAn overview again of the components in DC2:\nThats it for the \u0026ldquo;moving parts\u0026rdquo; involved in both datacenters for this practice.\nTKG management cluster deployment # Now finally for the fun parts. Deployment. As I have mentioned in the previous chapters, I will deploy the TKG management cluster in datacenter 1. But before I do the actual deployment I will need to explain a little around how a TKG cluster is reached, whether its the management cluster or the workload clusters.\nKubernetes API endpoint - exposing services inside the kubernetes clusters (tkg clusters) # A Kubernetes cluster consist usually of 1 or 3 controlplane nodes. This is where the Kubernetes API endpoint lives. When interacting with Kubernetes we are using the exposed Kubernetes APIs to tell it declaratively (some say in a nice way) to realise something we want it to do. This api endpoint is usually exposed on port 6443, and will always be available on the control plane nodes, not on the worker nodes. So the first criteria to be met is connectivity to the control plane nodes on port 6443 (or ssh into the controlplane nodes themselves on port 22 and work with the kube-api from there, but not ideal). We want to reach the api from a remote workstation to be more flexible and effective in how we interact with the Kubernetes API. When having just 1 controlplane node it is probably just ok to reach this one controlplane node and send our api calls directly but with just one controlplane node this can create some issues down the road when we want to replace/upgrade this one node, it can change (most likely will) IP address. Meaning our kubeconfig context/automation tool needs to be updated accordingly. So what we want is a virtual ip address that will stay consistent across the lifetime of the Kubernetes cluster. The same is also when we have more than one controlplane node, 3 is a common number of controlplane nodes in production. We cant have an even number of controlplane nodes as we want quorum. We want to have 1 consistent IP address to reach either just the one controlplane node\u0026rsquo;s Kubernetes API or 1 consistent IP address loadbalanced across all three controlplane nodes. To achieve that we need some kind of loadbalancer that can create this virtual ip address for us to expose the Kubernetes API consistently. In TKG we can use NSX Advanced Loabalancer for this purpose, or a simpler approach like Kube-VIP. I dont want to go into a big writeup on the difference between these two other than they are not comparable to each other. Kube-VIP will not loadbalance the Kubernetes API between the 3 control-plane nodes, it will just create a virtual ip in the same subnet as the controlplane nodes and be placed on one of the controlplane nodes, stay there until the node fails and move over to the other control-plane nodes. While NSX ALB will loadbalance the Kuberntes API endpoint between all three control-plane nodes and the IP address is automatically allocated on provisioning. Kube-VIP is statically assigned.\nWhy I am mentioning this? Why could I not just focus on NSX Advanced Loadbalancer that can cover all my needs? That is because in this specific post I am hitting a special use-case where I have my TKG management cluster placed in one datacenter managed by its own NSX-T, while I want to deploy and manage TKG workload clusters in a completely different datacenter also managed by its own NSX-T. By using NSX Advanced Loabalancer as my API endpoint VIP provider in combination with NSX-T Clouds (Avi Clouds) I am currently not allowed to override the control-plane network (API endpoint). It is currently not possible to override or select a different NSX-T Tier1 for the the control-plane network, as these are different due to two different NSX-T environments, I can name the Tier-1 routers identically in both datacenters, but its not so easily fooled \u0026#x1f604; So my option to work around this is to use Kube-VIP. Kube-VIP allows me to configure manually the API endpoint IP for my workload clusters. I will try to explain a bit more how the NSX ALB integration works in TKG below.\nWhat about the services I want to expose from the different workload-clusters like servicetype loadBalancer and Ingress? That is a different story, there we can use NSX Advanced Loadbalancer as much as we want and in a very flexible way too. The reason for that is that the Kubernetes API endpoint VIP or controlplane network is something that is managed and controlled by the TKG management cluster while whats coming from the inside of a working TKG workload cluster is completely different. Using NSX Advanced Loadbalancer in TKG or in any other Kubernetes platform like native upstream Kubernetes we use a component called AKO (Avi Kubernetes Operator) that handles all the standard Kubernetes requests like servicetype loadBalancer and Ingress creation and forwards them to the NSX ALB controller to realize them. In TKG we have AKO running in the management cluster that is responsible for the services being exposed from inside the TKG management cluster, but also assigning the VIP for the workload clusters Kubernetes API (controlplane network). As soon as we have our first TKG workload cluster, this comes with its own AKO that is responsible for all the services in the workload cluster it runs in, it will not have anything to do with the controlplane network and the AKO running in the TKG management cluster. So we can actually adjust this AKO instance to match our needs there without being restricted to what the AKO instance in the TKG management cluster is configured with.\nIn a TKG workload cluster there is a couple of ways to get AKO installed. One option is to use the AKO Operator running in the TKG management cluster to deploy it automatically on TKG workload cluster provisioning. This approach is best if you want TKG to handle the lifecycle of the AKO instance, like upgrades and it is very hands-off. We need to define an AkoDeploymentConfig in the TKG management cluster that defines the AKO settings for the respective TKG workload cluster or clusters if they can share the same settings. This is based on labels so its very easy to create the ADC for a series of clusters or specific cluster by applying the correct label on the cluster. The other option is to install AKO via Helm, this gives you full flexibility but is a manual process that needs to be done on all TKG workload clusters that needs AKO installed. I tend to lean on the ADC approach as I cant see any limitation this approach has compared to the AKO via Helm approach. ADC also supports AviInfraSettings which gives you further flexibility and options.\nWith that out of the way let us get this TKG management cluster deployed already\u0026hellip;\nTKG management cluster deployment - continued # I will not cover any of the pre-reqs to deploy TKG, for that have a look here, I will just go straight to it. My TKG managment cluster bootstrap yaml manifest. Below I will paste my yaml for the TKG mgmt cluster with some comments that I have done to make use of Kube-VIP for the controlplane, aka Kubernetes API endpoint.\n#! --------------- #! Basic config #! ------------- CLUSTER_NAME: tkg-stc-mgmt-cluster CLUSTER_PLAN: dev INFRASTRUCTURE_PROVIDER: vsphere ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; CLUSTER_CIDR: 100.96.0.0/11 SERVICE_CIDR: 100.64.0.0/13 TKG_IP_FAMILY: ipv4 DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; CLUSTER_API_SERVER_PORT: 6443 #Added for Kube-VIP VSPHERE_CONTROL_PLANE_ENDPOINT: 10.13.20.100 #Added for Kube-VIP - specify a static IP in same subnet as nodes VSPHERE_CONTROL_PLANE_ENDPOINT_PORT: 6443 #Added for Kube-VIP VIP_NETWORK_INTERFACE: \u0026#34;eth0\u0026#34; #Added for Kube-VIP # VSPHERE_ADDITIONAL_FQDN: AVI_CONTROL_PLANE_HA_PROVIDER: false #Set to false to use Kube-VIP instead AVI_ENABLE: \u0026#34;true\u0026#34; #I still want AKO to be installed, but not used for controplane endpoint #! --------------- #! vSphere config #! ------------- VSPHERE_DATACENTER: /cPod-NSXAM-STC VSPHERE_DATASTORE: /cPod-NSXAM-STC/datastore/vsanDatastore VSPHERE_FOLDER: /cPod-NSXAM-STC/vm/TKGm VSPHERE_INSECURE: \u0026#34;false\u0026#34; VSPHERE_NETWORK: /cPod-NSXAM-STC/network/ls-tkg-mgmt VSPHERE_PASSWORD: \u0026#34;password\u0026#34; VSPHERE_RESOURCE_POOL: /cPod-NSXAM-STC/host/Cluster/Resources #VSPHERE_TEMPLATE: /Datacenter/vm/TKGm/ubuntu-2004-kube-v1.23.8+vmware.2 VSPHERE_SERVER: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa ssh-public key VSPHERE_TLS_THUMBPRINT: vcenter SHA1 VSPHERE_USERNAME: username@domain.net #! --------------- #! Node config #! ------------- OS_ARCH: amd64 OS_NAME: ubuntu OS_VERSION: \u0026#34;20.04\u0026#34; VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; CONTROL_PLANE_MACHINE_COUNT: 1 WORKER_MACHINE_COUNT: 2 #! --------------- #! Avi config #! ------------- AVI_CA_DATA_B64: AVI Controller Base64 Certificate AVI_CLOUD_NAME: stc-nsx-cloud AVI_CONTROLLER: 172.24.3.50 # Network used to place workload clusters\u0026#39; endpoint VIPs #AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 #AVI_CONTROL_PLANE_NETWORK_CIDR: 10.13.102.0/24 # Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) AVI_DATA_NETWORK: vip-tkg-wld-l7 AVI_DATA_NETWORK_CIDR: 10.13.103.0/24 # Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.13.101.0/24 AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 # Network used to place management clusters\u0026#39; endpoint VIPs #AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 #AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.13.100.0/24 AVI_NSXT_T1LR: Tier-1 AVI_CONTROLLER_VERSION: 22.1.2 AVI_LABELS: \u0026#34;{adc-enabled: \u0026#39;true\u0026#39;}\u0026#34; #Added so I can select easily which workload cluster that will use this AKO config AVI_PASSWORD: \u0026#34;password\u0026#34; AVI_SERVICE_ENGINE_GROUP: stc-nsx AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: tkgm-se-group AVI_USERNAME: admin AVI_DISABLE_STATIC_ROUTE_SYNC: false AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true AVI_INGRESS_SHARD_VS_SIZE: SMALL AVI_INGRESS_SERVICE_TYPE: NodePortLocal #! --------------- #! Proxy config #! ------------- TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; #! --------------------------------------------------------------------- #! Antrea CNI configuration #! --------------------------------------------------------------------- # ANTREA_NO_SNAT: false # ANTREA_TRAFFIC_ENCAP_MODE: \u0026#34;encap\u0026#34; # ANTREA_PROXY: false # ANTREA_POLICY: true # ANTREA_TRACEFLOW: false ANTREA_NODEPORTLOCAL: true ANTREA_PROXY: true ANTREA_ENDPOINTSLICE: true ANTREA_POLICY: true ANTREA_TRACEFLOW: true ANTREA_NETWORKPOLICY_STATS: false ANTREA_EGRESS: true ANTREA_IPAM: false ANTREA_FLOWEXPORTER: false ANTREA_SERVICE_EXTERNALIP: false ANTREA_MULTICAST: false #! --------------------------------------------------------------------- #! Machine Health Check configuration #! --------------------------------------------------------------------- ENABLE_MHC: \u0026#34;true\u0026#34; ENABLE_MHC_CONTROL_PLANE: true ENABLE_MHC_WORKER_NODE: true MHC_UNKNOWN_STATUS_TIMEOUT: 5m MHC_FALSE_STATUS_TIMEOUT: 12m #! --------------------------------------------------------------------- #! Identity management configuration #! --------------------------------------------------------------------- IDENTITY_MANAGEMENT_TYPE: none All the configs above should match the datacenter 1 environment so the TKG management cluster can be deployed. Lets deploy it using Tanzu CLI from my TKG bootstrap client:\ntanzu mc create -f tkg-mgmt-bootstrap.yaml As soon as it is deployed grab the k8s config and add it to your context:\ntanzu mc kubeconfig get --admin --export-file stc-tkgm-mgmt-cluster.yaml The IP address used for the Kubernetes API endpoint is the controlplane IP defined above:\nVSPHERE_CONTROL_PLANE_ENDPOINT: 10.13.20.100 We can also see this IP being assigned to my one controlplane node in the vCenter view:\nNow just have a quick look inside the TKG mgmt cluster and specifically after AKO and eventual ADC:\ntkg-bootstrap-vm:~/Kubernetes-library/examples/ingress$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE avi-system ako-0 1/1 Running 0 8h capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-5fb8fbc6c7-rqkzf 1/1 Running 0 8h capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-78c559f48c-cj2dm 1/1 Running 0 8h capi-system capi-controller-manager-84fbb669c-bhk4j 1/1 Running 0 8h capv-system capv-controller-manager-5f46567b86-pccf5 1/1 Running 0 8h cert-manager cert-manager-5d8d7b4dfb-gj6h2 1/1 Running 0 9h cert-manager cert-manager-cainjector-7797ff666f-zxh5l 1/1 Running 0 9h cert-manager cert-manager-webhook-59969cbb8c-vpsgr 1/1 Running 0 9h kube-system antrea-agent-6xzvh 2/2 Running 0 8h kube-system antrea-agent-gsfhc 2/2 Running 0 8h kube-system antrea-agent-t5gzb 2/2 Running 0 8h kube-system antrea-controller-74b468c659-hcrgp 1/1 Running 0 8h kube-system coredns-5d4666ccfb-qx5qt 1/1 Running 0 9h kube-system coredns-5d4666ccfb-xj47b 1/1 Running 0 9h kube-system etcd-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h kube-system kube-apiserver-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h kube-system kube-controller-manager-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h kube-system kube-proxy-9d7b9 1/1 Running 0 9h kube-system kube-proxy-kd8h8 1/1 Running 0 9h kube-system kube-proxy-n7zwx 1/1 Running 0 9h kube-system kube-scheduler-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h kube-system kube-vip-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h kube-system metrics-server-b468f4d5f-hvtbg 1/1 Running 0 8h kube-system vsphere-cloud-controller-manager-fnsvh 1/1 Running 0 8h secretgen-controller secretgen-controller-697cb6c657-lh9rr 1/1 Running 0 8h tanzu-auth tanzu-auth-controller-manager-d75d85899-d8699 1/1 Running 0 8h tkg-system-networking ako-operator-controller-manager-5bbb9d4c4b-2bjsk 1/1 Running 0 8h tkg-system kapp-controller-9f9f578c7-dpzgk 2/2 Running 0 9h tkg-system object-propagation-controller-manager-5cbb94894f-k56w5 1/1 Running 0 8h tkg-system tanzu-addons-controller-manager-79f656b4c7-m72xw 1/1 Running 0 8h tkg-system tanzu-capabilities-controller-manager-5868c5f789-nbkgm 1/1 Running 0 8h tkg-system tanzu-featuregates-controller-manager-6d567fffd6-647s5 1/1 Running 0 8h tkg-system tkr-conversion-webhook-manager-6977bfc965-gjjbt 1/1 Running 0 8h tkg-system tkr-resolver-cluster-webhook-manager-5c8484ffd8-8xc8n 1/1 Running 0 8h tkg-system tkr-source-controller-manager-57c56d55d9-x6vsz 1/1 Running 0 8h tkg-system tkr-status-controller-manager-55b4b845b9-77snb 1/1 Running 0 8h tkg-system tkr-vsphere-resolver-webhook-manager-6476749d5d-5pxlk 1/1 Running 0 8h vmware-system-csi vsphere-csi-controller-585bf4dc75-wtlw2 7/7 Running 0 8h vmware-system-csi vsphere-csi-node-ldrs6 3/3 Running 2 (8h ago) 8h vmware-system-csi vsphere-csi-node-rwgpw 3/3 Running 4 (8h ago) 8h vmware-system-csi vsphere-csi-node-rx8f6 3/3 Running 4 (8h ago) 8h There is an AKO pod running. Are there any ADCs created?\ntkg-bootstrap-vm:~/Kubernetes-library/examples/ingress$ k get adc NAME AGE install-ako-for-all 8h install-ako-for-management-cluster 8h Lets have a look inside both of them, first out install-aka-for-all and then install-ako-for-management-cluster\n# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: annotations: kapp.k14s.io/identity: v1;/networking.tkg.tanzu.vmware.com/AKODeploymentConfig/install-ako-for-all;networking.tkg.tanzu.vmware.com/v1alpha1 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.tkg.tanzu.vmware.com/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;AKODeploymentConfig\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685446688101132090\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.8329c3602ed02133e324fc22d58dcf28\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;install-ako-for-all\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;adminCredentialRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-credentials\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;certificateAuthorityRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-ca\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterSelector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;adc-enabled\u0026#34;:\u0026#34;true\u0026#34;}},\u0026#34;controlPlaneNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.101.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-mgmt-l7\u0026#34;},\u0026#34;controller\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;dataNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.103.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-wld-l7\u0026#34;},\u0026#34;extraConfigs\u0026#34;:{\u0026#34;disableStaticRouteSync\u0026#34;:false,\u0026#34;ingress\u0026#34;:{\u0026#34;defaultIngressController\u0026#34;:false,\u0026#34;disableIngressClass\u0026#34;:true,\u0026#34;nodeNetworkList\u0026#34;:[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-mgmt\u0026#34;}]},\u0026#34;networksConfig\u0026#34;:{\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;Tier-1\u0026#34;}},\u0026#34;serviceEngineGroup\u0026#34;:\u0026#34;stc-nsx\u0026#34;}}\u0026#39; kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 creationTimestamp: \u0026#34;2023-05-30T11:38:45Z\u0026#34; finalizers: - ako-operator.networking.tkg.tanzu.vmware.com generation: 2 labels: kapp.k14s.io/app: \u0026#34;1685446688101132090\u0026#34; kapp.k14s.io/association: v1.8329c3602ed02133e324fc22d58dcf28 name: install-ako-for-all resourceVersion: \u0026#34;4686\u0026#34; uid: 0cf0dd57-b193-40d5-bb03-347879157377 spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: stc-nsx-cloud clusterSelector: matchLabels: adc-enabled: \u0026#34;true\u0026#34; controlPlaneNetwork: cidr: 10.13.101.0/24 name: vip-tkg-mgmt-l7 controller: 172.24.3.50 controllerVersion: 22.1.3 dataNetwork: cidr: 10.13.103.0/24 name: vip-tkg-wld-l7 extraConfigs: disableStaticRouteSync: false ingress: defaultIngressController: false disableIngressClass: true nodeNetworkList: - networkName: ls-tkg-mgmt networksConfig: nsxtT1LR: Tier-1 serviceEngineGroup: stc-nsx This is clearly configured for my datacenter 1, and will not match my datacenter 2 environment. Also notice the label, if I do create cluster and apply this label I will get the \u0026ldquo;default\u0026rdquo; ADC applied which will not match what I have to use in datacenter 2.\nLets have a look at the last one:\n# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: annotations: kapp.k14s.io/identity: v1;/networking.tkg.tanzu.vmware.com/AKODeploymentConfig/install-ako-for-management-cluster;networking.tkg.tanzu.vmware.com/v1alpha1 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.tkg.tanzu.vmware.com/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;AKODeploymentConfig\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685446688101132090\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.3012c3c8e0fa37b13f4916c7baca1863\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;install-ako-for-management-cluster\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;adminCredentialRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-credentials\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;certificateAuthorityRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-ca\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterSelector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;cluster-role.tkg.tanzu.vmware.com/management\u0026#34;:\u0026#34;\u0026#34;}},\u0026#34;controlPlaneNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.101.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-mgmt-l7\u0026#34;},\u0026#34;controller\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;dataNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.101.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-mgmt-l7\u0026#34;},\u0026#34;extraConfigs\u0026#34;:{\u0026#34;disableStaticRouteSync\u0026#34;:false,\u0026#34;ingress\u0026#34;:{\u0026#34;defaultIngressController\u0026#34;:false,\u0026#34;disableIngressClass\u0026#34;:true,\u0026#34;nodeNetworkList\u0026#34;:[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-mgmt\u0026#34;}]},\u0026#34;networksConfig\u0026#34;:{\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;Tier-1\u0026#34;}},\u0026#34;serviceEngineGroup\u0026#34;:\u0026#34;tkgm-se-group\u0026#34;}}\u0026#39; kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 creationTimestamp: \u0026#34;2023-05-30T11:38:45Z\u0026#34; finalizers: - ako-operator.networking.tkg.tanzu.vmware.com generation: 2 labels: kapp.k14s.io/app: \u0026#34;1685446688101132090\u0026#34; kapp.k14s.io/association: v1.3012c3c8e0fa37b13f4916c7baca1863 name: install-ako-for-management-cluster resourceVersion: \u0026#34;4670\u0026#34; uid: c41e6e39-2b0f-4fa4-9245-0eec1bcf6b5d spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: stc-nsx-cloud clusterSelector: matchLabels: cluster-role.tkg.tanzu.vmware.com/management: \u0026#34;\u0026#34; controlPlaneNetwork: cidr: 10.13.101.0/24 name: vip-tkg-mgmt-l7 controller: 172.24.3.50 controllerVersion: 22.1.3 dataNetwork: cidr: 10.13.101.0/24 name: vip-tkg-mgmt-l7 extraConfigs: disableStaticRouteSync: false ingress: defaultIngressController: false disableIngressClass: true nodeNetworkList: - networkName: ls-tkg-mgmt networksConfig: nsxtT1LR: Tier-1 serviceEngineGroup: tkgm-se-group The same is true for this one. Configured for my datacenter 1, only major difference being a different dataNetwork. So if I decide to deploy a workload cluster in the same datacenter it would be fine, but I dont want that I want my workload cluster to be in a different datacenter.. Lets do that..\nTKG workload cluster deployment - with corresponding ADC # Before I deploy my workload cluster I will create a \u0026ldquo;custom\u0026rdquo; ADC specific for the datacenter 2 where I will deploy the workload cluster. Lets paste my ADC for the dc 2 workload cluster:\napiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: name: ako-tkg-wdc-cloud spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: wdc-nsx-cloud clusterSelector: matchLabels: avi-cloud: \u0026#34;wdc-nsx-cloud\u0026#34; controller: 172.24.3.50 dataNetwork: cidr: 10.101.221.0/24 name: tkg-wld-1-apps extraConfigs: cniPlugin: antrea disableStaticRouteSync: false # required ingress: defaultIngressController: true disableIngressClass: false # required nodeNetworkList: # required - cidrs: - 10.101.13.0/24 networkName: ls-tkg-wdc-wld-1 serviceType: NodePortLocal # required shardVSSize: SMALL # required l4Config: autoFQDN: default networksConfig: nsxtT1LR: /infra/tier-1s/Tier-1 serviceEngineGroup: wdc-se-group In this ADC I will configure the dataNetwork for the VIP network I have defined in the NSX ALB DC 2 cloud, pointing to the NSX-T Tier1 (yes they have the same name as in DC1, but they are not the same), nodeNetworkList matching where my workload cluster nodes will be placed in DC 2. And also notice the label, for my workload cluster to use this ADC I will need to apply this label either during provisioning or label it after creation. Apply the ADC:\nk apply -f ako-wld-cluster-1.wdc.cloud.yaml Is it there:\namarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm$ k get adc NAME AGE ako-tkg-wdc-cloud 20s install-ako-for-all 9h install-ako-for-management-cluster 9h Yes it is.\nNow prepare the TKG workload cluster manifest to match the DC 2 environment and apply it, also making sure aviAPIServerHAProvider is set to false.\napiVersion: cpi.tanzu.vmware.com/v1alpha1 kind: VSphereCPIConfig metadata: name: wdc-tkgm-wld-cluster-1 namespace: ns-wdc-1 spec: vsphereCPI: ipFamily: ipv4 mode: vsphereCPI tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 vmNetwork: excludeExternalSubnetCidr: 10.101.13.100/32 excludeInternalSubnetCidr: 10.101.13.100/32 --- apiVersion: csi.tanzu.vmware.com/v1alpha1 kind: VSphereCSIConfig metadata: name: wdc-tkgm-wld-cluster-1 namespace: ns-wdc-1 spec: vsphereCSI: config: datacenter: /cPod-NSXAM-WDC httpProxy: \u0026#34;\u0026#34; httpsProxy: \u0026#34;\u0026#34; noProxy: \u0026#34;\u0026#34; region: null tlsThumbprint: vCenter SHA-1 of DC2 useTopologyCategories: false zone: null mode: vsphereCSI --- apiVersion: run.tanzu.vmware.com/v1alpha3 kind: ClusterBootstrap metadata: annotations: tkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.25.7---vmware.2-tkg.1 name: wdc-tkgm-wld-cluster-1 namespace: ns-wdc-1 spec: additionalPackages: - refName: metrics-server* - refName: secretgen-controller* - refName: pinniped* cpi: refName: vsphere-cpi* valuesFrom: providerRef: apiGroup: cpi.tanzu.vmware.com kind: VSphereCPIConfig name: wdc-tkgm-wld-cluster-1 csi: refName: vsphere-csi* valuesFrom: providerRef: apiGroup: csi.tanzu.vmware.com kind: VSphereCSIConfig name: wdc-tkgm-wld-cluster-1 kapp: refName: kapp-controller* --- apiVersion: v1 kind: Secret metadata: name: wdc-tkgm-wld-cluster-1 namespace: ns-wdc-1 stringData: password: password vCenter User username: user@vcenter.net --- apiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: annotations: osInfo: ubuntu,20.04,amd64 tkg.tanzu.vmware.com/cluster-controlplane-endpoint: 10.101.13.100 #here is the VIP for the workload k8s API - by Kube-VIP tkg/plan: dev labels: tkg.tanzu.vmware.com/cluster-name: wdc-tkgm-wld-cluster-1 avi-cloud: \u0026#34;wdc-nsx-cloud\u0026#34; name: wdc-tkgm-wld-cluster-1 namespace: ns-wdc-1 spec: clusterNetwork: pods: cidrBlocks: - 20.10.0.0/16 services: cidrBlocks: - 20.20.0.0/16 topology: class: tkg-vsphere-default-v1.0.0 controlPlane: metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu replicas: 1 variables: - name: cni value: antrea - name: controlPlaneCertificateRotation value: activate: true daysBefore: 90 - name: auditLogging value: enabled: false - name: apiServerPort value: 6443 - name: podSecurityStandard value: audit: baseline deactivated: false warn: baseline - name: apiServerEndpoint value: 10.101.13.100 #Here is the K8s API endpoint provided by Kube-VIP - name: aviAPIServerHAProvider value: false - name: vcenter value: cloneMode: fullClone datacenter: /cPod-NSXAM-WDC datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 folder: /cPod-NSXAM-WDC/vm/TKGm network: /cPod-NSXAM-WDC/network/ls-tkg-wdc-wld-1 resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources server: vcsa.cpod-nsxam-wdc.az-wdc.cloud-garage.net storagePolicyID: \u0026#34;\u0026#34; template: /cPod-NSXAM-WDC/vm/ubuntu-2004-efi-kube-v1.25.7+vmware.2 tlsThumbprint: vCenter SHA1 DC2 - name: user value: sshAuthorizedKeys: - ssh-rsa public key - name: controlPlane value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: worker value: count: 2 machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 version: v1.25.7+vmware.2-tkg.1 workers: machineDeployments: - class: tkg-worker metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 replicas: 2 Notice the label, it should match what you defined in the ADC.. avi-cloud: \u0026ldquo;wdc-nsx-cloud\u0026rdquo;\nNow apply the cluster.\ntanzu cluster create -f wdc-tkg-wld-cluster-1.yaml After a cup of coffee it should be ready.\nLets check the cluster from the mgmt cluster context:\nlinux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get cluster -n ns-wdc-1 wdc-tkgm-wld-cluster-1 NAME PHASE AGE VERSION wdc-tkgm-wld-cluster-1 Provisioned 13m v1.25.7+vmware.2 Grab the k8s config and switch to the workload cluster context.\ntanzu cluster kubeconfig get wdc-tkgm-wld-cluster-1 --namespace ns-wdc-1 --admin --export-file wdc-tkgm-wld-cluster-1-k8s-config.yaml Check the nodes:\nlinux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME wdc-tkgm-wld-cluster-1-md-0-jkcjw-d7795fbb5-gxnpc Ready \u0026lt;none\u0026gt; 9h v1.25.7+vmware.2 10.101.13.26 10.101.13.26 Ubuntu 20.04.6 LTS 5.4.0-144-generic containerd://1.6.18-1-gdbc99e5b1 wdc-tkgm-wld-cluster-1-md-0-jkcjw-d7795fbb5-ph5j4 Ready \u0026lt;none\u0026gt; 9h v1.25.7+vmware.2 10.101.13.42 10.101.13.42 Ubuntu 20.04.6 LTS 5.4.0-144-generic containerd://1.6.18-1-gdbc99e5b1 wdc-tkgm-wld-cluster-1-s49w2-mz85r Ready control-plane 9h v1.25.7+vmware.2 10.101.13.41 10.101.13.41 Ubuntu 20.04.6 LTS 5.4.0-144-generic containerd://1.6.18-1-gdbc99e5b1 In my vCenter in DC 2?\nLets see them running pods:\nlinux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE avi-system ako-0 1/1 Running 0 8h kube-system antrea-agent-rhmsn 2/2 Running 0 8h kube-system antrea-agent-ttk6f 2/2 Running 0 8h kube-system antrea-agent-tw6t7 2/2 Running 0 8h kube-system antrea-controller-787994578b-7v2cl 1/1 Running 0 8h kube-system coredns-5d4666ccfb-b2j85 1/1 Running 0 8h kube-system coredns-5d4666ccfb-lr97g 1/1 Running 0 8h kube-system etcd-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h kube-system kube-apiserver-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h kube-system kube-controller-manager-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h kube-system kube-proxy-9m2zh 1/1 Running 0 8h kube-system kube-proxy-rntv8 1/1 Running 0 8h kube-system kube-proxy-t7z49 1/1 Running 0 8h kube-system kube-scheduler-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h kube-system kube-vip-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h kube-system metrics-server-c6d9969cb-7h5l7 1/1 Running 0 8h kube-system vsphere-cloud-controller-manager-b2rkl 1/1 Running 0 8h secretgen-controller secretgen-controller-cd678b84c-cdntv 1/1 Running 0 8h tkg-system kapp-controller-6c5dfccc45-7nhl5 2/2 Running 0 8h tkg-system tanzu-capabilities-controller-manager-5bf587dcd5-fp6t9 1/1 Running 0 8h vmware-system-csi vsphere-csi-controller-5459886d8c-5jzlz 7/7 Running 0 8h vmware-system-csi vsphere-csi-node-6pfbj 3/3 Running 4 (8h ago) 8h vmware-system-csi vsphere-csi-node-cbcpm 3/3 Running 4 (8h ago) 8h vmware-system-csi vsphere-csi-node-knk8q 3/3 Running 2 (8h ago) 8h There is an AKO pod running. So far so good. How does the AKO configmap look like?\n# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, # and an empty file will abort the edit. If an error occurs while saving this file will be # reopened with the relevant failures. # apiVersion: v1 data: apiServerPort: \u0026#34;8080\u0026#34; autoFQDN: default cloudName: wdc-nsx-cloud clusterName: ns-wdc-1-wdc-tkgm-wld-cluster-1 cniPlugin: antrea controllerIP: 172.24.3.50 controllerVersion: 22.1.3 defaultIngController: \u0026#34;true\u0026#34; deleteConfig: \u0026#34;false\u0026#34; disableStaticRouteSync: \u0026#34;false\u0026#34; fullSyncFrequency: \u0026#34;1800\u0026#34; logLevel: INFO nodeNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-wdc-wld-1\u0026#34;,\u0026#34;cidrs\u0026#34;:[\u0026#34;10.101.13.0/24\u0026#34;]}]\u0026#39; nsxtT1LR: /infra/tier-1s/Tier-1 serviceEngineGroupName: wdc-se-group serviceType: NodePortLocal shardVSSize: SMALL useDefaultSecretsOnly: \u0026#34;false\u0026#34; vipNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;tkg-wld-1-apps\u0026#34;,\u0026#34;cidr\u0026#34;:\u0026#34;10.101.221.0/24\u0026#34;}]\u0026#39; kind: ConfigMap metadata: annotations: kapp.k14s.io/identity: v1;avi-system//ConfigMap/avi-k8s-config;v1 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;apiServerPort\u0026#34;:\u0026#34;8080\u0026#34;,\u0026#34;autoFQDN\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;cloudName\u0026#34;:\u0026#34;wdc-nsx-cloud\u0026#34;,\u0026#34;clusterName\u0026#34;:\u0026#34;ns-wdc-1-wdc-tkgm-wld-cluster-1\u0026#34;,\u0026#34;cniPlugin\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;controllerIP\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;controllerVersion\u0026#34;:\u0026#34;22.1.3\u0026#34;,\u0026#34;defaultIngController\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;deleteConfig\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;disableStaticRouteSync\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;fullSyncFrequency\u0026#34;:\u0026#34;1800\u0026#34;,\u0026#34;logLevel\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;nodeNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;ls-tkg-wdc-wld-1\\\u0026#34;,\\\u0026#34;cidrs\\\u0026#34;:[\\\u0026#34;10.101.13.0/24\\\u0026#34;]}]\u0026#34;,\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;/infra/tier-1s/Tier-1\u0026#34;,\u0026#34;serviceEngineGroupName\u0026#34;:\u0026#34;wdc-se-group\u0026#34;,\u0026#34;serviceType\u0026#34;:\u0026#34;NodePortLocal\u0026#34;,\u0026#34;shardVSSize\u0026#34;:\u0026#34;SMALL\u0026#34;,\u0026#34;useDefaultSecretsOnly\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;vipNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;tkg-wld-1-apps\\\u0026#34;,\\\u0026#34;cidr\\\u0026#34;:\\\u0026#34;10.101.221.0/24\\\u0026#34;}]\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685448627039212099\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.ae838cced3b6caccc5a03bfb3ae65cd7\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;avi-k8s-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;avi-system\u0026#34;}}\u0026#39; kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 creationTimestamp: \u0026#34;2023-05-30T12:10:34Z\u0026#34; labels: kapp.k14s.io/app: \u0026#34;1685448627039212099\u0026#34; kapp.k14s.io/association: v1.ae838cced3b6caccc5a03bfb3ae65cd7 name: avi-k8s-config namespace: avi-system resourceVersion: \u0026#34;2456\u0026#34; uid: 81bbe809-f5a1-45b8-aef2-a83ff36a3dd1 That looks good, the question now will it blend?\nSo far I dont have anything in my NSX ALB dashboard. What happens then if I create some servicetype loadBalancer or Ingresses? Lets have a look. First check, do I have an IngressClass?\nlinux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 10m I certainly do. I will now deploy an application with serviceType loadBalancer and an application using Ingress.\nHere they are:\nlinux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get svc -n yelb NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-server ClusterIP 20.20.103.179 \u0026lt;none\u0026gt; 6379/TCP 10s yelb-appserver ClusterIP 20.20.242.204 \u0026lt;none\u0026gt; 4567/TCP 10s yelb-db ClusterIP 20.20.202.153 \u0026lt;none\u0026gt; 5432/TCP 10s yelb-ui LoadBalancer 20.20.123.18 10.101.221.10 80:30119/TCP 10s linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get ingress -n fruit NAME CLASS HOSTS ADDRESS PORTS AGE ingress-example avi-lb fruit-tkg.you-have.your-domain.here 10.101.221.11 80 50s Looking at the Ingres I can also see NSX ALB has been so kind to register DNS record for it also.\nHow does it look inside my NSX ALB in DC1?\nAnd where are the above NSX ALB Service Engines deployed?\nIn my vCenter in DC-2.\nWell that was it. Thanks for reading.\n","date":"30 May 2023","externalUrl":null,"permalink":"/2023/05/30/tanzu-kubernetes-grid-2.2-remote-workload-clusters/","section":"Posts","summary":"In this post I will quickly go through how to use a Tanzu Kubernetes Grid management cluster to deploy and manage workload clusters in edge/remote locations.","title":"Tanzu Kubernetes Grid 2.2 \u0026 Remote Workload Clusters","type":"posts"},{"content":"","date":"30 May 2023","externalUrl":null,"permalink":"/tags/tkg/","section":"Tags","summary":"","title":"Tkg","type":"tags"},{"content":"This post will explore a bit further how we can utilize the three-zone setup configured here for TKG cluster and application placement. It will not be a very long post, but showing how it can be done and which values to use. In the \u0026ldquo;part 1\u0026rdquo; post I enabled a three-zone Supervisor deployment, meaning the three Supervisor Control Plane VMs will be distributed evenly across my three vSphere Clusters. To use a three-zone deployment we need to have three vSphere Zones defined. Each of these vSphere zones is described as a \u0026ldquo;Failure Domain\u0026rdquo; and becomes a value we can use when we decide the placement of both TKG clusters and applications inside our TKG clusters. So basically this post will describe how I can take advantage of this when I deploy my workload clusters and how can I decide where my applications will be placed.\nWorkload cluster placement # My Supervisor cluster is already deployed in my three vSphere Zones, just waiting for me to give it something to do. I have created a vSphere Namespace for my TKG cluster called ns-three-zone-1. I want to use a different workload network than my Supervisor workload network is placed on, that is a benefit when using NSX-T.\nTo give some context, this is how my environment is looking before deploying the TKG cluster:\nNow I just need to log into the supervisor, prepare my TKG yaml manifest and deploy my TKG cluster.\nLog in to Supervisor\nlinuxvm01:~/$ kubectl vsphere login --server=10.101.90.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below Password: Logged in successfully. You have access to the following contexts: 10.101.90.2 If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator. To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` linuxvm01:~/$ Now that I am logged in, I will check if my vSphere Zones are available by issuing the following command:\nlinuxvm01:~/three-zones$ k get vspherezones.topology.tanzu.vmware.com NAME AGE wdc-zone-1 19d wdc-zone-2 19d wdc-zone-3 19d linuxvm01:~/three-zones$ So it seems, now I need to use these names/labels in my TKG yaml manifest. In my first deployment I will use the example from the official VMware documentation here with some additions from my side like run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu I will be using the API v1beta1 with kubectl not Tanzu CLI.\nLet us have a look at it and edit it accordingly:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: three-zone-cluster-1 #My own name on the cluster namespace: ns-three-zone-1 #My vSphere Namespace spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] #Edited by me pods: cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] #Edited by me serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.24.9+vmware.1-tkg.4 #My latest available TKR version controlPlane: replicas: 1 # only one controlplane (saving resources and time) metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: #muliple node pools are used machineDeployments: - class: node-pool name: node-pool-1 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-1 #named after my vSphere zone - class: node-pool name: node-pool-2 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-2 #named after my vSphere zone - class: node-pool name: node-pool-3 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-3 #named after my vSphere zone variables: - name: vmClass value: best-effort-small - name: storageClass value: all-vsans #my zonal storageclass Lets apply and see what happens. What I am expecting is the worker nodes should be placed according to the plan above, 1 worker pr vSphere cluster. The control plane node will be placed random.\nlinuxvm01:~/three-zones$ k apply -f three-zone-cluster-1.yaml cluster.cluster.x-k8s.io/three-zone-cluster-1 created And the results are in:\nAll three worker nodes were placed in their respective vSphere Zones (vSphere clusters) as configured in the yaml. The control plane node was just randomly placed in vSphere zone 3.\nThats it for this task. Now I want to deploy nearly the same, but with 3 control plane nodes. Where will they be placed?\nHere is the cluster definition:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: three-zone-cluster-1 #My own name on the cluster namespace: ns-three-zone-1 #My vSphere Namespace spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] #Edited by me pods: cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] #Edited by me serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.24.9+vmware.1-tkg.4 #My latest available TKR version controlPlane: replicas: 3 # should be spread evenly across zones metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: #muliple node pools are used machineDeployments: - class: node-pool name: node-pool-1 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-1 #named after my vSphere zone - class: node-pool name: node-pool-2 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-2 #named after my vSphere zone - class: node-pool name: node-pool-3 replicas: 1 #only 1 worker here metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu #failure domain the machines will be created in #maps to a vSphere Zone; name must match exactly failureDomain: wdc-zone-3 #named after my vSphere zone variables: - name: vmClass value: best-effort-small - name: storageClass value: all-vsans #my zonal storageclass And in vCenter where is the control plane nodes placed:\nApplication placement # After your workload cluster has been deployed as specified above you also want to utilize the different vSphere Zones for application placement. Start by switching into the context of your workload cluster:\nlinuxvm01:~/three-zones$ kubectl vsphere login --server=10.101.90.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-namespace ns-three-zone-1 --tanzu-ku bernetes-cluster-name three-zone-cluster-1 linuxvm01:~/three-zones$ k config current-context three-zone-cluster-1 Now that I am in the correct context, lets check the nodes status, and specifically if there are any labels of interest or relevance to the vSphere Zones.\nlinuxvm01:~/three-zones$ k get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS three-zone-cluster-1-h9nxh-9xsvp Ready control-plane 4m25s v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-2,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-h9nxh-9xsvp,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-2 three-zone-cluster-1-h9nxh-bqqzd Ready control-plane 16m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-3,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-h9nxh-bqqzd,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-3 three-zone-cluster-1-h9nxh-kkvkz Ready control-plane 10m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-1,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-h9nxh-kkvkz,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-1 three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz Ready \u0026lt;none\u0026gt; 13m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-1,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz,kubernetes.io/os=linux,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-1 three-zone-cluster-1-node-pool-2-prg6m-84d45c4bd-vwhns Ready \u0026lt;none\u0026gt; 11m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-2,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-node-pool-2-prg6m-84d45c4bd-vwhns,kubernetes.io/os=linux,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-2 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 Ready \u0026lt;none\u0026gt; 11m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-3,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6,kubernetes.io/os=linux,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-3 In both the worker nodes and control plane nodes we have the labels:\nlinuxvm01:~/three-zones$ k get nodes --show-labels NAME LABELS three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz topology.kubernetes.io/zone=wdc-zone-1 three-zone-cluster-1-node-pool-2-prg6m-84d45c4bd-vwhns topology.kubernetes.io/zone=wdc-zone-2 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 topology.kubernetes.io/zone=wdc-zone-3 These labels can then be used when we deploy our applications. I will be using node affinity in my example below. For more information on pod to node placement see here.\nNow I want to deploy an application called Yelb that consist of four pods.\nI will define the yelp-db, yelp-appserver and yelb-cache pod to be placed in my vSphere Zone 3. The yelb-ui I will define to be placed in my vSphere Zone 1.\nBelow is my yelp application yaml manifest.\napiVersion: v1 kind: Service metadata: name: redis-server labels: app: redis-server tier: cache namespace: yelb spec: type: ClusterIP ports: - port: 6379 selector: app: redis-server tier: cache --- apiVersion: v1 kind: Service metadata: name: yelb-db labels: app: yelb-db tier: backenddb namespace: yelb spec: type: ClusterIP ports: - port: 5432 selector: app: yelb-db tier: backenddb --- apiVersion: v1 kind: Service metadata: name: yelb-appserver labels: app: yelb-appserver tier: middletier namespace: yelb spec: type: ClusterIP ports: - port: 4567 selector: app: yelb-appserver tier: middletier --- apiVersion: v1 kind: Service metadata: name: yelb-ui labels: app: yelb-ui tier: frontend namespace: yelb spec: type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 80 selector: app: yelb-ui tier: frontend --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-ui namespace: yelb spec: selector: matchLabels: app: yelb-ui replicas: 1 template: metadata: labels: app: yelb-ui tier: frontend spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-1 containers: - name: yelb-ui image: registry.guzware.net/yelb/yelb-ui:0.3 imagePullPolicy: Always ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-server namespace: yelb spec: selector: matchLabels: app: redis-server replicas: 1 template: metadata: labels: app: redis-server tier: cache spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-3 containers: - name: redis-server image: registry.guzware.net/yelb/redis:4.0.2 ports: - containerPort: 6379 --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-db namespace: yelb spec: selector: matchLabels: app: yelb-db replicas: 1 template: metadata: labels: app: yelb-db tier: backenddb spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-3 containers: - name: yelb-db image: registry.guzware.net/yelb/yelb-db:0.3 ports: - containerPort: 5432 --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-appserver namespace: yelb spec: selector: matchLabels: app: yelb-appserver replicas: 1 template: metadata: labels: app: yelb-appserver tier: middletier spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-3 containers: - name: yelb-appserver image: registry.guzware.net/yelb/yelb-appserver:0.3 ports: - containerPort: 4567 This is the section I define where the deployments should be placed:\naffinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - wdc-zone-X And now I apply my application, and where will the different pods be placed:\nlinuxvm01:~/three-zones$ k apply -f yelb-lb-zone-affinity.yaml service/redis-server created service/yelb-db created service/yelb-appserver created service/yelb-ui created deployment.apps/yelb-ui created deployment.apps/redis-server created deployment.apps/yelb-db created deployment.apps/yelb-appserver created Check pod information with -o wide\nlinuxvm01:~/three-zones$ k get pods -n yelb -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES redis-server-6cc65b47bd-sndht 1/1 Running 0 70s 20.40.3.8 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-appserver-84d4784595-jw7m5 1/1 Running 0 70s 20.40.3.10 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-db-7f888657dd-nt427 1/1 Running 0 70s 20.40.3.9 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; yelb-ui-6597db5d9b-972h7 1/1 Running 0 70s 20.40.1.6 three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As I can see from this output all pods execpt my frontend-ui pod has been placed in vSphere Zone 3. Now, if you read the official documentation from Kubernets.io pod placemement can be done in different ways according to different needs. Worth reading.\nThis concludes this post.\n","date":"3 May 2023","externalUrl":null,"permalink":"/2023/05/03/vsphere-with-tanzu-and-multi-zones-part2/","section":"Posts","summary":"In this post I will quickly go through how we can use vSphere with Tanzu on three different vSphere clusters - Multi Zones for app placement.","title":"vSphere with Tanzu and Multi Zones part2","type":"posts"},{"content":" vSphere with Tanzu and Multi-Zone # This post will be a brief introduction of how we can deploy and configure vSphere with Tanzu on three vSphere clusters (vSphere Zones) to achieve better availability of the TKG clusters and Supervisor controlplane nodes. This feature came with Sphere 8, not available in vSphere 7. There are some requirements that needs to be addressed for this to work in addition to the pre-reqs for deploying vSphere with Tanzu on a single vSphere cluster. I will list them below later.\nFrom the official VMware documentation:\nYou can deploy a Supervisor on three vSphere Zones to provide cluster-level high-availability that protects your Kubernetes workloads against cluster-level failure. A vSphere Zone maps to one vSphere cluster that you can setup as an independent failure domain. In a three-zone deployment, all three vSphere clusters become one Supervisor. You can also deploy a Supervisor on one vSphere cluster, which will automatically create a vSphere Zone and map it to the cluster, unless you use a vSphere cluster that is already mapped to a zone. In a single cluster deployment, the Supervisor only has high availability on host level that is provided by vSphere HA.\nOn a three-zone Supervisor you can run Kubernetes workloads on Tanzu Kubernetes Grid clusters and VMs created by using the VM service. A three-zone Supervisor has the following components:\nSupervisor control plane VM. Three Supervisor control plane VMs in total are created on the Supervisor. In a three-zone deployment, one control plane VM resides on each zone. The three Supervisor control plane VMs are load balanced as each one of them has its own IP address. Additionally, a floating IP address is assigned to one of the VMs and a 5th IP address is reserved for patching purposes. vSphere DRS determines the exact placement of the control plane VMs on the ESXi hosts part of the Supervisor and migrates them when needed. Tanzu Kubernetes Grid and Cluster API. Modules running on the Supervisor and enable the provisioning and management of Tanzu Kubernetes Grid clusters. Virtual Machine Service. A module that is responsible for deploying and running stand-alone VMs and VMs that make up Tanzu Kubernetes Grid clusters. Planning and deployment of a three-zone Supervisor # Before heading into the actual deployment, some pre-requirements needs to be in place. I will go through the important ones below such as storage, network and user-roles.\nvSphere ESXi clusters # A deployment of a three-zone Supervisor may also be referred to as multi-zone. This can maybe lead to the understanding that we may deploy vSphere with Tanzu multi-zone on 2 vSphere ESXi clusters or even 4 or higher number of vSphere clusters. The only number of vSphere clusters supported in a multi-zone Supervisor deployment (three-zone) is 3 vSphere clusters.\nFrom the official documentation:\nCreate three vSphere clusters with at least 2 hosts. For storage with vSAN, the cluster must have 3 or 4 hosts. Configure storage with vSAN or other shared storage solution for each cluster. Enable vSphere HA and vSphere DRS on Fully Automate or Partially Automate mode. Configure each cluster as an independent failure domain. Configure networking with NSX or vSphere Distributed Switch (vDS) networking for the clusters. Network # To be able to deploy a three-zone Supervisor an important requirement on the networking side is that all vSphere clusters to be used are connected to the same VDS. One VDS shared across all three vSphere clusters. From the official documentation:\nIn a three-zone Supervisor configured with NSX as the networking stack, all hosts from all three vSphere clusters mapped to the zones must use be connected to the same VDS and participate in the same NSX Overlay Transport Zone. All hosts must be connected to the same L2 physical device.\nNot sure what is meant with connecting to the same L2 physical device though\u0026hellip; But anyhow, this constraint can be a limiting factor and something to be aware of. Some explanations:\nIf you have an NSX environment with one common Overlay transportzone across all vSphere clusters, but configured with three individual VDS switches (1 specific VDS pr vSphere cluster) it is currently not supported. Having individual VDS switches per cluster allows you to have only relevant portgroups in your vSphere clusters that are configured specifically for their respective cluster\u0026rsquo;s network, different vlans, placed in different racks and there is no L2 between them, only L3. The different vSphere clusters can be on different racks with different ToR switches, in a Spine-Leaf topology, different subnets/vlans configured for mgmt, vmotion, vSAN, Geneve tunnel VLAN, VM network and so on.\nAn example when having a dedicated VDS pr vSphere cluster:\nThe illustration above indicates that we have portgroups defined that is valid for the VLAN trunks configured to the attached ToR switches with the corresponding VLAN trunks. So there is no risk of using any of the portgroups defined in the respective VDS in any of the above two vSphere clusters to use VLAN portgoups that has not been defined in the ToR switches trunk-ports. That means you will not end up with any unrelevant portgroups not valid for the respective cluster they are being used in. All this depends on the vlan trunks configured in the ToR switches. There is no need to have portgroups spanning across different vSphere clusters with unrelevant vlan tags. They will just end up not getting anywhere, and create a lot of \u0026ldquo;noise\u0026rdquo; for the admin managing these portgroups.\nThe example below illustrates one shared VDS across all vSphere clusters, where we have differentiating ToR VLAN trunks, configured per rack.\nAs we are using the same VDS across all clusters, all portgroups defined in this shared VDS is accessible and visible for all ESXi hosts participating in this VDS. This means they can easily be used, but they may not be the correct portgroups to be used for this cluster as the underlaying ToR switches may not have the correct vlan trunks for the ESXi hosts uplinks (pNICs).\nThat is just something to be aware of. If your ESXi hosts have been configured with more than two pNICs it is also possible to have one dedicated VDS per cluster for services specificially for the respective ESXi clusters and one shared for services that are configured identically across the clusters.\nStorage # From the official documentation:\nWhen you prepare storage resources for the three-zone Supervisor, keep in mind the following considerations:\nStorage in all three zones does not need to be of the same type. However, having uniform storage in all three clusters provides a consistent performance. For the namespace on the three-zone Supervisor, use a storage policy that is compliant with the shared storage in each of the clusters. The storage policy must be topology aware. Do not remove topology constraints from the storage policy after assigning it to the namespace. Do not mount zonal datastores on other zones. A three-zone Supervisor does not support the following items: Cross-zonal volumes vSAN File volumes (ReadWriteMany Volumes) Static volume provisioning using Register Volume API Workloads that use vSAN Data Persistence platform vSphere Pod vSAN Stretched Clusters VMs with vGPU and instance storage Permissions # From the official documentation:\nvSphere Namespaces required privileges:\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Allows disk decommission operations Allows for decommissioning operations of data stores. Data stores Namespaces.ManageDisks Backup Workloads component files Allows for backing up the contents of the etcd cluster (used only in VMware Cloud on AWS). Clusters Namespaces.Backup List accessible namespaces Allows listing the accessible vSphere Namespaces. Clusters Namespaces.ListAccess Modify cluster-wide configuration Allows modifying the cluster-wide configuration, and activating and deactivating cluster namespaces. Clusters Namespaces.ManageCapabilities Modify cluster-wide namespace self-service configuration Allows modifying the namespace self-service configuration. Clusters (for activating and deactivating)Templates(for modifying the configuration)vCenter Server(for creating a template) Namespaces.SelfServiceManage Modify namespace configuration Allows modifying namespace configuration options such as resource allocation and user permissions. Clusters Namespaces.Manage Toggle cluster capabilities Allows manipulating the state of cluster capabilities (used internally only for VMware Cloud on AWS). Clusters NA Upgrade clusters to newer versions Allows initiation of the cluster upgrade. Clusters Namespaces.Upgrade vSphere Zones Privileges\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Attach and Detach vSphere objects for vSphere Zones Allows for adding and removing clusters from a vSphere Zone. Clusters Zone.ObjectAttachable Create, Update and Delete vSphere Zones and their associations Allows for creating and deleting a vSphere Zone. Clusters Zone.Manage Supervisor Services Privileges\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Manage Supervisor Services Allows for creating, updating, or deleting a Supervisor Service. Also allows for installing a Supervisor Service on a Supervisor, and creating or deleting a Supervisor Service version. Clusters SupervisorServices.Manage Deployment time # Now that we have gone through the requirements I will start the actual installation/deployment steps of vSphere with Tanzu in a three-zone setup. This post is assuming an already configured and working vSphere environment and NSX installation. Will only focus on the actual deployment of vSphere with Tanzu in a three-zone setup.\nThis is how my environment looks like on a vSphere level before enabling the three-zone Supervisor:\nMy vSphere cluster consists of three vSphere cluster with 4 ESXi hosts each. They are all connected to the same Distributed Virtual Switch, but with different portgroups as their backend vlans is different between the clusters. The only common portgroups they share is all the overlay segments created from NSX (they are all part of the same Overlay TransportZone). Each cluster has its own VSAN datastore, not shared or stretched between the clusters. VSAN local to every vSphere cluster.\nEnable three-zone Supervisor # vSphere Zones # Before I can go ahead and enable a three-zone Supervisor I need to create the vSphere Zones which is done here:\nClick on the vCenter in the inventory tree -\u0026gt; Configure \u0026gt; vSphere Zones. From there I need to create three zones representing my three vSphere clusters. Click Add New vSphere Zone:\nThen select the cluster it should apply to and finish. The end result looks like this:\nStorage policy # Now I need to create a storage policy for the Supervisor. This policy needs to be a zonal policy. Head over to Policies and Profiles here:\nClick on VM Storage Policies:\nAnd click Create:\nGive it a name:\nI am using VSAN so in the next step (2.) I select rules for VSAN storage and the important bit needed for three-zone deployment is the Enable consumption domain\nThen on step 3. I will leave it default unless I have some special VSAN policies I want to apply.\nUnder step 4 the Storage topology type is Zonal\nIn step 5 all the vSAN storages should be listed as compatible\nThen it is just the review and finish\nNow that the storage policy is in place next up is the actual deployment of the three-zone Supervisor.\nDeploy the three-zone Supervisor # When all the prerequisites have been done, the actual enablement of the Supervisor is not so different from a regual Supervisor enablement, but let us go through the steps anyway.\nHead over to Workload Management\nStart the deployment wizard, I am using NSX so I am selecting NSX as the network stack\nIn step 2 I select vSphere Zone Deployment, select all my vSphere Zones, the give the Supervisor a name.\nStep 2 is the only step that is done different from a single-zone deployment.\nIn step 3 I select my newly created \u0026ldquo;zonal\u0026rdquo; policy, notice that we only have to select the Control Plane Storage Policy\nThen in step 4 it is the Supervisor management network configurations, here I am using my ls-mgmt NSX overlay segment which is common/shared across all esxi hosts/vSphere cluster.\nIn step 5 its the Supervisor Workload network configurations.\nThen in the last step its review and finish time, or put in a dns name for the supervisor k8s api endpoint.\nIt should start to deploy as soon as clicking the finish button:\nNow let us head back to vCenter inventory view and check whats going on there.\nAs you can see from the screenshot above, the three supervisors will be distributed across my three vSphere clusters, one Supervisor per vSphere cluster.\nThis concludes the enabling of a three-zone Supervisor cluster. Next step is to deploy your TKC or guest clusters.\nDeploy a TKC/guest cluster in a three-zone # Deploying a TKC cluster in a three-zone is not any different than a single zone, but we need to create storage policy for the workload cluster to be used for deployment.\nThe storage policy I have created look like this and is applied on the vSphere Namespace I create. I will quickly go through the step below:\nThats it, the same policy configuration used for the Supervisor deployment. Then it is all about creating a vSphere Namespace and apply a cluster to it.\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: wdc-cluster-small-cidr namespace: test-small-subnet spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] pods: cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.23.8---vmware.2-tkg.2-zshippable controlPlane: replicas: 1 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: machineDeployments: - class: node-pool name: node-pool-01 replicas: 2 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu variables: - name: vmClass value: best-effort-small #machineclass, get the available classes by running \u0026#39;k get virtualmachineclass\u0026#39; in vSphere ns context - name: storageClass value: all-vsans And the result should the same as for the Supervisor cluster, the workload clusters control plane and worker nodes should be distributed across your vSphere cluster.\nHappy deployment\n","date":"3 May 2023","externalUrl":null,"permalink":"/2023/05/03/vsphere-with-tanzu-multi-three-zones-and-nsx/","section":"Posts","summary":"In this post I will quickly go through how to deploy vSphere with Tanzu on three different vSphere clusters - Multi Zones","title":"vSphere with Tanzu, Multi (three) Zones and NSX","type":"posts"},{"content":" Tanzu with vSphere using NSX with multiple T0s # In this post I will go through how to configure Tanzu with different NSX T0 routers for IP separation use cases, network isolation and multi-tenancy. The first part will involve spinning up dedicated NSX Tier-0 routers by utlizing several NSX Edges and NSX Edge Clusters. The second part will involve using NSX VRF-Tier0. Same needs, two different approaches, and some different configs in NSX.\nSome background to why this is a useful feature: In vSphere with Tanzu with NSX we have the option to override network setting pr vSphere Namespace. That means we can place TKC/Workload clusters on different subnets/segments for ip separation and easy NSX Distributed Firewall policy creation (separation by environments DEV, TEST, PROD etc), but we can also override and define separate NSX Tier-0 routers for separation all the way out to the physical infrastructure. In some environments this is needed as there is guidelines/policies for certain network classifications to be separated, and filtered in physical firewall/security perimeters. Although NSX comes with a powerful and advanced distributed firewall, including Gateway firewall (on Tier1 and Tier0) there is nothing in the way for NSX to be combined in such environments, it just allows for more granular firewall policies before the traffic is eventually shipped out to the perimeter firewalls.\nThe end-goal would be something like this (high level):\nBefore jumping into the actual configuration of this setup I will need to explain a couple of things, as there are some steps that needs to be involved in getting this to work. Lets continue this discussion/monologue in the chapter below.\nRouting considerations # Tanzu with vSphere consists of a Supervisor Controlplane VM cluster (3 nodes). These three are configured with two network interfaces, interface ETH0 is placed in the management network. This management network can be NSX overlay segments, or regular VLAN backed segments or VDS/vSwitch portgroups. Its main responsibility is to communicate with the vCenter server API for vm/node creation etc. The second interface (ETH1) on these Supervisor Controlpane VMs is the Workload network. This network\u0026rsquo;s responsibilities are among a couple of things the Kubernetes API endpoint where we can interact with the Kubernetes API to create workload clusters, and here is an important note to remember: It is also used to communicate with the workload clusters being created. If there is no communication from this network to the workload cluster being created, workload cluster creation will fail. So needles to say, it is important that this communication works. When we deploy or enable the Workload Control Plane/Supervisor cluster on a fresh vCenter server and with NSX as the networking stack we are presented with a choice how the workload network can be configured, and the option I am interested in here is the NAT or no NAT option.\nThis option is very easy to select (selected by default) and very easy to deselect, but the impact it does is big. Especially when we in this post are discussing TKG with multi-Tier-0/VRF-T0.\nWhat this option does is deciding whether the workload network in your vSphere Namespace will be routed or not. Meaning that the controlplane and worker nodes actual ip addresses will be directly reachable and exposed (routed) in your network or if it will be masked by NAT rules so they will not be exposed with their real ip addresses in the network. The same approach as a POD in Kubernetes, default it will be Source NAT\u0026rsquo;ed through its worker node it is running on when it is doing egress traffic (going out and wants to make contact with something). So with NAT enabled on the workload network in a vSphere Namespace the workers will not be using their real ip addresses when they are communicating out, they will be masked by a NAT rule created in NSX automatically. This is all fine, if you want to use NAT. But if you dont want to use NAT you will have to disable this option and prepare all your vSphere Namespace workoad networks to be of ip subnets you are prepared to route in your network. That is also very fine. And as long as you dont expect to be running 500000+ worker nodes you will have available routed RFC1918 addresses to your disposal.\nThe reason I would like to touch upon the above subject is that it will define how we create our routing rules between a Supervisor cluster a vSphere Namespace workload network with NAT enabled or not.\nNow I get to the part where it really get interesting. If you decide to use the NAT\u0026rsquo;ed approach and NSX VRF-T0 when you create your first vSphere Namespace (not the default inital system vSphere Namespace, but the first vSphere Namespace you can create after WCP has been installed) NCP (NSX Container Plugin) will automatically create a couple of static routes so the Supervisor Cluster workload network can reach the workload networks in your vSphere Namespace placed under a different Tier-0 than the default workload network is placed, here a VRF Tier-0. These routes will be defined with VRF Route Leaking, meaning they will not go out of its parent Tier-0 to reach certain subnets. And with a NAT enabled workload cluster that is perfect as the we dont have care about the NAT rules created on the workload network, they can talk to each other with their real ip addresses. I will explain this a bit later on. Well that is fine and all, but sometimes VRF Tier-0 is not the best option, we need to use dedicated Tier-0 routers on different NSX edge cluster/edges then there is no automatic creation for these static routes. But where and how do I define these static routes manually? On the Tier-0s themselves? In the physical routers the Tier-0s are peering with (using static or BGP)? Yes, both options are possible. If we decide to create these rules in the Tier-0s themselves we need to create a linknet between the Tier-0s to they can point to each other with their respective subnet routes (can be a regular overlay segment used as L2 between the different Tier-0s).\nBut, there is always a but \u0026#x1f604;. With a NAT enabled vSphere Namespace, the workload network is not allowed to use its own IP address when going out of the Tier-0, it will be using an IP address from the egress subnet defined, or will it? First, when NAT is enabled on a vSphere Namespace, NCP will create a rule in NSX saying that if you want to talk to your other workload cluster network buddies, you dont have to hide your true identity, you are allowed to use your real name when you want to talk to your buddies. See below for how these rules looks like:\nWait a minute, what does this mean then? Exactly, this means that when it communicates with the other vSphere Namespace network destinations IP/Subnets (see above) it will not be NAT\u0026rsquo;ed. It will use its real ip address. And that ladies and gentlemen is why creating the static routes directly between the Tier-0s using a linknet between is an easy solution when using NAT in the vSphere Namespace network. Why, how etc, still dont understand? Ok, let me first show you an example below of two such static routes.\nThe above screenshot illustrates a static route defined on the Tier-0 where the Supervisor workload network is placed below behind a Tier-1 gateway. The ip subnet is the workload network cidr/subnet of a vSphere Namespace placed on a different Tier-0 router (its actual ip subnet, not the NAT ip addresses).\nThe above static route is created on the second Tier-0 where the vSphere Namespace number 2 is created and placed. This route is pointing to the actual IP subnet/cidr of the Supervisor workload network, not the NAT address. Just to mention it, common for both these routes is that they are using the linknet interfaces of the Tier-0s respectively. But how? We have this no SNAT rule, but at the same time we have a route-map denying any segment used by a NAT enable vSphere Namespace workload network to be advertised/exposed outside the Tier-0. Yes, exactly, outside the Tier-0. By using the linknet between the Tier-0s for our static routes we are simply telling the Tier-0 routers to go to their respective Tier-0 when it needs to find the respective vSphere Namespace workload networks. The idea here is that the Tier-0s are the \u0026ldquo;all-seeing-eye\u0026rdquo; and they truly are. All the T1 gateways configured by NCP/WCP will enable 4 route advertisements, all connected segments, NAT IPs, LB VIPS and Static Routes. This means that all the Tier1 gateways will happily tell the Tier-0 about their networks/subnets/ip addresses they know of to the Tier-0. So when we just create a static route like the two above (e.g 10.101.82.32/27 via linknet interface) the Tier-0 router that is defined as next-hop know exactly where this network is located, which T1 gateway it needs to send it to regardless of NAT rules and route-maps.\nNice, right? This means we dont need to interfere with these static routes in the physical routers, they were supposed to be NAT\u0026rsquo;ed right? So we dont want them in our physical routers anyway. This traffic will then never leave the \u0026ldquo;outside\u0026rdquo; of the Tier-0s via their uplinks connected to their respective BGP/Upstream routers as we are using the \u0026ldquo;linknet\u0026rdquo; between them. This will be true as long as the destination subnet is for the subnets defined in the no-nat rules and we have defined the static routes to go over the linknet.\nTo further show evidence of how this looks, let us do a \u0026ldquo;traffic walk\u0026rdquo; from the Supervisor Cluster Control Plane vm\u0026rsquo;s workload network to the other workload cluster networks on the second Tier-0 router. First an attempt to draw this:\nThe diagram above tries to illustrate what the actual IP address being used in regards of the destination subnet. If we quickly take a look at the no-snat rules created in NSX again:\nThe first 4 no-snat rules tells the Tier-1 router to NOT do any SNAT on the source IP if the source subnet happens to be 10.101.80/23 (SVC workload network) and the destination happens to be any of the four defined subnets in each NO-SNAT rules (10.101.80/23 itself, 10.101.82.0/24, 10,101.90.0/24 its own ingress cidr, and 10.101.83.0/24).\nSo if I do a tcpdump on one of my workload clusters control plane nodes, and fitering on ip coming from the supervisor control planes workload network. Will I then see the NATed address or will I see their real IP?\nvmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo tcpdump src net 10.101.80.0/27 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 12:39:06.351632 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1537746084:1537746122, ack 2981756064, win 6516, options [nop,nop,TS val 2335069578 ecr 3559626446], length 38 12:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 91, win 6516, options [nop,nop,TS val 2335069583 ecr 3559634990], length 0 12:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 7589, win 6477, options [nop,nop,TS val 2335069583 ecr 3559634990], length 0 12:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 38:73, ack 7589, win 6516, options [nop,nop,TS val 2335069584 ecr 3559634990], length 35 12:39:06.400033 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 7620, win 6516, options [nop,nop,TS val 2335069626 ecr 3559634991], length 0 12:39:06.794326 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [S], seq 2927440799, win 64240, options [mss 1460,sackOK,TS val 2841421479 ecr 0,nop,wscale 7], length 0 12:39:06.797334 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 907624276, win 502, options [nop,nop,TS val 2841421483 ecr 3559635431], length 0 12:39:06.797334 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [F.], seq 0, ack 1, win 502, options [nop,nop,TS val 2841421483 ecr 3559635431], length 0 12:39:06.800095 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 2, win 502, options [nop,nop,TS val 2841421486 ecr 3559635434], length 0 12:39:06.809982 IP 10.101.80.3.58013 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1460837974:1460838009, ack 2147601163, win 1312, options [nop,nop,TS val 1221062723 ecr 4234259718], length 35 If I filter on port 6443:\nvmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo tcpdump port 6443 tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 12:40:57.817560 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1537768767:1537768802, ack 2982048853, win 6516, options [nop,nop,TS val 2335181040 ecr 3559745035], length 35 12:40:57.817560 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 35:174, ack 1, win 6516, options [nop,nop,TS val 2335181041 ecr 3559745035], length 139 12:40:57.817643 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [.], ack 35, win 501, options [nop,nop,TS val 3559746454 ecr 2335181040], length 0 12:40:57.817677 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [.], ack 174, win 501, options [nop,nop,TS val 3559746454 ecr 2335181041], length 0 12:40:57.819719 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [P.], seq 1:90, ack 174, win 501, options [nop,nop,TS val 3559746456 ecr 2335181041], length 89 12:40:57.864302 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 90, win 6516, options [nop,nop,TS val 2335181086 ecr 3559746456], length 0 12:40:58.590194 IP 10.101.92.3.4120 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1810063827:1810063865, ack 3070977968, win 5820, options [nop,nop,TS val 1030353737 ecr 937884951], length 38 What do I see? I see the source address being the real IP addresses from the Supervisor Control Plane VMs (10.101.80.x). I also see 10.101.92.3 which happens to be the workload clusters own ingress.\nAfter these static routes have been added I also just want to do a traceroute from the supervisor vm to the workload cluster control plane node to show how I have altered the next hops it will use to get there:\nroot@422080039f397c9aa239cf40e4535f0d [ ~ ]# traceroute -T -p 22 -i eth1 10.101.82.34 traceroute to 10.101.82.34 (10.101.82.34), 30 hops max, 60 byte packets 1 _gateway (10.101.80.1) 0.265 ms 0.318 ms 0.295 ms 2 100.64.0.4 (100.64.0.4) 1.546 ms 1.518 ms 1.517 ms 3 10.101.240.13 (10.101.240.13) 9.603 ms 9.551 ms 9.568 ms 4 * * * 5 10.101.82.34 (10.101.82.34) 10.378 ms 10.296 ms 10.724 ms Now if I do the same traceflow from the same Supervisor vm, but to the workload cluster\u0026rsquo;s vip (10.101.92.0/24) which is exposed via BGP\u0026hellip;\nroot@422080039f397c9aa239cf40e4535f0d [ ~ ]# traceroute -T -p 22 -i eth1 10.101.92.3 traceroute to 10.101.92.3 (10.101.92.3), 30 hops max, 60 byte packets 1 _gateway (10.101.80.1) 0.229 ms 0.263 ms 0.237 ms 2 100.64.0.4 (100.64.0.4) 1.025 ms 1.006 ms 0.972 ms 3 10.101.4.1 (10.101.4.1) 1.405 ms 1.408 ms 1.524 ms 4 10.101.7.13 (10.101.7.13) 2.123 ms 2.037 ms 2.019 ms 5 10.101.92.3 (10.101.92.3) 2.562 ms 2.197 ms 2.499 ms It will not take the same route to get there. It will actually go all the way out to the physical BGP peers, and then over to the second Tier-0 router.\nBut if I do the same traceroute from my workload cluster nodes, traceroute the Supervisor workload VIP (which is also exposed via BGP)?\nvmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo traceroute -T -p 22 10.101.90.2 traceroute to 10.101.90.2 (10.101.90.2), 30 hops max, 60 byte packets 1 _gateway (10.101.82.33) 0.494 ms 1.039 ms 1.015 ms 2 100.64.0.0 (100.64.0.0) 1.460 ms 1.451 ms 1.441 ms 3 10.101.240.10 (10.101.240.10) 3.120 ms 3.108 ms 3.099 ms 4 10.101.90.2 (10.101.90.2) 3.115 ms 3.105 ms 3.095 ms It will go over the Tier-0 linknet. Why, because on the second Tier-0 router I have created two static routes pointing to both the Supervisor workload cluster subnet and Supervisor VIP altering the next-hops. See below:\nAnd also, did I mention that there will be created a route-map on the Tier-0 for the vSphere Namespace Networks with corresponding IP-prefix lists prohibiting the workload networks ip subnets to be advertised through BGP/OSPF from the Tier-0 and its upstream bgp neigbours.\nHow will this go then, if we cant by any reason use a linknet between the Tier-0s for these static routes and need to define them in the physical routers? Well that is an interesting question. Let us try to dive into that topic also. Did I mention that we can also decide to disable NAT completely? Well that is also a perfectly fine option. This could also give other benefits for the environments where it is is needed to have these routes in the physical network due to policies, requirements etc. We can create much more granular firewall policies in the perimeter firewalls when we know each node will egress with their actual IP instead of being masked by a NAT ip address. If being masked by a NAT ip address we cant for sure really know which node it is, we can only know that it potentially comes from any node in a vSphere Namespace where this egress subnet is defined (and that can potentially be a couple). Remember how the SNAT rules look like (maybe not as I haven\u0026rsquo;t shown a screenshot of it yet \u0026#x1f604;)?\nAlso, we dont need to create any static routes, it will be auto advertised by BGP (if using BGP) each time we create a new vSphere Namespace.\nBut, there is a possibilty to still use static routes, or inject them via BGP in the physical network. We need to define the static routes with the correct subnets, pointing to the NAT\u0026rsquo;ed vSphere Namespace workload network and egress subnet/cidr. So in my physical router I would need to create these rules:\nip route 10.101.80.0/23 10.101.4.10 #Workload network Default vSphere Namespace via Tier-0 Uplink(s) ip route 10.101.91.0/24 10.101.4.10 #Egress cidr Default vSphere Namespace via Tier-0 Uplink(s) These are the needed static routes for the Supervisor Workload and egress network to be configured in the physical router, if you happen to create a vSphere Namespace which is not NAT\u0026rsquo;ed these are the only routes needed. If your other vSphere Namespace is also NAT\u0026rsquo;ed you need to create the routes accordingly for this Namespace also.\nIllustration of static routes in the physical network between two Tier-0s and two NAT enabled vSphere Namespaces:\nIf using routed or a NAT disabled vSphere Namespace, the life will be much easier if using BGP on your Tier-0s.\nThere was a reason you decided to use NAT? When defining these static routes the network which are supposed to not be routed outside the Tier-0s will now suddenly be routed and exposed in your network. Is that something you want? Then you maybe should opt for no NAT anyway?\nSome notes on BFD with NSX and BGP/Static routes here\nThis part can be a bit confusing if not fully understanding how network traffic works in Tanzu with vSphere and NSX. Hopefully I managed to explain it so its more understandable.\nAs long as we are only using just one Tier-0 router for all our vSphere Namespaces, regardless of how many different subnets we decide to create pr vSphere Namespace they will be known by the same Tier-0 as the Tier-1 will be default configured to advertise to the Tier-0 its connected networks, yes it also advertise NAT IPs and LoadBalancer IPs but these are also configured on the Tier-0 to be further advertised to the outside world. Its only the Tier-0 that can be configured with BGP, as it is only the Tier-0 that can be configured to talk to the outside world (external network) by a SR T0 using interfaces on the NSX edges (VM or Bare-Metal). This means there is no need for us to create any routes either on the Tier-1 or Tier-0 when creating different vSphere Namespaces with different subnets. But I would not have anything to write about if we just decided to use only one Tier-0 router, would I? \u0026#x1f601;\nNow when all this is clear as day, let us head over to the actual installation and configuration of this.\nNSX and Tanzu configurations with different individual Tier-0s # I assume a fully functional Tanzu environment is running with the default Workload network configured with NAT and NSX is the networking component. For this exercise I have prepared my lab to look like this \u0026ldquo;networking wise\u0026rdquo;:\nI will deploy two new vSphere Namespaces where I select override Supervisor Network and choose the Tier-0-2 which is another Tier-0 router than my Supervisor workload network is on (this is using the first Tier-0 router)\nIn my lab I use the following IP addresses for the following components:\nTanzu Management network: 10.101.10.0/24 - connected to a NSX overlay segment - manually created by me Tanzu Workload network (the default Workload network): 10.101.80.0/23 (could be smaller) - will be created automatically as a NSX overlay segment. Ingress: 10.101.90.0/24 Egress: 10.101.91.0/24 I am doing NAT on this network (important to have in mind for later) The first Tier-0 has been configured to use uplinks on vlan 1014 in the following cidr: 10.101.4.0/24 The second Tier-0 (Tier-0-2) will be using uplink on vlan 1017 in the follwing cidr: 10.101.7.0/24 Second vSphere Namespace - will be using Tier-0-2 and NAT disabled Second vSphere Namespace Workload network: 10.101.82.0/24 Second vSphere Namespace ingress network: 10.101.92.0/24 Third vSphere Namespace - will be using Tier-0-2 and NAT enabled Third vSphere Namespace Workload network: 10.101.83.0/24 Third vSphere Namespace ingress network: 10.101.93.0/24 Third vSphere Namespace egress network: 10.101.94.0/24 (where the NAT rules will be created) Using dedicated Tier-0 means we need to deploy additional edges, either in the same NSX edge cluster or a new edge cluster. This can generate some compute and admin overhead. But in some environments its not \u0026ldquo;allowed\u0026rdquo; to share two different network classifications over same devices. So we need separate edges for our different Tier-0s. But again, with TKGs we cannot deploy our TKC clusters on other vSphere clusters than our Supervisor cluster has been configured on, so the different TKC cluster will end up on the same shared compute nodes (ESXi). But networking wise they are fully separated.\nDeploy new Edge(s) to support a new Tier-0 # As this is my lab, I will not deploy redundant amount of Edges, but will stick with one Edge just to get connectivity up and working. NSX Edge do not support more than 1 SR T0 pr Edge, so we need 1:1 mapping between the SR T0 and Edge. And take into consideration if running this in production we must accommodate potential edge failovers, so we should atleast have two edges responsible for a T0. If running two Tier-0 in an edge cluster we should have 4 edges (if one of them fail).\nThe first thing we need to do is to deploy a new Edge vm from the NSX manager. The new edge will be part of my \u0026ldquo;common\u0026rdquo; overlay transportzone as I cant deploy any TKC cluster on other vSphere clusters than where my Supervisor cluster has been enabled. For the VLAN transportzones one can reuse the existing Edge vlan transportzone and the same profile so they get their correct TEP VLAN. For the Uplinks it can be same VLAN trunkport (VDS or NSX VLAN segment) if the vlan trunk range includes the VLAN for the new T0 uplink.\nSo my new edge for this second T0 will be deployed like this:\nAfter the Edge has been deployed its time to create a Edge cluster. Now we need to create a new segment for the coming new Tier-0 router and the Tier-0 linknet:\nThe first segment has been configured to use the edge-vlan-transportzone, and this vlan will be used to peer with the upstream router. The second segment is just a layer 2 overlay segment to be used for link between the two Tier-0s.\nNow we can go ahead and create the new Tier-0:\nGive the Tier-0 a name, select your new Edge cluster. Save and go back to edit it. We need to add the two interfaces uplink to upstream router and link interface between the two tier-0s: Give the interfaces a name, IP address and select the segments we created above for the new Tier-0 uplinks. Select the Edge node and Save Now we have the interfaces, to test if it is up and running you can ping it from your upstream router. The only interface that can be reached is the uplink interface 10.101.7.13. Next configure BGP and the BGP peering with your upstream router:\nThe last thing we need to do in our newly created Tier-0 is to create two static routes that can help us reach the Workload Network on the Supervisor Control Plane nodes on their actual IP addresses (remember our talk above?). On the newly created Tier-0 (Tier-0-2) click on Routing -\u0026gt; Static Routes and add the following route (Supervisor workload network):\nThe two routes created is the Supervisor workload network cidr and the actual ingress vip /32.\nAnd the next-hop is defined with the ip of the other (first) Tier-0 interface on the \u0026ldquo;linknet\u0026rdquo; interface between the T0s (not configured on the first Titer-0 yet):\nAdd and Save.\nNow on the first Tier-0 we need a second interface (or two depending on the amount of edges) in the linknet we created earlier.\nName it, ip address Select the edge(s) that will have this/these new interface(s). Save.\nNext up is the route: These route should point to the vSphere workload network cidrs we defined when we created the vSphere Namespaces. The correct cidr is something we get when we create the vSphere Namespace (it is based on the Subnet prefix you configure)\nAnd next-hop (yes you guessed correct) is the linknet interface on the new Tier-0.\nI create the static routes for both the vSphere Namespaces, and I know that it will start using the second /27 subnet in the /24 workload network cidr for the first cluster in each namespace.\nSo we should have something like this now:\nAs mentioned above, these routes is maybe easier to create after we have created the vSphere Network with the correct network definition as we can see them being realized in the NSX manager.\nBy adding these static routes on the T0 level as I have done, means this traffic will never leave the Tier-0s, it will go over the linknet between the Tier-0s\nThese routes are necessary for the Supervisor and the TKC cluster to be able to reach each others. If they cant, deployment of the TKC clusters will fail, it will just deploy the first Control Plane node and stop there)\nCreate a vSphere Namespace to use our new Tier-0 # Head over to vCenter -Workload Management and create a new Namespace:\nAs soon as that is done, go ahead and create the third vSphere Namespace with NAT enabled:\nGive the NS\u0026rsquo;es a dns compliant name, select the Override Supervisor network settings. From the dropdown select our new Tier-0 router. Uncheck NAT on the vSphere NS number 2 (dont need NAT). Fill in the IP addresses you want to use for the TKC worker nodes, and then the ingress cidr you want. On the vSphere NS number 2 enable NAT and populate an egress cidr also.\nClick Create. Wait a couple of second and head over to NSX and check what has been created there.\nIn the NSX Manager you should now see the following:\nNetwork Topology:\nSegments\nThese network is automatically created by NCP, and the first /27 segment in all vSphere Namespace created will be reserved for Supervisor services, like vSphere pods etc. The second /27 segment is the first available network for the workload clusters. This will in my case start at 10.101.82.32/27 and 10.101.83.32/27 accordingy.\nUnder LoadBalancing we also got a couple of new objects:\nThis is our ingress for the workload cluster Kubernetes API.\nUnder Tier-1 gateways we have new Tier-1 gateways:\nIf you take a closer look at the new Tier-1s, you probably would expect them to be created on the new edge-cluster you created and placed your Tier-0 on? No, its not doing that.\nIt used the edge-cluster the Supervisor has been configured to use. Thats pr design. So dont try to troubleshoot this. Reboot the Edge nodes, NSX managers, ESXi hosts etc. It is like that.\nNow it is time to deploy your new TKC cluster with the new Tier-0. Its the same procedure as every other TKC cluster. Give it a name and place it in the correct Namespace:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: stc-tkc-cluster-dmz namespace: stc-ns-dmz spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] pods: cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.23.8---vmware.2-tkg.2-zshippable controlPlane: replicas: 1 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: machineDeployments: - class: node-pool name: node-pool-01 replicas: 2 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu variables: - name: vmClass value: best-effort-small #machineclass, get the available classes by running \u0026#39;k get virtualmachineclass\u0026#39; in vSphere ns context - name: storageClass value: vsan-default-storage-policy Then it is just running:\nkubectl apply -f yaml.file And a couple of minutes later (if all preps have been done correctly) you should have a new TKC cluster using the new T0.\nNow, if the network conditions not were right, the TKC cluster would never be finished. It would stop stop at deploying the first control plane node. But to quickly verify connectivity from the supervisor controlplane vm and the tkc controlplane vm I will SSH into both (described below under troubleshooting) and do a curl against their K8s API VIP respectively:\nFrom one of the supervisor vms \u0026ldquo;curling\u0026rdquo; both vSphere NS workload networks k8s api vip from its workload network interface:\nroot@422068ece368739850023f7a81cf5e14 [ ~ ]# curl --interface eth1 https://10.101.93.1:6443 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. root@422068ece368739850023f7a81cf5e14 [ ~ ]# curl --interface eth1 https://10.101.92.1:6443 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. root@422068ece368739850023f7a81cf5e14 [ ~ ]# From the controlplane node of the TKC workload cluster:\nvmware-system-user@wdc-vrf-cluster-1-2k8tp-gb5g2:~$ curl https://10.101.90.2:6443 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.haxx.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. NSX and Tanzu configurations with NSX VRF # In NSX-T 3.0 VRF was a new feature, and configuring it was a bit cumbersome, but already from NSX-T 3.1 adding and configuring a VRF Tier-0 is very straightforward. The benefit of using VRF is that it does not dictate the requirement of additional NSX Edges, and we can create many VRF T0s. We can \u0026ldquo;reuse\u0026rdquo; the same Edges that has already been configured with a Tier-0. Instead a VRF T0 will be linked to that already existing Tier-0 which will then be the Parent Tier-0. Some settings will be inherited from the parent Tier-0 like BGP AS number for NSX-T versions before 4.1. From NSX-T 4.1 it is now also possible to override the BGP AS number in the VRF-T0, its no longer tied to the parent T0s BGP AS. We can achieve ip-separation by using individual uplinks on the VRF Tier-0s and peer to different upstream routers than our parent Tier-0. The VRF Tier0 will have its own Tier-1 linked to it. So all the way from the physical world to the VM we have a dedicated ip network. To be able to configure VRF Tier-0 we need to make sure the uplinks our Edges have been configured with have the correct vlan trunk range so we can create dedicated VRF Tier0 uplink segments in their respective vlan. The VRF Tier0 will use the same \u0026ldquo;physical\u0026rdquo; uplinks as the Edges have been configured with, but using different VLAN for the Tier-0 uplinks. I will go through how I configre VRF T0 in my environment. Pr default there is no route leakage between the parent Tier-0 and the VRF-T0 created, if you want to exhange routes between them we need to create those static routes ourselves. Read more about NSX VRF here.\nFrom NSX-T 4.1 it is now also possible to override the BGP AS number in the VRF-T0, its no longer tied to the parent T0s BGP AS\nIn this part of my lab I use the following IP addresses for the following components:\nTanzu Management network: 172.21.103.0/24 - connected to a VDS port group - manually created by me Tanzu Workload network (the initial Workload network): 10.103.100.0/23 - will be created automatically as a NSX overlay segment. Ingress: 10.103.200.0/24 Egress: 10.103.201.0/24 I am doing NAT on this network (important to have in mind for later) The first Tier-0 has been configured to use uplinks on vlan 1034 in the following cidr: 10.103.4.0/24 The VRF Tier-0 will be using uplink on vlan 1035 in the follwing cidr: 10.103.5.0/24 Here is a digram showing high-level how VRF-T0 looks like:\nThe Edge VM network config:\nConfigure VRF Tier-0 in NSX # Head over the NSX manager -\u0026gt; Networking -\u0026gt; Tier-0 Gateways and click Add Gateway:\nThen give it a name and select the parent Tier0:\nClick save.\nNow head over to Segments and create the VRF-Tier0 Uplink segment:\nGive it a name, select the Edge VLAN Transportzone and enter the VLAN for the VRF T0-uplink (you can also create a vlan Trunk range here instead of creating two distinct segments for both uplinks). In my lab I will only use one uplink.\nClick save\nNow head back to your VRF T0 again and add a interface:\nGive it a name, select external, enter the IP for the uplink you will use to peer with your upstream router, then select the segment created earlier. Select the Edge that will get this interface. Notice also the Access VLAN ID field. There is no need to enter the VLAN here as we only defined one VLAN in our segment, had we created a VLAN range we need to define a VLAN here. It discovers the correct VLAN as we can see. Click save. Remember that for this VLAN to \u0026ldquo;come through\u0026rdquo; the Edge needs to be on a trunk-port that allows this VLAN.\nYou can verify the L2 connectivity from your router:\nroot@cpodrouter-v7n31 [ ~ ]# ping 10.103.5.10 PING 10.103.5.10 (10.103.5.10) 56(84) bytes of data. 64 bytes from 10.103.5.10: icmp_seq=1 ttl=64 time=4.42 ms 64 bytes from 10.103.5.10: icmp_seq=2 ttl=64 time=0.627 ms 64 bytes from 10.103.5.10: icmp_seq=3 ttl=64 time=0.776 ms ^C --- 10.103.5.10 ping statistics --- 3 packets transmitted, 3 received, 0% packet loss, time 10ms rtt min/avg/max/mdev = 0.627/1.939/4.416/1.752 ms Now that we have verified that its time for BGP to configured in our upstream router and in our VRF Tier-0. I have already configured my upstream router to accept my VRF T0 as a BGP neighbour, I just need to confgure BGP on my new VRF Tier-0. In the VRF Tier-0 go to BGP and add a bgp neighbour (notice that we need to enable BGP, not enabled by default, and you cant change the BGP as number):\nClick save.\nNeighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.103.4.10 4 66803 336 345 0 0 0 05:30:33 3 10.103.5.10 4 66803 2 19 0 0 0 00:01:38 0 172.20.0.1 4 65700 445 437 0 0 0 07:09:43 74 My new neighbour has jouined the party. Now just make sure it will advertise the needed networks. Lets configure that: In the VRF T0, click route re-distribution and SET\nNow my new VRF-Tier 0 is ready to route and accept new linked Tier-1s. How does it look like in the NSX map?\nLooking good.\nLet us get back to this picture when we have deployed a TKC cluster on it.\nCreate a vSphere Namespace to use our new VRF Tier-0 # This will be the same approach as above here only difference is we are selecting a VRF Tier0 instead. Here I have selected the VRF Tier-0 and defined the network for it. I have disabled NAT.\nNow what have happened in NSX? Lets have a look. The network topology has been updated:\nA new Tier-1 has been created:\nAnd ofcourse the loadbalancer interface:\nBut the most interesting part is the static routes being created. Let us have a look at these.\nIn the VRF T0 it has created two additonal static routes:\nThose to routes above points to the Supervisor Workload network and the Supervisor Ingress network. Next hop is:\nThese are the Tier0-Tier-1 transit net interface:\nWhat static routes have been configured on the parent Tier-0?\nAnd next-hop is:\nThese routes are pointing to the new vSphere Namespace network, and Ingress network we defined to use the new VRF-Tier0.\nHigh-level overview of the static routes being created automatically by NCP:\nWhen the TKC cluster is deployed the NSX map will look like this:\nA new segment as been added (vnet-domain-c8:5135e3cc-aca4-4c99-8f9f-903e68496937-wdc-ns-1-vrf-wdc-cl-58aaa-0), which is the segment where the TKC workers have been placed. Notice that it is using a /27 subnet as defined in the Namespace Subnet Prefix above. The first segment (/27 chunk) (seg-domain-xxxxx) is always reserved for the Supervisor Services/vSphere Pods. As I decided not to use NAT I can reach the worker nodes IP addresses directly from my management jumpbox (if allowed routing/firewall wise). Note that ping is default disabled/blocked. So to test connectivity try port 22 with SSH/curl/telnet etc.\nandreasm@linuxvm01:~/tkgs_vsphere7$ ssh 10.103.51.34 The authenticity of host \u0026#39;10.103.51.34 (10.103.51.34)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:qonxA8ySCbic0YcCAg9i2pLM9Wpb+8+UGpAcU1qAXHs. Are you sure you want to continue connecting (yes/no/[fingerprint])? But before you can reach it directly you need to allow this with a firewall rule in NSX as there is a default block rule here:\nIn order to \u0026ldquo;override\u0026rdquo; this rule we need to create a rule earlier in the NSX Distributed Firewall. Below is just a test rule I created, its far to open/liberal of course:\nThe group membership in the above rules is just the vnet-domain-c8:5135e3cc-aca4-4c99-8f9f-903e68496937-wdc-ns-1-vrf-wdc-cl-58aaa-0 segment where my TKC workers in this namespace will reside. So if I scale down/up this cluster the content will be dynamically updated. I dont have to update the rule or security group, its done automatic.\nFirewall openings - network diagram # I will get back and update this section with a table and update the diagram with more details.\nTroubleshooting # To troubleshoot networking scenarios with Tanzu it can sometimes help to SSH into the Supervisor Controlplane VMs and the TKC worker nodes. When I tested out this multi Tier-0 setup I had an issue that only the control plane node of my TKC cluster were being spun up, it never came to deploying the worker nodes. I knew it had to do with connectivity between the Supervisor and TKC. I used NSX Traceflow to verify that connectivity worked as intended which my traceflow in NSX did show me, but still it did not work. So sometimes it is better to see whats going on from the workloads perspective themselves.\nSSH Supervisor VM # To log in to the Supervisor VMs we need the root password. This password can be retreived from the vCenter server. SSH into the vCenter server:\nroot@vcsa [ /lib/vmware-wcp ]# ./decryptK8Pwd.py Read key from file Connected to PSQL Cluster: domain-c35:dd5825a9-8f62-4823-9347-a9723b6800d5 IP: 172.21.102.81 PWD: PASSWORD-IS-HERE ------------------------------------------------------------ Cluster: domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5 IP: 10.101.10.21 PWD: PASSWORD-IS-HERE ------------------------------------------------------------ Now that we have the root password one can log into the Supervisor VM with SSH and password through the Management Interface (the Workload Interface IP is probably behind NAT so is not reachable OOB):\nandreasm@andreasm:~/from_ubuntu_vm/tkgs/tkgs-stc-cpod$ ssh root@10.101.10.22 The authenticity of host \u0026#39;10.101.10.22 (10.101.10.22)\u0026#39; can\u0026#39;t be established. ED25519 key fingerprint is SHA256:vmeHlDgquXrZTK3yyevmY2QfISW1WNoTC5TZJblw1J4. This key is not known by any other names Are you sure you want to continue connecting (yes/no/[fingerprint])? And from in here we can use some basic troubleshooting tools to verify if the different networks can be reached from the Supervisor VM. In the example below I try to verify if it can reach the K8s API VIP for the TKC cluster deployed behind the new Tier-0. I am adding \u0026ndash;interface eth1 as I want to specifically use the Workload Network interface on the SVM.\ncurl --interface eth1 https://10.13.52.1:6443 The respons should be immediate, if not you have network reachability issues:\ncurl: (28) Failed to connect to 10.13.52.1 port 6443 after 131108 ms: Couldn\u0026#39;t connect to server What you should see is this:\nroot@423470e48788edd2cd24398f794c5f7b [ ~ ]# curl --interface eth1 https://10.13.52.1:6443 curl: (60) SSL certificate problem: unable to get local issuer certificate More details here: https://curl.se/docs/sslcerts.html curl failed to verify the legitimacy of the server and therefore could not establish a secure connection to it. To learn more about this situation and how to fix it, please visit the web page mentioned above. SSH TKC nodes # The nodes in a TKC cluster can also be SSH\u0026rsquo;ed into. If you dont do NAT on your vSphere Namespace network they can be reach directly on their IPs (if from where your SSH jumpbox is allowed routing wise/firewall wise). But if you are NAT\u0026rsquo;ing then you have to place your SSH jumpbox in the same segment as the TKC nodes you want to SSH into. Or add a second interface on your jumpbox placed in this network. The segment is created in NSX and is called something like this:\nTo get the password for the TKC nodes you can get them with kubectl like this: Put yourselves in the context of the namespace where your workload nodes is deployed:\nandreasm@andreasm:~$ vsphere-kubectl login --server=10.101.11.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-namespace ns-wdc-1-nat andreasm@andreasm:~$ k config current-context tkc-cluster-nat Then get the SSH secret:\nandreasm@andreasm:~$ k get secrets NAME TYPE DATA AGE default-token-fqvbp kubernetes.io/service-account-token 3 127d tkc-cluster-1-antrea-data-values Opaque 1 127d tkc-cluster-1-auth-svc-cert kubernetes.io/tls 3 127d tkc-cluster-1-ca cluster.x-k8s.io/secret 2 127d tkc-cluster-1-capabilities-package clusterbootstrap-secret 1 127d tkc-cluster-1-encryption Opaque 1 127d tkc-cluster-1-etcd cluster.x-k8s.io/secret 2 127d tkc-cluster-1-extensions-ca kubernetes.io/tls 3 127d tkc-cluster-1-guest-cluster-auth-service-data-values Opaque 1 127d tkc-cluster-1-kapp-controller-data-values Opaque 2 127d tkc-cluster-1-kubeconfig cluster.x-k8s.io/secret 1 127d tkc-cluster-1-metrics-server-package clusterbootstrap-secret 0 127d tkc-cluster-1-node-pool-01-bootstrap-j2r7s-fgmm2 cluster.x-k8s.io/secret 2 42h tkc-cluster-1-node-pool-01-bootstrap-j2r7s-r5lcm cluster.x-k8s.io/secret 2 42h tkc-cluster-1-node-pool-01-bootstrap-j2r7s-w96ft cluster.x-k8s.io/secret 2 42h tkc-cluster-1-pinniped-package clusterbootstrap-secret 1 127d tkc-cluster-1-proxy cluster.x-k8s.io/secret 2 127d tkc-cluster-1-sa cluster.x-k8s.io/secret 2 127d tkc-cluster-1-secretgen-controller-package clusterbootstrap-secret 0 127d tkc-cluster-1-ssh kubernetes.io/ssh-auth 1 127d tkc-cluster-1-ssh-password Opaque 1 127d tkc-cluster-1-ssh-password-hashed Opaque 1 127d I am interested in this one:\ntkc-cluster-1-ssh-password So I will go ahead and retrieve the content of it:\nandreasm@andreasm:~$ k get secrets tkc-cluster-1-ssh-password -oyaml apiVersion: v1 data: ssh-passwordkey: aSx--redacted---KJS= #Here is the ssh password in base64 kind: Secret metadata: creationTimestamp: \u0026#34;2022-12-08T10:52:28Z\u0026#34; name: tkc-cluster-1-ssh-password namespace: stc-tkc-ns-1 ownerReferences: - apiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster name: tkc-cluster-1 uid: 4a9c6137-0223-46d8-96d2-ab3564e375fc resourceVersion: \u0026#34;499590\u0026#34; uid: 75b163a3-4e62-4b33-93de-ae46ee314751 type: Opaque Now I just need to decode the base64 encoded pasword:\nandreasm@andreasm:~$ echo \u0026#39;aSx--redacted---KJS=\u0026#39; |base64 --decode passwordinplaintexthere=andreasm@andreasm:~$ Now we can use this password to log in to the TKC nodes with the user: vmware-system-user\nssh vmware-system-user@10.101.51.34 DCLI - VMware Datacenter CLI # If you happen to find different tasks easier to perform from CLI instead of GUI I will show here how to create a new vSphere Namespace by using DCLI from the VCSA appliance (vCenter Server). For more information and reference on dcli look here.\nLog in to your vCenter hosting your Supervisor Cluster with SSH and enter shell:\nandreasm@andreasm:~/$ ssh root@vcsa.cpod-v7n31.az-wdc.cloud-garage.net VMware vCenter Server 7.0.3.01000 Type: vCenter Server with an embedded Platform Services Controller (root@vcsa.cpod-v7n31.az-wdc.cloud-garage.net) Password: Connected to service * List APIs: \u0026#34;help api list\u0026#34; * List Plugins: \u0026#34;help pi list\u0026#34; * Launch BASH: \u0026#34;shell\u0026#34; Command\u0026gt; shell Shell access is granted to root root@vcsa [ ~ ]# Type *dcli \u0026ndash;help\u0026quot; to see some options:\nroot@vcsa [ ~ ]# dcli --help usage: dcli [+server SERVER] [+vmc-server] [+nsx-server [NSX_SERVER]] [+org-id ORG_ID] [+sddc-id SDDC_ID] [+interactive] [+prompt PROMPT] [+skip-server-verification | +cacert-file CACERT_FILE] [+username USERNAME] [+password PASSWORD] [+logout] [+filter FILTER [FILTER ...]] [+formatter {yaml,yamlc,table,xml,xmlc,json,jsonc,jsonp,html,htmlc,csv}] [+verbose] [+log-level {debug,info,warning,error}] [+log-file LOG_FILE] [+generate-json-input] [+generate-required-json-input] [+json-input JSON_INPUT] [+credstore-file CREDSTORE_FILE] [+credstore-add | +credstore-list | +credstore-remove] [+session-manager SESSION_MANAGER] [+configuration-file CONFIGURATION_FILE] [+more] [args [args ...]] VMware Datacenter Command Line Interface positional arguments: args CLI command optional arguments: +server SERVER Specify VAPI Server IP address/DNS name (default: \u0026#39;http://localhost/api\u0026#39;) +vmc-server Switch to indicate connection to VMC server (default VMC URL: \u0026#39;https://vmc.vmware.com\u0026#39;) +nsx-server [NSX_SERVER] Specify NSX on VMC Server or on-prem instance IP address/DNS name (default: \u0026#39;None\u0026#39;) +org-id ORG_ID Specify VMC organization id to connect to NSX instance. Works together with +sddc-id. (default: \u0026#39;None\u0026#39;) +sddc-id SDDC_ID Specify VMC SDDC id to connect to NSX instance. Works together with +org-id. (default: \u0026#39;None\u0026#39;) +interactive Open a CLI shell to invoke commands +prompt PROMPT Prompt for cli shell (default: dcli\u0026gt; ) +skip-server-verification Skip server SSL verification process (default: False) +cacert-file CACERT_FILE Specify the certificate authority certificates for validating SSL connections (format: PEM) (default: \u0026#39;\u0026#39;) +username USERNAME Specify the username for login (default: \u0026#39;\u0026#39;) +password PASSWORD Specify password explicitly (default: False) +logout Requests delete session and remove from credentials store if stored. (default: False) +filter FILTER [FILTER ...] Provide JMESPath expression to filter command output. More info on JMESPath here: http://jmespath.org +formatter {yaml,yamlc,table,xml,xmlc,json,jsonc,jsonp,html,htmlc,csv} Specify the formatter to use to format the command output +verbose Prints verbose output +log-level {debug,info,warning,error} Specify the verbosity for log file. (default: \u0026#39;info\u0026#39;) +log-file LOG_FILE Specify dcli log file (default: \u0026#39;/var/log/vmware/vapi/dcli.log\u0026#39;) +generate-json-input Generate command input template in json +generate-required-json-input Generate command input template in json for required fields only +json-input JSON_INPUT Specifies json value or a json file for command input +credstore-file CREDSTORE_FILE Specify the dcli credential store file (default: \u0026#39;/root/.dcli/.dcli_credstore\u0026#39;) +credstore-add Store the login credentials in credential store without prompting +credstore-list List the login credentials stored in credential store +credstore-remove Remove login credentials from credential store +session-manager SESSION_MANAGER Specify the session manager for credential store remove operation +configuration-file CONFIGURATION_FILE Specify the dcli configuration store file (default: \u0026#39;/root/.dcli/.dcli_configuration\u0026#39;) +more Flag for page-wise output root@vcsa [ ~ ]# Enter DCLI interactive mode: All commands in dcli have autocomplete\nroot@vcsa [ ~ ]# dcli +i +server vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net +skip-server-verification Welcome to VMware Datacenter CLI (DCLI) usage: \u0026lt;namespaces\u0026gt; \u0026lt;command\u0026gt; To auto-complete and browse DCLI namespaces: [TAB] If you need more help for a command: vcenter vm get --help If you need more help for a namespace: vcenter vm --help To execute dcli internal command: env For detailed information on DCLI usage visit: http://vmware.com/go/dcli dcli\u0026gt; Below shows how autocomplete works:\nroot@vcsa [ ~ ]# dcli +i +server vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net +skip-server-verification Welcome to VMware Datacenter CLI (DCLI) usage: \u0026lt;namespaces\u0026gt; \u0026lt;command\u0026gt; To auto-complete and browse DCLI namespaces: [TAB] If you need more help for a command: vcenter vm get --help If you need more help for a namespace: vcenter vm --help To execute dcli internal command: env For detailed information on DCLI usage visit: http://vmware.com/go/dcli dcli\u0026gt; com vmware vcenter n \u0026gt; namespacemanagement \u0026gt; namespaces \u0026gt; network dcli\u0026gt; com vmware vcenter namespaces instances list list getv2 update delete listv2 createv2 set We can tab to autocomplete and/or use the \u0026ldquo;dropdown\u0026rdquo; list to scroll through the different options. Nice feature.\nCreate a vSphere Namespace from DCLI, selecting a VRF T0, configure name, network etc (as you would do from the GUI of vCenter in Workload Management):\ndcli\u0026gt; com vmware vcenter namespaces instances create --cluster domain-c8 --namespace stc-cluster-vrf2 --namespace-network-network-ingress-cidrs \u0026#39;[{\u0026#34;address\u0026#34;: \u0026#34;10.13.54. 0\u0026#34;, \u0026#34;prefix\u0026#34;:24}]\u0026#39; --namespace-network-network-load-balancer-size SMALL --namespace-network-network-namespace-network-cidrs \u0026#39;[{\u0026#34;address\u0026#34;: \u0026#34;10.13.53.0\u0026#34;, \u0026#34;prefix\u0026#34;:24}]\u0026#39; - -namespace-network-network-provider NSXT_CONTAINER_PLUGIN --namespace-network-network-nsx-tier0-gateway vrf-1 --namespace-network-network-routed-mode true --namespace-n etwork-network-subnet-prefix-length 28 dcli\u0026gt; To get the \u0026ndash;cluster domain id run this:\ndcli\u0026gt; com vmware vcenter namespaces instances list |---------|-----------------------------------|----------------|-----------|----------------------|-------------| |cluster |stats |namespace |description|self_service_namespace|config_status| |---------|-----------------------------------|----------------|-----------|----------------------|-------------| |domain-c8||--------|-----------|------------||stc-cluster-vrf | |False |RUNNING | | ||cpu_used|memory_used|storage_used|| | | | | | ||--------|-----------|------------|| | | | | | ||0 |0 |0 || | | | | | ||--------|-----------|------------|| | | | | |domain-c8||--------|-----------|------------||stc-cluster-vrf2| |False |RUNNING | | ||cpu_used|memory_used|storage_used|| | | | | | ||--------|-----------|------------|| | | | | | ||0 |0 |0 || | | | | | ||--------|-----------|------------|| | | | | |---------|-----------------------------------|----------------|-----------|----------------------|-------------| dcli\u0026gt; And seconds later the vSphere Namespace is created\nvCenter API - with Postman # vCenter has a nice feature included, the API Explorer. This can be found here:\nClick on the hamburger Menu, and find Developer Center:\nAnd from here we have all the API available to us:\nIts a looooong list of available APIs.\nTo be able to authenticate against vCenter with Postman we must create an API Key. So the first we need to do is \u0026ldquo;login\u0026rdquo; with post using the following api (this uses a username and password with sufficient acces to vCenter):\nhttps://{{vcenter-fqdn}}/api/session In Postman one should create an environment that contains the vCenter IP/FQDN, username and password. So the first action is to POST this API to get the API Key, making sure you set Authorization to Basic Auth from your environment:\nThe response from this POST should be a token. From now you need to use this token to interact with vCenter API. Change the authentication to API Key and use vmware-api-session-id as Key and Token as value.\nNow lets try a GET and see if it works:\nThat worked out fine \u0026#x1f604;\nWhat about creating a vSphere Namespace from Postman?\nThats very easy, below is an example to create a new vSphere Namespace, and pointing it to my VRF Tier-0 router:\n{ \u0026#34;access_list\u0026#34;: [ { \u0026#34;domain\u0026#34;: \u0026#34;cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;OWNER\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;andreasm\u0026#34;, \u0026#34;subject_type\u0026#34;: \u0026#34;USER\u0026#34; } ], \u0026#34;cluster\u0026#34;: \u0026#34;domain-c8\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;stc-cluster-vrf2\u0026#34;, \u0026#34;namespace_network\u0026#34;: { \u0026#34;network\u0026#34;: { \u0026#34;ingress_cidrs\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;10.13.54.0\u0026#34;, \u0026#34;prefix\u0026#34;: 24 } ], \u0026#34;load_balancer_size\u0026#34;: \u0026#34;SMALL\u0026#34;, \u0026#34;namespace_network_cidrs\u0026#34;: [ { \u0026#34;address\u0026#34;: \u0026#34;10.13.53.0\u0026#34;, \u0026#34;prefix\u0026#34;: 24 } ], \u0026#34;nsx_tier0_gateway\u0026#34;: \u0026#34;vrf-1\u0026#34;, \u0026#34;routed_mode\u0026#34;: true, \u0026#34;subnet_prefix_length\u0026#34;: 28 }, \u0026#34;network_provider\u0026#34;: \u0026#34;NSXT_CONTAINER_PLUGIN\u0026#34; } } Paste this into Postman (Body - Raw) and POST it to the following path https://{{vcenter-fqdn}}/api/vcenter/namespaces/instances and the new vSphere Namespace should be created in a jiff.\nAnd in vCenter our new Namespace:\nFor references to the APIs in vCenter and a whole lot of details and explanations have a look here!\n","date":"14 April 2023","externalUrl":null,"permalink":"/2023/04/14/tanzu-with-vsphere-and-different-tier-0s/","section":"Posts","summary":"This post will take you trough how to configre Tanzu with vSphere using NSX with multiple Tier-0 and VRF Tier-0 gateways. It will also show you how to do this using DCLI and API","title":"Tanzu with vSphere and different Tier-0s","type":"posts"},{"content":" Tanzu Kubernetes Grid # This post will go through how to deploy TKG 2.1, the management cluster, a workload cluster (or two), and the necessary preparations to be done on the underlaying infrastructure to support TKG 2.1. In this post I will use vSphere 8 with vSAN, Avi LoadBalancer, and NSX. So what we want to end up with it something like this:\nPreparations before deployment # This post will assume the following:\nvSphere is already installed configured. See more info here and here\nNSX has already been configured (see this post for how to configure NSX). Segments used for both Management cluster and Workload clusters should have DHCP server available. We dont need DHCP for Workload Cluster, but Management needs DHCP. NSX can provide DHCP server functionality for this use *\nNSX Advanced LoadBalancer has been deployed (and configured with a NSX cloud). See this post for how to configure this. **\nImport the VM template for TKG, see here\nA dedicated Linux machine/VM we can use as the bootstrap host, with the Tanzu CLI installed. See more info here\n(*) TKG 2.1 is not tied to NSX the same way as TKGs - So we can choose to use NSX for Security only or the full stack with networking and security. The built in NSX loadbalancer will not be used, I will use the NSX Advanced Loadbalancer (Avi)\n(**) I want to use the NSX cloud in Avi as it gives several benefits such as integration into the NSX manager where Avi automatically creates security groups, tags and services to easily be used in security policy creation and automatic \u0026ldquo;route plumbing\u0026rdquo; for the VIPs.\nTKG Management cluster - deployment # The first step after all the pre-requirements have been done is to prepare a bootstrap yaml for the management cluster. I will post an example file here and go through what the different fields means and why I have configured them and why I have uncommented some of them. Start by logging into the bootstrap machine, or if you decide to create the bootstrap yaml somewhere else go ahead but we need to copy it over to the bootstrap machine when we are ready to create the the management cluster.\nTo get started with a bootstrap yaml file we can either grab an example from here or in your bootstrap machine there is a folder which contains a default config you can start out with:\nandreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ ll total 120 drwxrwxr-x 18 andreasm andreasm 4096 Mar 24 09:10 ./ drwx------ 9 andreasm andreasm 4096 Mar 16 11:32 ../ drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 ako/ drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 bootstrap-kubeadm/ drwxrwxr-x 4 andreasm andreasm 4096 Mar 16 06:52 cert-manager/ drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 cluster-api/ -rw------- 1 andreasm andreasm 1293 Mar 16 06:52 config.yaml -rw------- 1 andreasm andreasm 32007 Mar 16 06:52 config_default.yaml drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 control-plane-kubeadm/ drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-aws/ drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-azure/ drwxrwxr-x 6 andreasm andreasm 4096 Mar 16 06:52 infrastructure-docker/ drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 infrastructure-ipam-in-cluster/ drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-oci/ drwxrwxr-x 4 andreasm andreasm 4096 Mar 16 06:52 infrastructure-tkg-service-vsphere/ drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-vsphere/ drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 kapp-controller-values/ -rwxrwxr-x 1 andreasm andreasm 64 Mar 16 06:52 providers.sha256sum* -rw------- 1 andreasm andreasm 0 Mar 16 06:52 v0.28.0 -rw------- 1 andreasm andreasm 747 Mar 16 06:52 vendir.lock.yml -rw------- 1 andreasm andreasm 903 Mar 16 06:52 vendir.yml drwxrwxr-x 8 andreasm andreasm 4096 Mar 16 06:52 ytt/ drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 yttcb/ drwxrwxr-x 7 andreasm andreasm 4096 Mar 16 06:52 yttcc/ andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ The file you should be looking for is called config_default.yaml . It could be a smart choice to use this as it will include the latest config parameters following the TKG version you have downloaded (Tanzu CLI).\nNow copy this file to a folder of preference and start to edit it. Below is a copy of an example I am using:\n#! --------------- #! Basic config #! ------------- CLUSTER_NAME: tkg-stc-mgmt-cluster #Name of the TKG mgmt cluster CLUSTER_PLAN: dev #Dev or Prod, defines the amount of control plane nodes of the mgmt cluster INFRASTRUCTURE_PROVIDER: vsphere #We are deploying on vSphere, could be AWS, Azure ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; #Customer Experience Improvement Program - set to true if you will participate ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; #Audit logging should be true in production environments CLUSTER_CIDR: 100.96.0.0/11 #Kubernetes Cluster CIDR SERVICE_CIDR: 100.64.0.0/13 #Kubernetes Services CIDR TKG_IP_FAMILY: ipv4 #ipv4 or ipv6 DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; #Yes to deploy standalone tkg mgmt cluster on vSphere #! --------------- #! vSphere config #! ------------- VSPHERE_DATACENTER: /cPod-NSXAM-STC #Name of vSphere Datacenter VSPHERE_DATASTORE: /cPod-NSXAM-STC/datastore/vsanDatastore #Name and path of vSphere datastore to be used VSPHERE_FOLDER: /cPod-NSXAM-STC/vm/TKGm #Name and path to VM folder VSPHERE_INSECURE: \u0026#34;false\u0026#34; #True if you dont want to verify vCenter thumprint below VSPHERE_NETWORK: /cPod-NSXAM-STC/network/ls-tkg-mgmt #A network portgroup (VDS or NSX Segment) for VM placement VSPHERE_CONTROL_PLANE_ENDPOINT: \u0026#34;\u0026#34; #Required if using Kube-Vip, I am using Avi Loadbalancer for this VSPHERE_PASSWORD: \u0026#34;password\u0026#34; #vCenter account password for account defined below VSPHERE_RESOURCE_POOL: /cPod-NSXAM-STC/host/Cluster/Resources #If you want to use a specific vSphere Resource Pool for the mgmt cluster. Leave it as is if not. VSPHERE_SERVER: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net #DNS record to vCenter Server VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa sdfgasdgadfgsdg sdfsdf@sdfsdf.net # your bootstrap machineSSH public key VSPHERE_TLS_THUMBPRINT: 22:FD # Your vCenter SHA1 Thumbprint VSPHERE_USERNAME: user@vspheresso/or/ad/user/domain #A user with the correct permissions #! --------------- #! Node config #! ------------- OS_ARCH: amd64 OS_NAME: ubuntu OS_VERSION: \u0026#34;20.04\u0026#34; VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; CONTROL_PLANE_MACHINE_COUNT: 1 WORKER_MACHINE_COUNT: 2 #! --------------- #! Avi config #! ------------- AVI_CA_DATA_B64: #Base64 of the Avi Certificate AVI_CLOUD_NAME: stc-nsx-cloud #Name of the cloud defined in Avi AVI_CONTROL_PLANE_HA_PROVIDER: \u0026#34;true\u0026#34; #True as we want to use Avi as K8s API endpoint AVI_CONTROLLER: 172.24.3.50 #IP or Hostname Avi controller or controller cluster # Network used to place workload clusters\u0026#39; endpoint VIPs - If you want to use a separate vip for Workload clusters Kubernetes API endpoint AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 #Corresponds with network defined in Avi AVI_CONTROL_PLANE_NETWORK_CIDR: 10.13.102.0/24 #Corresponds with network defined in Avi # Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) AVI_DATA_NETWORK: vip-tkg-wld-l7 #Corresponds with network defined in Avi AVI_DATA_NETWORK_CIDR: 10.13.103.0/24 #Corresponds with network defined in Avi # Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.13.101.0/24 #Corresponds with network defined in Avi AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 #Corresponds with network defined in Avi # Network used to place management clusters\u0026#39; endpoint VIPs AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 #Corresponds with network defined in Avi AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.13.100.0/24 #Corresponds with network defined in Avi AVI_NSXT_T1LR: /infra/tier-1s/Tier-1 #Path to the NSX T1 you have configured, click on three dots in NSX on the T1 to get the full path. AVI_CONTROLLER_VERSION: 22.1.2 #Latest supported version of Avi for TKG 2.1 AVI_ENABLE: \u0026#34;true\u0026#34; # Enables Avi as Loadbalancer for workloads AVI_LABELS: \u0026#34;\u0026#34; #When used Avi is enabled only workload cluster with corresponding label AVI_PASSWORD: \u0026#34;password\u0026#34; #Password for the account used in Avi, username defined below AVI_SERVICE_ENGINE_GROUP: stc-nsx #Service Engine group for Workload clusters if you want to have separate groups for Workload clusters and Management cluster AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: tkgm-se-group #Dedicated Service Engine group for management cluster AVI_USERNAME: admin AVI_DISABLE_STATIC_ROUTE_SYNC: true #Pod network reachable or not from the Avi Service Engines AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true #If you want to use AKO as default ingress controller, false if you plan to use other ingress controllers also. AVI_INGRESS_SHARD_VS_SIZE: SMALL #Decides the amount of shared vs pr ip. AVI_INGRESS_SERVICE_TYPE: NodePortLocal #NodePortLocal only when using Antrea, otherwise NodePort or ClusterIP AVI_CNI_PLUGIN: antrea #! --------------- #! Proxy config #! ------------- TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; #! --------------------------------------------------------------------- #! Antrea CNI configuration #! --------------------------------------------------------------------- # ANTREA_NO_SNAT: false # ANTREA_TRAFFIC_ENCAP_MODE: \u0026#34;encap\u0026#34; # ANTREA_PROXY: false # ANTREA_POLICY: true # ANTREA_TRACEFLOW: false ANTREA_NODEPORTLOCAL: true ANTREA_PROXY: true ANTREA_ENDPOINTSLICE: true ANTREA_POLICY: true ANTREA_TRACEFLOW: true ANTREA_NETWORKPOLICY_STATS: false ANTREA_EGRESS: true ANTREA_IPAM: false ANTREA_FLOWEXPORTER: false ANTREA_SERVICE_EXTERNALIP: false ANTREA_MULTICAST: false #! --------------------------------------------------------------------- #! Machine Health Check configuration #! --------------------------------------------------------------------- ENABLE_MHC: \u0026#34;true\u0026#34; ENABLE_MHC_CONTROL_PLANE: true ENABLE_MHC_WORKER_NODE: true MHC_UNKNOWN_STATUS_TIMEOUT: 5m MHC_FALSE_STATUS_TIMEOUT: 12m #! --------------------------------------------------------------------- #! Identity management configuration #! --------------------------------------------------------------------- IDENTITY_MANAGEMENT_TYPE: none #I have disabled this, use kubeconfig instead #LDAP_BIND_DN: CN=Andreas M,OU=Users,OU=GUZWARE,DC=guzware,DC=local #LDAP_BIND_PASSWORD: \u0026lt;encoded:UHNAc=\u0026gt; #LDAP_GROUP_SEARCH_BASE_DN: DC=guzware,DC=local #LDAP_GROUP_SEARCH_FILTER: (objectClass=group) #LDAP_GROUP_SEARCH_GROUP_ATTRIBUTE: member #LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn #LDAP_GROUP_SEARCH_USER_ATTRIBUTE: distinguishedName #LDAP_HOST: guzad07.guzware.local:636 #LDAP_ROOT_CA_DATA_B64: LS0tLS1CRUd #LDAP_USER_SEARCH_BASE_DN: DC=guzware,DC=local #LDAP_USER_SEARCH_FILTER: (objectClass=person) #LDAP_USER_SEARCH_NAME_ATTRIBUTE: uid #LDAP_USER_SEARCH_USERNAME: uid #OIDC_IDENTITY_PROVIDER_CLIENT_ID: \u0026#34;\u0026#34; #OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \u0026#34;\u0026#34; #OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM: \u0026#34;\u0026#34; #OIDC_IDENTITY_PROVIDER_ISSUER_URL: \u0026#34;\u0026#34; #OIDC_IDENTITY_PROVIDER_NAME: \u0026#34;\u0026#34; #OIDC_IDENTITY_PROVIDER_SCOPES: \u0026#34;\u0026#34; #OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \u0026#34;\u0026#34; For additional explanations of the different values see here\nWhen you feel you are ready with the bootstrap yaml file its time to deploy the management cluster. From your bootstrap machine where Tanzu CLI have been installed enter the following command:\ntanzu mc create --file path/to/cluster-config-file.yaml For more information around this process have a look here\nThe first thing that happens is some validation checks, if those pass it will continue to build a local bootstrap cluster on your bootstrap machine before building the TKG Management cluster in your vSphere cluster.\nNote! If you happen to use a an IP range within 172.16.0.0/12 on your computer you are accessing the bootstrap machine through you should edit the default Docker network. Otherwise you will loose connection to your bootstrap machine. This is done like this:\nAdd or edit, if it exists, the /etc/docker/daemon.json file with the following content:\n{ \u0026#34;default-address-pools\u0026#34;: [ {\u0026#34;base\u0026#34;:\u0026#34;192.168.0.0/16\u0026#34;,\u0026#34;size\u0026#34;:24} ] } Restart docker service or reboot the machine.\nNow back to the tanzu create process, you can monitor the progress from the terminal of your bootstrap machine, and you should after a while see machines being cloned from your template and powered on. In the Avi controller you should also see a new virtual service being created:\nThe ip address depicted above is the sole control plane node as I am deploying a TKG management cluster using plan dev. If the progress in your bootstrap machine indicates that it is done, you can check the status with the following command:\ntanzu mc get This will give you this output:\nNAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.9+vmware.1 management dev v1.24.9---vmware.1-tkg.1 Details: NAME READY SEVERITY REASON SINCE MESSAGE /tkg-stc-mgmt-cluster True 8d ├─ClusterInfrastructure - VSphereCluster/tkg-stc-mgmt-cluster-xw6xs True 8d ├─ControlPlane - KubeadmControlPlane/tkg-stc-mgmt-cluster-wrxtl True 8d │ └─Machine/tkg-stc-mgmt-cluster-wrxtl-gkv5m True 8d └─Workers └─MachineDeployment/tkg-stc-mgmt-cluster-md-0-vs9dc True 3d3h ├─Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-55c649d9fc-gnpz4 True 8d └─Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-55c649d9fc-gwfvt True 8d Providers: NAMESPACE NAME TYPE PROVIDERNAME VERSION WATCHNAMESPACE caip-in-cluster-system infrastructure-ipam-in-cluster InfrastructureProvider ipam-in-cluster v0.1.0 capi-kubeadm-bootstrap-system bootstrap-kubeadm BootstrapProvider kubeadm v1.2.8 capi-kubeadm-control-plane-system control-plane-kubeadm ControlPlaneProvider kubeadm v1.2.8 capi-system cluster-api CoreProvider cluster-api v1.2.8 capv-system infrastructure-vsphere InfrastructureProvider vsphere v1.5.1 When cluster is ready deployed and before we can access it with our kubectl cli tool we must set the context to it.\nkubectl config use-context my-mgmnt-cluster-admin@my-mgmnt-cluster But you probably have a dedicated workstation you want to acces the cluster from, then you can export the kubeconfig like this:\ntanzu mc kubeconfig get --admin --export-file MC-ADMIN-KUBECONFIG Now copy the file to your workstation and accessed the cluster from there.\nTip! Test out this tool to easy manage your Kubernetes configs: https://github.com/sunny0826/kubecm\nThe above is a really great tool:\namarqvardsen@amarqvards1MD6T:~$ kubecm switch --ui-size 10 Use the arrow keys to navigate: ↓ ↑ → ← and / toggles search Select Kube Context 😼 tkc-cluster-1(*) tkgs-cluster-1-admin@tkgs-cluster-1 wdc-2-tkc-cluster-1 10.13.200.2 andreasmk8slab-admin@andreasmk8slab-pinniped ns-wdc-3 tkc-cluster-1-routed tkg-mgmt-cluster-admin@tkg-mgmt-cluster stc-tkgm-mgmt-cluster ↓ tkg-wld-1-cluster-admin@tkg-wld-1-cluster --------- Info ---------- Name: tkc-cluster-1 Cluster: 10.13.202.1 User: wcp:10.13.202.1:andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net Now your TKG management cluster is ready and we can deploy a workload cluster.\nIf you noticed some warnings around conciliation during deployment, you can check whether they failed or not by issuing this command after you have gotten the kubeconfig context in place to the Management cluster with this command:\nandreasm@tkg-bootstrap:~$ kubectl get pkgi -A NAMESPACE NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE stc-tkgm-ns-1 stc-tkgm-wld-cluster-1-kapp-controller kapp-controller.tanzu.vmware.com 0.41.5+vmware.1-tkg.1 Reconcile succeeded 7d22h stc-tkgm-ns-2 stc-tkgm-wld-cluster-2-kapp-controller kapp-controller.tanzu.vmware.com 0.41.5+vmware.1-tkg.1 Reconcile succeeded 7d16h tkg-system ako-operator ako-operator-v2.tanzu.vmware.com 0.28.0+vmware.1-tkg.1-zshippable Reconcile succeeded 8d tkg-system tanzu-addons-manager addons-manager.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tanzu-auth tanzu-auth.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tanzu-cliplugins cliplugins.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tanzu-core-management-plugins core-management-plugins.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tanzu-featuregates featuregates.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tanzu-framework framework.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkg-clusterclass tkg-clusterclass.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkg-clusterclass-vsphere tkg-clusterclass-vsphere.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkg-pkg tkg.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-antrea antrea.tanzu.vmware.com 1.7.2+vmware.1-tkg.1-advanced Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-capabilities capabilities.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-load-balancer-and-ingress-service load-balancer-and-ingress-service.tanzu.vmware.com 1.8.2+vmware.1-tkg.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-metrics-server metrics-server.tanzu.vmware.com 0.6.2+vmware.1-tkg.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-pinniped pinniped.tanzu.vmware.com 0.12.1+vmware.2-tkg.3 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-secretgen-controller secretgen-controller.tanzu.vmware.com 0.11.2+vmware.1-tkg.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-tkg-storageclass tkg-storageclass.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-vsphere-cpi vsphere-cpi.tanzu.vmware.com 1.24.3+vmware.1-tkg.1 Reconcile succeeded 8d tkg-system tkg-stc-mgmt-cluster-vsphere-csi vsphere-csi.tanzu.vmware.com 2.6.2+vmware.2-tkg.1 Reconcile succeeded 8d tkg-system tkr-service tkr-service.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkr-source-controller tkr-source-controller.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d tkg-system tkr-vsphere-resolver tkr-vsphere-resolver.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d TKG Workload cluster deployment # Now that we have done all the initial configs to support our TKG environment on vSphere, NSX and Avi, to deploy a workload cluster is as simple as loading a game on the Commodore 64 \u0026#x1f4fc; From your bootstrap machine make sure you are in the context of your TKG Managment cluster:\nandreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ kubectl config current-context tkg-stc-mgmt-cluster-admin@tkg-stc-mgmt-cluster I you prefer to deploy your workload clusters in its own Kubernetes namespace go ahead and create a namespace for your workload cluster like this:\nkubectl create ns \u0026#34;name-of-namespace\u0026#34; Now to create a workload cluster, this also needs a yaml definition file. The easiest way to achieve such a file is to re-use the bootstramp yaml we created for our TKG Management cluster. For more information deploying a workload cluster in TKG read here.By using the Tanzu CLI we can convert this bootstrap file to a workload cluster yaml definiton file, this is done like this:\ntanzu cluster create stc-tkgm-wld-cluster-1 --namespace=stc-tkgm-ns-1 --file tkg-mgmt-bootstrap-tkg-2.1.yaml --dry-run \u0026gt; stc-tkg-wld-cluster-1.yaml The command above read the bootstrap yaml file we used to deploy the TKG management cluster, converts it into a yaml file we can use to deploy a workload cluster. It alse removes unnecessary fields not needed for our workload cluster. I am also using the \u0026ndash;namespace field to point the config to use the correct namespace and automatically put that into the yaml file. then I am pointing to the TKG Management bootstrap yaml file and finally the \u0026ndash;dry-run command to pipe it to a file called stc-tkg-wld-cluster-1.yaml. The result should look something like this:\napiVersion: cpi.tanzu.vmware.com/v1alpha1 kind: VSphereCPIConfig metadata: name: stc-tkgm-wld-cluster-1 namespace: stc-tkgm-ns-1 spec: vsphereCPI: ipFamily: ipv4 mode: vsphereCPI tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 --- apiVersion: csi.tanzu.vmware.com/v1alpha1 kind: VSphereCSIConfig metadata: name: stc-tkgm-wld-cluster-1 namespace: stc-tkgm-ns-1 spec: vsphereCSI: config: datacenter: /cPod-NSXAM-STC httpProxy: \u0026#34;\u0026#34; httpsProxy: \u0026#34;\u0026#34; noProxy: \u0026#34;\u0026#34; region: null tlsThumbprint: 22:FD useTopologyCategories: false zone: null mode: vsphereCSI --- apiVersion: run.tanzu.vmware.com/v1alpha3 kind: ClusterBootstrap metadata: annotations: tkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.24.9---vmware.1-tkg.1 name: stc-tkgm-wld-cluster-1 namespace: stc-tkgm-ns-1 spec: additionalPackages: - refName: metrics-server* - refName: secretgen-controller* - refName: pinniped* cpi: refName: vsphere-cpi* valuesFrom: providerRef: apiGroup: cpi.tanzu.vmware.com kind: VSphereCPIConfig name: stc-tkgm-wld-cluster-1 csi: refName: vsphere-csi* valuesFrom: providerRef: apiGroup: csi.tanzu.vmware.com kind: VSphereCSIConfig name: stc-tkgm-wld-cluster-1 kapp: refName: kapp-controller* --- apiVersion: v1 kind: Secret metadata: name: stc-tkgm-wld-cluster-1 namespace: stc-tkgm-ns-1 stringData: password: Password username: andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net --- apiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: annotations: osInfo: ubuntu,20.04,amd64 tkg/plan: dev labels: tkg.tanzu.vmware.com/cluster-name: stc-tkgm-wld-cluster-1 name: stc-tkgm-wld-cluster-1 namespace: stc-tkgm-ns-1 spec: clusterNetwork: pods: cidrBlocks: - 100.96.0.0/11 services: cidrBlocks: - 100.64.0.0/13 topology: class: tkg-vsphere-default-v1.0.0 controlPlane: metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu replicas: 1 variables: - name: controlPlaneCertificateRotation value: activate: true daysBefore: 90 - name: auditLogging value: enabled: false - name: podSecurityStandard value: audit: baseline deactivated: false warn: baseline - name: apiServerEndpoint value: \u0026#34;\u0026#34; - name: aviAPIServerHAProvider value: true - name: vcenter value: cloneMode: fullClone datacenter: /cPod-NSXAM-STC datastore: /cPod-NSXAM-STC/datastore/vsanDatastore folder: /cPod-NSXAM-STC/vm/TKGm network: /cPod-NSXAM-STC/network/ls-tkg-mgmt #Notice this - if you want to place your workload clusters in a different network change this to your desired portgroup. resourcePool: /cPod-NSXAM-STC/host/Cluster/Resources server: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net storagePolicyID: \u0026#34;\u0026#34; template: /cPod-NSXAM-STC/vm/ubuntu-2004-efi-kube-v1.24.9+vmware.1 tlsThumbprint: 22:FD - name: user value: sshAuthorizedKeys: - ssh-rsa 88qv2fowMT65qwpBHUIybHz5Ra2L53zwsv/5yvUej48QLmyAalSNNeH+FIKTkFiuX/WjsHiCIMFisn5dqpc/6x8= - name: controlPlane value: machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 - name: worker value: count: 2 machine: diskGiB: 20 memoryMiB: 4096 numCPUs: 2 version: v1.24.9+vmware.1 workers: machineDeployments: - class: tkg-worker metadata: annotations: run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu name: md-0 replicas: 2 Read through the result, edit if you find something you would like to change. If you want to deploy your workload cluster on a different network than your Management cluster edit this field to reflect the correct portgroup in vCenter:\nnetwork: /cPod-NSXAM-STC/network/ls-tkg-mgmt Now that the yaml defintion is ready we can create the first workload cluster like this:\ntanzu cluster create --file stc-tkg-wld-cluster-1.yaml You can monitor the progress from the terminal of your bootstrap machine. When done check your cluster status with Tanzu CLI (remember to either use -n \u0026ldquo;nameofnamespace\u0026rdquo; or just -A):\nandreasm@tkg-bootstrap:~$ tanzu cluster list -A NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 Further verifications can be done with this command:\nandreasm@tkg-bootstrap:~$ tanzu cluster get stc-tkgm-wld-cluster-1 -n stc-tkgm-ns-1 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES TKR stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; v1.24.9---vmware.1-tkg.1 Details: NAME READY SEVERITY REASON SINCE MESSAGE /stc-tkgm-wld-cluster-1 True 7d22h ├─ClusterInfrastructure - VSphereCluster/stc-tkgm-wld-cluster-1-lzjxq True 7d22h ├─ControlPlane - KubeadmControlPlane/stc-tkgm-wld-cluster-1-22z8x True 7d22h │ └─Machine/stc-tkgm-wld-cluster-1-22z8x-jjb66 True 7d22h └─Workers └─MachineDeployment/stc-tkgm-wld-cluster-1-md-0-2qmkw True 3d3h ├─Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-6c4789d7b5-lj5wl True 7d22h └─Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-6c4789d7b5-wb7k9 True 7d22h If everything is green its time to get the kubeconfig for the cluster so we can start consume it. This is done like this:\ntanzu cluster kubeconfig get stc-tkgm-wld-cluster-1 --namespace stc-tkgm-ns-1 --admin --export-file stc-tkgm-wld-cluster-1-k8s-config.yaml Now you can copy this to your preferred workstation and start consuming.\nNote! The kubeconfigs I have used here is all admin privileges and is not something you will use in production where you want to have granular user access. I will create a post around user management in both TKGm and TKGs later.\nThe next sections will cover how to upgrade TKG, some configs on the workload clusters themselves around AKO and Antrea.\nAntrea configs # If there is a feature you would like to enable in Antrea in one of your workload clusters, we need to create an AntreaConfig by using the AntreaConfig CRD (this is one way of doing it) and apply it on the Namespace where your workload cluster resides. This is the same approach as we do in vSphere 8 with Tanzu - see here\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: stc-tkgm-wld-cluster-1-antrea-package # notice the naming-convention cluster name-antrea-package namespace: stc-tkgm-ns-1 # your vSphere Namespace the TKC cluster is in. spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: true Egress: true NodePortLocal: true AntreaTraceflow: true NetworkPolicyStats: true Avi/AKO configs # In TKGm we can override the default AKO settings by using AKODeploymentConfig CRD. We apply this configuration from the TKG Managment cluster on the respective Workload cluster by using labels. An example of such a config yaml:\napiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 kind: AKODeploymentConfig metadata: name: ako-stc-tkgm-wld-cluster-1 spec: adminCredentialRef: name: avi-controller-credentials namespace: tkg-system-networking certificateAuthorityRef: name: avi-controller-ca namespace: tkg-system-networking cloudName: stc-nsx-cloud clusterSelector: matchLabels: ako-stc-wld-1: \u0026#34;ako-l7\u0026#34; controller: 172.24.3.50 dataNetwork: cidr: 10.13.103.0/24 name: vip-tkg-wld-l7 controlPlaneNetwork: cidr: 10.13.102.0/24 name: vip-tkg-wld-l4 extraConfigs: cniPlugin: antrea disableStaticRouteSync: false # required ingress: defaultIngressController: true disableIngressClass: false # required nodeNetworkList: # required - cidrs: - 10.13.21.0/24 networkName: ls-tkg-wld-1 serviceType: NodePortLocal # required shardVSSize: SMALL # required l4Config: autoFQDN: default networksConfig: nsxtT1LR: /infra/tier-1s/Tier-1 serviceEngineGroup: tkgm-se-group Notice the:\nclusterSelector: matchLabels: ako-stc-wld-1: \u0026#34;ako-l7\u0026#34; We need to apply this label to our workload cluster. From the TKG management cluster list all your clusters:\namarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get cluster -A NAMESPACE NAME PHASE AGE VERSION stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Provisioned 7d23h v1.24.9+vmware.1 stc-tkgm-ns-2 stc-tkgm-wld-cluster-2 Provisioned 7d17h v1.24.9+vmware.1 tkg-system tkg-stc-mgmt-cluster Provisioned 8d v1.24.9+vmware.1 Apply the above label:\nkubectl label cluster -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 ako-stc-wld-1=ako-l7 Now run the get cluster command again but with the value \u0026ndash;show-labels to see if it has been applied:\namarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get cluster -A --show-labels NAMESPACE NAME PHASE AGE VERSION LABELS stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Provisioned 7d23h v1.24.9+vmware.1 ako-stc-wld-1=ako-l7,cluster.x-k8s.io/cluster-name=stc-tkgm-wld-cluster-1,networking.tkg.tanzu.vmware.com/avi=ako-stc-tkgm-wld-cluster-1,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.1,tkg.tanzu.vmware.com/cluster-name=stc-tkgm-wld-cluster-1,topology.cluster.x-k8s.io/owned= Looks good. Then we can apply the AKODeploymentConfig above.\nk apply -f ako-wld-cluster-1.yaml Verify if the AKODeploymentConfig has been applied:\namarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get akodeploymentconfigs.networking.tkg.tanzu.vmware.com NAME AGE ako-stc-tkgm-wld-cluster-1 7d21h ako-stc-tkgm-wld-cluster-2 7d6h install-ako-for-all 8d install-ako-for-management-cluster 8d Now head back your workload cluster and check the AKO pod whether it has been restarted, if you dont want to wait you can always delete the pod to speed up the changes. To verify the changes have a look at the ako configmap like this:\namarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get configmaps -n avi-system avi-k8s-config -oyaml apiVersion: v1 data: apiServerPort: \u0026#34;8080\u0026#34; autoFQDN: default cloudName: stc-nsx-cloud clusterName: stc-tkgm-ns-1-stc-tkgm-wld-cluster-1 cniPlugin: antrea controllerIP: 172.24.3.50 controllerVersion: 22.1.2 defaultIngController: \u0026#34;true\u0026#34; deleteConfig: \u0026#34;false\u0026#34; disableStaticRouteSync: \u0026#34;false\u0026#34; fullSyncFrequency: \u0026#34;1800\u0026#34; logLevel: INFO nodeNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-wld-1\u0026#34;,\u0026#34;cidrs\u0026#34;:[\u0026#34;10.13.21.0/24\u0026#34;]}]\u0026#39; nsxtT1LR: /infra/tier-1s/Tier-1 serviceEngineGroupName: tkgm-se-group serviceType: NodePortLocal shardVSSize: SMALL vipNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;vip-tkg-wld-l7\u0026#34;,\u0026#34;cidr\u0026#34;:\u0026#34;10.13.103.0/24\u0026#34;}]\u0026#39; kind: ConfigMap metadata: annotations: kapp.k14s.io/identity: v1;avi-system//ConfigMap/avi-k8s-config;v1 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;apiServerPort\u0026#34;:\u0026#34;8080\u0026#34;,\u0026#34;autoFQDN\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterName\u0026#34;:\u0026#34;stc-tkgm-ns-1-stc-tkgm-wld-cluster-1\u0026#34;,\u0026#34;cniPlugin\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;controllerIP\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;controllerVersion\u0026#34;:\u0026#34;22.1.2\u0026#34;,\u0026#34;defaultIngController\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;deleteConfig\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;disableStaticRouteSync\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;fullSyncFrequency\u0026#34;:\u0026#34;1800\u0026#34;,\u0026#34;logLevel\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;nodeNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;ls-tkg-wld-1\\\u0026#34;,\\\u0026#34;cidrs\\\u0026#34;:[\\\u0026#34;10.13.21.0/24\\\u0026#34;]}]\u0026#34;,\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;/infra/tier-1s/Tier-1\u0026#34;,\u0026#34;serviceEngineGroupName\u0026#34;:\u0026#34;tkgm-se-group\u0026#34;,\u0026#34;serviceType\u0026#34;:\u0026#34;NodePortLocal\u0026#34;,\u0026#34;shardVSSize\u0026#34;:\u0026#34;SMALL\u0026#34;,\u0026#34;vipNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;vip-tkg-wld-l7\\\u0026#34;,\\\u0026#34;cidr\\\u0026#34;:\\\u0026#34;10.13.103.0/24\\\u0026#34;}]\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1678977773033139694\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.ae838cced3b6caccc5a03bfb3ae65cd7\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;avi-k8s-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;avi-system\u0026#34;}}\u0026#39; kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 creationTimestamp: \u0026#34;2023-03-16T14:43:11Z\u0026#34; labels: kapp.k14s.io/app: \u0026#34;1678977773033139694\u0026#34; kapp.k14s.io/association: v1.ae838cced3b6caccc5a03bfb3ae65cd7 name: avi-k8s-config namespace: avi-system resourceVersion: \u0026#34;19561\u0026#34; uid: 1baa90b2-e5d7-4177-ae34-6c558b5cfe29 It should reflect the changes we applied\u0026hellip;\nAntrea RBAC # Antrea comes with a list of Tiers where we can place our Antrea Native Policies. These can also be used to restrict who is allowed to apply policies and not. See this page for more information for now. I will update this section later with my own details - including the integration with NSX.\nUpgrade TKG (from 2.1 to 2.1.1) # When a new TKG relase is available we can upgrade to use this new release. The steps I have followed are explained in detail here. I recommend to always follow the updated information there.\nTo upgrade TKG these are the typical steps:\nDownload the latest Tanzu CLI - from my.vmware.com Download the latest Tanzu kubectl - from my.vmware.com Download the latest Photon or Ubuntu OVA VM template - from my.vmware.com Upgrade the TKG Management cluster Upgrade the TKG Workload clusters So lets get into it.\nUpgrade CLI tools and dependencies # I have already downloaded the Ubuntu VM image for version 2.1.1 into my vCenter and converted it to a template. I have also downloaded the Tanzu CLI tools and Tanzu kubectl for version 2.1.1. Now I need to install the Tanzu CLI and Tanzu kubectl. So I will getting back into my bootstrap machine used previously where I already have Tanzu CLI 2.1 installed.\nThe first thing I need to is to delete the following file:\n~/.config/tanzu/tkg/compatibility/tkg-compatibility.yaml Extract the downloaded Tanzu CLI 2.1.1 packages (this will create a cli folder where you are placed. So if you want to use another folder create this first and extract the file in there) :\ntar -xvf tanzu-cli-bundle-linux-amd64.tar.gz andreasm@tkg-bootstrap:~/tanzu$ tar -xvf tanzu-cli-bundle-linux-amd64.2.1.1.tar.gz cli/ cli/core/ cli/core/v0.28.1/ cli/core/v0.28.1/tanzu-core-linux_amd64 cli/tanzu-framework-plugins-standalone-linux-amd64.tar.gz cli/tanzu-framework-plugins-context-linux-amd64.tar.gz cli/ytt-linux-amd64-v0.43.1+vmware.1.gz cli/kapp-linux-amd64-v0.53.2+vmware.1.gz cli/imgpkg-linux-amd64-v0.31.1+vmware.1.gz cli/kbld-linux-amd64-v0.35.1+vmware.1.gz cli/vendir-linux-amd64-v0.30.1+vmware.1.gz Navigate to the cli folder and install the different packages.\nInstall Tanzu CLI:\nandreasm@tkg-bootstrap:~/tanzu/cli$ sudo install core/v0.28.1/tanzu-core-linux_amd64 /usr/local/bin/tanzu Initialize the Tanzu CLI:\nandreasm@tkg-bootstrap:~/tanzu/cli$ tanzu init ℹ Checking for required plugins... ℹ Installing plugin \u0026#39;secret:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;login:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;management-cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;telemetry:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Successfully installed all required plugins ✔ successfully initialized CLI Verify version:\nandreasm@tkg-bootstrap:~/tanzu/cli$ tanzu version version: v0.28.1 buildDate: 2023-03-07 sha: 0e6704777-dirty Now the Tanzu plugins:\nandreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin clean ✔ successfully cleaned up all plugins andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin sync ℹ Checking for required plugins... ℹ Installing plugin \u0026#39;management-cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;secret:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;telemetry:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;cluster:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;kubernetes-release:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;login:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;feature:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; ✖ [unable to fetch the plugin metadata for plugin \u0026#34;login\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;package\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;pinniped-auth\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;isolated-cluster\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64] andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin sync ℹ Checking for required plugins... ℹ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;login:v0.28.1\u0026#39; ℹ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Successfully installed all required plugins ✔ Done Note! I had to run the comand twice as I ecountered an issue on first try. Now list the plugins:\nandreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin list Standalone Plugins NAME DESCRIPTION TARGET DISCOVERY VERSION STATUS isolated-cluster isolated-cluster operations default v0.28.1 installed login Login to the platform default v0.28.1 installed pinniped-auth Pinniped authentication operations (usually not directly invoked) default v0.28.1 installed management-cluster Kubernetes management-cluster operations kubernetes default v0.28.1 installed package Tanzu package management kubernetes default v0.28.1 installed secret Tanzu secret management kubernetes default v0.28.1 installed telemetry Configure cluster-wide telemetry settings kubernetes default v0.28.1 installed Plugins from Context: tkg-stc-mgmt-cluster NAME DESCRIPTION TARGET VERSION STATUS cluster Kubernetes cluster operations kubernetes v0.28.0 installed feature Operate on features and featuregates kubernetes v0.28.0 installed kubernetes-release Kubernetes release operations kubernetes v0.28.0 installed Install the Tanzu kubectl:\nandreasm@tkg-bootstrap:~/tanzu$ gunzip kubectl-linux-v1.24.10+vmware.1.gz andreasm@tkg-bootstrap:~/tanzu$ chmod ugo+x kubectl-linux-v1.24.10+vmware.1 andreasm@tkg-bootstrap:~/tanzu$ sudo install kubectl-linux-v1.24.10+vmware.1 /usr/local/bin/kubectl Check version:\nandreasm@tkg-bootstrap:~/tanzu$ kubectl version WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.10+vmware.1\u0026#34;, GitCommit:\u0026#34;b980a736cbd2ac0c5f7ca793122fd4231f705889\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-01-24T15:36:34Z\u0026#34;, GoVersion:\u0026#34;go1.19.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Kustomize Version: v4.5.4 Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.9+vmware.1\u0026#34;, GitCommit:\u0026#34;d1d7c19c9b6265a8dcd1b2ab2620ec0fc7cee784\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-12-14T06:23:39Z\u0026#34;, GoVersion:\u0026#34;go1.18.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Install the Carvel tools. From the cli folder first out is ytt. Install ytt:\nandreasm@tkg-bootstrap:~/tanzu/cli$ gunzip ytt-linux-amd64-v0.43.1+vmware.1.gz andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x ytt-linux-amd64-v0.43.1+vmware.1 andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./ytt-linux-amd64-v0.43.1+vmware.1 /usr/local/bin/ytt andreasm@tkg-bootstrap:~/tanzu/cli$ ytt --version ytt version 0.43.1 Instal kapp:\nandreasm@tkg-bootstrap:~/tanzu/cli$ gunzip kapp-linux-amd64-v0.53.2+vmware.1.gz andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x kapp-linux-amd64-v0.53.2+vmware.1 andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./kapp-linux-amd64-v0.53.2+vmware.1 /usr/local/bin/kapp andreasm@tkg-bootstrap:~/tanzu/cli$ kapp --version kapp version 0.53.2 Succeeded Install kbld:\nandreasm@tkg-bootstrap:~/tanzu/cli$ gunzip kbld-linux-amd64-v0.35.1+vmware.1.gz andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x kbld-linux-amd64-v0.35.1+vmware.1 andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./kbld-linux-amd64-v0.35.1+vmware.1 /usr/local/bin/kbld andreasm@tkg-bootstrap:~/tanzu/cli$ kbld --version kbld version 0.35.1 Succeeded Install imgpkg:\nandreasm@tkg-bootstrap:~/tanzu/cli$ gunzip imgpkg-linux-amd64-v0.31.1+vmware.1.gz andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x imgpkg-linux-amd64-v0.31.1+vmware.1 andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./imgpkg-linux-amd64-v0.31.1+vmware.1 /usr/local/bin/imgpkg andreasm@tkg-bootstrap:~/tanzu/cli$ imgpkg --version imgpkg version 0.31.1 Succeeded We have done the verification of the different versions, but we should have Tanzu cli version v0.28.1\nUpgrade the TKG Management cluster # Now we can proceed with the upgrade process. One important document to check is this! Known Issues\u0026hellip; Check whether you are using environments, if you happen to use them we need to unset them.\nandreasm@tkg-bootstrap:~/tanzu/cli$ printenv I am clear here and will now start the upgrading of my standalone TKG Management cluster Make sure you are in the context of the TKG management cluster and that you have converted the new Ubuntu VM image as template.\nandreasm@tkg-bootstrap:~$ kubectl config current-context tkg-stc-mgmt-cluster-admin@tkg-stc-mgmt-cluster If not, use the following command:\nandreasm@tkg-bootstrap:~$ tanzu login ? Select a server [Use arrows to move, type to filter] \u0026gt; tkg-stc-mgmt-cluster() + new server andreasm@tkg-bootstrap:~$ tanzu login ? Select a server tkg-stc-mgmt-cluster() ✔ successfully logged in to management cluster using the kubeconfig tkg-stc-mgmt-cluster ℹ Checking for required plugins... ℹ All required plugins are already installed and up-to-date Here goes: (To start the upgrade of the management cluster)\nandreasm@tkg-bootstrap:~$ tanzu mc upgrade Upgrading management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; to TKG version \u0026#39;v2.1.1\u0026#39; with Kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;. Are you sure? [y/N]: Eh\u0026hellip;. yes\u0026hellip;\nProgress:\nandreasm@tkg-bootstrap:~$ tanzu mc upgrade Upgrading management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; to TKG version \u0026#39;v2.1.1\u0026#39; with Kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;. Are you sure? [y/N]: y Validating the compatibility before management cluster upgrade Validating for the required environment variables to be set Validating for the user configuration secret to be existed in the cluster Warning: unable to find component \u0026#39;kube_rbac_proxy\u0026#39; under BoM Upgrading management cluster providers... infrastructure-ipam-in-cluster provider\u0026#39;s version is missing in BOM file, so it would not be upgraded Checking cert-manager version... Cert-manager is already up to date Performing upgrade... Scaling down Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-system\u0026#34; Scaling down Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; Scaling down Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; Scaling down Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capv-system\u0026#34; Deleting Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-system\u0026#34; Installing Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-system\u0026#34; Deleting Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; Installing Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; Deleting Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; Installing Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; Deleting Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capv-system\u0026#34; Installing Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;v1.5.3\u0026#34; TargetNamespace=\u0026#34;capv-system\u0026#34; Management cluster providers upgraded successfully... Preparing addons manager for upgrade Upgrading kapp-controller... Adding last-applied annotation on kapp-controller... Removing old management components... Upgrading management components... ℹ Updating package repository \u0026#39;tanzu-management\u0026#39; ℹ Getting package repository \u0026#39;tanzu-management\u0026#39; ℹ Validating provided settings for the package repository ℹ Updating package repository resource ℹ Waiting for \u0026#39;PackageRepository\u0026#39; reconciliation for \u0026#39;tanzu-management\u0026#39; ℹ \u0026#39;PackageRepository\u0026#39; resource install status: Reconciling ℹ \u0026#39;PackageRepository\u0026#39; resource install status: ReconcileSucceeded ℹ Updated package repository \u0026#39;tanzu-management\u0026#39; in namespace \u0026#39;tkg-system\u0026#39; ℹ Installing package \u0026#39;tkg.tanzu.vmware.com\u0026#39; ℹ Updating package \u0026#39;tkg-pkg\u0026#39; ℹ Getting package install for \u0026#39;tkg-pkg\u0026#39; ℹ Getting package metadata for \u0026#39;tkg.tanzu.vmware.com\u0026#39; ℹ Updating secret \u0026#39;tkg-pkg-tkg-system-values\u0026#39; ℹ Updating package install for \u0026#39;tkg-pkg\u0026#39; ℹ Waiting for \u0026#39;PackageInstall\u0026#39; reconciliation for \u0026#39;tkg-pkg\u0026#39; ℹ \u0026#39;PackageInstall\u0026#39; resource install status: ReconcileSucceeded ℹ Updated installed package \u0026#39;tkg-pkg\u0026#39; Cleanup core packages repository... Core package repository not found, no need to cleanup Upgrading management cluster kubernetes version... Upgrading kubernetes cluster to `v1.24.10+vmware.1` version, tkr version: `v1.24.10+vmware.1-tkg.2` Waiting for kubernetes version to be updated for control plane nodes... Waiting for kubernetes version to be updated for worker nodes... In vCenter we should start see some action also:\nTwo control plane nodes:\nNo longer:\nmanagement cluster is opted out of telemetry - skipping telemetry image upgrade Creating tkg-bom versioned ConfigMaps... Management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; successfully upgraded to TKG version \u0026#39;v2.1.1\u0026#39; with kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39; ℹ Checking for required plugins... ℹ Installing plugin \u0026#39;kubernetes-release:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Installing plugin \u0026#39;feature:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; ℹ Successfully installed all required plugins Well, it finished successfully.\nLets verify with Tanzu CLI:\nandreasm@tkg-bootstrap:~$ tanzu cluster list --include-management-cluster -A NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Looks good, notice the different versions. Management cluster is upgraded to latest version, workload clusters are still on its older version. They are up next.\nLets do a last check before we head to Workload cluster upgrade.\nandreasm@tkg-bootstrap:~$ tanzu mc get NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Details: NAME READY SEVERITY REASON SINCE MESSAGE /tkg-stc-mgmt-cluster True 17m ├─ClusterInfrastructure - VSphereCluster/tkg-stc-mgmt-cluster-xw6xs True 8d ├─ControlPlane - KubeadmControlPlane/tkg-stc-mgmt-cluster-wrxtl True 17m │ └─Machine/tkg-stc-mgmt-cluster-wrxtl-csrnt True 24m └─Workers └─MachineDeployment/tkg-stc-mgmt-cluster-md-0-vs9dc True 10m ├─Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-54554f9575-7hdfc True 14m └─Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-54554f9575-ng9lx True 7m4s Providers: NAMESPACE NAME TYPE PROVIDERNAME VERSION WATCHNAMESPACE caip-in-cluster-system infrastructure-ipam-in-cluster InfrastructureProvider ipam-in-cluster v0.1.0 capi-kubeadm-bootstrap-system bootstrap-kubeadm BootstrapProvider kubeadm v1.2.8 capi-kubeadm-control-plane-system control-plane-kubeadm ControlPlaneProvider kubeadm v1.2.8 capi-system cluster-api CoreProvider cluster-api v1.2.8 capv-system infrastructure-vsphere InfrastructureProvider vsphere v1.5.3 Congrats, head over to next level \u0026#x1f604;\nUpgrade workload cluster # This procedure is much simpler, almost as simple as starting a game in MS-DOS 6.2 requiring a bit over 600kb convential memory. Make sure your are still in the TKG Management cluster context.\nAs done above list out the cluster you have and notice the versions they are on now.:\nandreasm@tkg-bootstrap:~$ tanzu cluster list --include-management-cluster -A NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Check if there are any new releases available from the management cluster:\nandreasm@tkg-bootstrap:~$ tanzu kubernetes-release get NAME VERSION COMPATIBLE ACTIVE UPDATES AVAILABLE v1.22.17---vmware.1-tkg.2 v1.22.17+vmware.1-tkg.2 True True v1.23.16---vmware.1-tkg.2 v1.23.16+vmware.1-tkg.2 True True v1.24.10---vmware.1-tkg.2 v1.24.10+vmware.1-tkg.2 True True There is one there.. v1.24.10 and its compatible.\nLets check whether there are any updates ready for our workload cluster:\nandreasm@tkg-bootstrap:~$ tanzu cluster available-upgrades get -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 NAME VERSION COMPATIBLE v1.24.10---vmware.1-tkg.2 v1.24.10+vmware.1-tkg.2 True It is\u0026hellip;\nLets upgrade it:\nandreasm@tkg-bootstrap:~$ tanzu cluster upgrade -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Upgrading workload cluster \u0026#39;stc-tkgm-wld-cluster-1\u0026#39; to kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;, tkr version \u0026#39;v1.24.10+vmware.1-tkg.2\u0026#39;. Are you sure? [y/N]: y Upgrading kubernetes cluster to `v1.24.10+vmware.1` version, tkr version: `v1.24.10+vmware.1-tkg.2` Waiting for kubernetes version to be updated for control plane nodes... y for YES\nSit back and wait for the upgrade process is to do its thing. You can monitor the output from the current terminal, and if something is happening in vCenter. Clone operations, power on, power off and delete.\nAnd the result is in:\nWaiting for kubernetes version to be updated for worker nodes... Cluster \u0026#39;stc-tkgm-wld-cluster-1\u0026#39; successfully upgraded to kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39; We have a winner.\nLets quickly check with Tanzu CLI:\nandreasm@tkg-bootstrap:~$ tanzu cluster get stc-tkgm-wld-cluster-1 -n stc-tkgm-ns-1 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES TKR stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.10+vmware.1 \u0026lt;none\u0026gt; v1.24.10---vmware.1-tkg.2 Details: NAME READY SEVERITY REASON SINCE MESSAGE /stc-tkgm-wld-cluster-1 True 11m ├─ClusterInfrastructure - VSphereCluster/stc-tkgm-wld-cluster-1-lzjxq True 8d ├─ControlPlane - KubeadmControlPlane/stc-tkgm-wld-cluster-1-22z8x True 11m │ └─Machine/stc-tkgm-wld-cluster-1-22z8x-mtpgs True 15m └─Workers └─MachineDeployment/stc-tkgm-wld-cluster-1-md-0-2qmkw True 39m ├─Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-58c5764865-7xvfn True 8m31s └─Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-58c5764865-c7rqj True 3m29s Couldn\u0026rsquo;t be better. Thats it then. Its Friday so have a great weekend and thanks for reading.\n","date":"22 March 2023","externalUrl":null,"permalink":"/2023/03/22/tanzu-kubernetes-grid-2.1/","section":"Posts","summary":"Tanzu Kubernetes Grid # This post will go through how to deploy TKG 2.","title":"Tanzu Kubernetes Grid 2.1","type":"posts"},{"content":"","date":"22 March 2023","externalUrl":null,"permalink":"/tags/tkg-2.1/","section":"Tags","summary":"","title":"TKG 2.1","type":"tags"},{"content":"","date":"4 March 2023","externalUrl":null,"permalink":"/categories/blog/","section":"Categories","summary":"","title":"Blog","type":"categories"},{"content":"","date":"4 March 2023","externalUrl":null,"permalink":"/tags/github/","section":"Tags","summary":"","title":"Github","type":"tags"},{"content":"","date":"4 March 2023","externalUrl":null,"permalink":"/categories/github/","section":"Categories","summary":"","title":"Github","type":"categories"},{"content":" Hugo on Github: # I have been running my blog page locally on Kubernetes for a long time now. And it has worked very well. But one always have to try something new, and I have always wanted to explore the option to host it on Github to have one maintenance task less to worry about. To get started with this I got some absolutely great help from my colleague Robert who put me into the right track to get this project rolling. In short this post will cover how I did it (with the help from Robert). The moving parts used in this post is Github, Github Pages, Hugo, git, git submodules, a DNS record for my custom domain name and a Linux terminal with git installed. In Github we will end up with two repositories, one for the Hugo files themselves, and one which will be used our Github Page (the actual webpage of your blog). The goal is to be able to add content and update your blog with just a few commands and it is live. Preparations in Github # In my Github account I create two repositories, one for the \u0026ldquo;Hugo\u0026rdquo; contents itself (config, themes, contents etc) and one repository which will host the actual Github page itself. Lets dive into the details \u0026#x1f604;\nIf not already a Github user, head over to Github.com and create yourself a user account. When logged into your Github account, create two repositories:\nOne repository is where you have all your Hugo files, content, posts, pages, config folders, css, archetypes and public folder when generating your pages. This repository is created like this in Github:\nAs this is a free Github account the only option is a Public repository.\nNow the second repositiory is created identically but with one important difference, the repository name. This has to start with your username and the github domain github.io (discard the red warning in example below, I already have my repository created using same name). This repository will be used to host your blog\u0026rsquo;s frontpage/webpage. This is referred to as Github Pages\nNext we need to clone into our two newly created repositories.\nGit # To clone a public repo there is no need to authenticate, but you would like to create your content locally and push them to your remote git repo so we need to authenticate to our Github account. Github dont use password and username for git authentication, but instead SSH keys. And when cloning into your repo one need to use the correct way to clone it for the authentication to work. More on that later. First prepare your git environment on your workstation.\nSSH keys # On your workstation generate your SSH keys (if not already done):\nssh-keygen -t rsa -b 4096 -C \u0026#34;andreasm@ubuntulaptop\u0026#34; Answer default to prompts, enter a desired passphrase if wanted leave empty without any passphrase.\nNow that the SSH keys as generated copy the content from the ~/.ssh/id_rsa.pub by issuing something like this cat ~/.ssh/id_rsa.pub and copy the wole content. Go into your Github account click on your user top right corner and -\u0026gt; settings\nThen SSH and GPT keys:\nAnd then New SSH Key and paste your SSH key. Give it a name.\nNow your Github account can authenticate your workstation by using its public SSH key. To use this approach one have to clone into a project by using git clone git@github.com:andreasm80/blog.local.git where the git@github.com:andreasm80/blog.local.git is found from your repository by clicking at the green Code on your repo:\nGit - continued # Now that the SSH keys are configured, our workstation is prepared to authenticate against our Github repositories. Create a folder in your Linux workstation, called something with github or whatever you like. Enter into that folder. Now enter the following command:\ngit clone git@github.com:andreasm80/blog.local.git If you dont specify a folder at the end of the command, git will just create a folder with the same name as the repository your are cloning. If you dont want that add a folder name at the end like this:\ngit clone git@github.com:andreasm80/blog.local.git localfoldername Now enter into the newly created folder: cd blog.local To verify that you are \u0026ldquo;linked\u0026rdquo; to the correct repository run the following command:\ngit remote -vvv origin\tgit@github.com:andreasm80/blog.local.git (fetch) origin\tgit@github.com:andreasm80/blog.local.git (push) I already have a Hugo \u0026ldquo;project\u0026rdquo; folder where I have my blog page content stored. Instead of just re-using this folder as is I copy all the content to this new folder. Then I delete the public folder (in the new folder) as this will be recreated later. Before pushing all the newly copied content into the cloned github folder above I need to do some preparation for git.\ngit config --global user.email \u0026#34;email@email.com\u0026#34; #is used to sign your commits git config --global user.name \u0026#34;AndreasM\u0026#34; #is used to sign your commits - can be whatever name you want Then I need to tell git which files it should track, commit and push when I am ready. To do this type in:\ngit add . #notice the \u0026#34;.\u0026#34; This means I will add all files in the folder. If you dont tell git this it will not commit and push them. You can check this with the following command:\nandreasm@ubuntu:~/git status On branch main Your branch is up to date with \u0026#39;origin/main\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: public (new commits) Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) content/post/2023-03-04-running-hugo-on-github/ no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Now you can commit by doing this command:\ngit commit -s -m \u0026#34;comment-description\u0026#34; # the -s is for signoff and the -m is the comment/message And the last thing to do now is to push the files locally to your remote github repository.\ngit push If you go into your Github page now you will see all your files there in the repo above, but still there is no working blog-page yet.\nGit submodules # As explained above, we need to create some kind of \u0026ldquo;softlink\u0026rdquo; for the folder public to point to the repository we will use as our web-page. In git we can use submodules for that. The reason for that is each time you generate your Hugo content, Hugo will create a public folder which contains the actual HTML files. And instead of copying the content of this folder each time you generate your webpage we link this folder to our Github Page repository. This is how to enable git submodules.\nWhile still in your blog.local folder (root of the first repository) enter the following:\ngit submodule add git@github.com:andreasm80/andreasm80.github.io.git public/ #the url is from my second repo being used for Github Pages With the above command I am instructing git to create a submodule point to my remote Github repository I will use as my Github Page and also pointing it to the local folder public.\nThen the next command is to initialize the submodule:\ngit submodule init #or a specific module as below git submodule init public/ In the current folder we should have file called .gitmodules have a look inside and it should contain something like this:\n[submodule \u0026#34;public\u0026#34;] path = public url = git@github.com:andreasm80/andreasm80.github.io.git This is how my folder structure looks like now:\ngithub/blog.local archetypes assets config content .git .gitmodules go.mod go.sum .hugo_build.lock layouts LICENSE public #this folder was created by the git submodules command README.md resources static Check the status on the submodule you have created:\ngit submodule status +6afb3af12e86416ad8ff255d9042d89bd9ddc719 public (heads/master) # status symbols infront, sha, name of submodule and branch Commit the changes we have done so far in our blog.local repo:\ngit add . git commit -s -m \u0026#34;added submodule public\u0026#34; git push Head over to your Github blog.local repo and you should see that the public folder is quite different from the others:\nAnd if you click on it you will be redirected to the repository of your second repository used for your Github Page.\nNow back to the CLI terminal\u0026hellip;\nTo have something to populate the public folder with we can now run the `hugo -v -D\u0026rsquo; command to generate our webpage. It will write everything needed in our public folder.\nhugo -v -D Now cd into the public folder. Check the repo it is pointing to:\ngit remote -vvv origin\tgit@github.com:andreasm80/andreasm80.github.io.git (fetch) origin\tgit@github.com:andreasm80/andreasm80.github.io.git (push) Check if there is something new or untracked here:\ngit status Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 2023/03/04/hosting-my-blog-on-github/ tags/static-site-generator/ no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Run the following commands:\ngit add . Changes to be committed: (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) new file: content/post/2023-03-04-running-hugo-on-github/images/GitHub-Logo300px.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305091118809.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305091558280.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305092142364.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305094906285.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100514339.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100657366.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100847587.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305101357702.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305112013507.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305112521903.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305113016848.png new file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305113102660.png new file: content/post/2023-03-04-running-hugo-on-github/index.md modified: public git commit -s -m \u0026#34;added content\u0026#34; git push Now Github should start build your blog-page. This can be seen under Actions here:\nUpdating my blog with this content:\nFor backup reasons it could also be smart to commit and push the blog.local folder also.\nGo back one level (from public to blog.local folder, root)\ngit status git add . git commit -s -m \u0026#34;added-content\u0026#34; git push New machine - set up environment # If you need to start from scratch, you have a new machine or whatever you dont have to go through all the steps above, you only need to add the SSH key, creating the git config -global settings. After that It is just as simple as doing this:\ngit clone --recurse-submodules git@github.com:andreasm80/blog.local.git #this will also clone submodules # then it is just regular git commands: git add . git commit -s -m \u0026#34;message\u0026#34; git push Custom domain - Github Pages # If you would like to present your Github Page (aka blog page) on a different domain you happen to own head over to settings in your Github Pages repository:\nThe on the left side click on Pages\nAnd in Pages type in your custom domain here and enable Enforce HTTPS:\nWhen you click save Github will place a file called CNAME in the root of your Github pages repository (mine andreasm80.github.io) where the content is the dns record you have entered in the Custom Domain field above. So you would need to fetch this locally with git to be in sync again.\n# you can either enter your submodule(s) directory and run: git fetch # or you can stay in the \u0026#34;root\u0026#34; folder and enter: git submodule foreach \u0026#39;git fetch\u0026#39; #which will do a fetch on all submodules Now you need to go to your DNS provider and add a CNAME pointing to your Gitub pages repository name, in my case that is andreasm80.github.io. So I have created this cname record:\nGithub will manage the certificate for your Github Page, so you dont have to worry about that either. After some minutes/hours (depending on how fast DNS is updated) your blog page will be resolved on the custom domain. Mine is https://blog.andreasm.io\n","date":"4 March 2023","externalUrl":null,"permalink":"/2023/03/04/hosting-my-blog-on-github/","section":"Posts","summary":"Hugo on Github: # I have been running my blog page locally on Kubernetes for a long time now.","title":"Hosting my blog on Github","type":"posts"},{"content":"","date":"4 March 2023","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"4 March 2023","externalUrl":null,"permalink":"/tags/static-site-generator/","section":"Tags","summary":"","title":"Static-Site-Generator","type":"tags"},{"content":" Antrea Egress: # What is Egress when we talk about Kubernetes? Well if a pod wants to communicate to the outside world, outside the Kubernetes cluster it runs in, out of the worker node the pod resides on, this is egress traffic (definition \u0026ldquo;the action of going out of or leaving a place\u0026rdquo; and in network terminology means the direction is outward from itself).\nWhy does egress matter? Well, usually when the pods communicate out, they will use the IP address of the worker node they currently is deployed on. Thats means the actual pod IP is not the address you should be expecting when doing network inspection, tcpdump, firewall rules etc, it is the Kubernetes worker nodes IP addresses. What we call this network feature is NAT, Network Address Translation. All Kubernetes worker nodes will take the actual POD IP and translate it to its own IP before sending the traffic out of itself. And as we know, we don\u0026rsquo;t know where the pod will be deployed, and the pods can be many and will relocate so in certain environments it can be hard, not granular enough to create firewall rules in the perimeter firewall to allow or block traffic from a certain pod when needed when we only can use the IP addresses of the worker nodes.\nThats where Antrea Egress comes in. With Antrea Egress we have the option to dictate which specific IP address the POD can use when communication out by using an IP address that is not its POD IP address but a valid and allowed IP address in the network. You can read more on the Antrea Egress feature here\nAs the diagram below will illustrate, when pods communicate out, the will all get their POD IP addresses translated into the worker node\u0026rsquo;s IP address. And the firewall between worker node and the SQL server are only able to allow or block the IP address of the worker node. That means we potentially allow or block all pods coming from this node, or nodes if we allow the range of all the worker nodes.\nOfcourse we can use Antrea Native Policies which I have written about here or VMware NSX with NCP, and VMware NSX with Antrea Integration to do fine grained security from source. But still there are environments we need to handle rules in perimeter firewalls.\nSo, this post will show how to enable Antrea Egress in vSphere 8 with Tanzu. With the current release of Antrea there is only support of using the same L2 network as worker nodes for the Antrea Egress IP-Pool.\nAs we can see in the diagram above, Antrea Egress has been configured with an IP-Pool the pods can get if we apply Antrea Egress IPs for them to use. It will then take a free IP from the Egress IP Pool and which is within the same L2 subnet as the workers are configured on. This is very easy to do and achieve. No need to create static routes, Antrea takes care of the IP mapping. With this in place the firewall rule is now very strict, I can allow only the IP 10.10.1.40 (which is the IP the POD got from Antrea Egress Pool) and block the worker node ip address.\nBut\u0026hellip;. I wanted to go a bit further and make use of L3 anyway for my Antrea Egress IP-Pool by utilizing BGP. Thats where the fun starts and this article is actually about. What I would like to achieve is that the IP address pool I configfure with Antrea Egress is something completely different from what the workers are using, not even the same L2 subnet but a completely different subnet. That means we need to involve some clever routing, and some configuration done on the worker nodes as its actually their IP addresses that becomes the gateway for our Antrea Egress subnets.\nSomething like this:\nThe diagram above shows a pod getting an IP address from the Egress pool which is something completely different from what subnet the worker node itself has. What Antrea does is creating a virtual interface on the worker node and assigns all the relevant ip addresses that are being used by Antrea Egress on that interface. They will use the default route on the worker node itself when going out, but the only component in the network that does know about this Egress subnet is the worker node itself, so it needs to tell this to his buddy routers out there. Either we create a static route on the router (could be the next hop of the worker node, the closest one, or some other hop in the infrastructure) or use BGP. Static route is more or less useless, too many ip addresses to update each time an egress ip is being applied, it could be on any worker node etc. So BGP is the way to go.\nThe Diagram below illustrates what happens if we dont tell our network routers where this network comes from and where it can be reached. It will egress out, but no one knows the way back.\nAs soon as the routers are informed of the address to this IP address they will be more than happy to deliver it for us, thats their job. Imagine being a postman delivering a packet somewhere in a country without any direction, address etc to narrow down his search field. In the scenario above the return traffic will most likely be sent out via a default route to the Internet and never to be seen again \u0026#x1f604;\nSo after we have been so kind to update with the exact delivery address below, we will get our mail again.\nEnough explanation already, get to the actual config of this.\nConfigure Antrea Egress in TKC (vSphere 8) # Deploy your TKC cluster, it must be Ubuntu os for this to work:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: wdc-2-tkc-cluster-1 # give your tkc cluster a name namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] pods: cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.23.8---vmware.2-tkg.2-zshippable controlPlane: replicas: 1 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: machineDeployments: - class: node-pool name: node-pool-01 replicas: 3 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu variables: - name: vmClass value: best-effort-medium - name: storageClass value: vsan-default-storage-policy Apply the correct Antrea configs, enable the Egress feature:\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: wdc-2-tkc-cluster-1-antrea-package namespace: wdc-2-ns-1 spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: false Egress: true #This needs to be enabled NodePortLocal: true AntreaTraceflow: true NetworkPolicyStats: true Log in to your newly created TKC cluster:\nkubectl-vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name tkc-cluster-1 --tanzu-kubernetes-cluster-namespace ns-1 Delete the Antrea Controller and Agent pods. Now that we have done the initial config of our TKC cluster its time to test Antrea Egress within same subnet as worker nodes just to verify that it works.\nFrom now on you should stay in the context of your newly created TKC cluster.\nVerify Antrea Egress works with L2 # To be able to use Antrea Egress we need to first start with an IP-Pool definition. So I create my definition like this:\napiVersion: crd.antrea.io/v1alpha2 kind: ExternalIPPool metadata: name: antrea-ippool-l2 #just a name of this specific pool spec: ipRanges: - start: 10.102.6.40 # make sure not to use already used ips end: 10.102.6.50 # should not overlap with worker nodes # - cidr: 10.101.112.0/32 # or you can define a whole range with cidr /32, /27 etc nodeSelector: {} # you can remove the brackets and define which nodes you want below by using labels # matchLabels: # egress-l2: antrea-egress-l2 Apply your yaml definition above:\nandreasm@linuxvm01:~/antrea/egress$ k apply -f ippool.wdc2.tkc.cluster-1.yaml externalippool.crd.antrea.io/antrea-ippool-l2 created Then we need to define the actual Egress itself. What we do with this config is selecting which pod that should get an Egress ip, from wich Antrea Egress IP pool (we can have several). So here is my example:\napiVersion: crd.antrea.io/v1alpha2 kind: Egress metadata: name: antrea-egress-l2 #just a name of this specific Egress config spec: appliedTo: podSelector: matchLabels: app: ubuntu-20-04 ###Which pods should get Egress IPs externalIPPool: antrea-ippool-l2 ###The IP pool I defined above. Before I apply it I will just make sure that I have a pod running the these labels, if not I will deploy it and then apply the Egress. So before I apply it I will show pinging from my pod to my jumpbox VM to identify which IP it is using before applying the Egress. And the apply the Egress and see if IP changes from the POD.\nMy ubuntu pod is up and running, I have entered the shell on it and initiates a ping from my pod to my jumpbox VM:\nSo here I can see the POD identifies itself with IP 10.102.6.15. Well which worker is that?:\nandreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n prod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-20-04-c9776f965-t8nmf 1/1 Running 0 20h 20.40.1.2 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 20h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 That is this worker: wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds.\nSo far so good. Now let me apply the Egress on this POD.\nandreasm@linuxvm01:~/antrea/egress$ k apply -f antrea.egress.l2.yaml egress.crd.antrea.io/antrea-egress-l2 created Now how does the ping look like?\nThis was as expected was it not? The POD now identifies itself with the IP 10.102.6.40 which happens to be the first IP in the range defined in the pool. Well, this is cool. Now we now that Antrea Egress works. But as mentioned, I want this to be done with a different subnet than the worker nodes. Se lets see have we can do that as \u0026ldquo;seemless\u0026rdquo; as possible, as we dont want to SSH into the worker nodes and do a bunch of manual installation, configuration and so on. No, we use Kubernetes for our needs here also.\nConfigure FRR on worker nodes # What I want to achieve is to deploy FRR here on my worker nodes unattended to enable BGP pr worker node to my upstream BGP router (remember, to inform about the Egress network no one knows about). The TKC workers are managed appliances, they can be deleted, scaled up and down (more workers, fewer workers.) And Deploying something manual on them are just waste of time. So we need something that deploy FRR automatically on the worker nodes.\nFRR is easy to deploy and configure, and it is included in the Ubuntu default repo (one reason I wanted to use Ubuntu as worker os). FRR is a very good routing protocol suite in Linux and is deployed easy on Ubuntu with \u0026ldquo;apt install frr\u0026rdquo;. FRR can be configured to use BGP which is the routing protocol I want to use. FRR needs two config files, daemons and frr.conf. frr.conf is individual pr node (specific IP addresses) so we need to take that into consideration also. So how can I deploy FRR on the worker nodes with their individal configuration files to automatically establish a BGP neighbourship with my Upstream router, and without logging into the actual worker nodes themselves?\nBelow diagram just illustrating a tkc worker node with FRR installed and BGP configured:\nKubernetes and Daemonset. # I have created three Daemonset definition files, one for the actual deployment of FRR on all the nodes:\nThen I have created on Daemonset definition to copy the frr.conf and daemons file for the specific worker nodes and the last definition file is used to uninistall everything on the worker nodes themselves (apt purge frr) if needed.\nLets start by just deploy FRR on the workers themselves.\nHere is the defintion for that:\n--- apiVersion: apps/v1 kind: DaemonSet metadata: namespace: kube-system name: node-custom-setup labels: k8s-app: node-custom-setup annotations: command: \u0026amp;cmd apt-get update -qy \u0026amp;\u0026amp; apt-get install -qy frr spec: selector: matchLabels: k8s-app: node-custom-setup template: metadata: labels: k8s-app: node-custom-setup spec: hostNetwork: true initContainers: - name: init-node command: - nsenter - --mount=/proc/1/ns/mnt - -- - sh - -c - *cmd image: alpine:3.7 securityContext: privileged: true hostPID: true containers: - name: wait image: pause:3.1 hostPID: true hostNetwork: true tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master updateStrategy: type: RollingUpdate Before I apply the above definiton I have logged into one of my TKC worker node and just wants to show that there is no FRR installed:\nsh-5.0# cd /etc/frr sh: cd: /etc/frr: No such file or directory sh-5.0# systemctl status frr Unit frr.service could not be found. sh-5.0# hostname wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds sh-5.0# Now apply:\nandreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f deploy-frr.yaml daemonset.apps/node-custom-setup configured andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system NAME READY STATUS RESTARTS AGE antrea-agent-4jrks 2/2 Running 0 20h antrea-agent-4khkr 2/2 Running 0 20h antrea-agent-4wxb5 2/2 Running 0 20h antrea-agent-ccglp 2/2 Running 0 20h antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 20h coredns-7d8f74b498-j5sjt 1/1 Running 0 21h coredns-7d8f74b498-mgqrm 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h kube-proxy-44qxn 1/1 Running 0 21h kube-proxy-4x72n 1/1 Running 0 21h kube-proxy-shhxb 1/1 Running 0 21h kube-proxy-zxhdb 1/1 Running 0 21h kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h metrics-server-6777988975-cxnpv 1/1 Running 0 21h node-custom-setup-5rlkr 1/1 Running 0 34s #There they are node-custom-setup-7gf2v 1/1 Running 0 62m #There they are node-custom-setup-b4j4l 1/1 Running 0 62m #There they are node-custom-setup-wjpgz 0/1 Init:0/1 0 1s #There they are Now what has happened on the TKC worker nodes itself:\nsh-5.0# cd /etc/frr/ sh-5.0# pwd /etc/frr sh-5.0# ls daemons frr.conf support_bundle_commands.conf vtysh.conf sh-5.0# systemctl status frr ● frr.service - FRRouting Loaded: loaded (/lib/systemd/system/frr.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2023-02-20 17:53:33 UTC; 1min 36s ago Wow that looks good. But the frr.conf is more or less empty so it doesnt do anything right now.\nA note on FRR config on the worker nodes # Before jumping into this section I would like to elaborate a bit around the frr.conf files being copied. If you are expecting that all worker nodes will be on same BGP AS number and your next-hop BGP neighbors are the same ones and in the same L2 as your worker node (illustrated above) you could probably go with the same config for all worker nodes. Then you can edit the same definition used for the FRR deployment to also copy and install the config in the same operation. The steps I do below describes individual config pr worker node. If you need different BGP AS numbers, multi-hop (next-hop is several hops away), individual update-source interfaces is configured then you need individual frr.config pr node.\nIndividual FRR config on the worker nodes # I need to \u0026ldquo;inject\u0026rdquo; the correct config for each worker node. So I label each and one with their unique label like this: (I map the names node1-\u0026gt;lowest-ip)\nI have already configured my upstream bgp router to accept my workers as soon as they are configured and ready. This is how this looks.\nrouter bgp 65802 bgp router-id 172.20.0.102 redistribute connected neighbor 10.102.6.15 remote-as 66889 neighbor 10.102.6.16 remote-as 66889 neighbor 10.102.6.17 remote-as 66889 neighbor 172.20.0.1 remote-as 65700 ! address-family ipv6 exit-address-family exit cpodrouter-nsxam-wdc-02# show ip bgp summary BGP router identifier 172.20.0.102, local AS number 65802 RIB entries 147, using 16 KiB of memory Peers 4, using 36 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.102.6.15 4 66889 1191 1197 0 0 0 never Active # Node1 not Established yet 10.102.6.16 4 66889 0 0 0 0 0 never Active # Node2 not Established yet 10.102.6.17 4 66889 1201 1197 0 0 0 never Active # Node3 not Established yet 172.20.0.1 4 65700 19228 19172 0 0 0 01w4d09h 65 To verify again, this is the current output of frr.conf on node1:\nsh-5.0# cat frr.conf # default to using syslog. /etc/rsyslog.d/45-frr.conf places the log # in /var/log/frr/frr.log log syslog informational sh-5.0# This is the definition I use to copy the daemons and frr.conf for the individual worker nodes:\n--- apiVersion: apps/v1 kind: DaemonSet metadata: namespace: kube-system name: node-frr-config labels: k8s-app: node-frr-config annotations: command: \u0026amp;cmd cp /tmp/wdc-2.node1.frr.conf /etc/frr/frr.conf \u0026amp;\u0026amp; cp /tmp/daemons /etc/frr \u0026amp;\u0026amp; systemctl restart frr spec: selector: matchLabels: k8s-app: node-frr-config template: metadata: labels: k8s-app: node-frr-config spec: nodeSelector: nodelabel: wdc2-node1 #Here is my specific node selection done hostNetwork: true initContainers: - name: copy-file image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cp /var/nfs/wdc-2.node1.frr.conf /var/nfs/daemons /data\u0026#39;] volumeMounts: - name: nfs-vol mountPath: /var/nfs # The mountpoint inside the container - name: node-vol mountPath: /data - name: init-node command: - nsenter - --mount=/proc/1/ns/mnt - -- - sh - -c - *cmd image: alpine:3.7 securityContext: privileged: true hostPID: true containers: - name: wait image: pause:3.1 hostPID: true hostNetwork: true tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master volumes: - name: nfs-vol nfs: server: 10.101.10.99 path: /home/andreasm/antrea/egress/FRR/nfs - name: node-vol hostPath: path: /tmp type: Directory updateStrategy: type: RollingUpdate Notice this:\nnodeSelector: nodelabel: wdc2-node1 This is used to select the correct node after I have labeled them like this:\nandreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k label node wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds nodelabel=wdc2-node1 node/wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds labeled So its just about time to apply the configs pr node:\nandreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f wdc2.frr.node1.config.yaml daemonset.apps/node-custom-setup configured andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system NAME READY STATUS RESTARTS AGE antrea-agent-4jrks 2/2 Running 0 21h antrea-agent-4khkr 2/2 Running 0 21h antrea-agent-4wxb5 2/2 Running 0 21h antrea-agent-ccglp 2/2 Running 0 21h antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 21h coredns-7d8f74b498-j5sjt 1/1 Running 0 21h coredns-7d8f74b498-mgqrm 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h kube-proxy-44qxn 1/1 Running 0 21h kube-proxy-4x72n 1/1 Running 0 21h kube-proxy-shhxb 1/1 Running 0 21h kube-proxy-zxhdb 1/1 Running 0 21h kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h metrics-server-6777988975-cxnpv 1/1 Running 0 21h node-custom-setup-w4mg5 0/1 Init:0/2 0 4s Now what does my upstream router say:\ncpodrouter-nsxam-wdc-02# show ip bgp summary BGP router identifier 172.20.0.102, local AS number 65802 RIB entries 149, using 16 KiB of memory Peers 4, using 36 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.102.6.15 4 66889 1202 1209 0 0 0 00:00:55 2 #Hey I am a happy neighbour 10.102.6.16 4 66889 0 0 0 0 0 never Active 10.102.6.17 4 66889 1201 1197 0 0 0 never Active 172.20.0.1 4 65700 19249 19194 0 0 0 01w4d10h 65 Total number of neighbors 4 Total num. Established sessions 2 Total num. of routes received 67 Then I just need to deploy on the other two workers.\nMy upstream bgp router is very happy to have new established neighbours:\ncpodrouter-nsxam-wdc-02# show ip bgp summary BGP router identifier 172.20.0.102, local AS number 65802 RIB entries 153, using 17 KiB of memory Peers 4, using 36 KiB of memory Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.102.6.15 4 66889 1221 1232 0 0 0 00:01:01 2 10.102.6.16 4 66889 11 16 0 0 0 00:00:36 2 10.102.6.17 4 66889 1225 1227 0 0 0 00:00:09 2 172.20.0.1 4 65700 19254 19205 0 0 0 01w4d10h 65 Total number of neighbors 4 Antrea IP Pool outside subnet of worker nodes # Lets apply an IP pool which resides outside worker nodes subnet and apply Egress on my test pod again.\nHere is the IP pool config:\napiVersion: crd.antrea.io/v1alpha2 kind: ExternalIPPool metadata: name: antrea-ippool-l3 spec: ipRanges: - start: 10.102.40.41 end: 10.102.40.51 # - cidr: 10.102.40.0/24 nodeSelector: {} # matchLabels: # egress-l3: antrea-egress-l3 And the Egress:\napiVersion: crd.antrea.io/v1alpha2 kind: Egress metadata: name: antrea-egress-l3 spec: appliedTo: podSelector: matchLabels: app: ubuntu-20-04 externalIPPool: antrea-ippool-l3 Apply it and check the IP address from the POD\u0026hellip;.\nWell, how about that?\nI know my workers reside on these ip addresses, but my POD is using a completely different IP address:\nandreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 What about the routing table in my upstream bgp router:\n*\u0026gt; 10.102.40.41/32 10.102.6.17 0 0 66889 ? Well have you seen..\nObjective accomplished\u0026mdash;-\u0026gt;\n","date":"20 February 2023","externalUrl":null,"permalink":"/2023/02/20/antrea-egress/","section":"Posts","summary":"Antrea Egress: # What is Egress when we talk about Kubernetes?","title":"Antrea Egress","type":"posts"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/tags/kubernetes-management/","section":"Tags","summary":"","title":"Kubernetes-Management","type":"tags"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/categories/kubernetes-management/","section":"Categories","summary":"","title":"Kubernetes-Management","type":"categories"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/tags/rancher/","section":"Tags","summary":"","title":"Rancher","type":"tags"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/categories/rancher/","section":"Categories","summary":"","title":"Rancher","type":"categories"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/tags/rke2/","section":"Tags","summary":"","title":"Rke2","type":"tags"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/categories/rke2/","section":"Categories","summary":"","title":"RKE2","type":"categories"},{"content":"","date":"5 February 2023","externalUrl":null,"permalink":"/tags/suse/","section":"Tags","summary":"","title":"Suse","type":"tags"},{"content":" Rancher by Suse - short introduction # From the offical Rancher docs:\nRancher is a container management platform built for organizations that deploy containers in production. Rancher makes it easy to run Kubernetes everywhere, meet IT requirements, and empower DevOps teams.\nRun Kubernetes Everywhere\nKubernetes has become the container orchestration standard. Most cloud and virtualization vendors now offer it as standard infrastructure. Rancher users have the choice of creating Kubernetes clusters with Rancher Kubernetes Engine (RKE) or cloud Kubernetes services, such as GKE, AKS, and EKS. Rancher users can also import and manage their existing Kubernetes clusters created using any Kubernetes distribution or installer.\nMeet IT Requirements\nRancher supports centralized authentication, access control, and monitoring for all Kubernetes clusters under its control. For example, you can:\nUse your Active Directory credentials to access Kubernetes clusters hosted by cloud vendors, such as GKE. Setup and enforce access control and security policies across all users, groups, projects, clusters, and clouds. View the health and capacity of your Kubernetes clusters from a single-pane-of-glass. Rancher can manage already existing Kubernetes clusters by importing them into Rancher, but Rancher can also do fully automated deployments of complete Kubernetes clusters.\nThe initial parts of this post will be focusing on getting Rancher itself deployed, then I will do some automated Kubernetes deployments from Rancher. The goal is to showcase how quick and easy it is to to deploy Kubernetes using Rancher. If it is not too obvious when reading these cluster creation chapters, trust me, deploying and managing Kubernetes cluster with Rancher is both fun and easy. After the initial chapters I will dive into some more technical topics.\nThis post is not meant to be a comprehensive article on all features/configurations possible with Rancher, look at it more as an unboxing Rancher post.\nMy environments used in this post # In this post I will be using two platforms where I run and deploy my Kubernetes clusters. One platform is VMware vSphere the other is using Proxmox. Proxmox is running home in my lab, vSphere is running in another remote place and I am accessing it using IPsec VPN. In this post I will deploy Rancher in a Tanzu Kubernetes Cluster deployed in my vSphere lab. I already have Rancher running in one of my Kubernetes clusters in my Proxmox lab. The Rancher deployment in vSphere is just used to go through the installation of Rancher (I had already done the installation of Rancher before creating this post and didnt want to tear it down). The Rancher instance in my lab will be used for both importing existing Kubernetes clusters and the automated deployments of Kubernetes clusters on both the vSphere environment and Proxmox environment.\nIn a very high level diagram it should look like this:\nWhere Rancher sits # As I will proceed through this post by adding and deploying Kubernetes clusters and using Rancher accessing these clusters, it will make sense to illustrate how this looks like. It may be beneficial to the overall understanding to have as much context as possible of whats going on and how things work when reading through the post.\nRunning Rancher on Kubernetes makes Rancher Highly Available, it will be distributed across multiple Kubernetes worker nodes and also benefit from Kubernetes lifecycle management, self-healing etc. Rancher will become a critical endpoint so managing and ensuring availability to this endpoint is critical. Exposing the Rancher using a HA capable loadbalancer is something to consider. Loosing a singleton instance loadbalancer means loosing access to Rancher. To expose Rancher one can use VMware Avi Loadbalancer with its distributed architecture (several Service Engines in Active/Active) or one can use Traefik loadbalancer, HAProxy Nginx, just to name a few. Then the underlaying physical compute hosts should consist of more than one host of course.\nOne should thrive to make the Rancher API endpoint as robust as possible as this will be THE endpoint to use to access and manage the Rancher managed Kubernetes Clusters.\nAs one can imagine, managing multiple clusters using Rancher there will be some requests to this endpoint. So performance and resilience is key.\nAs soon as I have authenticated through Rancher I can access my Kubernetes clusters. Rancher will then work as a proxy and forward my requests to my respective Kubernetes clusters.\n{\u0026#34;kind\u0026#34;:\u0026#34;Event\u0026#34;,\u0026#34;authorization.k8s.io/decision\u0026#34;:\u0026#34;allow\u0026#34;,\u0026#34;authorization.k8s.io/reason\u0026#34;:\u0026#34;RBAC: allowed by ClusterRoleBinding \\\u0026#34;globaladmin-user-64t5q\\\u0026#34; of ClusterRole \\\u0026#34;cluster-admin\\\u0026#34; to User \\\u0026#34;user-64t5q\\\u0026#34;\u0026#34;}} Installing Rancher on an existing Kubernetes Cluster using Helm # Before I can start using Rancher I need to install it. Rancher can be deployed using Docker for poc/testing purposes and on (more or less any) Kubernetes platform for production use. If one started out testing Rancher on Docker one can actually migrate from Docker to Kubernetes at a later stage also. In this post I will deploy Rancher on Kubernetes. I have already deployed a Kubernetes cluster I intend to deploy Rancher on. The Kubernetes cluster that will be used is provisioned by vSphere with Tanzu, running in my vSphere cluster using the method I describe in this post. vSphere with Tanzu is also its own Kubernetes management platform of course. I have deployed 3 control plane nodes and 3 worker nodes. All persistent storage will be handled by the the vSphere Cluster VSAN storage. Ingress will be taken care of by VMware Avi loadbalancer.\nHere is my TKC cluster I will install rancher on:\nThe cluster is ready with all the necessary backend services like the Avi loadbalancer providing loadbalancer services and Ingress rules. The first thing I need is to add the Helm repo for Rancher.\n# latest - recommended for testing the newest features andreasm@linuxmgmt01:~$ helm repo add rancher-latest https://releases.rancher.com/server-charts/latest \u0026#34;rancher-latest\u0026#34; has been added to your repositories Create a namespace for Rancher:\nandreasm@linuxmgmt01:~$ kubectl create namespace cattle-system namespace/cattle-system created I will bring my own Certificate, so skipping Cert-Manager. This is my Rancher Helm value yaml:\n# Additional Trusted CAs. # Enable this flag and add your CA certs as a secret named tls-ca-additional in the namespace. # See README.md for details. additionalTrustedCAs: false antiAffinity: preferred topologyKey: kubernetes.io/hostname # Audit Logs https://rancher.com/docs/rancher/v2.x/en/installation/api-auditing/ # The audit log is piped to the console of the rancher-audit-log container in the rancher pod. # https://rancher.com/docs/rancher/v2.x/en/installation/api-auditing/ # destination stream to sidecar container console or hostPath volume # level: Verbosity of logs, 0 to 3. 0 is off 3 is a lot. auditLog: destination: sidecar hostPath: /var/log/rancher/audit/ level: 0 maxAge: 1 maxBackup: 1 maxSize: 100 # Image for collecting rancher audit logs. # Important: update pkg/image/export/resolve.go when this default image is changed, so that it\u0026#39;s reflected accordingly in rancher-images.txt generated for air-gapped setups. image: repository: \u0026#34;rancher/mirrored-bci-micro\u0026#34; tag: 15.4.14.3 # Override imagePullPolicy image # options: Always, Never, IfNotPresent pullPolicy: \u0026#34;IfNotPresent\u0026#34; # As of Rancher v2.5.0 this flag is deprecated and must be set to \u0026#39;true\u0026#39; in order for Rancher to start addLocal: \u0026#34;true\u0026#34; # Add debug flag to Rancher server debug: false # When starting Rancher for the first time, bootstrap the admin as restricted-admin restrictedAdmin: false # Extra environment variables passed to the rancher pods. # extraEnv: # - name: CATTLE_TLS_MIN_VERSION # value: \u0026#34;1.0\u0026#34; # Fully qualified name to reach your Rancher server hostname: rancher-01.my.domain.net ## Optional array of imagePullSecrets containing private registry credentials ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ imagePullSecrets: [] # - name: secretName ### ingress ### # Readme for details and instruction on adding tls secrets. ingress: # If set to false, ingress will not be created # Defaults to true # options: true, false enabled: true includeDefaultExtraAnnotations: true extraAnnotations: {} ingressClassName: \u0026#34;avi-lb\u0026#34; # Certain ingress controllers will will require the pathType or path to be set to a different value. pathType: ImplementationSpecific path: \u0026#34;/\u0026#34; # backend port number servicePort: 80 # configurationSnippet - Add additional Nginx configuration. This example statically sets a header on the ingress. # configurationSnippet: | # more_set_input_headers \u0026#34;X-Forwarded-Host: {{ .Values.hostname }}\u0026#34;; tls: # options: rancher, letsEncrypt, secret source: secret secretName: tls-rancher-ingress ### service ### # Override to use NodePort or LoadBalancer service type - default is ClusterIP service: type: \u0026#34;\u0026#34; annotations: {} ### LetsEncrypt config ### # ProTip: The production environment only allows you to register a name 5 times a week. # Use staging until you have your config right. letsEncrypt: # email: none@example.com environment: production ingress: # options: traefik, nginx class: \u0026#34;\u0026#34; # If you are using certs signed by a private CA set to \u0026#39;true\u0026#39; and set the \u0026#39;tls-ca\u0026#39; # in the \u0026#39;rancher-system\u0026#39; namespace. See the README.md for details privateCA: false # http[s] proxy server passed into rancher server. # proxy: http://\u0026lt;username\u0026gt;@\u0026lt;password\u0026gt;:\u0026lt;url\u0026gt;:\u0026lt;port\u0026gt; # comma separated list of domains or ip addresses that will not use the proxy noProxy: 127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local # Override rancher image location for Air Gap installs rancherImage: rancher/rancher # rancher/rancher image tag. https://hub.docker.com/r/rancher/rancher/tags/ # Defaults to .Chart.appVersion # rancherImageTag: v2.0.7 # Override imagePullPolicy for rancher server images # options: Always, Never, IfNotPresent # Defaults to IfNotPresent # rancherImagePullPolicy: \u0026lt;pullPolicy\u0026gt; # Number of Rancher server replicas. Setting to negative number will dynamically between 0 and the abs(replicas) based on available nodes. # of available nodes in the cluster replicas: 3 # Set priorityClassName to avoid eviction priorityClassName: rancher-critical # Set pod resource requests/limits for Rancher. resources: {} # # tls # Where to offload the TLS/SSL encryption # - ingress (default) # - external tls: ingress systemDefaultRegistry: \u0026#34;\u0026#34; # Set to use the packaged system charts useBundledSystemChart: false # Certmanager version compatibility certmanager: version: \u0026#34;\u0026#34; # Rancher custom logos persistence customLogos: enabled: false volumeSubpaths: emberUi: \u0026#34;ember\u0026#34; vueUi: \u0026#34;vue\u0026#34; ## Volume kind to use for persistence: persistentVolumeClaim, configMap volumeKind: persistentVolumeClaim ## Use an existing volume. Custom logos should be copied to the volume by the user # volumeName: custom-logos ## Just for volumeKind: persistentVolumeClaim ## To disables dynamic provisioning, set storageClass: \u0026#34;\u0026#34; or storageClass: \u0026#34;-\u0026#34; # storageClass: \u0026#34;-\u0026#34; accessMode: ReadWriteOnce size: 1Gi # Rancher post-delete hook postDelete: enabled: true image: repository: rancher/shell tag: v0.1.23 namespaceList: - cattle-fleet-system - cattle-system - rancher-operator-system # Number of seconds to wait for an app to be uninstalled timeout: 120 # by default, the job will fail if it fail to uninstall any of the apps ignoreTimeoutError: false # Set a bootstrap password. If leave empty, a random password will be generated. bootstrapPassword: \u0026#34;MyPassword\u0026#34; livenessProbe: initialDelaySeconds: 60 periodSeconds: 30 readinessProbe: initialDelaySeconds: 5 periodSeconds: 30 global: cattle: psp: # will default to true on 1.24 and below, and false for 1.25 and above # can be changed manually to true or false to bypass version checks and force that option enabled: \u0026#34;\u0026#34; # helm values to use when installing the rancher-webhook chart. # helm values set here will override all other global values used when installing the webhook such as priorityClassName and systemRegistry settings. webhook: \u0026#34;\u0026#34; # helm values to use when installing the fleet chart. # helm values set here will override all other global values used when installing the fleet chart. fleet: \u0026#34;\u0026#34; I choose secret under TLS, so before I install Rancher I can create the secret with my own certificate.\nandreasm@linuxmgmt01:~$ k create secret -n cattle-system tls tls-rancher-ingress --cert=tls.crt --key=tls.key secret/tls-rancher-ingress created Now I can deploy Rancher with my value.yaml above.\nhelm install -n cattle-system rancher rancher-latest/rancher -f rancher.values.yaml NAME: rancher LAST DEPLOYED: Mon May 6 08:24:36 2024 NAMESPACE: cattle-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: Rancher Server has been installed. NOTE: Rancher may take several minutes to fully initialize. Please standby while Certificates are being issued, Containers are started and the Ingress rule comes up. Check out our docs at https://rancher.com/docs/ If you provided your own bootstrap password during installation, browse to https://rancher-01.my-domain.net to get started. If this is the first time you installed Rancher, get started by running this command and clicking the URL it generates: ``` echo https://rancher-01.my-domain.net/dashboard/?setup=$(kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}\u0026#39;) ``` To get just the bootstrap password on its own, run: ``` kubectl get secret --namespace cattle-system bootstrap-secret -o go-template=\u0026#39;{{.data.bootstrapPassword|base64decode}}{{ \u0026#34;\\n\u0026#34; }}\u0026#39; ``` Happy Containering! Now its time to log into the the url mentioned above using the provided bootstrap password (if entered):\nBy entering the bootstrap password above will log you in. If logging out, next time it will look like this:\nBefore going to the next chapter, in my Kubernetes cluster now I will have some additional namespaces created, services and deployments. Below is some of them:\n# Namespaces NAME STATUS AGE cattle-fleet-clusters-system Active 3h9m cattle-fleet-local-system Active 3h8m cattle-fleet-system Active 3h10m cattle-global-data Active 3h10m cattle-global-nt Active 3h10m cattle-impersonation-system Active 3h9m cattle-provisioning-capi-system Active 3h8m cattle-system Active 3h24m cluster-fleet-local-local-1a3d67d0a899 Active 3h8m fleet-default Active 3h10m fleet-local Active 3h10m local Active 3h10m p-4s8sk Active 3h10m p-n8cmn Active 3h10m # Deployments and services NAME READY STATUS RESTARTS AGE pod/helm-operation-9bzv8 0/2 Completed 0 3m22s pod/helm-operation-bvjrq 0/2 Completed 0 8m10s pod/helm-operation-njvlg 0/2 Completed 0 8m22s pod/rancher-5498b85476-bpfcn 1/1 Running 0 9m25s pod/rancher-5498b85476-j6ggn 1/1 Running 0 9m25s pod/rancher-5498b85476-xg247 1/1 Running 0 9m25s pod/rancher-webhook-7d876fccc8-6m8tk 1/1 Running 0 8m7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/rancher ClusterIP 10.10.227.75 \u0026lt;none\u0026gt; 80/TCP,443/TCP 9m26s service/rancher-webhook ClusterIP 10.10.81.6 \u0026lt;none\u0026gt; 443/TCP 8m8s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/rancher 3/3 3 3 9m27s deployment.apps/rancher-webhook 1/1 1 1 8m9s NAME DESIRED CURRENT READY AGE replicaset.apps/rancher-5498b85476 3 3 3 9m26s replicaset.apps/rancher-webhook-7d876fccc8 1 1 1 8m8s NAME CLASS HOSTS ADDRESS PORTS AGE rancher avi-lb rancher-01.my-domain.net 192.168.121.10 80, 443 9m47s Rancher is ready to be used.\nCreate and manage Kubernetes clusters # As soon as I have logged in the first thing I would like to do is to create a Kubernetes cluster. Rancher supports many methods of creating a Kubernetes cluster. Rancher can create clusters in a hosted Kubernetes provider such as Amazon EKS, Azure AKS, Google GKE and more. Rancher can create clusters and provision the nodes on Amazon EC2, Azure, DigitalOcean, Harvester, Linode, VMware vSphere and more. If you dont use any of them, it can create clusters on exisiting nodes like VMs deployed with a Linux (or even Windows..) OS waiting to be configured to do something (I will go through that later) or using Elemental (more on that later also).\nUnder drivers, see the list of possible Cluster Drivers and Node Drivers:\nI happen to have vSphere as my virtualization platform which is natively supported by Rancher and will start by provisioning a Kubernetes cluster using the vSphere cloud. I also have another hypervisor platform running in my lab (Proxmox) which does not have a native driver for Rancher, instead I will make Rancher deploy Kubernetes on existing VM nodes. I will go through how I deploy Kubernetes clusters on vSphere and on Proxmox using the Custom \u0026ldquo;cloud\u0026rdquo; on existing nodes.\nRancher Kubernetes distributions # Rancher is fully capable of doing automated provisining of Kubernetes clusters, as I will go through in this post, as much as it can manage already exisiting Kubernetes clusters by importing them.\nWhen Rancher is doing automated provisions of Kubernetes on supported clouds or onto existing nodes it uses its own Kubernetes distribution. In Rancher one can choose between three distributions called RKE, RKE2 and K3s. In this post I will only use RKE2 which stands for Rancher Kubernetes Engine. RKE2 is Rancher\u0026rsquo;s next-generation Kubernetes distribution. I may touch upon the K3s distribution also, as this is very much focused on edge use cases due to its lightweight.\nFor more information on RKE2 head over here\nFor more information on K3s head over here\nAt the current time of writing this post I am using Rancher 2.8.3 and the latest RKE2/Kubernetes release is 1.28.8 which is not so far away from Kubernetes upstream which is currently at v1.30.\nCreate RKE2 clusters using vSphere # Using vSphere as the \u0026ldquo;cloud\u0026rdquo; in Rancher do have some benefits. From the docs\nFor Rancher to spin up and create RKE2 clusters on vSphere I need to create a VM template using the OS of choice in my vSphere cluster. According to the official RKE2 documentation RKE2 should work on any Linux distribution that uses systemd and iptables. There is also a RKE2 Support Matrix for all OS versions that have been validated with RKE2 here (linked to v1.28 as this post uses 1.28 but v1.29 is already out at the time of writing this post).\nI went with Ubuntu 22.04 as my Linux distribution\nThere is a couple of ways Rancher can utilize vSphere to manage the template. Below is a screenshot of the possible options of the time of writing this post:\nIn this section I will go through the two methods Deploy from template: Content Library and deploy from template: Data Center\nCreate Cloud Credentials # The first thing I will do before getting to the actual cluster creation is to create credentials for the user and connection information to my vCenter server. I will go to the Cluster Management and Cloud Credentials section in Rancher:\nClick create in the top right corner and select VMware vSphere:\nFill in the relevant information about your vCenter server including the credentials with the right permissions (see permissions here using some very old screenshots from vCenter..):\nRKE2 clusters using vSphere Content Library template # I decided to start with the method of deploying RKE2 clusters using the Content Library method as I found that one to be the easiest and fastest method (its just about uploading the image to the content library and thats it). The concept behind this is to create a Content Library in your vCenter hosting a Cloud Image template for the OS of choice. This should be as minimal as possible, with zero to none configs. All the needed configs are done by Rancher when adding the nodes to your cluster later.\nAs mentioned above, I went with Ubuntu Cloud image. Ubuntu Cloud images can be found here.\nI will go with this OVA image (22.04 LTS Jammy Jellyfish):\nIn my vCenter I create a Content Library:\nLocal content library:\nSelect my datastore to host the content:\nFinish:\nNow I need to enter the content library by clicking on the name to add my desired Ubuntu Cloud Image version:\nI will select Source File URL and paste the below URL:\nhttps://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.ova\nClick on Import and it will download the OVA from the URL\nIt may complain about SSL certificate being untruste.. Click Actions and Continue:\nThe template should be downloading now:\nThats it from the vCenter side. The cloud image is ready as a template for Rancher to use. Now I will head over to Rancher and create my RKE2 cluster on vSphere based on this template.\nI will go to Cluster Management using the top left menu or directy from the homepage clicking on manage\nAs I have done the preparations in vSphere I am now ready to create my RKE2 cluster on vSphere. Head to Clusters in the top left corner and click create in the top right corner then select VMware vSphere:\nThe cluster I want to create is one control-plane node and one worker node.\nGive the cluster a name and description if you want. Then select the roles, at a minimum one have to deploy one node with all roles etcd, Control Plane and Worker. Give the pool a name, I will go with default values here. Machine count I will go with default 1 here also. I can scale it later \u0026#x1f604;. Then make sure to select the correct datacenter in your vCenter, select the Resource Pool. If you have resource pools, if not just select the correct cluster (sfo-m01-cl01)/Resources which will be the cluster resource pool (root). Then select the correct Datastore. Then I will left the other fields default except the Creation method and Network. Under creation method I have selected Deploy from template: Content Library the correct Content Library and the jammy-server-cloudimg template I have uploaded/downloaded to the Content Library. Click Add Network and select the correct vCenter Network Portgroup you want the node to be placed in.\nThats it for pool-1. I will scroll a bit down, click on the + sign and add a second pool for the nodes to have only the role worker.\nI will repeat the steps above for pool-2. They will be identical except for the pool name and only role Worker selected.\nFor the sake of keeping it simple, I will leave all the other fields default and just click create. I will now sit back and enjoy Rancher creating my RKE2 cluster on vSphere.\nIn vCenter:\nAnd cluster is ready:\nThere was of course a bunch of option I elegantly skipped during the creation of this cluster. But the point, again, was to show how quickly and easily I could bring up a RKE2 cluster on vSphere. And so it was.\nSupplemental information\nTo make further customization of the Ubuntu Cloud Image one can use the #cloud-config (cloud-init), and the vSphere vApp function.\nvApp:\nWhen it comes to the Ubuntu Cloud Image it does not contain any default username and password. So how can I access SSH on them? Well, from the Rancher UI one can access SSH directly or download the SSH keys. One can even copy paste into the SSH shell from there. SSH through the UI is very neat feature.\nSSH Shell:\nFor IP allocation I have configured DHCP in my NSX-T environment for the segment my RKE2 nodes will be placed in and all nodes will use dynamically allocated IP addresses in that subnet.\nRKE2 clusters using vSphere VM template # The other method of preparing a template for Rancher to use is to install the Ubuntu Server as a regular VM. That involves doing the necessary configs, power it down and convert it to template. I thought this would mean a very minimal config to be done. But not quite.\nMy first attempt did not go so well. I installed Ubuntu, then powered it down and converted it to a template. Tried to create a RKE2 cluster with it from Rancher, it cloned the amount of control plane nodes and worker nodes I had defined, but there it stopped. So I figured I must have missed something along the way.\nThen I found this post here which elegantly described the things I need in my template before \u0026ldquo;handing\u0026rdquo; it over to Rancher. So below is what I did in my Ubuntu Template by following the blog post above.\nIn short, this is what I did following the post above.\nOn my template:\nsudo apt-get install -y curl wget git net-tools unzip ca-certificates cloud-init cloud-guest-utils cloud-image-utils cloud-initramfs-growroot open-iscsi openssh-server open-vm-tools net-tools apparmor and:\nsudo dpkg-reconfigure cloud-init Deselected everything except \u0026ldquo;NoCloud\u0026rdquo;\nThen I ran the script:\n#!/bin/bash # Cleaning logs. if [ -f /var/log/audit/audit.log ]; then cat /dev/null \u0026gt; /var/log/audit/audit.log fi if [ -f /var/log/wtmp ]; then cat /dev/null \u0026gt; /var/log/wtmp fi if [ -f /var/log/lastlog ]; then cat /dev/null \u0026gt; /var/log/lastlog fi # Cleaning udev rules. if [ -f /etc/udev/rules.d/70-persistent-net.rules ]; then rm /etc/udev/rules.d/70-persistent-net.rules fi # Cleaning the /tmp directories rm -rf /tmp/* rm -rf /var/tmp/* # Cleaning the SSH host keys rm -f /etc/ssh/ssh_host_* # Cleaning the machine-id truncate -s 0 /etc/machine-id rm /var/lib/dbus/machine-id ln -s /etc/machine-id /var/lib/dbus/machine-id # Cleaning the shell history unset HISTFILE history -cw echo \u0026gt; ~/.bash_history rm -fr /root/.bash_history # Truncating hostname, hosts, resolv.conf and setting hostname to localhost truncate -s 0 /etc/{hostname,hosts,resolv.conf} hostnamectl set-hostname localhost # Clean cloud-init cloud-init clean -s -l Powered off the VM, converted it to a template in vSphere.\nStill after doing the steps described in the post linked to above my deployment failed. It turned out to be insufficient disk capacity. So I had to update my template in vSphere to use bigger disk. I was too conservative (or cheap) when I created it, I extended it to 60gb to be on the safe side. Could that have been a \u0026ldquo;no-issue\u0026rdquo; if I had read the official documentation/requirements, yes.\nI saw that when I connected to the control plane and checked the cattle-cluster-agent (it never started):\nandreasm@linuxmgmt01:~/$ k --kubeconfig test-cluster.yaml describe pod -n cattle-system cattle-cluster-agent-5cc77b7988-dwrtj Name: cattle-cluster-agent-5cc77b7988-dwrtj Namespace: cattle-system Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning Failed 6m24s kubelet Failed to pull image \u0026#34;rancher/rancher-agent:v2.8.3\u0026#34;: failed to pull and unpack image \u0026#34;docker.io/rancher/rancher-agent:v2.8.3\u0026#34;: failed to extract layer sha256:3a9df665b61dd3e00c0753ca43ca9a0828cb5592ec051048b4bbc1a3f4488e05: write /var/lib/rancher/rke2/agent/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/108/fs/var/lib/rancher-data/local-catalogs/v2/rancher-partner-charts/8f17acdce9bffd6e05a58a3798840e408c4ea71783381ecd2e9af30baad65974/.git/objects/pack/pack-3b07a8b347f63714781c345b34c0793ec81f2b86.pack: no space left on device: unknown Warning Failed 6m24s kubelet Error: ErrImagePull Warning FailedMount 4m15s (x5 over 4m22s) kubelet MountVolume.SetUp failed for volume \u0026#34;kube-api-access-cvwz2\u0026#34; : failed to fetch token: Post \u0026#34;https://127.0.0.1:6443/api/v1/namespaces/cattle-system/serviceaccounts/cattle/token\u0026#34;: dial tcp 127.0.0.1:6443: connect: connection refused Normal Pulling 4m7s kubelet Pulling image \u0026#34;rancher/rancher-agent:v2.8.3\u0026#34; Warning Evicted 3m59s kubelet The node was low on resource: ephemeral-storage. Threshold quantity: 574385774, available: 108048Ki. After doing the additional changes in my template I gave the RKE2 cluster creation another attempt.\nFrom the Cluster Management, click Create\nSelect VMware vSphere:\nI will start by defining \u0026ldquo;pool-1\u0026rdquo; as my 3 control plane nodes. Select vCenter Datacenter, Datastore and resource pool (if no resource pool is defined in vCenter, select just Resources which is \u0026ldquo;root\u0026rdquo;/cluster-level)\nThen define the \u0026ldquo;pool-1\u0026rdquo; instance options like cpu, memory, disk, network and the created vm template:\nAdd a second pool by clicking on the + sign here:\nThen confgure the amount of Machine count, roles (worker only) and vSphere placement as above (unless one have several clusters and want to distribute the workers.. That is another discussion).\nConfigure the pool-2 instance cpu, memory, network, vm template etc.. Here one can define the workers to be a bit more beefier (resource wise) than the control plane nodes.\nThen it is the cluster config itself. There is a lot of details I will not cover now, will cover these a bit later. Now the sole purpose is to get a cluster up and running as easy and fast as possible. I will leave everything default, except pod CIDR and service CIDR. Will get into some more details later in the post.\nPod CIDR and Service CIDR\nWhen done, click create bottom right corner.\nFrom vCenter:\nHere is my cluster starting to be built.\nRancher cluster creation status\nThats it. Cluster is up and running. There was a couple of additional tasks that needed to be done, but it worked. If one does not have the option to use a Cloud Image, then this will also work. The experience using a Cloud Image was by far the easiest and most flexible approach.\nCreating a RKE2 cluster on existing nodes - non native cloud provider # If I dont have a vSphere environment or other supported Cloud provider, but another platform for my virtalization needs, one alternative is then to prepare some VMs with my preferred operating system I can tell Rancher to deploy RKE2 on. The provisioning and managing of these VMs will be handled by OpenTofu. Lets see how that works.\nI have already a post covering how I am deploying VMs and Kubernetes using OpenTofu and Kubespray on my Proxmox cluster. For this section I will use the OpenTofu part just to quickly deploy the VMs on Proxmox I need to build or bring up my Kubernetes (RKE2) cluster.\nMy OpenTofu project is already configured, it should deploy 6 VMs, 3 control plane nodes and 3 workers. I will kick that task off like this:\nandreasm@linuxmgmt01:~/terraform/proxmox/rancher-cluster-1$ tofu apply plan proxmox_virtual_environment_file.ubuntu_cloud_init: Creating... proxmox_virtual_environment_file.ubuntu_cloud_init: Creation complete after 1s [id=local:snippets/ubuntu.cloud-config.yaml] proxmox_virtual_environment_vm.rke2-worker-vms-cl01[0]: Creating... proxmox_virtual_environment_vm.rke2-cp-vms-cl01[2]: Creating... proxmox_virtual_environment_vm.rke2-cp-vms-cl01[1]: Creating... proxmox_virtual_environment_vm.rke2-worker-vms-cl01[2]: Creating... proxmox_virtual_environment_vm.rke2-worker-vms-cl01[1]: Creating... proxmox_virtual_environment_vm.rke2-cp-vms-cl01[0]: Creating... ... proxmox_virtual_environment_vm.rke2-cp-vms-cl01[2]: Creation complete after 1m37s [id=1023] Apply complete! Resources: 7 added, 0 changed, 0 destroyed. And my vms should start popping up in my Proxmox ui:\nNow I have some freshly installed VMs running, it is time to hand them over to Rancher to do some magic on them. Lets head over to Rancher and create a Custom cluster.\nCreating RKE2 cluster on existing nodes - \u0026ldquo;custom cloud\u0026rdquo; # In Rancher, go to Cluster Management or from the homepage, create and select Custom\nGive the cluster a name, and change what you want according to your needs. I will only change the Container Network and the pod/services CIDR.\nClick create:\nNow it will tell you to paste a registration command on the node you want to be the controlplane, etc and worker node.\nClick on the cli command to get it into the clipboard. I will start by preparing the first node with all three roles control-plane, etcd and worker. Then I will continue with the next two. After my controlplane is ready and consist of three nodes I will change the parameter to only include worker node and do the last three nodes.\nThen go ahead and ssh into my intented control plane node and paste the command:\nubuntu@ubuntu:~$ curl -fL https://rancher-dev.my-domain.net/system-agent-install.sh | sudo sh -s - --server https://rancher-dev.my-domain.net --label \u0026#39;cattle.io/os=linux\u0026#39; --token p5fp6sv4qbskmhl7mmpksbxcgs9gk9dtsqkqg9q8q8ln5f56l5jlpb --etcd --controlplane --worker % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 32228 0 32228 0 0 368k 0 --:--:-- --:--:-- --:--:-- 370k [INFO] Label: cattle.io/os=linux [INFO] Role requested: etcd [INFO] Role requested: controlplane [INFO] Role requested: worker [INFO] Using default agent configuration directory /etc/rancher/agent [INFO] Using default agent var directory /var/lib/rancher/agent [INFO] Successfully tested Rancher connection [INFO] Downloading rancher-system-agent binary from https://rancher-dev.my-domain.net/assets/rancher-system-agent-amd64 [INFO] Successfully downloaded the rancher-system-agent binary. [INFO] Downloading rancher-system-agent-uninstall.sh script from https://rancher-dev.my-doamin.net/assets/system-agent-uninstall.sh [INFO] Successfully downloaded the rancher-system-agent-uninstall.sh script. [INFO] Generating Cattle ID [INFO] Successfully downloaded Rancher connection information [INFO] systemd: Creating service file [INFO] Creating environment file /etc/systemd/system/rancher-system-agent.env [INFO] Enabling rancher-system-agent.service Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service. [INFO] Starting/restarting rancher-system-agent.service ubuntu@ubuntu:~$ Now, lets just wait in the Rancher UI and monitor the progress.\nRepeat same operation for the two last control plane nodes.\nFor the worker nodes I will adjust the command to only deploy worker node role:\nubuntu@ubuntu:~$ curl -fL https://rancher-dev.my-domain.net/system-agent-install.sh | sudo sh -s - --server https://rancher-dev.my-domain.net --label \u0026#39;cattle.io/os=linux\u0026#39; --token jfjd5vwfdqxkszgg25m6w49hhm4l99kjk2xhrm2hp75dvwhrjnrt2d --worker % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 32228 0 32228 0 0 370k 0 --:--:-- --:--:-- --:--:-- 374k [INFO] Label: cattle.io/os=linux [INFO] Role requested: worker [INFO] Using default agent configuration directory /etc/rancher/agent [INFO] Using default agent var directory /var/lib/rancher/agent [INFO] Successfully tested Rancher connection [INFO] Downloading rancher-system-agent binary from https://rancher-dev.my-domain.net/assets/rancher-system-agent-amd64 [INFO] Successfully downloaded the rancher-system-agent binary. [INFO] Downloading rancher-system-agent-uninstall.sh script from https://rancher-dev.my-doamin.net/assets/system-agent-uninstall.sh [INFO] Successfully downloaded the rancher-system-agent-uninstall.sh script. [INFO] Generating Cattle ID [INFO] Successfully downloaded Rancher connection information [INFO] systemd: Creating service file [INFO] Creating environment file /etc/systemd/system/rancher-system-agent.env [INFO] Enabling rancher-system-agent.service Created symlink /etc/systemd/system/multi-user.target.wants/rancher-system-agent.service → /etc/systemd/system/rancher-system-agent.service. [INFO] Starting/restarting rancher-system-agent.service Repeat for all the worker nodes needed.\nAfter some minutes, the cluster should be complete with all control plane nodes and worker nodes:\nThe whole process, two tasks really, from deploying my six VMs to a fully working Kubernetes cluster running was just shy of 10 minutes. All I had to do was to provison the VMs, go into Rancher create a RKE2 cluster, execute the registration command on each node.\nDeploying Kubernetes clusters couldn\u0026rsquo;t be more fun actually.\nImport existing Kubernetes Clusters # As mentioned earlier, Rancher can also import existing Kubernetes clusters for central management. If one happen to have a bunch of Kubernetes clusters deployed, no central management of them, add them to Rancher.\nHere is how to import existing Kubernetes clusters.\nHead over to Cluster Management in the Rancher UI, go to Clusters and find the Import Existing button:\nI will select Generic as I dont have any clusters running in Amazon, Azure nor Google.\nGive the cluster a name (name as it will show in Rancher and ignore my typo tanzuz) and description. I am using the default admin account in Rancher.\nClick create in the bottom right corner.\nIt will now show me how I can register my already existing cluster into Rancher:\nNow I need to get into the context of the Kubernetes cluster I want to import and execute the following command:\nkubectl apply -f https://rancher-dev.my-domain.net/v3/import/knkh9n4rvhgpkxxxk7c4rhblhkqxfjxcq82nm7zcgfdcgqsl7bqdg5_c-m-qxpbc6tp.yaml In the correct context:\nandreasm@linuxmgmt01:~$ kubectl apply -f https://rancher-dev.my-domain.net/v3/import/knkh9n4rvhgpkxxxk7c4rhblhkqxfjxcq82nm7zcgfdcgqsl7bqdg5_c-m-qxpbc6tp.yaml clusterrole.rbac.authorization.k8s.io/proxy-clusterrole-kubeapiserver created clusterrolebinding.rbac.authorization.k8s.io/proxy-role-binding-kubernetes-master created Warning: resource namespaces/cattle-system is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. namespace/cattle-system configured serviceaccount/cattle created clusterrolebinding.rbac.authorization.k8s.io/cattle-admin-binding created secret/cattle-credentials-603e85e created clusterrole.rbac.authorization.k8s.io/cattle-admin created Warning: spec.template.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchExpressions[0].key: beta.kubernetes.io/os is deprecated since v1.14; use \u0026#34;kubernetes.io/os\u0026#34; instead deployment.apps/cattle-cluster-agent created service/cattle-cluster-agent created In the Rancher UI:\nIt took a couple of seconds, and the cluster was imported.\nNow just having a look around.\nNodes:\nDoing kubectl from the UI:\nNavigating the Rancher UI # So far I have only created and imported clusters into Rancher. The current inventory in Rancher is a couple of clusters both imported and created by Rancher:\nIts time to explore Rancher.\nWhat is the information Rancher can give me on my new RKE2 cluster. Lets click around.\nClick on Explore\nRancher Cluster Dashboard # The first thing I get to when clicking on Explore is the Cluster Dashboard. It contains a brief status of the cluster like CPU and Memory usage, total resources, pods running (55 of 660?) and latest events. Very nice set of information.\nBut in the top right corner, what have we there? Install Monitoring and Add Cluster Badge. Lets try the Cluster Badge\nI can customize the cluster badge.. Lets see how this looks like\nIt adds a color and initials on the cluster list on the left side menu, making it easy to distinguish and sort the different Kubernetes clusters.\nNow, what about the Install Monitoring.\nA set of available monitoring tools, ready to be installed. This is really great, and qualifies for dedicated sections later in this post.\nOn left side menu, there is additional information:\nPods:\nWhat about executing directly into shell of a pod?\nNodes:\nConfigMaps:\nI have an action menu on every object available in the left side menus, I can edit configMaps, secrets, drain node etc right there from the Rancher UI.\nIf I know the object name, I can just search for it also:\nSo much information readily available directly after deploying the cluster. This was just a quick overview, there are several more fields to explore.\nAccessing my new RKE2 cluster deployed on vSphere # Now that my clusters is up and running, how can I access it? SSH into one of the control plane nodes and grab the kubeconfig? Well that is possible, but is it a better way?\nYes it is\u0026hellip;\nClick on the three dots at the end of the cluster:\nLets try the Kubectl Shell\nOr just copy the KubeConfig to Clipboard, paste it in a file on your workstation and access your cluster like this:\nandreasm@linuxmgmt01:~/$ k get nodes -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME rke2-cluster-1-pool1-47454a8f-6npvs Ready control-plane,etcd,master,worker 127m v1.28.8+rke2r1 192.168.150.98 \u0026lt;none\u0026gt; Ubuntu 22.04.4 LTS 5.15.0-105-generic containerd://1.7.11-k3s2 rke2-cluster-1-pool1-47454a8f-9tv65 Ready control-plane,etcd,master,worker 123m v1.28.8+rke2r1 192.168.150.54 \u0026lt;none\u0026gt; Ubuntu 22.04.4 LTS 5.15.0-105-generic containerd://1.7.11-k3s2 rke2-cluster-1-pool1-47454a8f-xpjqf Ready control-plane,etcd,master,worker 121m v1.28.8+rke2r1 192.168.150.32 \u0026lt;none\u0026gt; Ubuntu 22.04.4 LTS 5.15.0-105-generic containerd://1.7.11-k3s2 rke2-cluster-1-pool2-b5e2f49f-2mr5r Ready worker 119m v1.28.8+rke2r1 192.168.150.97 \u0026lt;none\u0026gt; Ubuntu 22.04.4 LTS 5.15.0-105-generic containerd://1.7.11-k3s2 rke2-cluster-1-pool2-b5e2f49f-9qnl2 Ready worker 119m v1.28.8+rke2r1 192.168.150.99 \u0026lt;none\u0026gt; Ubuntu 22.04.4 LTS 5.15.0-105-generic containerd://1.7.11-k3s2 rke2-cluster-1-pool2-b5e2f49f-tfw2x Ready worker 119m v1.28.8+rke2r1 192.168.150.100 \u0026lt;none\u0026gt; Ubuntu 22.04.4 LTS 5.15.0-105-generic containerd://1.7.11-k3s2 andreasm@linuxmgmt01:~/$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE cattle-fleet-system fleet-agent-595ff97fd6-vl9t4 1/1 Running 0 124m cattle-system cattle-cluster-agent-6bb5b4f9c6-br6lk 1/1 Running 0 123m cattle-system cattle-cluster-agent-6bb5b4f9c6-t5822 1/1 Running 0 123m cattle-system rancher-webhook-bcc8984b6-9ztqj 1/1 Running 0 122m kube-system cloud-controller-manager-rke2-cluster-1-pool1-47454a8f-6npvs 1/1 Running 0 127m kube-system cloud-controller-manager-rke2-cluster-1-pool1-47454a8f-9tv65 1/1 Running 0 123m kube-system cloud-controller-manager-rke2-cluster-1-pool1-47454a8f-xpjqf 1/1 Running 0 120m kube-system etcd-rke2-cluster-1-pool1-47454a8f-6npvs 1/1 Running 0 127m kube-system etcd-rke2-cluster-1-pool1-47454a8f-9tv65 1/1 Running 0 123m kube-system etcd-rke2-cluster-1-pool1-47454a8f-xpjqf 1/1 Running 0 120m kube-system helm-install-rke2-canal-4cxc6 0/1 Completed 0 127m kube-system helm-install-rke2-coredns-vxvpj 0/1 Completed 0 127m kube-system helm-install-rke2-ingress-nginx-f5g88 0/1 Completed 0 127m kube-system helm-install-rke2-metrics-server-66rff 0/1 Completed 0 127m kube-system helm-install-rke2-snapshot-controller-crd-9ncn6 0/1 Completed 0 127m kube-system helm-install-rke2-snapshot-controller-rrhbw 0/1 Completed 1 127m kube-system helm-install-rke2-snapshot-validation-webhook-jxgr5 0/1 Completed 0 127m kube-system kube-apiserver-rke2-cluster-1-pool1-47454a8f-6npvs 1/1 Running 0 127m kube-system kube-apiserver-rke2-cluster-1-pool1-47454a8f-9tv65 1/1 Running 0 123m kube-system kube-apiserver-rke2-cluster-1-pool1-47454a8f-xpjqf 1/1 Running 0 120m kube-system kube-controller-manager-rke2-cluster-1-pool1-47454a8f-6npvs 1/1 Running 0 127m kube-system kube-controller-manager-rke2-cluster-1-pool1-47454a8f-9tv65 1/1 Running 0 123m kube-system kube-controller-manager-rke2-cluster-1-pool1-47454a8f-xpjqf 1/1 Running 0 120m kube-system kube-proxy-rke2-cluster-1-pool1-47454a8f-6npvs 1/1 Running 0 127m kube-system kube-proxy-rke2-cluster-1-pool1-47454a8f-9tv65 1/1 Running 0 123m kube-system kube-proxy-rke2-cluster-1-pool1-47454a8f-xpjqf 1/1 Running 0 120m kube-system kube-proxy-rke2-cluster-1-pool2-b5e2f49f-2mr5r 1/1 Running 0 119m kube-system kube-proxy-rke2-cluster-1-pool2-b5e2f49f-9qnl2 1/1 Running 0 119m kube-system kube-proxy-rke2-cluster-1-pool2-b5e2f49f-tfw2x 1/1 Running 0 119m kube-system kube-scheduler-rke2-cluster-1-pool1-47454a8f-6npvs 1/1 Running 0 127m kube-system kube-scheduler-rke2-cluster-1-pool1-47454a8f-9tv65 1/1 Running 0 123m kube-system kube-scheduler-rke2-cluster-1-pool1-47454a8f-xpjqf 1/1 Running 0 120m kube-system rke2-canal-7lmxr 2/2 Running 0 119m kube-system rke2-canal-cmrm2 2/2 Running 0 123m kube-system rke2-canal-f9ztm 2/2 Running 0 119m kube-system rke2-canal-gptjt 2/2 Running 0 119m kube-system rke2-canal-p9dj4 2/2 Running 0 121m kube-system rke2-canal-qn998 2/2 Running 0 126m kube-system rke2-coredns-rke2-coredns-84b9cb946c-q7cjn 1/1 Running 0 123m kube-system rke2-coredns-rke2-coredns-84b9cb946c-v2lq7 1/1 Running 0 126m kube-system rke2-coredns-rke2-coredns-autoscaler-b49765765-lj6mp 1/1 Running 0 126m kube-system rke2-ingress-nginx-controller-24g4k 1/1 Running 0 113m kube-system rke2-ingress-nginx-controller-9gncw 1/1 Running 0 123m kube-system rke2-ingress-nginx-controller-h9slv 1/1 Running 0 119m kube-system rke2-ingress-nginx-controller-p98mx 1/1 Running 0 120m kube-system rke2-ingress-nginx-controller-rfvnm 1/1 Running 0 125m kube-system rke2-ingress-nginx-controller-xlhd8 1/1 Running 0 113m kube-system rke2-metrics-server-544c8c66fc-n8r9b 1/1 Running 0 126m kube-system rke2-snapshot-controller-59cc9cd8f4-5twmc 1/1 Running 0 126m kube-system rke2-snapshot-validation-webhook-54c5989b65-8fzx7 1/1 Running 0 126m This goes for both Rancher automated deployed Kubernetes clusters and imported clusters. Same API endpoint to access them all.\nIf I would access my Tanzu Kubernetes clusters I would have to use the kubectl vsphere login command and from there enter the correnct vSphere Namespace context to get to my TKC cluster. By just adding the Tanzu cluster to Rancher I can use the same endpoint to access this cluster two. Have a quick look at how the kubectl config:\napiVersion: v1 kind: Config clusters: - name: \u0026#34;tanzuz-cluster-1\u0026#34; cluster: server: \u0026#34;https://rancher-dev.my-domain.net/k8s/clusters/c-m-qxpbc6tp\u0026#34; users: - name: \u0026#34;tanzuz-cluster-1\u0026#34; user: token: \u0026#34;kubeconfig-user-64t5qq4d67:l8mdcs55ggf8kz4zlrcpth8tk48j85kp9qjslk5zrcmjczw6nqr7fw\u0026#34; contexts: - name: \u0026#34;tanzuz-cluster-1\u0026#34; context: user: \u0026#34;tanzuz-cluster-1\u0026#34; cluster: \u0026#34;tanzuz-cluster-1\u0026#34; current-context: \u0026#34;tanzuz-cluster-1\u0026#34; I accessing my TKC cluster through Rancher API endpoint.\nManaging Kubernetes clusters from Rancher # Though making Kubernetes cluster deployments an easy task is important, handling day two operations is even more important. Let\u0026rsquo;s explore some typical administrative tasks that needs to be done on a Kubernetes cluster during its lifetime using Rancher.\nScaling nodes horizontally - up and down # There will come a time when there is a need to scale the amount of worker nodes in a cluster. Lets see how we can do that in Rancher.\nIf I manually want to scale the amount of worker nodes up, I will go to Cluster Management and Clusters, then click on the name of my cluster to be scaled:\nBy clicking on the cluster name it will take me directly to the Machine pools, on the right there is some obvious -/+ buttons. By clicking on the + sign once it will scale with 1 additional worker node in that pool. Lets test\nI will now scale my rke2-cluster-2-vsphere with from one worker node to two worker nodes.\nIn my vCenter:\nCluster is now scaled with one additional worker node:\nManual autoscaling is not always sufficient as the cluster is too dynamic so Rancher does have support Node Autoscaling\nAutoscaling: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/rancher/README.md\nScaling nodes vertically - add cpu, ram and disk # What if I need to change the cpu, memory and disk resources on the nodes? An unexpected change in the workload dictates the nodes to be handle to change that and scaling horizontally does not solve it. Then I need an easy way to change the current nodes resource.\nFrom the Cluster Manager and Clusters view, click on the name of the cluster that needs to have its nodes resources changed, and from there click Config:\nSelect the pool you want to change the resources on, I will select pool2 as thats where my worker nodes are residing.\nThen in the top right corner click on the three dots and select edit config to allow for the required fields to be edited.\nFrom there I will adjust the memory and cpu on the pool2 from 2 cpus to 4 and from 4096mb memory to 6144mb memory. Then click save.\nThis triggers a rollout of a new worker nodes until all nodes have been replace with new nodes using the new cpu and memory config:\nThats it\nUpgrading Kubernetes clusters # This is probably one of the most important task maintaining a Kubernetes cluster, keeping pace with Kubernetes updates and performing Kubernetes upgrades. New Kubernetes release is coming approximately every 3 months.\nSo how can I upgrade my Kubernetes cluster using Rancher?\nAlmost same approach as above, but instead of changing any of the CPU and Memory resources I will scroll down and select the new Kubernetes version I want to upgrade to:\nI have a cluster currently running version 1.27.12 and want to upgrade to 1.28.8. I will now select the latest version available and then save:\nContinue.\nThis will not trigger a rolling upgrade, instead it will do a in-line upgrade by cordon node by node and the upgrade the node.\nMy control plane nodes has been upgrade to 1.28.8 already.\nAll nodes have been upgraded:\nThis took just a couple of seconds (I was suspecting that something was wrong actually), it really only just took a couple of seconds. Everything worked as it should afterwards, I must say a relatively smoth process.\nInstalling another Ingress controller than the default Ngninx - Avi Kubernetes Operator # In my vSphere environment I already have Avi Loadbalancer running, Avi can also be extended into Kubernetes to automate K8s service creation and also Ingress and Gateway API. So in this section I will create a cluster with AKO as my Ingress controller.\nBy default Rancher installs Nginx as the Ingress controller. To install my own Ingress controller, like AKO, I will deselect Nginx in the cluster creation wizard:\nDeploy the cluster, wait for it be finished.\nNow that I have a cluster deployed without any ingressClasses or Ingress controller I can go on an deploy the ingress controller of my choice. As from my understanding there is currently no support for OCI Helm Chart repositories as of yet in Rancher I need to deploy AKO using the cli approach. Otherwise I could just have added the AKO repo through the GUI and deploy it as an APP.\nFrom my linux machine, I have grabbed the context of my newly created K8s cluster without any Ingress installed. Then I will just follow the regular AKO installation instructions:\nandreasm@linuxmgmt01:~/rancher$ k get ingressclasses.networking.k8s.io No resources found andreasm@linuxmgmt01:~/rancher$ k create ns avi-system namespace/avi-system created andreasm@linuxmgmt01:~/rancher$ helm show values oci://projects.registry.vmware.com/ako/helm-charts/ako --version 1.11.3 \u0026gt; values.yaml Pulled: projects.registry.vmware.com/ako/helm-charts/ako:1.11.3 Digest: sha256:e4d0fcd80ae5b5377edf510d3d085211181a3f6b458a17c8b4a19d328e4cdfe6 # Will now edit the values.yaml file to accomodate my Avi config settings # After edit I will deploy AKO andreasm@linuxmgmt01:~/rancher$ helm install --generate-name oci://projects.registry.vmware.com/ako/helm-charts/ako --version 1.11.3 -f values.yaml -n avi-system Pulled: projects.registry.vmware.com/ako/helm-charts/ako:1.11.3 Digest: sha256:e4d0fcd80ae5b5377edf510d3d085211181a3f6b458a17c8b4a19d328e4cdfe6 NAME: ako-1715330716 LAST DEPLOYED: Fri May 10 10:45:19 2024 NAMESPACE: avi-system STATUS: deployed REVISION: 1 TEST SUITE: None # Check for pods and Ingress Classes andreasm@linuxmgmt01:~/rancher$ k get pods -n avi-system NAME READY STATUS RESTARTS AGE ako-0 1/1 Running 0 39s andreasm@linuxmgmt01:~/rancher$ k get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 47s Now I have an Ingress controller installed in my cluster.\nHow about going back to the Rancher UI and create and Ingress using my newly deployed ingress controller?\nI will go into my cluster, Service Discovery -\u0026gt; Ingresses\nNow lets try to create an Ingress:\nTarget Service and Ports are selected from the dropdown lists as long as the services are available from within the same namespace as I create my Ingress. Thats really nice.\nSelecting my IngressClass from the dropdown.\nThen click Create:\nandreasm@linuxmgmt01:~/rancher$ k get ingress -n fruit NAME CLASS HOSTS ADDRESS PORTS AGE fruit-ingress avi-lb fruit-1.my-domain.net 192.168.121.13 80 35s In my Avi controller I should now have a new Virtual Service using the fqdn name:\nInstalling Custom CNI - bootstrapping new clusters using Antrea CNI # What if I want to use a CNI that is not in the list of CNIs in Rancher?\nRancher provide a list of tested CNIs to choose from when deploying a new RKE2 cluster, but one is not limited to only choose one of these CNIs. It is possible to create a cluster using your own preferred CNI. So in this chapter I will go through how I can deploy a new RKE2 cluster using Antrea as my CNI.\nBefore I can bootstrap a cluster with Antrea (or any custom CNI) I need to tell Rancher to not deploy any CNI from the list of provided CNIs. This involves editing the cluster deployment using the YAML editor in Rancher. There is two ways I can install Antrea as the CNI during cluster creation/bootsrap, one is using the Antrea deployment manifest the other is using Helm Charts. The Helm Chart approach is the recommended way if you ask me, but one may not always have the option use Helm Chart that is way I have included both approaches. Below I will go through both of them, starting with the Antrea manifest. I need to complete one of these steps before clicking on the create cluster button.\nAdd-on Config using manifest # After I have gone through the typical cluster creation steps, node pool config, cluster config etc I will need to add my Antrea manifest in the Add-On Config section.\nSo I will be going into the Cluster Configuration section and Add-On Config\nHere I can add the deployment manifest for Antrea by just copy paste the content directly from the Antrea repo here. This will always take you to the latest version of Antrea. Do the necessary configs in the deployment yaml like enabling or disabling the Antrea feature gates, if needed. They can be changed post cluster provision too.\nDisregard the already populated Calico Configuration section:\nThis will be removed at a later stage completely. I need to paste my Antrea deployment manifest under Additional Manifest\nWhen done proceed to the step which involves disabling the default CNI that Rancher wants to deploy. It is not possible set CNI to none from the UI so I need to configure the yaml directly to tell it to not install any CNI. That is done by following the steps below.\nClick edit as yaml\nWithin my cluster yaml, there is certain blocks I need to edit and remove. Below is a \u0026ldquo;default\u0026rdquo; yaml using Calico as CNI, then I will paste the yaml which I have edited to allow me to install my custom CNI. See comments in the yamls by extending the code-field. I have only pasted parts of the yaml that is involved to make it \u0026ldquo;shorter\u0026rdquo;.\nThe \u0026ldquo;untouched\u0026rdquo; yaml:\n# Before edited - expand field for all content to display apiVersion: provisioning.cattle.io/v1 kind: Cluster metadata: name: rke2-cluster-4-antrea annotations: field.cattle.io/description: RKE Cluster with Antrea # key: string labels: {} # key: string namespace: fleet-default spec: cloudCredentialSecretName: cattle-global-data:cc-qsq7b clusterAgentDeploymentCustomization: kubernetesVersion: v1.28.8+rke2r1 localClusterAuthEndpoint: caCerts: \u0026#39;\u0026#39; enabled: false fqdn: \u0026#39;\u0026#39; rkeConfig: chartValues: ## Need to add {} due to remove the below line rke2-calico: {} ### This needs to be removed etcd: disableSnapshots: false s3: # bucket: string # cloudCredentialName: string # endpoint: string # endpointCA: string # folder: string # region: string # skipSSLVerify: boolean snapshotRetention: 5 snapshotScheduleCron: 0 */5 * * * machineGlobalConfig: cluster-cidr: 10.10.0.0/16 cni: calico ### Need to set this to none disable-kube-proxy: false etcd-expose-metrics: false service-cidr: 10.20.0.0/16 profile: null The edited yaml:\n# Before edited - expand field for all content to display apiVersion: provisioning.cattle.io/v1 kind: Cluster metadata: name: rke2-cluster-4-antrea annotations: field.cattle.io/description: RKE Cluster with Antrea # key: string labels: {} # key: string namespace: fleet-default spec: cloudCredentialSecretName: cattle-global-data:cc-qsq7b clusterAgentDeploymentCustomization: kubernetesVersion: v1.28.8+rke2r1 localClusterAuthEndpoint: caCerts: \u0026#39;\u0026#39; enabled: false fqdn: \u0026#39;\u0026#39; rkeConfig: chartValues: {} etcd: disableSnapshots: false s3: # bucket: string # cloudCredentialName: string # endpoint: string # endpointCA: string # folder: string # region: string # skipSSLVerify: boolean snapshotRetention: 5 snapshotScheduleCron: 0 */5 * * * machineGlobalConfig: cluster-cidr: 10.10.0.0/16 cni: none disable-kube-proxy: false ## See comment below etcd-expose-metrics: false service-cidr: 10.20.0.0/16 profile: null After editing the yaml as indicated above it is time to click Create.\nAfter a couple of minutes the cluster should be getting green and ready to consume with Antrea as the CNI.\nNext up is the Helm Chart approach.\nAdd-on Config using Helm Charts # I will follow almost the same approach as above, but instead of providing the Antrea manifest, I will be providing the Antrea Helm Chart repo.\nSo to save some digital ink, head over to Add-On Config and populate the Additional Manifest with the following content:\nBelow is the yaml:\napiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: antrea namespace: kube-system spec: bootstrap: true # this is important chart: antrea targetNamespace: kube-system repo: https://charts.antrea.io version: v1.15.1 # if I want a specific version to be installed Below is a template to see which key/values that can be used:\napiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: #string namespace: default # annotations: key: string # labels: key: string spec: # authPassCredentials: boolean # authSecret: # name: string # backOffLimit: int # bootstrap: boolean # chart: string # chartContent: string # createNamespace: boolean # dockerRegistrySecret: # name: string # failurePolicy: string # helmVersion: string # jobImage: string # podSecurityContext: # fsGroup: int # fsGroupChangePolicy: string # runAsGroup: int # runAsNonRoot: boolean # runAsUser: int # seLinuxOptions: # type: string # level: string # role: string # user: string # seccompProfile: # type: string # localhostProfile: string # supplementalGroups: # - int # sysctls: # - name: string # value: string # windowsOptions: # gmsaCredentialSpec: string # gmsaCredentialSpecName: string # hostProcess: boolean # runAsUserName: string # repo: string # repoCA: string # repoCAConfigMap: # name: string # securityContext: # allowPrivilegeEscalation: boolean # capabilities: # add: # - string # drop: # - string # privileged: boolean # procMount: string # readOnlyRootFilesystem: boolean # runAsGroup: int # runAsNonRoot: boolean # runAsUser: int # seLinuxOptions: # type: string # level: string # role: string # user: string # seccompProfile: # type: string # localhostProfile: string # windowsOptions: # gmsaCredentialSpec: string # gmsaCredentialSpecName: string # hostProcess: boolean # runAsUserName: string # set: map[] # targetNamespace: string # timeout: string # valuesContent: string # version: string Then edit the cluster yaml to disable the Rancher provided CNI, as described above.\nClick create\nBelow is a complete yaml of the cluster being provisioned with Antrea as the CNI using Helm Chart option.\napiVersion: provisioning.cattle.io/v1 kind: Cluster metadata: name: rke2-cluster-4-antrea annotations: field.cattle.io/description: RKE2 using Antrea labels: {} namespace: fleet-default spec: cloudCredentialSecretName: cattle-global-data:cc-qsq7b clusterAgentDeploymentCustomization: appendTolerations: overrideResourceRequirements: defaultPodSecurityAdmissionConfigurationTemplateName: \u0026#39;\u0026#39; defaultPodSecurityPolicyTemplateName: \u0026#39;\u0026#39; fleetAgentDeploymentCustomization: appendTolerations: overrideAffinity: overrideResourceRequirements: kubernetesVersion: v1.28.9+rke2r1 localClusterAuthEndpoint: caCerts: \u0026#39;\u0026#39; enabled: false fqdn: \u0026#39;\u0026#39; rkeConfig: additionalManifest: |- apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: antrea namespace: kube-system spec: bootstrap: true # This is important chart: antrea targetNamespace: kube-system repo: https://charts.antrea.io chartValues: {} etcd: disableSnapshots: false s3: snapshotRetention: 5 snapshotScheduleCron: 0 */5 * * * machineGlobalConfig: cluster-cidr: 10.10.0.0/16 cni: none disable-kube-proxy: false etcd-expose-metrics: false service-cidr: 10.20.0.0/16 profile: null machinePools: - name: pool1 etcdRole: true controlPlaneRole: true workerRole: true hostnamePrefix: \u0026#39;\u0026#39; quantity: 1 unhealthyNodeTimeout: 0m machineConfigRef: kind: VmwarevsphereConfig name: nc-rke2-cluster-4-antrea-pool1-gh2nj drainBeforeDelete: true machineOS: linux labels: {} - name: pool2 etcdRole: false controlPlaneRole: false workerRole: true hostnamePrefix: \u0026#39;\u0026#39; quantity: 1 unhealthyNodeTimeout: 0m machineConfigRef: kind: VmwarevsphereConfig name: nc-rke2-cluster-4-antrea-pool2-s5sv4 drainBeforeDelete: true machineOS: linux labels: {} machineSelectorConfig: - config: protect-kernel-defaults: false registries: configs: {} mirrors: {} upgradeStrategy: controlPlaneConcurrency: \u0026#39;1\u0026#39; controlPlaneDrainOptions: deleteEmptyDirData: true disableEviction: false enabled: false force: false gracePeriod: -1 ignoreDaemonSets: true skipWaitForDeleteTimeoutSeconds: 0 timeout: 120 workerConcurrency: \u0026#39;1\u0026#39; workerDrainOptions: deleteEmptyDirData: true disableEviction: false enabled: false force: false gracePeriod: -1 ignoreDaemonSets: true skipWaitForDeleteTimeoutSeconds: 0 timeout: 120 machineSelectorConfig: - config: {} __clone: true By doing the steps above, I should shortly have a freshly installed Kubernetes cluster deployed using Antrea as my CNI.\n# Run kubectl commands inside here # e.g. kubectl get all \u0026gt; k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE cattle-fleet-system fleet-agent-7cf5c7b6cb-cbr4c 1/1 Running 0 35m cattle-system cattle-cluster-agent-686798b687-jq9kp 1/1 Running 0 36m cattle-system cattle-cluster-agent-686798b687-q97mx 1/1 Running 0 34m cattle-system dashboard-shell-6tldq 2/2 Running 0 7s cattle-system helm-operation-95wr6 0/2 Completed 0 35m cattle-system helm-operation-jtgzs 0/2 Completed 0 29m cattle-system helm-operation-qz9bc 0/2 Completed 0 34m cattle-system rancher-webhook-7d57cd6cb8-4thw7 1/1 Running 0 29m cattle-system system-upgrade-controller-6f86d6d4df-x46gp 1/1 Running 0 35m kube-system antrea-agent-9qm2d 2/2 Running 0 37m kube-system antrea-agent-dr2hz 2/2 Running 1 (33m ago) 34m kube-system antrea-controller-768b4dbcb5-7srnw 1/1 Running 0 37m kube-system cloud-controller-manager-rke2-cluster-5-antrea-pool1-fea898fd-t7fhv 1/1 Running 0 38m kube-system etcd-rke2-cluster-5-antrea-pool1-fea898fd-t7fhv 1/1 Running 0 38m kube-system helm-install-antrea-vtskp 0/1 Completed 0 34m kube-system helm-install-rke2-coredns-8lrrq 0/1 Completed 0 37m kube-system helm-install-rke2-ingress-nginx-4bbr4 0/1 Completed 0 37m kube-system helm-install-rke2-metrics-server-8xrbh 0/1 Completed 0 37m kube-system helm-install-rke2-snapshot-controller-crd-rt5hb 0/1 Completed 0 37m kube-system helm-install-rke2-snapshot-controller-pncxt 0/1 Completed 0 37m kube-system helm-install-rke2-snapshot-validation-webhook-zv9kl 0/1 Completed 0 37m kube-system kube-apiserver-rke2-cluster-5-antrea-pool1-fea898fd-t7fhv 1/1 Running 0 38m kube-system kube-controller-manager-rke2-cluster-5-antrea-pool1-fea898fd-t7fhv 1/1 Running 0 38m kube-system kube-proxy-rke2-cluster-5-antrea-pool1-fea898fd-t7fhv 1/1 Running 0 38m kube-system kube-proxy-rke2-cluster-5-antrea-pool2-209a1e89-9sjt7 1/1 Running 0 34m kube-system kube-scheduler-rke2-cluster-5-antrea-pool1-fea898fd-t7fhv 1/1 Running 0 38m kube-system rke2-coredns-rke2-coredns-84b9cb946c-6hrfv 1/1 Running 0 37m kube-system rke2-coredns-rke2-coredns-84b9cb946c-z2bb5 1/1 Running 0 34m kube-system rke2-coredns-rke2-coredns-autoscaler-b49765765-spj7d 1/1 Running 0 37m kube-system rke2-ingress-nginx-controller-8p7rz 1/1 Running 0 34m kube-system rke2-ingress-nginx-controller-zdbvm 0/1 Pending 0 37m kube-system rke2-metrics-server-655477f655-5n7sp 1/1 Running 0 37m kube-system rke2-snapshot-controller-59cc9cd8f4-6mtbs 1/1 Running 0 37m kube-system rke2-snapshot-validation-webhook-54c5989b65-jprqr 1/1 Running 0 37m Now with Antrea installed as the CNI I can go ahead and use the rich set of feature gates Antrea brings to table.\nTo enable and disable the Antrea FeatureGates either edit the configMap using kubectl edit configmap -n kube-system antrea-config or use the Rancher UI:\nUpgrading or changing version of the Antrea CNI using Rancher # When I deploy Antrea using the Helm Chart Add-On Config, upgrading or changing the version of Antrea is a walk in the park.\nIn the cluster I want to upgrade/downgrade Antrea I first need to add the Antrea Helm Repo under App -\u0026gt; Repositories:\nThen head back to Installed Apps under Apps and click on antrea\nClick top right corner:\nFollow the wizard and select the version you want, then click next.\nChange values or dont, and click Upgrade:\nSit back and enjoy:\nNow I just downgraded from version 2.0.0 to 1.15.1, the same approach is true for upgrading to a newer version of course. But why upgrade when one can downgrade, one need to have some fun in life\u0026hellip;\nIn the list of Installed Apps view Rancher will notify me if there is a new release available. This is a nice indicator whenever there is a new release:\nNeuVector # NeuVector is new for me, by having a quick look at what it does it did spark my interest. From the official docs page:\nNeuVector provides a powerful end-to-end container security platform. This includes end-to-end vulnerability scanning and complete run-time protection for containers, pods and hosts, including:\nCI/CD Vulnerability Management \u0026amp; Admission Control. Scan images with a Jenkins plug-in, scan registries, and enforce admission control rules for deployments into production. Violation Protection. Discovers behavior and creates a whitelist based policy to detect violations of normal behavior. Threat Detection. Detects common application attacks such as DDoS and DNS attacks on containers. DLP and WAF Sensors. Inspect network traffic for Data Loss Prevention of sensitive data, and detect common OWASP Top10 WAF attacks. Run-time Vulnerability Scanning. Scans registries, images and running containers orchestration platforms and hosts for common (CVE) as well as application specific vulnerabilities. Compliance \u0026amp; Auditing. Runs Docker Bench tests and Kubernetes CIS Benchmarks automatically. Endpoint/Host Security. Detects privilege escalations, monitors processes and file activity on hosts and within containers, and monitors container file systems for suspicious activity. Multi-cluster Management. Monitor and manage multiple Kubernetes clusters from a single console. Other features of NeuVector include the ability to quarantine containers and to export logs through SYSLOG and webhooks, initiate packet capture for investigation, and integration with OpenShift RBACs, LDAP, Microsoft AD, and SSO with SAML. Note: Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container.\nIf I head over to any of my clusters in Rancher under Apps -\u0026gt; Charts I will find a set of ready to install applications. There I also find NeuVector:\nLets try an installation of NeuVector. I will keep everything at default for now. As one can imagine there is some configurations that should be done if deploying it in production, like Ingress, Certificate, PVC etc. By just deploying with all default values it will be exposed using NodePort, selfsigned certificate and no PVC.\n\u0026gt; k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE cattle-neuvector-system neuvector-controller-pod-5b8b6f7df-cctd7 1/1 Running 0 53s cattle-neuvector-system neuvector-controller-pod-5b8b6f7df-ql5dz 1/1 Running 0 53s cattle-neuvector-system neuvector-controller-pod-5b8b6f7df-tmb77 1/1 Running 0 53s cattle-neuvector-system neuvector-enforcer-pod-lnsfl 1/1 Running 0 53s cattle-neuvector-system neuvector-enforcer-pod-m4gjp 1/1 Running 0 53s cattle-neuvector-system neuvector-enforcer-pod-wr5j4 1/1 Running 0 53s cattle-neuvector-system neuvector-manager-pod-779f9976f8-cmlh5 1/1 Running 0 53s cattle-neuvector-system neuvector-scanner-pod-797cc57f7d-8lztx 1/1 Running 0 53s cattle-neuvector-system neuvector-scanner-pod-797cc57f7d-8qhr7 1/1 Running 0 53s cattle-neuvector-system neuvector-scanner-pod-797cc57f7d-gcbr9 1/1 Running 0 53s NeuVector is up and running.\nLets have a quick look inside NeuVector\nDefault password is admin\nFirst glance:\nDashboard\nNetwork Activity\nNetwork Rules\nSettings\nFor more information head over to the official NeuVector documentation page here\nNeuvector Rancher UI Extension\nNeuvector can also be installed as an extension in Rancher:\nIf I now head into the cluster I have deployed NeuVector, I will get a great overview from NeuVector directly in the Rancher UI:\nThat is very neat. And again, it was just one click to enable this extension.\nMonitoring # Keeping track of whats going on in the whole Kubernetes estate is crucial, everything from Audit logs, performance, troubleshooting.\nI will try out the Monitoring app that is available under Monitoring tools, or Cluster tools here:\nAgain, using default values:\nSUCCESS: helm upgrade --install=true --namespace=cattle-monitoring-system --timeout=10m0s --values=/home/shell/helm/values-rancher-monitoring-103.1.0-up45.31.1.yaml --version=103.1.0+up45.31.1 --wait=true rancher-monitoring /home/shell/helm/rancher-monitoring-103.1.0-up45.31.1.tgz 2024-05-11T08:19:06.403230781Z --------------------------------------------------------------------- Now I suddenly can access it all from the Monitoring section within my cluster:\nNotice the Active Alerts.\nFrom the Cluster Dashboard:\nLets see whats going on in and with my cluster:\nAlertManager\nGrafana\nA bunch of provided dashboards:\nThe Rancher API # One can interact with Rancher not only using the great Rancher UI, but using the CLI, kubectl and from external applications.\nLets start with the CLI. The binary for the CLI can be downloaded from the Rancher UI by going to about - here:\nI download the tar file for mac, extract it using tar -zxvf rancher-darwin-amd64-v2.8.3.tar.gz and then copy/and move it to /user/local/bin/ folder.\nBefore I can use th CLI to connect to my Rancher endpoint I need to create an api token here:\nNo Scopes as I want access to them all\nNow I need to copy these keys, so I have them. They will expire in 90 days. If I loose them I have to create new ones.\nandreasm@linuxmgmt01:~/rancher$ rancher login https://rancher-dev.my-domain.net --token token-m8k88:cr89kvdq4xfsl4pcnn4rh5xkdcw2rhvz46cxlrpw29qblqxbsnz NUMBER CLUSTER NAME PROJECT ID PROJECT NAME PROJECT DESCRIPTION 1 rke2-cluster-3-ako c-m-6h64tt4t:p-dt4qw Default Default project created for the cluster 2 rke2-cluster-3-ako c-m-6h64tt4t:p-rpqhc System System project created for the cluster 3 rke2-cluster-1 c-m-d85dvj2l:p-gpczf System System project created for the cluster 4 rke2-cluster-1 c-m-d85dvj2l:p-l6cln Default Default project created for the cluster 5 rke2-cluster-2-vsphere c-m-ddljkdhx:p-57cpk Default Default project created for the cluster 6 rke2-cluster-2-vsphere c-m-ddljkdhx:p-fzdtp System System project created for the cluster 7 k8s-prod-cluster-2 c-m-pjl4pxfl:p-8gc9x System System project created for the cluster 8 k8s-prod-cluster-2 c-m-pjl4pxfl:p-f2kkb Default Default project created for the cluster 9 tanzuz-cluster-1 c-m-qxpbc6tp:p-mk49f System System project created for the cluster 10 tanzuz-cluster-1 c-m-qxpbc6tp:p-s428w Default Default project created for the cluster 11 rke2-cluster-proxmox-1 c-m-tllp6sd6:p-nrxgl Default Default project created for the cluster 12 rke2-cluster-proxmox-1 c-m-tllp6sd6:p-xsmbd System System project created for the cluster 13 local local:p-tqk2t Default Default project created for the cluster 14 local local:p-znt9n System System project created for the cluster Select a Project: Selecting project rke2-cluster-2-vsphere ID 5:\nSelect a Project:5 INFO[0123] Saving config to /Users/andreasm/.rancher/cli2.json andreasm@linuxmgmt01:~/rancher$ rancher kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE calico-system calico-kube-controllers-7fdf877475-svqt8 1/1 Running 0 22h #Switch between contexts andreasm@linuxmgmt01:~/rancher$ rancher context switch NUMBER CLUSTER NAME PROJECT ID PROJECT NAME PROJECT DESCRIPTION 1 rke2-cluster-3-ako c-m-6h64tt4t:p-dt4qw Default Default project created for the cluster 2 rke2-cluster-3-ako c-m-6h64tt4t:p-rpqhc System System project created for the cluster 3 rke2-cluster-1 c-m-d85dvj2l:p-gpczf System System project created for the cluster 4 rke2-cluster-1 c-m-d85dvj2l:p-l6cln Default Default project created for the cluster 5 rke2-cluster-2-vsphere c-m-ddljkdhx:p-57cpk Default Default project created for the cluster 6 rke2-cluster-2-vsphere c-m-ddljkdhx:p-fzdtp System System project created for the cluster 7 k8s-prod-cluster-2 c-m-pjl4pxfl:p-8gc9x System System project created for the cluster 8 k8s-prod-cluster-2 c-m-pjl4pxfl:p-f2kkb Default Default project created for the cluster 9 tanzuz-cluster-1 c-m-qxpbc6tp:p-mk49f System System project created for the cluster 10 tanzuz-cluster-1 c-m-qxpbc6tp:p-s428w Default Default project created for the cluster 11 rke2-cluster-proxmox-1 c-m-tllp6sd6:p-nrxgl Default Default project created for the cluster 12 rke2-cluster-proxmox-1 c-m-tllp6sd6:p-xsmbd System System project created for the cluster 13 local local:p-tqk2t Default Default project created for the cluster 14 local local:p-znt9n System System project created for the cluster Select a Project: More information on the CLI reference page here\nMore information on the API here\nkubectl\nUsing kubectl without the Rancher CLI is of course possible, just grab the contexts from your different cluster you have access to from the Rancher UI, copy or download them and put into the .kube/config or point to them as below and interact with Rancher managed Kubernetes clusters as they were any other Kubernetes cluster:\nandreasm@linuxmgmt01:~/rancher$ k --kubeconfig rke-ako.yaml get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE avi-system ako-0 1/1 Running 0 24h calico-system calico-kube-controllers-67f857444-qwtd7 1/1 Running 0 25h calico-system calico-node-4k6kl 1/1 Running 0 25h calico-system calico-node-wlzpt 1/1 Running 0 25h calico-system calico-typha-75d7fbfc6c-bhgxj 1/1 Running 0 25h cattle-fleet-system fleet-agent-7cf5c7b6cb-kqgqm 1/1 Running 0 25h cattle-system cattle-cluster-agent-8548cbfbb7-b4t5f 1/1 Running 0 25h cattle-system cattle-cluster-agent-8548cbfbb7-hvdcx 1/1 Running 0 25h cattle-system rancher-webhook-7d57cd6cb8-gwvr4 1/1 Running 0 25h cattle-system system-upgrade-controller-6f86d6d4df-zg7lv 1/1 Running 0 25h fruit apple-app 1/1 Running 0 24h fruit banana-app 1/1 Running 0 24h kube-system cloud-controller-manager-rke2-cluster-3-ako-pool1-b6cddca0-tf84w 1/1 Running 0 25h kube-system etcd-rke2-cluster-3-ako-pool1-b6cddca0-tf84w 1/1 Running 0 25h kube-system helm-install-rke2-calico-crd-w2skj 0/1 Completed 0 25h kube-system helm-install-rke2-calico-lgcdp 0/1 Completed 1 25h kube-system helm-install-rke2-coredns-xlrtx 0/1 Completed 0 25h kube-system helm-install-rke2-metrics-server-rb7qj 0/1 Completed 0 25h kube-system helm-install-rke2-snapshot-controller-crd-bhrq5 0/1 Completed 0 25h kube-system helm-install-rke2-snapshot-controller-wlftf 0/1 Completed 0 25h kube-system helm-install-rke2-snapshot-validation-webhook-jmnm2 0/1 Completed 0 25h kube-system kube-apiserver-rke2-cluster-3-ako-pool1-b6cddca0-tf84w 1/1 Running 0 25h kube-system kube-controller-manager-rke2-cluster-3-ako-pool1-b6cddca0-tf84w 1/1 Running 0 25h kube-system kube-proxy-rke2-cluster-3-ako-pool1-b6cddca0-tf84w 1/1 Running 0 25h kube-system kube-proxy-rke2-cluster-3-ako-pool2-50bb46e0-tj2kp 1/1 Running 0 25h kube-system kube-scheduler-rke2-cluster-3-ako-pool1-b6cddca0-tf84w 1/1 Running 0 25h kube-system rke2-coredns-rke2-coredns-84b9cb946c-jj2gk 1/1 Running 0 25h kube-system rke2-coredns-rke2-coredns-84b9cb946c-wq4hs 1/1 Running 0 25h kube-system rke2-coredns-rke2-coredns-autoscaler-b49765765-c86ps 1/1 Running 0 25h kube-system rke2-metrics-server-544c8c66fc-hkdh9 1/1 Running 0 25h kube-system rke2-snapshot-controller-59cc9cd8f4-np4lt 1/1 Running 0 25h kube-system rke2-snapshot-validation-webhook-54c5989b65-9zk7w 1/1 Running 0 25h tigera-operator tigera-operator-779488d655-7x25x 1/1 Running 0 25h Authentication and RBAC with Rancher # Rancher provides a robust Authentication and RBAC out of the box. For more information on authentication and RBAC in Rancher head over here here\nRancher can integrate with the following external authentication services, in addition to local user authentication:\nMicrosoft Active Directory Microsoft Azure AD Microsoft AD FS GitHub FreeIPA OpenLDAP PingIdentity KeyCloak (OIDC) KeyCloak ((SAML) Okta Google OAuth Shibboleth RBAC is an important part in managing a Kubernetes estate. I will cover RBAC in Rancher in more depth in another blog post.\nOutro # My experience with Rancher has been very positive, I would say way over the expectations I had before I started writing this post. It (Rancher) just feels like a well thought through product, solid and complete. It brings a lot of built-in features and tested applications, but its not limiting me to do things differently nor add my own \u0026ldquo;custom\u0026rdquo; things to it. Delivering a product with so many features out-of-the-box, without setting to many limitations on what I can do \u0026ldquo;outside-the-box\u0026rdquo; is not an easy achievement, but Rancher gives me the feeling they have acheived it. I feel like whatever feature I wanted, Rancher had it, at the same time many features I even haven\u0026rsquo;t thought of, Rancher had it.\nA very powerful Kubernetes management platform. Its open-source and available.\nStay tuned for some follow-up posts where I deepdive into specific topics of Rancher. This will be fun.\n","date":"5 February 2023","externalUrl":null,"permalink":"/2023/02/05/taking-rancher-by-suse-for-a-spin/","section":"Posts","summary":"In this post I wil go through Rancher and test out some of the possibilties and capabilties managing Kubernetes using Rancher.","title":"Taking Rancher by SUSE for a Spin","type":"posts"},{"content":"","date":"2 February 2023","externalUrl":null,"permalink":"/tags/compliance/","section":"Tags","summary":"","title":"Compliance","type":"tags"},{"content":"","date":"2 February 2023","externalUrl":null,"permalink":"/categories/compliance/","section":"Categories","summary":"","title":"Compliance","type":"categories"},{"content":"","date":"2 February 2023","externalUrl":null,"permalink":"/tags/neuvector/","section":"Tags","summary":"","title":"Neuvector","type":"tags"},{"content":"","date":"2 February 2023","externalUrl":null,"permalink":"/categories/neuvector/","section":"Categories","summary":"","title":"NeuVector","type":"categories"},{"content":" NeuVector # After doing my post on Rancher here I wrote that I would do a dedicated post on NeuVector, among other topics. So this post will be a more deep dive post covering some of the features in NeuVector and how to configure and use them. NeuVector is a comprehensive security solution with many features covering several aspects, so I decided to divide this post into a 2-series post instead of covering everything in one big post.\nI have been working a lot with Kubernetes Network policies by utilizing the CNIs built-in network security features, especially Antrea Native Policies. I have also covered most of that in earlier posts in my blog. This time it will be a bit different angle and approach.\nUsing Antrea Native Policies or Kubernetes Network policies is more on the network layer than on the application layer of a pod in a Kubernetes infrastructure. In this post I will focus more on the application layer (including network application aware rules) by using the NeuVector security features. Both Antrea Native Policies (using the CNIs built in security policies) and NeuVector policies/rules will be used as combining the two will contribute to a more secure platform. It is again not meant to be a comprehensive post covering best practices but may open up for some ideas and thinking around Kubernetes security.\nHaving in mind that security is not just one place in the infrastructure, it needs to be everywhere. Its not sufficient to just focus on the network, we need visibility and security mechanisms in place in all areas of the infrastructure - end to end. In a Kubernetes environment this involves the build process to the registry where our images are hosted/uploaded. If that is not compliant we will potentially just push vulnerable images into our Kubernetes platform. It also involves what is allowed on the \u0026ldquo;wire\u0026rdquo; between nodes, pods, ingress and egress and whats going on inside the containers and nodes in form of processes, files, checking compliance and vulnerabilites on nodes and containers ++. NeuVector brings all these aspects together in one single management plane making it easier to manage security on all these entities.\nNeuVector Architecture # From the official NeuVector documentation page they already provide a diagram of the NeuVector Architecture:\nShort explanations of the components:\nController\nThe Controller manages the NeuVector Enforcer container cluster. It also provides REST APIs for the management console. Although typical test deployments have one Controller, multiple Controllers in a high-availability configuration is recommended. 3 controllers is the default in the Kubernetes Production deployment sample yaml.\nEnforcer\nThe Enforcer is a lightweight container that enforces the security policies. One enforcer should be deployed on each node (host), e.g. as a Daemon set.\nManager\nThe Manager is a stateless container that provides a web-UI (HTTPS only) console for users to manage the NeuVector security solution. More than one Manager container can be deployed as necessary.\nAll-in-One\nThe All-in-One container includes a Controller, an Enforcer and a Manager in one package. It\u0026rsquo;s useful for easy installation in single-node or small-scale deployments.\nScanner\nThe Scanner is a container which performs the vulnerability and compliance scanning for images, containers and nodes. It is typically deployed as a replicaset and can be scaled up to as many parallel scanners as desired in order to increase the scanning performance. The Controller assigns scanning jobs to each available scanner in a round-robin fashion until all scans are completed. The scanner also contains the latest CVE database and is updated regularly by NeuVector.\nUpdater\nThe Updater is a container which when run, updates the CVE database for NeuVector. NeuVector regularly publishes new scanner images to include the latest CVE for vulnerability scans. The updater re-deploys all scanner pods by taking the deployment to zero and scaling it back up, forcing a pull of an updated scanner image.\nNeuVector installation # NeuVector can be deployed many ways and on many different platforms, for an extensive list of supported methods and platforms see here.\nIf you happen to use Rancher, installing NeuVector is just a click in the Rancher UI. Ofcourse there are some options that should be configured to ensure access (Ingress) and data-persistence.\nI will quickly go through the installation of NeuVector using Rancher.\nFrom the Cluster Dashboard for the cluster you want to deploy NeuVector in head to Apps - Charts and find NeuVector and click on it:\nThen from the next window find the Install button in the top right corner:\nThen in the next step select the Project and click Next:\nIn the next step I will leave everything at default except the Ingress. Here I will select Ingress for the Manager and enter my hostname then when I am done click Install:\nDone.\nNeuvector is now installed and I should be able to reach the ui by grabbing the IP from the Ingress object or from within the Rancher UI:\nNAMESPACE NAME CLASS HOSTS ADDRESS PORTS AGE cattle-neuvector-system neuvector-webui-ingress avi-lb neuvector-1.my-domain.net 192.168.121.12 80 68s When logged in it will show the Dashboard providing a quick glance at the current state:\nNetwork Activity Dashboard # Out of the box NeuVector provides a very useful and handy graphical map with container traffic flows. I found this dashboard to be pretty impressive and powerful in the process of securing my containers with network rules. Instead of using all kinds of tools to verify network rules being blocked or allowed, the Network Activity dashboard could show me exactly what was going on. It saved a lot of time and I could see what was going on. If I wanted to know if one of my applications were being blocked or allowed and what kind of communication was going on, in and out of my containers, the map showed me that. I didnt have to guess anymore. I also got the impression that as a soon as new flow occurred it was updated in the map, no delay there. I will quickly go through what kind of information that is provided in the dashboard.\nI will begin by having a look in the top left corner of the map:\nHere I will find some useful settings to adjust the map to focus on what I will have a look at instead of the whole picture. From the left:\n\u0026#x2795; and \u0026#x2796; sign for zooming in and out the view \u0026lt;-\u0026gt; make everything fit to window \u0026#x1f50d; search for a node by name \u0026#x1f39b;\u0026#xfe0f; match a view to your area of interest (more on that later) \u0026#x1f441;\u0026#xfe0f;\u0026zwj;\u0026#x1f5e8;\u0026#xfe0f; hide namespace, group or endpoint \u0026#x1f4f7; take a snapshot of the view \u0026#x1f4d4; show legend \u0026#x1f504; refresh view - to show any new flows immediately I want to know whats going on with my demo application Yelb, that I have deployed in the namespace yelb. I will click on the parameters button \u0026#x1f39b;\u0026#xfe0f; button. There I will enter the namespace yelb to filter only on the namespace of interest, leaving all else default.\nClick apply\nThis should now only show me the groups (groups will be explained later) in the namespace yelb:\nNow the view may not be optimal to begin with, but I can drag the groups around the way I want them;\nThis gives me a better \u0026ldquo;logical\u0026rdquo; view of the groups in the namespace. The first thing I notice in this view is the direction of the flow. And this matches the exact behaviour of my application as I know it. But I only see the groups, not the containers. By right clicking on the group, eg yelb-ui in this case, I will be presented with a list of options. If I click details a get relevant information regarding which members are in the group, network and response rules this group have:\nI can select Expand and it will show me the individual containers.\nThis view is also useful if I have containers having conversations with each other in the same group and I want to see these flows, what type and action (allow/deny).\nI can now left click on one of the containers to focus the network flow going to/from it. I can also right click on it and it will provide me with another list of options:\nLets try a packet capture:\nNow click the middle button indicated by the red arrow to reveal the download button:\nThis will allow me to download a pcap file I can load into Wireshark:\nIf I want to see details of the specific container I select details on the container:\nWhen the group is expanded I can also see the unique sessions each container has, if any, like the example below:\nWhile on the other container there is a different set of sessions:\nOr any flows between the containers in the group:\nIf I just want to see the latest flows on a group I can click on the flow indicators:\nThis will tell me the application, client-ip/source, server-ip/destination, protocol/port and which rule id and action allow/deny.\nBy hovering over the flow indicator I can also see the observed application which in this case is Redis.\nFinally the legend:\nNow lets try out some of the security features in NeuVector, which will include the use of this map throughout the post.\nSecurity Policies # I wrote a post a while back using Antrea Native Policies to create a \u0026ldquo;zero trust\u0026rdquo; approach where all namespaces are per default not allowed to communicate with each other (zero-trust) and then I micro-segmented, using Antrea policies, a demo application called Yelb. I will follow the same approach here using Antrea policies to apply the foundation policies in my cluster then I will use NeuVector to further add security postures and explore what more I can do with NeuVector.\nNeuVector comes with a rich set of security features such as L7 network rules, WAF, Process Profile rules, Network Threat signatures, Config Drift rules just to name a few. In additon to the vast visibility NeuVector provides one can imagine this post will be a very fun and interesting post to do. I will try to cover most of these features in the sub-sections below.\nThis post will start by applying two Antrea Native polices to create a \u0026ldquo;minimum\u0026rdquo; foundation of security policies, then I will continue with the features of NeuVector.\nNeuVector Modes: Discover, Monitor and Protect # From the official NeuVector documentation:\nDiscover\nBy default, NeuVector starts in Discover mode. In this mode, NeuVector:\nDiscovers your container infrastructure, including containers, nodes, and hosts. Learns your applications and behaviors by observing conversations (network connections) between containers. Identifies separate services and applications running. Automatically builds a whitelist of Network Rules to protect normal application network behavior. Baselines the processes running in containers for each service and creates whitelist Process Profile Rules. NOTE\nTo determine how long to run a service in Discover mode, run test traffic through the application and review all rules for completeness. Several hours should be sufficient, but some applications may require a few days to be fully exercised. When in doubt, switch to Monitor mode and check for violations, which can then be converted to whitelist rules before moving to Protect mode.\nMonitor\nIn Monitor mode NeuVector monitors conversations and detects run-time violations of your Security Policy. In this mode, no new rules are created by NeuVector, but rules can manually be added at any time.\nWhen violations are detected, they are visible in the Network Activity map visually by a red line. Violations are also logged and displayed in the Notifications tab. Process profile rule and file access violations are logged into Notifications -\u0026gt; Security Events.\nIn the Network map you can click on any conversation (green, yellow, red line) to display more details about the type of connection and protocol last monitored. You can also use the Search and Filter by Group buttons in the lower right to narrow the display of your containers.\nProtect\nIn Protect mode, NeuVector enforcers will block (deny) any network violations and attacks detected. Violations are shown in the Network map with a red ‘x’ in them, meaning they have been blocked. Unauthorized processes and file access will also be blocked in Protect mode. DLP sensors which match will block network connections.\nAs I proceed with this post I will use a mix of all three modes, discover, monitor and protect, until I am satisfied and can enable protect on a global level.\nInfo: Changing the three modes above can be done on a global level, for both \u0026ldquo;Process Profile\u0026rdquo; modes and Network Security Policy modes. The three modes can also be adjusted on a per group level, this will not affect any network rules, but group rules. More on that later\nDiscovery to Monitor to Protect # I needed to add a dedicated section just for the Discover mode in NeuVector as I find this very useful. During the time I have been running NeuVector in my lab the Discover feature has helped me alot of understanding and telling me whats going on both inside and outside my containers. This includes network flows, from containers, nodes, external ip addresses to containers, nodes, external ip addresses and in and out of my Kubernetes cluster. In addition it also discovers files and processes inside my containers. I have a bunch of different test/demo containers in my lab and I am surprised how much actually going on inside them, many things I wasn\u0026rsquo;t aware of. With NeuVector I am no longer fumbling in the blind when it comes to locking down the \u0026ldquo;unwanted\u0026rdquo; as NeuVector tells me exactly whats going on. This is very valuable and takes away the guesswork in securing the Kubernetes estate. This makes it very easy to follow the good known (hopefully) and stop chasing the unknown. Putting this information into the context of the Network Activity Dashboard provides a very good overview and easy to manage.\nAfter NeuVector as done its Discovery phase I can move to Monitor (both network and processes/files or any of them) to wach any deviations. When I am satisfied I can move to Protect and ensure no new activities, processes/files are allowed.\nGroups # From the official documentation here\nThis menu is the key area to view and manage security rules and customize Groups for use in rules. It is also used to switch modes of Groups between Discover, Monitor, and Protect. Container Groups can have Process/File rules in a different mode than Network rules, as described here.\nNeuVector automatically creates Groups from your running applications. These groups start with the prefix \u0026rsquo;nv.\u0026rsquo; You can also manually add them using a CRD or the REST API and can be created in any mode, Discover, Monitor, or Protect. Network and Response Rules require these Group definitions. For automatically created Groups (\u0026rsquo;learned\u0026rsquo; groups starting with \u0026rsquo;nv\u0026rsquo;), NeuVector will learn the network and process rules and add them while in Discover mode. Custom Groups will not auto-learn and populate rules. Note: \u0026rsquo;nv.\u0026rsquo; groups start with zero drift enabled by default for process/file protections.\nNeuVector discovers and create groups, but one can also define and create custom groups:\nI will deploy a set of Ubuntu pods in the same namespace ubuntu-3 using the same image, but different names. Let see how NeuVector discovers them:\nI automatically get two new groups based on the name of the application/deployment.\nGroups are also used in Network Rules, both autocreated groups and custom groups.\nSettings -\u0026gt; change modes -\u0026gt; Discover, Monitor and Protect # If I go to settings -\u0026gt; configuration:\nThere is two settings that is valuable to be aware of. New Services Mode and Network Security Policy Mode controls two different layers, the first is whats going on inside an application, the other is whats going on in the network. So a short explanation of what these do.\nNew Services Mode\nIf new services are discovered by NeuVector, for example a previously unknown container starts running, it can be set to a default mode. In Discover mode, NeuVector will start to learn its behavior and build Rules. In Monitor, a violation will be generated when connections to the new service are detected. In Protect, all connections to the new service will be blocked unless the rules have been created prior.\nSource\nThe setting New Services Mode can be set to either Discover, Monitor, or Protect, and will dictate which mode NeuVector will set autocreated groups to. For example, if I deploy a new application, and if it has a unique name and/or a new namespace, it will be discovered as a new group, and the mode will be set according to the mode chosen at the time of creation.\nNotice above the three groups, they are created after the corresponding mode has been configured and reflects the mode at the time of discovery is done. Setting the mode to Discover it will allow the application to run everything it wants and NeuVector will just discover what it is doing and update the group inventory with discovered processes. By setting the mode to Monitor it will alert on any process rules being violated. By setting the mode to Protect for new services and one really wants zero-trust one can ensure no new services will be able to start at all as NeuVector has not been able to scan for any services and will just block all potential services. I will go through how this works later.\nGroups Policy Modes can also be manually changed, individually, by changing the mode here:\nThe mode on the Groups will not affect any Network rules defined. The mode on the Groups will only impact processes and files.\nNetwork Security Policy Mode\nThere is a global setting available in Settings -\u0026gt; Configuration to separately set the network protection mode for enforcement of network rules. Enabling this (default is disabled), causes all network rules to be in the protection mode selected (Discover, Monitor, Protect), while process/file rules remain in the protection mode for that Group, as displayed in the Policy -\u0026gt; Groups screen. In this way, network rules can be set to Protect (blocking), while process/file policy can be set to Monitor, or vice versa.\nSource\nThis setting is controlling the network rules \u0026ldquo;layer\u0026rdquo;. Default this setting is disabled, but by enabling it and starting with the Discover mode NeuVector will immediately start a discovery process for all network flows in and out of the Kubernetes estate. From there I will already have a good set of rules that will tell me whats going on and I can begin locking things down by using the information provided by NeuVector.\nSo to reflect upon these to settings. The first setting, New Services Mode, will dictate group modes when discovered and can have some rules (like Process Profile rules) that is necessary to be in protect mode, while the other setting, Network Security Policy Mode, can to be in Discover, Monitor or Protect without impacting the first one.\nNetwork Rules # Before getting into NeuVector network rules I have defined some Antrea Native policies as a minimum set of network security policies controlling what is allowed and not allowed inside my Kubernetes cluster, without impacting the Kubernetes cluster operation.\nThe two Antrea policies applied will allow communication between all namespace I have defined as \u0026ldquo;system-namespaces\u0026rdquo; and will drop any communication between any created user namespaces, including from any user created namespaces to system namespaces, except CoreDNS.\nBelow is the Antrea Policy I will apply to create the \u0026ldquo;zero-trust\u0026rdquo; between all user namespaces:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: strict-ns-isolation-except-system-ns spec: priority: 9 tier: securityops appliedTo: - namespaceSelector: # Selects all non-system Namespaces in the cluster matchExpressions: - {key: kubernetes.io/metadata.name, operator: NotIn, values: [default,avi-system,kube-node-lease,kube-public,kube-system,cattle-system,cattle-neuvector-system,cattle-monitoring-system,cattle-impersonation-system,cattle-fleet-system,cattle-dashboards]} ingress: - action: Pass from: - namespaces: match: Self # Skip ACNP evaluation for traffic from Pods in the same Namespace name: PassFromSameNS - action: Drop from: - namespaceSelector: {} # Drop from Pods from all other Namespaces name: DropFromAllOtherNS egress: - action: Pass to: - namespaces: match: Self # Skip ACNP evaluation for traffic to Pods in the same Namespace name: PassToSameNS - action: Drop to: - namespaceSelector: {} # Drop to Pods from all other Namespaces name: DropToAllOtherNS Certain essential services such as CoreDNS is also needed and I will apply a policy to allow for that, below is the policy:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: allow-all-dns-service spec: priority: 8 tier: securityops appliedTo: - namespaceSelector: {} egress: - action: Allow toServices: - name: rke2-coredns-rke2-coredns namespace: kube-system ports: - protocol: TCP port: 53 - protocol: UDP port: 53 name: \u0026#34;allow-dns-service-from-any\u0026#34; Applied:\nandreasm@linuxmgmt01:~/rancher$ k get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE allow-all-dns-service securityops 8 7 7 15m strict-ns-isolation-except-system-ns securityops 9 2 2 17m Now its time to continue with NeuVector.\nOne brilliant part of NeuVector is the Discover/Learned part. It will already provide me with a list of policies in Allow mode, based on findings it has done. These policies may be used as is or as an initial suggestion to how the the application interacts. Some of the rules it has created: I already have a demo application called Yelb running in the cluster. Lets filter on the app Yelb and see what NeuVector has found:\nThis looks to be a good starting point, what I do miss though is the specific ports the different pods/parts applications are using. I happen to know them as I know the application, but I suspect even NeuVector knows them, lets check Network Activity and filter on anything Yelb related:\nLets click on the flows between the pods yelb-ui, yelb-db, yelb-appserver and redis-server.\nyelb-ui to yelb-appserver\nYelb UI to Yelb Appserver is discovered using application HTTP on TCP/4567 - as the Appserver is running Ruby Sinatra lightweight web application.\nyelb-appserver to yelb-db\nYelb Appserver to Yelb DB is discovered using application PostgreSQL on port TCP/5432\nyelb-appserver to redis-server\nYelb Appserver to Redis Server is discovered using application Redis on port TCP/6379\nThe external to yelb-ui\nThen from external Neuvector has discovered the source IP addresses 192.168.120.100 and 192.168.120.101 is using application HTTP on port TCP/80 to my Yelb UI pod. These source addresses happens to be my Avi Service Engines providing the ServiceType Loadbalancer service for my Yelb ui which is the only ip addresses that should have access to the Yelb-ui pods in the first place.\nNeuVector discovers the correct applications and ports inside my Yelb demo application including the external access via the loadbalancer service.\nNow that I have all the ports also, I can go ahead and create a more specific rule. And, all the applications/ports discovered were correct as the application is using these ports:\nLets start by creating the new rules before enforcing anything (changing mode to protect):\nAfter the first rule has been created I will continue add the rest below the first by clicking on the + sign like this:\nAfter adding all rules:\nI have only created the rules necessary for the application itself. I have not defined any external to ui policy, so if I enforce this I will loose access to the application from outside. And then it is the DNS service. I already have a Antrea Native policy that gives me access to DNS, but if I enforce it in NeuVector, will NeuVector drop DNS? Lets see\nI will delete the rules created as part of the discovery process:\nWith only my three manually created rules in place, will NeuVector re-discover and create the other needed rules?\nYes, it re-discovers and re-creates the other rules as well.\nDNS will also be blocked by NeuVector if not defined/discovered and allowed, regardless of the Antrea policies, so I will have to create such a rule also in NeuVector in addition to the external rule.\nFor the external part I will create a specific group where only the IP addresses from my Avi Service Engines is added.\nNow I can also be very specific what the external source is.\nThis is how my Yelb network rules is looking right now:\nNow that I am satisfied with the network rules for Yelb it is now time to change mode to protect to enable the default deny policy at the bottom of all my rules. This policy will drop anything that has not been defined earlier.\nAll the previous rules NeuVector has provided by Discovery not related to the Yelb application will be left as is as it will just ensure all the other systems will continue to work.\nChanging the global mode from Discover to Protect:\nSelect protect\nThen Submit\nI can verify that my Yelb application still works:\nandreasm@linuxmgmt01:~/$ curl http://192.168.121.11 \u0026lt;!doctype html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Using NeuVector Network Activity dashboard is also very useful to see whats going on:\nInfo: Using NeuVector\u0026rsquo;s Network Activity dashboard is really powerful, its very instant, new flows are detected immediately and it provides vast information on whats going on. Lets dig into this dashboard later\nSome last verifcations that NeuVector actually enforces these rules, I will change the avi-se-vms to yelb-ui policy to drop:\nTrying to connect to my Yelb app now:\nandreasm@linuxmgmt01:~/$ curl http://192.168.121.11 curl: (56) Recv failure: Connection reset by peer From the network activity dashboard now I see this:\nWhat about between the pods in the Yelb namespace?\nFrom yelb-ui\nTo verify that I will now exec into the yelb-appserver pod do a curl to the yelb-ui pod on different ports:\nandreasm@linuxmgmt01:~/$ k exec -it -n yelb yelb-appserver-6dc7cd98-pvdln bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@yelb-appserver-6dc7cd98-pvdln:/app# curl 10.42.1.10 ^C root@yelb-appserver-6dc7cd98-pvdln:/app# curl 10.42.1.10:4567 ^C root@yelb-appserver-6dc7cd98-pvdln:/app# curl 10.42.1.10:5432 ^C And in network activity dashboard I will see this:\nFrom yelb-appserver to yelb-db using other ports than the allowed tcp/5432:\nandreasm@linuxmgmt01:~/$ k exec -it -n yelb yelb-appserver-6dc7cd98-pvdln bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@yelb-appserver-6dc7cd98-pvdln:/app# curl 10.42.1.8:4567 ^C This is looks pretty locked down for now. I have made sure there is no possibility for any of the pods in the Yelb namespace to have a conversation without using the application-id AND port they are allowed to use.\nIf I scale the application, e.g the UI pod, to several pods after I have put NeuVector to Protect, will they inherit the same rules?\nLets test that.\nScaling the yelb-ui to 4 pods in total:\nandreasm@linuxmgmt01:~/$ k scale deployment -n yelb yelb-ui --replicas 4 deployment.apps/yelb-ui scaled amarqvardsen@amarqvards1MD6T:~/github_repos/blog.local (main)$ k get pods -n yelb NAME READY STATUS RESTARTS AGE redis-server-84f4bf49b5-t7pdc 1/1 Running 0 3d22h yelb-appserver-6dc7cd98-pvdln 1/1 Running 0 3d22h yelb-db-84d6f6fc6c-wq2bf 1/1 Running 0 3d22h yelb-ui-f544fc74f-2zzlq 0/1 ContainerCreating 0 5s yelb-ui-f544fc74f-85x4v 1/1 Running 0 3d22h yelb-ui-f544fc74f-bj6n7 0/1 ContainerCreating 0 5s yelb-ui-f544fc74f-ddwpd 0/1 ContainerCreating 0 5s The additional yelb-ui pods will be added to the same group as I am using in the network rules below:\nI am now confident my network rules controlling the Yelb application will keep it in control. The result should look something like this:\nWhatever conversations the containers or pods in the Yelb application decides to do, they will be blocked unless they are using the application and ports that is allowed by the network rules.\nIf I do scale the yelb-ui pods from 1 to many, the Network Rules will automatically ensure that there is possible way for the ui pods to have a conversation more than what is allowed. And looking at the rules I have create I have not defined any rule indicating that any pod of same type should be allowed to communicate, and certainly not the yelb-ui pods. This restriction limits the lateral movement if one of the web containers should be compromised.\nI have simulated this behaviour by having to Ubuntu containers in the same namespace and used curl to access their web-service over tcp/80, then from the Network Activity dashboard:\nNeuVector does not enforce policies on the nodes themselves, so here one may have to look at Antrea Node Policies or NSX as a VM/node firewall solution. I do covere some of that in this post and this post.\nNext I will look at processes going on inside the containers themselves\u0026hellip;\nProcess Profile rules # This part is very interesting as it is capable of allowing or blocking the execution of processes inside a container. These rules are independent of any network rules and will only be enforced if the protect mode is set on the group level.\nIf I dont know whats going on inside my container, I can start out by leaving the autocreated group in Discovery mode. When I am satsified with whats being discovered I can decide to drop or allow certain processes necessary for the container to do its job.\nFor the purpose of this post I will spin up a Ubuntu pod in the namespace ubuntu, and let NeuVector do its discovery of whats going on.\nMy pod is up and NeuVector has discovered a lot of interesting processes already:\nThis Ubntu container do contain a lot of stuff and does not represent a typical pod which is usually configured to one specific purpose in life. The Ubuntu container in my case here is my \u0026ldquo;swiss army\u0026rdquo; knife that contains a lot of tools out of the box, so it does not represent a typical container (and not something you would like to have running in production anyway). But it do work as a really good example of what NeuVector is able to discover as soon as the contaner is up and running.\nA typical container may look more like one of my Yelb pods, like the yelb-db below:\nThis looks more like a purpose built container. I know it should run Postgres, and the Pause process is there to keep the pod up and running. If I wanted to lock this group down, not allowing any other processes to run/executed in the container I can just switch mode to Protect on this group and I can rest assure no other processes than Postgres is allowed to run.\nThis makes life much easier instead of chasing the unknown, I can protect the known and forget about all else.\nBack to my Ubuntu container again. I want to block a bunch of services on it. Lets just try to set the group in protect mode as is with whats already discovered.\nNotice I am using Basic instead of Zero Drift, more on that later.\nI will execute into the Ubuntu container and do a SSH session to an external server:\nandreasm@linuxmgmt01:~/$ k exec -it -n ubuntu ubuntu-20-04-7bb59fcd7f-sz7bj bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ubuntu-20-04-7bb59fcd7f-sz7bj:/# ssh andreasm@192.168.100.99 bash: /usr/bin/ssh: Operation not permitted Oops, that was not allowed. Let me check if SSH is on the list of allowed processes:\nIts not there, so it will not be allowed. Only rules defining what is allowed is allowed, and apparently SSH has not been discovered. So let me create a rule for SSH.\nprocess name, path and action -\u0026gt; allow. . Thats it.\nAfter clicking deploy I will try again:\nroot@ubuntu-20-04-7bb59fcd7f-sz7bj:/usr/bin# ssh andreasm@192.168.100.99 andreasm@192.168.100.99\u0026#39;s password: Welcome to Ubuntu 22.04.4 LTS (GNU/Linux 5.15.0-101-generic x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/pro Now I can SSH again.\nIt is also possible to delete or edit the autocreated Process Profile Rules:\nLets take one of the autocreated rules and change it from Allow to Deny.\nroot@ubuntu-20-04-7dc874975d-c5t48:/# ls bash: /usr/bin/ls: Operation not permitted The change is instant. I dont have to go in and out of the bash session. Now I no longer can do ls.\nZero Drift vs Basic # This setting can be set either globally or per group individually\nThis setting is very nice to know about and understand the difference between Zero Drift and Basic.\nLets take a closer look at how this setting works. If I change mode on my nv.ubuntu-20-04 group to protect and leaving Zero Drift enabled, and making sure bash is allowed in the Process rules:\nandreasm@linuxmgmt01:~/$ k exec -it -n ubuntu ubuntu-maintenance-7dc874975d-c5t48 bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. command terminated with exit code 137 it will still block the bash access. Why is that? Lets have a look at the event:\nThe Process Parent Name is runc and I was expecting bash\nAnd if I refer to the Zero Drift vs Basic config documentation:\nZero-Drift\nZero-drift automatically allows only processes which originate from the parent process that is in the original container image, and does not allow file updates or new files to be installed. When in Discover or Monitor mode, zero-drift will alert on any suspicious process or file activity. In Protect mode, it will block such activity. Zero-drift does not require processes to be learned or added to an allow-list. Disabling zero-drift for a group will cause the process and file rules listed for the group to take effect instead.\nThe problem is because the process does not originate from any parent process defined in the container image, or process tree. Doing kubectl exec -it -n ubuntu ubuntu-20-04 bash starts a new process not originating from anything that should be expected in the container image. Zero Drift will consider this as an unexpected process and block that.\nIf I compare the process tree in the ubuntu-20-04 container and the yelb-db container:\nI see that bash is not a parent process, sleep is, and bash is not coming from the sleep process with pid1 in the ubuntu-20-04 container.\nandreasm@linuxmgmt01:~$ k exec -it -n ubuntu ubuntu-22-04-5f775c647b-x4q9x bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ubuntu-22-04-5f775c647b-x4q9x:/app# ps -ef --forest UID PID PPID C STIME TTY TIME CMD root 321 0 0 10:11 pts/0 00:00:00 bash root 333 321 0 10:11 pts/0 00:00:00 \\_ ps -ef --forest root 1 0 0 09:51 ? 00:00:00 sleep infinity root@ubuntu-22-04-5f775c647b-x4q9x:/app# In the yelb-db container the postgres process is the parent process, and this process is expected and have its origin in the container image.\nandreasm@linuxmgmt01:~$ k exec -it -n yelb yelb-db-84d6f6fc6c-wq2bf bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@yelb-db-84d6f6fc6c-wq2bf:/# ps -ef --forest UID PID PPID C STIME TTY TIME CMD root 17470 0 2 10:04 pts/0 00:00:00 bash root 17476 17470 0 10:04 pts/0 00:00:00 \\_ ps -ef --forest postgres 1 0 0 May20 ? 00:00:21 postgres postgres 68 1 0 May20 ? 00:00:00 postgres: checkpointer process postgres 69 1 0 May20 ? 00:00:09 postgres: writer process postgres 70 1 0 May20 ? 00:00:09 postgres: wal writer process postgres 71 1 0 May20 ? 00:00:13 postgres: autovacuum launcher process postgres 72 1 0 May20 ? 00:00:18 postgres: stats collector process postgres 73 1 0 May20 ? 00:00:01 postgres: bgworker: logical replication launcher root@yelb-db-84d6f6fc6c-wq2bf:/# And if I take a look at the security event from the yelb-db container I will see that process parent name of postgres is postgres\nExecuting bash using kubectl will never be a parent process in my container and is clearly not a process that is supposed to have its origin from the container image itself. Hence, Zero Trust will never accept this behaviour.\nIf I absolutely need bash access when in need of troubleshooting, I have the option to set the group in Basic mode instead of Zero-Drift.\nBasic Mode\nZero-drift can be disabled, switching to Basic process protection. Basic protection enforces process/file activity based on the listed process and/or file rules for each Group. This means that there must be a list of process rules and/or file rules in place for protection to occur. Rules can be auto-created through Behavioral Learning while in Discover mode, manually created through the console or rest API, or programmatically created by applying a CRD. With Basic enabled if there are no rules in place, all activity will be alerted/blocked while in Monitor or Protect modes.\nBehavioral learning file access rules # If I want to see the typical file changes being done in a container, I can monitor that by create a file access rule in blocking state, and leave the group in Discover mode:\nI want to monitor everything:\nDeploy\nNow do what you need to do in your container, and check if the rule is being updated with content:\nBy just executing bash on my container, NeuVector will automatically add allowed applications in the File Access Rules:\nWhen I am satisifed with the learning \u0026ldquo;period\u0026rdquo; I can change the mode on my group to protection and set the File Access Rule to Monitor File Change. If needed adjust the filter to be much more specific.\nResponse rules # Response Rules provide a flexible, customizable rule engine to automate responses to important security events. Triggers can include Security Events, Vulnerability Scan results, CIS Benchmarks, Admission Control events and general Events. Actions include container quarantine, webhooks, and suppression of alerts.\nSource\nLet me quickly create a response rule to put my container in quarantine.\nI will head over to Response Rules on the menu on the left side:\nThen I will click Add to Top to create my own custom Response Rule. I will define it as simple as possible just to get a reaction.\nThe rule above will trigger as soon a container violates any of the allowed Process Profile rules.\nIn the group I selected in the rule above, I should now see the response rule:\nNow lets see if I can trigger an action on this rule.\nFrom the container ubuntu-22-04\nandreasm@linuxmgmt01:~/$ k exec -it -n ubuntu ubuntu-22-04-54c4d6774b-v9n6p bash kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. root@ubuntu-22-04-54c4d6774b-v9n6p:/app# vim maintenance.sh bash: /usr/bin/vim: Operation not permitted I tried to open vim editor and thats it. It should be catched by the Response Rule and put into quarantine. vim editor is not an allowed process to run in the container according to my Process Profile rules so it will trigger an event.\nMy ubuntu container is now isolated completely from the network. I can still interact with it from a Kubernetes/kubectl perspective but its no longer able to communicate over the network.\nLets see the event:\nIf I want to \u0026ldquo;un-quarantine\u0026rdquo; the pod I need to delete response rule that put it there in the first place:\nRegardless of the group being in Monitor or Protect mode, the Response Rule will be in effect.\nQuarantine – container is quarantined. Note that Quarantine means that all network traffic is blocked. The container will remain and continue to run - just without any network connections. Kubernetes will not start up a container to replace a quarantined container, as the api-server is still able to reach the container. Registry scan # As mentioned in the beginning of this post, making sure the image registry is also being scanned for vulnerabilities and images being compliant or not is also an important part in the whole chain of securing the Kubernetes ecosystem.\nI use Harbor registry in my lab and will see how I can enable NeuVector to scan my registry.\nI will head over to the Assets -\u0026gt; Registries and click +Add\nThere I will fill in the relevant information for NeuVector to use to scan my registry:\nI have selected Scan Layer, Periodic scan and added the filters ubuntu/* and yelb/* to only scan the images used in this post.\nNow I will start a Scan by clicking on Scan here:\nWhen the scan is done I will be presented with a list of images, including all versions/layers, with a corresponding vulnerabilities number:\nNow lets click on the first image and see whats really going on here and if there is something I can do..\nThis brings up a very comprehensive list, sorted into three different tabs that clearly explains the issues and what I need to do.\nVulnerabilities\nShowing the vulnerabilities with the highest severity first and then descending into the medium and low vulnerabilities:\nCompliance\nModules\nI clearly have a lot of job to do to update my images to sort out the vulnerabilities, compliance and module issues. Very useful information.\nWith Harbor there is also the option to use pluggable scanner interface, for more information head over here. This feature enables triggered scans on image push.\nOutro # This concludes part 1 of this 2 post series on NeuVector. There is much more to cover. In this post I have only covered how NeuVector Discovery works, the more typical security features like network rules, process rules, file access rules, response rules and how to use the NeuVector Activity Dashboard and registry scanning.\nIn the next post (part 2) I will go more into automation, and the features I haven\u0026rsquo;t covered so far like multiple cluster management, risk analysis, deep packet inspection, compliance, admission control, export/import, CRD, etc.\nI was really looking forward to do do this post, and I am now even more looking forward to do the next post. It has been a joy to explore NeuVector so far and still there is much more to explore. See you in the next post \u0026#x1f604;\n","date":"2 February 2023","externalUrl":null,"permalink":"/2023/02/02/neuvector-by-suse-security-where-it-is-needed-part-1/","section":"Posts","summary":"In this two post series I will explore and deep dive into NeuVector security features","title":"NeuVector by SUSE - security where it is needed - part 1","type":"posts"},{"content":"","date":"2 February 2023","externalUrl":null,"permalink":"/categories/suse/","section":"Categories","summary":"","title":"SUSE","type":"categories"},{"content":" Deploy Tanzu in vSphere 8 with VDS and Avi Loadbalancer: # This post will go through how to install Tanzu in vSphere 8 using vSphere VDS networking and Avi as loadbalancer. The goal is to deploy Tanzu by using vSphere Distributed Switch (no NSX this time) and utilize Avi as loadbalancer for Supervisor and workload cluster L4 endpoint (kubernetes API). When that is done I will go through how we also can extend this into L7 (Ingress) by using AKO in our workload clusters.\nThe below diagram is what we should end up with after the basic deployment of Tanzu and Avi:\nAssumptions # This post assumes we already have a vSphere environment up and running with vCenter, HA and DRS. Required network to support the basic vSphere stuff like vMotion and shared storage. And the hosts networking has been configured with a Distributed Switch with the corresponding vds portgroups for Management, Frontend network (VIP placement for kubernetes API endpoint) and workload network with corresponding VLANs. In vCenter a content library needs to be created, this is just a local library you give a meaningful name no subscriptions etc.\nAt least one Avi controller is deployed, no cloud added, just deployed and the initial configs done.\nPreparations on the Avi side of things # This part of the guide takes place on the newly configured Avi controller(s) which currently only has the initial configuration done\nAvi cloud configurations # To prepare Avi for this deployment we need to configure the vCenter cloud. This is done here:\nThere is a Default-Cloud object there we need to convert to a vCenter cloud. This is done by clicking on this button on the far right side: This will bring up the following options: Select VMware vCenter/vSphere NSX Then start populate the relevant vCenter information for your vCenter: When credentials is added slect content library and choose your content library from the list, then click connect and then Save \u0026amp; Relaunch\nWhen the dialog relaunches select the management network and ip address management. I have opted for DHCP (I have DHCP in my mgmt network, if not we can leverage Avi as IPAM provider for the mgmt network also.) This is used for the SE\u0026rsquo;s mgmt interface. Click save for now. Head over to the Template section to add IPAM and DNS.\nWhen using vCenter clouds the different portgroups is automatically added under networks. We need to configure some of them. But for now just create the IPAM and DNS profiles and we configure the networks later accordingly.\nThe DNS Profile (optional, only if you want to use Avi DNS service):\nClick save when done\nThe IPAM profile:\nSelect the Default-Cloud, then select from the list \u0026ldquo;Usable Networks\u0026rdquo; the Frontend network vds portgroup corresponding to the frontend network we want to use for our endpoint vips. Click save. You should have your profiles configured now:\nHead back to your cloud again and add your newly created IPAM and DNS profiles.\nAdd the profiles: Before you click finish, make sure you have selected \u0026ldquo;Prefer Static Routes vs Directly Connected Network\u0026rdquo; like this: Then click finish\u0026hellip;\nAvi network configs # Now its time to configure the networks for the SEs (VIP, dataplane). I will go ahead and configure both the Frontend VIP for kubernetes API endpoint, but also the workload network I will use when I add L7 functionality later. Head over to Cloud Resources: Scroll until you find your \u0026ldquo;Frontend-network\u0026rdquo; Click edit all the way to the right: In here we need to define the subnet (if not already auto discovered) and add and IP range for the SE\u0026rsquo;s and VIPS. We can decide to create just one range for both, or a range only for SEs, and one for VIPs only. To create a range for both SE and VIP do like this: If you want a specific range for SE and a specific for VIP do like this: Common for both is to deselect the DHCP Enabled option. What we have done now is to tell Avi that Avi is responsible for IP allocation to our SE\u0026rsquo;s when they are deployed and configured to give them each their IP in the Frontend network, and also \u0026ldquo;carve\u0026rdquo; out an IP for the VIP when a Virtual Service is created and IP allocation for that service is selected to auto-allocate. The same would go if you decided to not use DHCP for mgmt IP, you would need to defined the network and select only \u0026ldquo;Use for Service Engine\u0026rdquo; (no VIPs in the management network)\nAvi service engine group # Now its time to prepare the Default Service Engine Group. Head over to Cloud Resources - Service Engine Group\nClick the pencil on the far right side of the Default-Group and make the following changes:\nIn the Advanced tab: Here we select our vSphere cluster for the SE placement, vSphere shared storage, and the Prefix and vCenter folder placement (if you want). Now that is done.\nAvi VRF context # Now we need to create a static route for the SE dataplane to know which gateway will take them to the \u0026ldquo;backend-pool\u0026rdquo; (the services they are acting as loadbalancer for). This is usually the gateway for the networks in their respective subnet as the dataplane is residing in. Here I prepare the route for the Frontend network, and also the Workload network (so it is already done when moving to the step of enabling L7). Avi controller SSL certificate for Tanzu \u0026ldquo;integration\u0026rdquo; # The last step is to create a new certificate, or use your own signed certificate, for the Tanzu deployment to use. Head over to Templates - SSL/TLS Certificates. From here we click \u0026ldquo;Create\u0026rdquo; in the top right corner: I will go ahead and create a new self-signed certificate: It is important that you use the IP or FQDN of the controller under \u0026ldquo;Common Name\u0026rdquo; and under \u0026ldquo;Subject Alternate Name (SAN)\u0026rdquo;\nNow head over to Administration - Access Settings: Click edit on the pencil in the top right corner, remove the existing certificates under SSL/TLS Certificate: And replace with the one you created: Now the Avi config is done for this round. Next step is to enable Workload Management in vSphere\u0026hellip;\nEnable Workload Management in vSphere # This section will cover all the steps to enable Tanzu from vCenter, describing the selections made and the network configs.\nEnable workload management # Head over to your vCenter server and click here: (from the the \u0026ldquo;hamburger menu\u0026rdquo; top left corner)\nClick the Get Started button:\nStep 1: Select vSphere Distributed Switch Step 2: Select Cluster Deployment, give the supervisor cluster a name and give it a zone name: Step 3: Select your Storage Policy (If VSAN and you dont have created a specific VSAN policy for this use Default Storage Policy): Step 4: Type in the relevant info for your Avi Controller and copy paste the certificate from your Avi controller: The certificate is easily copied from the Avi controller by going to Templates - SSL/TLS Certificates and click the \u0026ldquo;down arrow\u0026rdquo;: Then copy the certificate: Paste the content in the Server Certificate field above (step 4) Step 5: Management Network Here we fill in the required information for the Supervisor nodes. Management IP for the nodes themselves (needs connectivity to both vCenter and ESXi hosts, could be in the same mgmt network as vCenter and ESXi). Select the corresponding vds portgroup, select either static or DHCP if you want to use DHCP. Step 6: Workload Network Select the correct vds portgroup for the workload network. The supervisor and the workload nodes will be placed here. Can be static or DHCP. Leave the default \u0026ldquo;Internal Network for Kubernetes Services\u0026rdquo;, that is for the internal services (clusterIP etc inside the K8s clusters, they will never be exposed outside). Fill in the necessary config if you go with static. Step 7: Review and Confirm and optionally give the Supervisor endpoint a DNS name which you later can register in your DNS service when we have the L4 IP for the kubernetes API endpoint. Click finish:\nThe whole summary: Now sit back and wait for the creation of the supervisor cluster, it can take a couple of minutes. After a while you can take a look in your Avi controller under Applications and see if something is being created there; You can monitor the process from the Workload management status view by clicking on the \u0026ldquo;Configuring (View) )\u0026rdquo;. You can continue work with your vCenter server and go back to this progress bar whenever you want by clicking the hamburger menu Workload management.\nIn your vCenter inventory you should also see the Supervisor VMs and Avi SE\u0026rsquo;s like this: When its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.102.7.11 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters. Lets dive into it.\nvSphere Namespace # vSphere with Tanzu workloads, including vSphere Pods, VMs, and Tanzu Kubernetes clusters, are deployed to a vSphere Namespace. You define a vSphere Namespace on a Supervisor and configure it with resource quota and user permissions. Depending on the DevOps needs and workloads they plan to run, you might also assign storage policies, VM classes, and content libraries for fetching the latest Tanzu Kubernetes releases and VM images. source\nCreate a vSphere namespace # Now that the Supervisor cluster is ready and running head back to your vCenter and create a vSphere namespace. Click create namespace (above) the select the supervisor to create the namespace on, and give your namespace a name then select the \u0026ldquo;workload network\u0026rdquo; you have defined for your workload placement. Now the namespace is being created.\nAdd additional workload networks # Sidenote there is also possible to add more \u0026ldquo;workload networks\u0026rdquo; after the Supervisor has been configured under Supervisor config if you want to add more \u0026ldquo;workload networks\u0026rdquo; for separation etc. To do that head over to Workload Management in vCenter:\nThen select the supervisor tab:\nClick on your supervisor cluster here: Then click the configure tab and go to network and add your additional workload network: After your namespace has been created we need to configure it with access permissions, datastores, content library and vmclasses: Create workload cluster # Afte the vSphere Namespace has been configured its time to deploy a workload cluster/TKC cluster (Tanzu Kubernetes Cluster cluster \u0026#x1f604; ). From your workstation/jumphost where you downloaded the cli tools login in to the supervisor with access rights to the Supervisor API. (administrator@vsphere.local will have access).\nCustom role in vCenter # I created a specific role in my vCenter with these privileges:\nAdded my \u0026ldquo;supervisor-manager\u0026rdquo; user in this role as global and in the top tree of my vCenter with inheritance. Also added it as \u0026ldquo;editor\u0026rdquo; in my wdc-2-ns-1 vSphere Namespace.\nkubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net When your are logged in it will give you this output and also put the kubernetes config in your ~/.kube/config file.\nandreasm@linuxvm01:~$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below Password: Logged in successfully. You have access to the following contexts: 10.102.7.11 wdc-2-ns-1 If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator. To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` When you are logged in prepare your yaml for your first workload cluster and apply it with kubectl apply -f nameof.yaml\nExample:\napiVersion: cluster.x-k8s.io/v1beta1 kind: Cluster metadata: name: wdc-2-tkc-cluster-1 # give your tkc cluster a name namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace spec: clusterNetwork: services: cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] pods: cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] serviceDomain: \u0026#34;cluster.local\u0026#34; topology: class: tanzukubernetescluster version: v1.23.8---vmware.2-tkg.2-zshippable controlPlane: replicas: 1 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu workers: machineDeployments: - class: node-pool name: node-pool-01 replicas: 3 metadata: annotations: run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu variables: - name: vmClass value: best-effort-medium - name: storageClass value: vsan-default-storage-policy As soon as I apply the above yaml it will deploy the corresponding tkc cluster in your vsphere environment:\nSit back and enjoy while your tkc cluster is being created for you. We can check the status in the vCenter gui:\nor via kubectl:\nandreasm@linuxvm01:~/tkc-wdc-01-vds$ k get cluster -n wdc-2-ns-1 NAME PHASE AGE VERSION wdc-2-tkc-cluster-1 Provisioned 12m v1.23.8+vmware.2 It is ready, now we need to log into it:\nandreasm@linuxvm01:~/tkc-wdc-01-vds$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name=wdc-2-tkc-cluster-1 --tanzu-kubernetes-cluster-namespace=wdc-2-ns-1 KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below Password: Logged in successfully. You have access to the following contexts: 10.102.7.11 wdc-2-ns-1 wdc-2-tkc-cluster-1 If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator. To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` Check if you are able to list ns and pods:\nandreasm@linuxvm01:~/tkc-wdc-01-vds$ k get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-77drs 2/2 Running 0 8m35s kube-system antrea-agent-j482r 2/2 Running 0 8m34s kube-system antrea-agent-thh5b 2/2 Running 0 8m35s kube-system antrea-agent-tz4fb 2/2 Running 0 8m35s kube-system antrea-controller-575845467f-pqgll 1/1 Running 0 8m35s kube-system coredns-7d8f74b498-ft7rf 1/1 Running 0 10m kube-system coredns-7d8f74b498-pqgp7 1/1 Running 0 7m35s kube-system docker-registry-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-6cvz9 1/1 Running 0 8m46s kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rgn29 1/1 Running 0 8m34s kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rsmfw 1/1 Running 0 9m kube-system etcd-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m kube-system kube-apiserver-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m kube-system kube-controller-manager-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m kube-system kube-proxy-67xjk 1/1 Running 0 8m46s kube-system kube-proxy-6fttt 1/1 Running 0 8m35s kube-system kube-proxy-m4wt8 1/1 Running 0 11m kube-system kube-proxy-rbsjw 1/1 Running 0 9m1s kube-system kube-scheduler-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m kube-system metrics-server-6f7c489795-scmm6 1/1 Running 0 8m36s secretgen-controller secretgen-controller-6966677567-4hngd 1/1 Running 0 8m26s tkg-system kapp-controller-55f9977c86-bqppj 2/2 Running 0 9m22s tkg-system tanzu-capabilities-controller-manager-cb4bc7978-qh9s8 1/1 Running 3 (87s ago) 7m54s vmware-system-auth guest-cluster-auth-svc-r4rk9 1/1 Running 0 7m48s vmware-system-cloud-provider guest-cluster-cloud-provider-859b8dc577-8jlth 1/1 Running 0 8m48s vmware-system-csi vsphere-csi-controller-6db86b997-l5glc 6/6 Running 0 8m46s vmware-system-csi vsphere-csi-node-7bdpl 3/3 Running 2 (7m34s ago) 8m34s vmware-system-csi vsphere-csi-node-q7zqr 3/3 Running 3 (7m32s ago) 8m46s vmware-system-csi vsphere-csi-node-r8v2c 3/3 Running 3 (7m34s ago) 8m44s vmware-system-csi vsphere-csi-node-zl4m2 3/3 Running 3 (7m40s ago) 8m46s andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get ns NAME STATUS AGE default Active 12m kube-node-lease Active 12m kube-public Active 12m kube-system Active 12m secretgen-controller Active 9m3s tkg-system Active 9m55s vmware-system-auth Active 12m vmware-system-cloud-provider Active 10m vmware-system-csi Active 10m vmware-system-tkg Active 12m By default we are not allowed to run anything on our newly created tkc cluster. We need to define some ClusterRoles. I will just apply a global clusterolres on my tkc cluster so I can do what I want with it like this: Apply the psp policy yaml:\napiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: psp:privileged rules: - apiGroups: [\u0026#39;policy\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: - vmware-system-privileged --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: all:psp:privileged roleRef: kind: ClusterRole name: psp:privileged apiGroup: rbac.authorization.k8s.io subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io kubectl apply -f roles.yaml clusterrole.rbac.authorization.k8s.io/psp:privileged created clusterrolebinding.rbac.authorization.k8s.io/all:psp:privileged created Now that I am allowed to deploy stuff I am ready to consume the newly cluster. But this blog was how to deploy Tanzu with VDS and Avi Loadbalancer. So far I have only covered the L4 part where Avi is providing me the K8s API endpoints, I will now jump over to the section where I configure both Avi and my tkc cluster to use Ingress (L7) also so I can publish/expose my applications with ingress. That means installing an additional component called AKO in my tkc cluster and configure Avi accordingly.\nConfigure Avi as Ingress controller (L7) # For Avi Ingress we need to deploy a component in our TKC cluster called AKO. AKO stands for Avi Kubernetes Operator and introduces the ability to translate our k8s api to the Avi controller so we can make our Avi automatically create vs services for us as soon as we request them from our TKC cluster. To deploy AKO we use Helm. In short we need to add the AKO helm repository, get the ako values, edit them to fit our environment, then install it by using Helm. So let us go through this step-by-step (I have also covered it a while back in an Upstream k8s cluster) but let us do it again here.\nCreate the namespace for the ako pod:\nk create ns avi-system namespace/avi-system created Then add the repo to Helm:\nhelm repo add ako https://projects.registry.vmware.com/chartrepo/ako Check the repo:\nandreasm@linuxvm01:~$ helm search repo\rNAME CHART VERSION\tAPP VERSION\tDESCRIPTION ako/ako 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator\rako/ako-operator\t1.3.1 1.3.1 A Helm chart for Kubernetes AKO Operator\rako/amko 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator Get the values.yaml:\nhelm show values ako/ako --version 1.8.2 \u0026gt; values.yaml Now its time to edit the value file. I will go through the values files, update it accordingly and adjust some configurations in Avi controller.\nThe values.yaml for ako chart:\n# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. replicaCount: 1 image: repository: projects.registry.vmware.com/ako/ako pullPolicy: IfNotPresent AKOSettings: primaryInstance: true enableEvents: \u0026#39;true\u0026#39; logLevel: WARN fullSyncFrequency: \u0026#39;1800\u0026#39; apiServerPort: 8080 deleteConfig: \u0026#39;false\u0026#39; disableStaticRouteSync: \u0026#39;false\u0026#39; clusterName: wdc-tkc-cluster-1 # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. enableEVH: false layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. namespaceSelector: labelKey: \u0026#39;\u0026#39; labelValue: \u0026#39;\u0026#39; servicesAPI: false vipPerNamespace: \u0026#39;false\u0026#39; NetworkSettings: nodeNetworkList: # nodeNetworkList: - networkName: \u0026#34;vds-tkc-workload-vlan-1026\u0026#34; # this is the VDS portgroup you have for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter cidrs: - 10.102.6.0/24 # this is the CIDR for your workers enableRHI: false nsxtT1LR: \u0026#39;\u0026#39; bgpPeerLabels: [] # bgpPeerLabels: # - peer1 # - peer2 vipNetworkList: - networkName: \u0026#34;vds-tkc-frontend-vlan-1027\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. cidr: 10.102.7.0/24 L7Settings: defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. noPGForSNI: false serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. passthroughShardSize: SMALL enableMCI: \u0026#39;false\u0026#39; L4Settings: defaultDomain: \u0026#39;\u0026#39; autoFQDN: default ControllerSettings: serviceEngineGroupName: Default-Group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.3 cloudName: Default-Cloud # The configured cloud name on the Avi controller. controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller tenantName: admin nodePortSelector: key: \u0026#39;\u0026#39; value: \u0026#39;\u0026#39; resources: limits: cpu: 350m memory: 400Mi requests: cpu: 200m memory: 300Mi podSecurityContext: {} rbac: pspEnable: false avicredentials: username: \u0026#39;admin\u0026#39; # username for the Avi controller password: \u0026#39;password\u0026#39; # password for the Avi controller authtoken: certificateAuthorityData: persistentVolumeClaim: \u0026#39;\u0026#39; mountPath: /log logFile: avi.log A word around the VIP network used for the L7/Ingress. As we deploy AKO as standalone we are not restricted to use only the components defined to support the install of Tanzu with vSphere, like service engine groups, vip networks etc. We could decide to create a separate VIP network by using a dedicated SE group for these networks. We could also decide to have the SE\u0026rsquo;s using a separate dataplane network than the VIP itself. If going this path there is some config steps that needs to be taken on the network side. Routing to the VIP addresses, either Avi can be configured by using BGP, or we create static routes in the physical routers. But as the VIPs are coming and going (applications are published, deleted, etc) these IPs change. So BGP would be the best option, or use an already defined VLAN as I am doing in this example. In my other post on using NSX and Avi with Tanzu I will show how to use NSX for BGP. Maybe I will update this post also by adding a section where I use BGP from Avi to my upstream router. But for now I will stick with using my VLAN I have called frontend which already have a gateway and a route defined. So all my VIPs will be reachable through this network.\nAntrea NodePortLocal # And another word around NodePortLocal. To be able to utilize NodePortLocal your Antrea config in the TKC cluster must be verified whether it is configured with NPL or not. So let us do instead of just assume something.\nandreasm@linuxvm01:~/ako/ako_vds$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml apiVersion: v1 data: antrea-agent.conf: | featureGates: AntreaProxy: true EndpointSlice: true Traceflow: true NodePortLocal: false AntreaPolicy: true FlowExporter: false NetworkPolicyStats: false Egress: false AntreaIPAM: false Multicast: false ServiceExternalIP: false trafficEncapMode: encap noSNAT: false tunnelType: geneve trafficEncryptionMode: none wireGuard: port: 51820 egress: {} serviceCIDR: 20.10.0.0/16 Well that was not good. So we need to enable it. Luckily, with Tanzu with vSphere its quite simple actually. Switch context to your vSphere Namespace, edit an antreaconfig, apply it.\nandreasm@linuxvm01:~/antrea$ k config use-context wdc-2-ns-1 Switched to context \u0026#34;wdc-2-ns-1\u0026#34;. andreasm@linuxvm01:~/antrea$ k get cluster NAME PHASE AGE VERSION wdc-2-tkc-cluster-1 Provisioned 3h26m v1.23.8+vmware.2 andreasm@linuxvm01:~/antrea$ k apply -f antreaconfig-wdc-2-nsx-1.yaml Warning: resource antreaconfigs/wdc-2-tkc-cluster-1-antrea-package is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. antreaconfig.cni.tanzu.vmware.com/wdc-2-tkc-cluster-1-antrea-package configured The antreaconfig I used:\napiVersion: cni.tanzu.vmware.com/v1alpha1 kind: AntreaConfig metadata: name: wdc-2-tkc-cluster-1-antrea-package # notice the naming-convention tkc cluster name-antrea-package namespace: wdc-2-ns-1 # your vSphere Namespace the TKC cluster is in. spec: antrea: config: featureGates: AntreaProxy: true EndpointSlice: false AntreaPolicy: true FlowExporter: false Egress: true NodePortLocal: true # Set this to true AntreaTraceflow: true NetworkPolicyStats: true Lets have a look at my Antrea config in my TKC cluster now:\nandreasm@linuxvm01:~/antrea$ k config use-context 10.102.7.11 wdc-2-ns-1 wdc-2-tkc-cluster-1 andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-tkc-cluster-1 Switched to context \u0026#34;wdc-2-tkc-cluster-1\u0026#34;. andreasm@linuxvm01:~/antrea$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml apiVersion: v1 data: antrea-agent.conf: | featureGates: AntreaProxy: true EndpointSlice: false Traceflow: true NodePortLocal: true # Yes, there it is AntreaPolicy: true FlowExporter: false NetworkPolicyStats: true Egress: true AntreaIPAM: false Multicast: false ServiceExternalIP: false trafficEncapMode: encap noSNAT: false tunnelType: geneve trafficEncryptionMode: none wireGuard: port: 51820 egress: exceptCIDRs: [] serviceCIDR: 20.10.0.0/16 But\u0026hellip; Even though our cluster config has been updated we need to delete the Antrea pods so they can restart and read their new configmap again.\nandreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-controller-575845467f-pqgll pod \u0026#34;antrea-controller-575845467f-pqgll\u0026#34; deleted andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent- antrea-agent-77drs antrea-agent-j482r antrea-agent-thh5b antrea-agent-tz4fb andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-77drs pod \u0026#34;antrea-agent-77drs\u0026#34; deleted andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-j482r pod \u0026#34;antrea-agent-j482r\u0026#34; deleted andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-thh5b pod \u0026#34;antrea-agent-thh5b\u0026#34; deleted andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-tz4fb pod \u0026#34;antrea-agent-tz4fb\u0026#34; deleted Configure Avi as Ingress controller (L7) - continue # Now that we have assured NodePortLocal is configured, its time to deploy AKO. I have also verified that I have the VIP network configured in Avi as I am using the existing network that is already defined. So install AKO then \u0026#x1f604;\nhelm install ako/ako --generate-name --version 1.8.2 -f ako.vds.wdc-2.values.yaml --namespace=avi-system NAME: ako-1675511021 LAST DEPLOYED: Sat Feb 4 11:43:42 2023 NAMESPACE: avi-system STATUS: deployed REVISION: 1 TEST SUITE: None Verify that the AKO pod is running:\nandreasm@linuxvm01:~/ako/ako_vds$ k get pods -n avi-system NAME READY STATUS RESTARTS AGE ako-0 1/1 Running 0 57s Check logs for some immediate messages that needs investigating before trying to deploy a test application.\nandreasm@linuxvm01:~/ako/ako_vds$ k logs -n avi-system ako-0 2023-02-04T11:43:51.240Z\tINFO\tapi/api.go:52\tSetting route for GET /api/status 2023-02-04T11:43:51.241Z\tINFO\tako-main/main.go:71\tAKO is running with version: v1.8.2 2023-02-04T11:43:51.241Z\tINFO\tapi/api.go:110\tStarting API server at :8080 2023-02-04T11:43:51.242Z\tINFO\tako-main/main.go:81\tWe are running inside kubernetes cluster. Won\u0026#39;t use kubeconfig files. 2023-02-04T11:43:51.265Z\tINFO\tlib/control_config.go:198\tako.vmware.com/v1alpha1/AviInfraSetting enabled on cluster 2023-02-04T11:43:51.270Z\tINFO\tlib/control_config.go:207\tako.vmware.com/v1alpha1/HostRule enabled on cluster 2023-02-04T11:43:51.273Z\tINFO\tlib/control_config.go:216\tako.vmware.com/v1alpha1/HTTPRule enabled on cluster 2023-02-04T11:43:51.290Z\tINFO\tako-main/main.go:150\tKubernetes cluster apiserver version 1.23 2023-02-04T11:43:51.296Z\tINFO\tutils/utils.go:168\tInitializing configmap informer in avi-system 2023-02-04T11:43:51.296Z\tINFO\tlib/dynamic_client.go:118\tSkipped initializing dynamic informers antrea 2023-02-04T11:43:51.445Z\tINFO\tk8s/ako_init.go:455\tSuccessfully connected to AVI controller using existing AKO secret 2023-02-04T11:43:51.446Z\tINFO\tako-main/main.go:261\tValid Avi Secret found, continuing .. 2023-02-04T11:43:51.866Z\tINFO\tcache/avi_ctrl_clients.go:71\tSetting the client version to 22.1.1 2023-02-04T11:43:51.866Z\tINFO\tako-main/main.go:279\tSEgroup name found, continuing .. 2023-02-04T11:43:53.015Z\tINFO\tcache/controller_obj_cache.go:2340\tAvi cluster state is CLUSTER_UP_NO_HA 2023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2901\tSetting cloud vType: CLOUD_VCENTER 2023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2904\tSetting cloud uuid: cloud-ae84c777-ebf8-4b07-878b-880be6b201b5 2023-02-04T11:43:53.176Z\tINFO\tlib/lib.go:291\tSetting AKOUser: ako-wdc-2-tkc-cluster-1 for Avi Objects 2023-02-04T11:43:53.177Z\tINFO\tcache/controller_obj_cache.go:2646\tSkipping the check for SE group labels 2023-02-04T11:43:53.332Z\tINFO\tcache/controller_obj_cache.go:3204\tSetting VRF global found from network vds-tkc-frontend-vlan-1027 2023-02-04T11:43:53.332Z\tINFO\trecord/event.go:282\tEvent(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;avi-system\u0026#34;, Name:\u0026#34;ako-0\u0026#34;, UID:\u0026#34;4b0ee7bf-e5f5-4987-b226-7687c5759b4a\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;41019\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ValidatedUserInput\u0026#39; User input validation completed. 2023-02-04T11:43:53.336Z\tINFO\tlib/lib.go:230\tSetting Disable Sync to: false 2023-02-04T11:43:53.338Z\tINFO\tk8s/ako_init.go:310\tavi k8s configmap created Looks good, let us try do deploy an application and expose it with Ingress\nI have two demo applications, banana and apple. Yaml comes below. I deploy them\nandreasm@linuxvm01:~/ako$ k create ns fruit namespace/fruit created andreasm@linuxvm01:~/ako$ k apply -f apple.yaml -f banana.yaml pod/apple-app created service/apple-service created pod/banana-app created service/banana-service created yaml for banana and fruit\nkind: Pod apiVersion: v1 metadata: name: banana-app labels: app: banana namespace: fruit spec: containers: - name: banana-app image: hashicorp/http-echo args: - \u0026#34;-text=banana\u0026#34; --- kind: Service apiVersion: v1 metadata: name: banana-service namespace: fruit spec: selector: app: banana ports: - port: 5678 # Default port for image kind: Pod apiVersion: v1 metadata: name: apple-app labels: app: apple namespace: fruit spec: containers: - name: apple-app image: hashicorp/http-echo args: - \u0026#34;-text=apple\u0026#34; --- kind: Service apiVersion: v1 metadata: name: apple-service namespace: fruit spec: selector: app: apple ports: - port: 5678 # Default port for image Then I apply the Ingress:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-example namespace: fruit # annotations: # ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; spec: ingressClassName: avi-lb rules: - host: fruit-tkgs.you-have.your-domain.here http: paths: - path: /apple pathType: Prefix backend: service: name: apple-service port: number: 5678 - path: /banana pathType: Prefix backend: service: name: banana-service port: number: 5678 k apply -f ingress-example.yaml You should more or less instantly notice a new virtual service in your Avi controller: And let us check the ingress in k8s:\nandreasm@linuxvm01:~/ako$ k get ingress -n fruit NAME CLASS HOSTS ADDRESS PORTS AGE\ringress-example avi-lb fruit-tkgs.you-have.your-domain.here 10.102.7.15 80 2m49s There it is, with the actual VIP it gets from Avi.\nHeres is the view of the application from the Dashboard view in Avi: Also notice that the SE\u0026rsquo;s now also places itself in the same network as the worker nodes, but still creates the VIP in the frontend-network.\nMeaning our network diagram will now look like this: Now, AKO comes with its own CRDs that one can work with. I will go through these in a separate post.\n","date":"26 October 2022","externalUrl":null,"permalink":"/2022/10/26/vsphere-8-with-tanzu-using-vds-and-avi-loadbalancer/","section":"Posts","summary":"Deploy Tanzu in vSphere 8 with VDS and Avi Loadbalancer: # This post will go through how to install Tanzu in vSphere 8 using vSphere VDS networking and Avi as loadbalancer.","title":"vSphere 8 with Tanzu using VDS and Avi Loadbalancer","type":"posts"},{"content":"","date":"26 October 2022","externalUrl":null,"permalink":"/tags/lodbalancing/","section":"Tags","summary":"","title":"Lodbalancing","type":"tags"},{"content":" Deploy Tanzu in vSphere 8 with NSX and Avi Loadbalancer: # This post will go through how to install Tanzu in vSphere 8 using NSX networking (including built in L4 loadbalancer) and Avi as L7 loadbalancer. The goal is to deploy Tanzu by using NSX for all networking needs, including the Kubernetes Api endpoint (L4) and utilize Avi as loadbalancer for all L7 (Ingress). The deployment of Tanzu with NSX is an automated process, but it does not include L7 loadbalancing. This post will quickly go through how to configure NSX and Avi to support this setup and also the actual configuration/deployment steps of Tanzu. The following components will be touched upon in this post: NSX, Tanzu, TKC, AKO, NCP, vCenter, AVI and Antrea. All networks needed for this deployment will be handled by NSX, except vCenter, NSX manager and Avi controller but including the management network for the supervisor cluster and Avi SE\u0026rsquo;s. In the end we will also have a quick look at how to use Antrea Egress in one of the TKC clusters.\nWe should end up with the following initial network diagram for this deployment (will update it later in the post reflecting several network for our TKC cluster with and without NAT (without NAT when using Egress): Preparations - NSX config # This post assumes a working vSphere environment with storage configured, vMotion network, vSAN (if using vSAN), HA, DRS enabled and configured. So this step will cover the basic NSX config for this use-case. NSX will need some network configured in the physical environment like the Geneve Tunnel VLAN, Uplink VLAN(s) for our T0 in addition to the most likely already defined management network for the placement of NSX managers, NSX edges and Avi controller and/or SE\u0026rsquo;s. So lets jump in.\nInitial configs in the NSX manager # The first NSX manager is already deployed. Accept the EULA and skip the NSX tutorial:\nWhen done, head over to System -\u0026gt; Licenses and add your license key. Then, still under system, head over to Appliances and add a cluster IP. Even though you only have 1 NSX manager for test/poc it can make sense to use cluster ip adding, removing nsx managers etc and still point to the same IP.\nClick on Set Virtual IP and type in your wanted cluster ip. Out of the box its the same layer 2 subnet as your controllers are placed in (it possible to use L3 also but that involves an external LoadBalancer, not the built in for this purpose).\nClick save and wait. After some minutes, try to log in via your new cluster IP. All the configs I will do will be used with this IP. It does not matter if you go directly to the NSX manager itself or the cluster IP for this post.\nAfter cluster IP is done, we need to add a Compute Manager which in our case is the vCenter server (not that you have any option besides vCenter). Still under System, go to Fabric expand and find Compute Manager. From there click Add Compute Manager:\nFill in the necessary information for your vCenter and make sure Service Account and Enable Trust is enabled. Next message will ask you to use a Thumprint the vCenter says it has. You could either just say ADD or actually go to vCenter and grab the thumbprint from there and verify or paste it in the SHA-256 field before clicking add.\nHere is how to get the thumbprint from vCenter:\nroot@vcsa [ ~ ]# openssl x509 -in /etc/vmware-vpx/ssl/rui.crt -fingerprint -sha256 -noout SHA256 Fingerprint=A1:F2:11:0F:47:D8:7B:02:D1:C9:B6:87:19:C0:65:15:B7:6A:6E:23:67:AD:0C:41:03:13:DA:91:A9:D0:B2:F6 Now when you are absolutely certain, add and wait.\nNSX Profiles: Uplink, Transport Zones and Transport Node Profiles # In NSX there is a couple of profiles that needs to be configured, profiles for the Transport nodes, Edge transport nodes, transport zones. Instead of configuring things individually NSX uses profiles so we have a consistent and central place to configure multiple components from. Let start with the Uplink profiles: Under Fabric head over to Profiles.\nHere we need to create two uplink profiles (one for the ESXi transport nodes and one for the NSX edge transport nodes). These profile will dictate the number of uplinks used, mtu size (only for the edges after NSX 3.1) ,vlan for the geneve tunnel and nic teaming. Here we also define multiple teaming policies if we want to dictate certain uplinks to be used for deterministic traffic steering. Which I will do.\nHost uplink:\nIn the \u0026ldquo;host\u0026rdquo; uplink profile we define the logical uplinks, (uplink-1 and uplink-2, but we could name them Donald-Duck-1 and 2 if we wanted). We define the default teaming-policy to be Load Balance Source as we want two vmkernels for the host-tep, and default active/active if we create a vlan segment without specifying a teaming policy in the segment. Then we add two more teaming policies with Failover Order and specify one uplink pr policy. The reason for that is because we will go on and create a VLAN segment later where we will place the Edge VM uplinks, and we need to have some control over wich uplink-\u0026gt;nic on the ESXi host the T0 Uplinks go, and we dont want to use teaming on these, and we dont want a standby uplink as BGP is supposed to handle failover for us. This way we steer T0 Uplink 1 out via uplink-1 and T0 Uplink 2 out via uplink-2 and the ESXi hosts has been configured to map uplink-1 to VDS uplink-1 which again is mapping to pNIC0 and uplink-2 to VDS uplink-2 to pNIC1 respectively. In summary we create a vlan segment on the host for the edge VM, then we create a vlan segment for the logical T0 later.\nThen we define the VLAN number for the Geneve tunnel. Notice we dont specify MTU size as this will adjust after what we have in our VDS which we will map to later, so our VDS must have minumum MTU 1700 defined (it works with 1600 also, but in NSX-T 3.2 and later there is a greenfield min MTU of 1700). We will use the same VLAN for both host and edge tep wich was supported from NSX-T 3.1 and forward. But to make that work we cant use VDS portgroups for the T0, it needs to be a NSX VLAN segment. More on that later.\nClick save.\nNext up is the \u0026ldquo;Edge\u0026rdquo; Uplink Profile, almost same procedure:\nThe biggest difference is the name of the specific teaming policies, and we specify a MTU size of 1700 as this is what I use in my VDS.\nNow over to Transport Zones\nHere we create three Transport Zones: 1 vlan TZ for host, 1 vlan TZ for edge and 1 overlay which is common for both Edge and Host transport nodes.\nCreate host-vlan-tz:\nIts probably not so obvious in the GUI, but we also define our teaming policies defined in our respective uplinks earlier. Here I enter manually the uplink policy names for my host transport zone so they can be available later when I create a VLAN segment for my T0 uplinks (on the host). Click save.\nWhen done its the edge-vlan-tz:\nCommon for both Transport Zones is VLAN.\nNow the last Transport Zone for now - the Overlay TZ (this one is easy):\nThe next step would be to create the Transport Node Profile, but first we need to create an IP-Pool for the TEP addresses. Head over to Networking and IP Address Pools :\nAdd IP address pool:\nIts only necessary to define the CIDR, IP Range and Gateway IP. Please make sure the range is sufficient to support the max amount of ESXi Transport nodes and Edge nodes you will have. 2 IPs pr device.\nNow back to System -\u0026gt; Fabric again and create the Transport Node Profile under Profile:\nHere we select the vCenter we added earlier, point to the correct VDS we want to use, select the host-profile we created and under IP assignment we use our newly created IP pool and map the uplinks to the corresponding VDS uplinks. Then ADD\nInstall NSX components on the ESXi hosts # To enable NSX after our initial configs has been done is fairly straight-forward. Head over to Nodes under System-\u0026gt;Fabric and select the first tab Host Transport Nodes. Select your vCenter under Managed by and select your cluster and click Configure NSX:\nSelect your (only?) transport-node profile your created above:\nClick apply and wait\u0026hellip;\nStatus in vCenter - the only status we will see.\nWhen everything is up and green as below, NSX is installed in our ESXi hosts and we are ready to create networks \u0026#x1f604;\nDeploy NSX Edges # Deploying a Edge is quite straight forward, and is done from the NSX manager under System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nBut I need to create two VLAN segments for Edge \u0026ldquo;data-path/uplink interfaces. As I want to use the same VLAN for both Host Tep and Edge TEP I need to do that. These two segments will only be used for the actual Edge VMs, not the T0 I am going to create later. In my lab I am using VLAN 1013 for TEP and VLAN 1014 and 1015 for T0 Uplink 1 and 2. So that means the first VLAN segment I create will have the VLAN Trunk range 1013-1014 and the second VLAN segment will use VLAN trunk 1013,1015 Head over to the Networking section/Segments in the NSX UI click Add Segment:\nSelect host-uplink-1 under Uplink Teaming Policy and add your VLAN ID under VLAN. Click save\nSame procedure again for the second segment:\nselect host-uplink-2 and the correct vlan trunk accordingly.\nThe result should be two VLAN segments, created in our host-vlan-tz (the host vlan transport zone created earlier)\nNow we can deploy our Edge(s).\nHead over to System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nClick add edge node and start the edge deployment wizard:\nGive the edge a name, then fill in a FQDN name. Not sure if that part have to be actually registered in DNS, but it probably does do any harm if you decide to. Choose a form factor. When using Tanzu it can be potentially many virtual services so you should at least go with Large. You can find more sizing recommendation on the VMware official NSX docs page. Next\nFill inn your username and passwords (if you want easier access to SSH shell for troubleshooting purposes enable SSH now).\nSelect your vCenter, cluster and datastore. The rest default.\nConfigure the basics\u0026hellip; This the part of the Edge that communicates with the NSX manager. Next we will configure the networking part that will be used for the T0 uplinks and TEP.\nHere we select the transport zones for the Edge to be part of. Note, it should be part of your overlay transport zone but not part of the host vlan transport zone. Here we have defined a Edge vlan transportzone to be used. This is the transport zone where the segment for the T0 to be created in. One of the reason is that we dont want the segment for the T0 to be visible for the host for potentially other workloads, and the segment is actually created in the Edge, thats where the T0 is realised (The SR part of the T0). Then we select the edge-profile we created earlier, the same IP pool as the hosts. Under uplinks we select the respective vlan segments uplink 1 and 2 created earlier.\nThen finish. It should take a couple of minutes to report ready in the NSX manager ui.\nStatus when ready for duty:\nThere should also be some activity in vCenter deploying the edge. When ready head over to Edge Clusters and create a cluster to put the Edge in. We need an Edge cluster and the edges in an edge cluster before we can do anything with them, even if we only deploy one edge (labs etc).\nThe T0 # Now that at least one Edge is up, we should create a T0 so we can make some external connectivity happen (even though NSX have its own networking components and we can create full L3 topology, we cant talk outside NSX from overlay without the Edges). Head over to Network and create a VLAN segment. This time the segment should be placed in the edge-vlan-tz as it is use for T0 uplinks only. Select teaming policy and correct vlan for the T0 uplink 1. I will only use 1 uplink in my lab so I will only create 1 segment for this, I only have 1 upstream router to peer to also.\nNext is heading over to Tier-0 and create a T0:\nThe selection is very limited at first, so give it a name and select HA mode, and edge cluster (the one that we created above).\nClick save and yes to continue edit:\nNow we need to add the interface(s) to the T0. The actual interfaces will be residing on the Edges, but we need to define them in the T0. Click on the 0 under Interfaces (its already two interfaces in the screenshot below).\nGive the interface a name, choose type External, give it the correct IP address to peer with the upstream router, and select the Edge VLAN segment created earlier which maps to the correct uplink (1 or 2). Then select the Edge node that shall have this interface configured.\nClick save. Now as an optional step SSH into the Edge selected above, go the correct vrf and ping the upstream router.\nGet the correct VRF (we are looking for the SR T0 part of the T0)\nEnter the vrf by typing vrf and the number, here it is vrf 1.\nListing the interface with get interfaces one should see the interface we configured above, and we can ping the upstream router to verify L2 connectivity.\nGood, now configure BGP. Expand BGP in your T0 settings view (same place as we configure the interface) adjust your BGP settings accordingly. Click save, enter again and add your BGP peers/neighbors bly clicking on neighbors.\nAdd the IP to the BGP peer you should use and adjust accordingly, like AS number. Click Save\nIt will become green directly, then if you click refresh it will become red, then refresh again it should be green again if everything is correct BGP config wise on both sides. Clicking on the (i) will give you the status also:\nFrom your upstream routes you should see a new neighbor established:\nThe last step on the T0 now is to configure it which networks it should advertise on BGP. That is done under Route re-distribution. In a Tanzu setup we need to advertise NAT, connected and LB VIP from our T1s. That is because Tanzu or NCP creates NAT rules, it creates some LB VIPS and we should also be able to reach our other Overlay segments we create under our T1 (which we have not created yet).\nNow that T0 is configured and peering with the upstream router, I can create segments directly under the T0, or create T1s and then segments connected to the T1 instead. If you do create segments directly attached to the T0 one must configure route advertisement accordingly. As the config above is not advertising any networks from T0, only T1.\nIn NSX there is a neat map over the Network Topology in NSX:\nDeploy Tanzu with NSX # Now that networking with NSX is configured and the foundation is ready. Its time to deploy the Supervisor cluster, or enable WCP, Workload Management. Head over to the hamburger menu in top left corner in your vCenter and select * Workload Management*\nClick on Get Started\nThen follow the wizard below:\nSelect NSX and Next\nSelect Cluster Deployment, choose your vCenter cluster, give the supervisor a name and below (not in picture above) enter a zone name.\nNext\nSelect your storage policies, Next\nIn Step 4 - Management Network we configure the network for the Supervisor Control Plane VMs. In my lab I have already created an overlay segment I call ls-mgmt with cidr 10.101.10.0/24 and gateway 10.101.10.1. So I will place my Supervisors also there. Not using DHCP, but just defining a start IP. The Supervisors will consist of three VMs, and a cluster IP. But it will use 5 IP addresses in total in this network. DNS, Search Domains and NTP should be your internal services. In screenshot above I have used an external NTP server. NEXT\nAbove I define the workload network, which the supervisor control plane vms also will be part of, but also for the vSphere Pods. This a default workload network where your TKC cluster can be deployed in (if not overriden when creating a vSphere namespace (later on that topic) ). Select the VDS you have used and configured for NSX. Select your Edge cluster. Add your DNS server(s), select the T0 router you created earlier in NSX. Then leave NAT-Mode enabled, we can create vSphere namespaces later where we override these settings. Then you define the Namespace Network. This is the network your vSphere pods will use, the workload network interface of the Supervisor ControlPlane Nodes, and your TKC cluster nodes. The CIDR size define how many IPs you will have available for your TKC cluster nodes, meaning also the total amount of nodes in this workload network. But dont despair, we can create additional vSphere namespaces and add more networks. So in the above example I give it a /20 cidr (unnecessary big actually, but why not). This Namespace Network will not be exposed to the outside world as NCP creates route-map rules on the T0 not allowing these to be advertised (we have NAT enabled). The Service CIDR is Kubernetes internal network for services. When we deploy a TKC cluster later we define other Kubernetes cluster and pod cidrs. Define the Ingress CIDR, this is the IP address range NCP will use to carve out LoadBalancer VIPs for the Kubernetes API endpoints, for the Supervisor Control Plane, the TKC clusters K8s Api endpoint and even the Service Type LoadBalancer services you decide to created. So all access TO the Supervisor Cluster API endpoint will be accessed through the IP address assigned from this CIDR. When we have NAT enabled it will also ask you to define a Egress CIDR which will be used by NSX to create SNAT rules for the worker nodes to use when communicate OUT. These NAT rules will be created automatically in NSX-T.\nNEXT\nSelect the size of your SVCP and give the SVCP API endpoint a name. This is something that can be registered in DNS when deployment is finished and we know the IP it gets.\nFinish and wait. Its the same (waiting) process as explained here\nIf everything goes well we should have a Supervisor cluster up and running in not that many minutes. 20-30 mins?\nWhen its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.101.11.2 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters.\nNSX components configured by WCP/NCP # Before heading over to next section, have a look in NSX and see what happended there:\nUnder Networks -\u0026gt; Segments you will notice networks like this:\nNotice the gateway ip and CIDR. These are created for each vSphere Namespace, the cidr 10.101.96.1/28 is carved out of the CIDR defined in the \u0026ldquo;default\u0026rdquo; network when deploying WCP.\nUnder Networks -\u0026gt; T-1 Gateways you will notice a couple of new T1 routers being created:\nUnder Networks -\u0026gt; Load Balancing:\nHere is all the L4 K8s API endpoints created, also the other Service Type Loadbalancer services you choose to expose in your TKC clusters.\nUnder Networks -\u0026gt; NAT and the \u0026ldquo;domain\u0026rdquo; T1 there will be auto-created NAT rules, depending on whether NAT is enabled or disabled pr Namespace.\nThen under Security -\u0026gt; Distributed Firewall there will be a new section:\nvSphere Namespace # When Supervisor is up and running next step is to create a vSphere Namespace. I will go ahead and create that, but will also use the \u0026ldquo;override network\u0026rdquo; to create a separate network for this Namespace and also disable NAT as I want to use this cluster for Antrea Egress explained here.\nA vSphere is a construct in vSphere to adjust indidivual access settings/permissions, resources, network settings or different networks for IP separation. Click on Create Namespace and fill in relevant info. I am choosing to Override Supervisor network settings.\nNotice when I disable NAT Mode there will no longer be a Egress IP to populate. Thats because the TKC nodes under this namespace will not get a NAT rule applied to them in NSX and will be communicating \u0026ldquo;externally\u0026rdquo; with their own actual IP address. Ingress will still be relevant as the NSX-T Loadbalancer will create the K8s API endpoint to reach the control plane endpoint your respective TKC clusters.\nThis option to adjust the networks in such a degree is a great flexibility with NSX. Tanzu with VDS does give you the option to select different VDS portgroups on separate vlans, but must be manually created, does not give you NAT, Ingress and Egress and VLAN/Routing must be in place in the physical environment. With NSX all the networking components are automatically created and it includes the NSX Distributed Firewall.\nAfter the vSphere Namespace has been created it is the same procedure to deploy TKC clusters regardless of using VDS or NSX. So instead of me repeating myself and saving the environment for digital ink I will refere to the process I have already described here\nConfigure Avi as Ingress controller (L7) with NSX as L4 LB # When using Antrea as the CNI in your TKC cluster (default in vSphere with Tanzu) make sure to enable NodePortLocal. This gives much better control and flexibility, follow how here and NodePortLocal explained here\nConfigure NSX cloud in Avi - network preparations # As this guide is using NSX as the underlaying networking platform instead of VDS as this article is using, we also have the benefit of configuring the NSX cloud in Avi instead of the vCenter Cloud. This cloud do come with some additional benefits like automatic Security Group creation in NSX, VIP advertisement through the already configured T0, SE placement dataplane/data network on separate network and VRF context.\nBut before we can consume the NSX cloud we need to configure it. Assuming the Avi controller has already been deployed and initial config is done (username and password, dns, etc) log in to the controller and head over to Administration -\u0026gt; User Credentials:\nAdd the credentials you want to use for both vCenter and NSX-T. The next step is involving NSX. Head over to NSX and create two new networks. One for SE management and one for SE dataplane (the network it will use to reach the backend servers they are loadbalancing). I will now just show a screenshot of three networks already created. One segment ls-avi-se-mgmt for SE mgmt, one segment ls-avi-generic-se-data for SE communication to backend pools, and one segment ls-avi-dns-se-data (I will get back to the last network later when enabling the Avi DNS service).\nThen head over to vCenter and create a local Content Library and call it what you want.\nWhen networks and credentials have been created, back in the Avi gui head over to Infrastructure -\u0026gt; Clouds:\nClick Create and select NSX-T\nStart by giving the NSX cloud a meaningfull name and give a prefix name for the objects being created in NSX by Avi. If you have several Avi controllers using same NSX manager etc. Easier to identify when looking in the NSX manager ui:\nThen proceed (scroll down if needed) to connect to you NSX manager:\nEnter the IP of your NSX manager, if you created a Cluster IP in the NSX manager use that one, and select the credentials for NSX manager create in the Avi earlier.\nConnect.\n!Note Before being able to continue this step we need to have defined some networks in NSX as explained above\u0026hellip;\nFill in the Transport zone where you have defined the segment for the management network, select the T1 this segment is attached to and then the segment. This network is created in NSX for the SE management interface. Then go to Data Networks and select the Overlay Transport Zone where the ls-avi-generic-se-data segment is created the ADD the T1 router for this segment from the dropdown list and the corresponding segment. (Ignore the second netowork in the screenshot above.) Then under vCenter Server click add and fill in relevant info to connect the vCenter server your NSX manager is using.\nGive it a meaningful name, it will either already have selected your NSX cloud otherwise select it from the list. Then the credentials and the Content Library from the dropdown.\nThe last section IPAM/DNS we will fill out later (click save):\nNow the cloud should be created.\nSE Groups # Head over to Cloud Resources -\u0026gt; Service Engine Groups and create a custom SE-group.\nSelect your newly created cloud from the dropdown list above next to Select Cloud. Click create blue button right side.\nChange the fields marked by a red line. And optionally adjust the Max. Number of Service Engines. This is just to restrict Avi to not deploy too many SE\u0026rsquo;s if it sees fit. Adjust the Virtual Services pr Service Engine to a higher number than 10 (if that is default). This all comes down to performance.\nJump to advanced:\nChange the Service Engine Name Prefix to something useful, then select the vCenter, cluster and datastore. Save\nAvi Networks # Now head over to Infrastructure -\u0026gt; VRF Contexts (below Service engine Groups) select correct cloud from dropdown and add a default gateway for the SE dataplane network.\nClick the pencil\nAdd a default route under Static Route and point to the gateway used for the SE dataplane ls-avi-generic-se-data . This is used for the SE\u0026rsquo;s to know where to go to reach the different pools it will loadbalance.\nNext we need to define the different networks for the SE\u0026rsquo;s to use (the dataplane network). Head over to Infrastructure -\u0026gt; Networks (again select correct cloud from dropdown)\nIf there is workload running in these networks they can already be auto-detected, if not you need to create them. In my lab I rely on Avi as the IPAM provider for my different services, a very useful feature in Avi. So the two networks we need to create/and or edit is the ls-avi-se-mgmt and the ls-avi-generic-se-data\nFirst out is the ls-avi-se-mgmt\nDeselect DHCP, we want to use Avi IPAM instead (and I dont have DHCP in these networks). Then fill in the CIDR (IP Subnet), deselect Use Static IP Address for VIPs and Service Engine add a range for the SE to be allow to get an IP from. Then select the Use for Service Engines. This will configure the IPAM pool to only be used for the dataplane for the SE\u0026rsquo;s in the management network. We dont want to have any VIPs here as it will only be used for the SE\u0026rsquo;s to talk to the Avi controller.\nThen it is the ls-avi-generic-se-data network\nSame as above just a different subnet, using the same T1 router defined in NSX.\nThe two above networks will only be used as dataplane network. Meaning they will not be used for any Virtual Service VIP. We also need to define 1 or more VIP networks. Create one more network:\nHere I specify a network which is only used for VS VIP\nAvi IPAM/DNS template # Now we need to inform our cloud which VIP networks we can use, that is done under Templates -\u0026gt; IPAM/DNS Profiles\nWhile we are here we create two profiles, one for DNS (for the DNS service) and IPAM profile for the VIP networks. Lets start with the IPAM profile. Click create right corner and select IPAM Profile:\nFill in name, select allocate IP in VRF and select your NSX cloud. Then click add and select your VIP networks defined above. Screenshot below also include a DNS-VIP network which I will use later. Click save.\nNow click create again and select DNS profile:\nGive it a name and type in the domain name you want Avi to use.\nNow go back to Infrastructure -\u0026gt; Clouds and edit your NSX cloud and add the newly added Profiles here:\nSave\nNow Avi is prepared and configured to handle requests from AKO on L7 service Ingress. Next step will be to deploy configure and deploy AKO in our TKC cluster. But first some short words around Avi DNS service.\nAvi DNS service # Getting a virtual service with a VIP is easy with Avi, but often we need a DNS record on these VS\u0026rsquo;es. Avi has a built in DNS service which automatically register a DNS record for your services. The simplest way to make this work out of the box is to create a Forward Zone in your DNS server to the DNS Service IP for a specific subdomain or domain. Then Avi will handle the DNS requests for these specific domains. To make use of Avi DNS service we should dedicate a SE group for this service, and create a dedicated VIP network for it. As we should use a dedicated SE group for the DNS service it would be nice to also have a dedicated SE dataplane network for these SE\u0026rsquo;s. So follow the steps I have done above to create a SE network for the SE service SE\u0026rsquo;s and add this to your cloud. The VIP network also needs to be added to the IPAM Profile created earlier. A note on the additional SE network, this also requires a dedicated T1 router in the NSX environment. So in your NSX environment create an additional T1 router, create segment for the DNS SE datanetwork. This is how to enable the DNS service in Avi after you have prepared the networks, and IPAM profile:\nHead over to Administration -\u0026gt; Settings -\u0026gt; DNS Service:\nThen create virtual service:\nSelect your cloud and configure the DNS service:\nThe VS VIP is configured with a static IP (outside of the DNS VIP IPAM range you have created)\nUnder advanced select the SE group:\nSave. Now the DNS VS is configured, go to templates and add a DNS profile:\nGive it a name, add your domain(s) here. Save\nHead over to the cloud and add your DNS profile.\nNow you just need to configure your backend DNS server to forward the requests for these domains to the Avi DNS VS IP. Using bind this can be done like this:\nzone \u0026#34;you-have.your-domain.here\u0026#34; { type forward; forward only; forwarders { 10.101.211.9; }; }; AKO in TKC # I have already deployed a TKC cluster, which is described here\nAlso make sure Antrea is configured with NodePortLocal as described also in the link above.\nSo for Avi to work as Ingress controller we need to deploy AKO (Avi Kubernetes Operator). I have also explained these steps here the only difference is how the value.yaml for AKO is configured. Below is how I have configured it to work in my NSX enabled environment with explanations:\n# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. replicaCount: 1 image: repository: projects.registry.vmware.com/ako/ako pullPolicy: IfNotPresent AKOSettings: primaryInstance: true enableEvents: \u0026#39;true\u0026#39; logLevel: WARN fullSyncFrequency: \u0026#39;1800\u0026#39; apiServerPort: 8080 deleteConfig: \u0026#39;false\u0026#39; disableStaticRouteSync: \u0026#39;false\u0026#39; clusterName: wdc-tkc-cluster-1-nsx # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. enableEVH: false layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. namespaceSelector: labelKey: \u0026#39;\u0026#39; labelValue: \u0026#39;\u0026#39; servicesAPI: false vipPerNamespace: \u0026#39;false\u0026#39; NetworkSettings: nodeNetworkList: # nodeNetworkList: - networkName: \u0026#34;vnet-domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5-ns-wdc-1-tkc-cluste-62397-0\u0026#34; # this is the NSX segment created for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter cidrs: - 10.101.112.32/27 # this is the CIDR for your current TKC cluster (make sure you are using right CIDR, seen from NSX) enableRHI: false nsxtT1LR: \u0026#39;Da-Tier-1\u0026#39; #The T1 router in NSX you have defined for the avi-se-dataplane network bgpPeerLabels: [] # bgpPeerLabels: # - peer1 # - peer2 vipNetworkList: - networkName: \u0026#34;vip-tkc-cluster-1-nsx-wdc-l7\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. cidr: 10.101.210.0/24 L7Settings: defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. noPGForSNI: false serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. passthroughShardSize: SMALL enableMCI: \u0026#39;false\u0026#39; L4Settings: defaultDomain: \u0026#39;\u0026#39; autoFQDN: default ControllerSettings: serviceEngineGroupName: nsx-se-generic-group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.2 cloudName: wdc-1-nsx # The configured cloud name on the Avi controller. controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller tenantName: admin nodePortSelector: key: \u0026#39;\u0026#39; value: \u0026#39;\u0026#39; resources: limits: cpu: 350m memory: 400Mi requests: cpu: 200m memory: 300Mi podSecurityContext: {} rbac: pspEnable: false avicredentials: username: \u0026#39;admin\u0026#39; # username for the Avi controller password: \u0026#39;password\u0026#39; # password for the Avi controller authtoken: certificateAuthorityData: persistentVolumeClaim: \u0026#39;\u0026#39; mountPath: /log logFile: avi.log Install AKO with this command:\nhelm install ako/ako --generate-name --version 1.8.2 -f values.yaml --namespace=avi-system Check the logs of the AKO pod if it encountered some issues or not by issuing the command:\nkubectl logs -n avi-system ako-o If there is no errors there its time to deploy a couple of test applications and the Ingress itself. This is already described here\nThats it. Now L7 is enabled on your TKC cluster with Avi as Ingress controller. There is much that can be configured with AKO CRDs. I will try to update my post here to go through the different possibilites. In the meantime much information is described here\nAviInfraSetting # If you need to have separate VIPs/different subnets for certain applications we can use AviInfraSetting to override the \u0026ldquo;default\u0026rdquo; settings configured in our values.yaml above. This is a nice feature to override some settings very easy. There is also the option to run several AKO instances pr TKC/k8s cluster like described here which I will go through another time. But now quickly AviInfraSetting\nLets say I want to enable BPG on certain services or adjust my Ingress to be exposed on a different VIP network.\nCreate a yaml definition for AviInfraSetting:\napiVersion: ako.vmware.com/v1alpha1 kind: AviInfraSetting metadata: name: enable-bgp-fruit spec: seGroup: name: Default-Group network: vipNetworks: - networkName: vds-tkc-frontend-l7-vlan-1028 cidr: 10.102.8.0/24 nodeNetworks: - networkName: vds-tkc-workload-vlan-1026 cidrs: - 10.102.6.0/24 enableRhi: true bgpPeerLabels: - cPodRouter In the example above I define the VIP network (here I can override the default confgured from value.yaml), the nodNetwork. Enable RHI, and define a label to be used for BGP (label is from BGP settings here):\nPeer:\nApply the above yaml. To use it, create an additional IngressClass like this:\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: avi-lb-bgp #name the IngressClass spec: controller: ako.vmware.com/avi-lb #default ingressclass from ako parameters: apiGroup: ako.vmware.com kind: AviInfraSetting #refer to the AviInfraSetting name: enable-bgp-fruit #the name of your AviInfraSetting applied Apply it, then when you apply your Ingress or update it refer to this ingressClass like this:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-example namespace: fruit spec: ingressClassName: avi-lb-bgp #Here you choose your specific IngressClass rules: - host: fruit-tkgs.you-have.your-domain.here http: paths: - path: /apple pathType: Prefix backend: service: name: apple-service port: number: 5678 - path: /banana pathType: Prefix backend: service: name: banana-service port: number: 5678 ","date":"26 October 2022","externalUrl":null,"permalink":"/2022/10/26/vsphere-8-with-tanzu-using-nsx-t-avi-loadbalancer/","section":"Posts","summary":"Deploy Tanzu in vSphere 8 with NSX and Avi Loadbalancer: # This post will go through how to install Tanzu in vSphere 8 using NSX networking (including built in L4 loadbalancer) and Avi as L7 loadbalancer.","title":"vSphere 8 with Tanzu using NSX-T \u0026 Avi LoadBalancer","type":"posts"},{"content":" What is AKO? # AKO is an operator which works as an ingress controller and performs Avi-specific functions in an OpenShift/Kubernetes environment with the Avi Controller. It runs as a pod in the cluster and translates the required OpenShift/Kubernetes objects to Avi objects and automates the implementation of ingresses/routes/services on the Service Engines (SE) via the Avi Controller. ref: link\nHow to install AKO # AKO is very easy installed with Helm. Four basic steps needs to be done.\nCreate a namespace for AKO in your kubernetes cluster: kubectl create ns avi-system Add AKO Helm reposistory: helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Get the current values for the versions you want: helm show values ako/ako --version 1.9.1 \u0026gt; values.yaml Deploy (after values have been edited to suit your environment): helm install ako/ako --generate-name --version 1.9.1 -f values.yaml -n avi-system AKO Helm values explained # Before deploying AKO there are some parameters that should be configured, or most likely the deployment will fail. Below is an example file where the different fields are explained:\n# Default values for ako. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: projects.registry.vmware.com/ako/ako #If using your own registry update accordingly pullPolicy: IfNotPresent ### This section outlines the generic AKO settings AKOSettings: primaryInstance: true # Defines AKO instance is primary or not. Value `true` indicates that AKO instance is primary. In a multiple AKO deployment in a cluster, only one AKO instance should be primary. Default value: true. enableEvents: \u0026#39;true\u0026#39; # Enables/disables Event broadcasting via AKO logLevel: WARN # enum: INFO|DEBUG|WARN|ERROR fullSyncFrequency: \u0026#39;1800\u0026#39; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. apiServerPort: 8080 # Internal port for AKO\u0026#39;s API server for the liveness probe of the AKO pod default=8080 deleteConfig: \u0026#39;false\u0026#39; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI disableStaticRouteSync: \u0026#39;false\u0026#39; # If the POD networks are reachable from the Avi SE, set this knob to true. clusterName: my-cluster # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT cniPlugin: \u0026#39;\u0026#39; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift|antrea|ncp enableEVH: false # This enables the Enhanced Virtual Hosting Model in Avi Controller for the Virtual Services layer7Only: false # If this flag is switched on, then AKO will only do layer 7 loadbalancing.Must be true if used in a TKC cluster / Tanzu with vSphere # NamespaceSelector contains label key and value used for namespacemigration # Same label has to be present on namespace/s which needs migration/sync to AKO namespaceSelector: labelKey: \u0026#39;\u0026#39; labelValue: \u0026#39;\u0026#39; servicesAPI: false # Flag that enables AKO in services API mode: https://kubernetes-sigs.github.io/service-apis/. Currently implemented only for L4. This flag uses the upstream GA APIs which are not backward compatible # with the advancedL4 APIs which uses a fork and a version of v1alpha1pre1 vipPerNamespace: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to create Parent VS per Namespace in EVH mode istioEnabled: false # This flag needs to be enabled when AKO is be to brought up in an Istio environment # This is the list of system namespaces from which AKO will not listen any Kubernetes or Openshift object event. blockedNamespaceList: [] # blockedNamespaceList: # - kube-system # - kube-public ipFamily: \u0026#39;\u0026#39; # This flag can take values V4 or V6 (default V4). This is for the backend pools to use ipv6 or ipv4. For frontside VS, use v6cidr ### This section outlines the network settings for virtualservices. NetworkSettings: ## This list of network and cidrs are used in pool placement network for vcenter cloud. ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. nodeNetworkList: [] # nodeNetworkList: # - networkName: \u0026#34;network-name\u0026#34; # cidrs: # - 10.0.0.1/24 # - 11.0.0.1/24 enableRHI: false # This is a cluster wide setting for BGP peering. nsxtT1LR: \u0026#39;\u0026#39; # T1 Logical Segment mapping for backend network. Only applies to NSX-T cloud. bgpPeerLabels: [] # Select BGP peers using bgpPeerLabels, for selective VsVip advertisement. # bgpPeerLabels: # - peer1 # - peer2 vipNetworkList: [] # Network information of the VIP network. Multiple networks allowed only for AWS Cloud. # vipNetworkList: # - networkName: net1 # cidr: 100.1.1.0/24 # v6cidr: 2002::1234:abcd:ffff:c0a8:101/64 # Setting this will enable the VS networks to use ipv6 ### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. L7Settings: defaultIngController: \u0026#39;true\u0026#39; noPGForSNI: false # Switching this knob to true, will get rid of poolgroups from SNI VSes. Do not use this flag, if you don\u0026#39;t want http caching. This will be deprecated once the controller support caching on PGs. serviceType: ClusterIP # enum NodePort|ClusterIP|NodePortLocal. NodePortLocal can only be used if Antrea is the CNI shardVSSize: LARGE # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL, DEDICATED passthroughShardSize: SMALL # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL enableMCI: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to start processing multi-cluster ingress objects. ### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. L4Settings: defaultDomain: \u0026#39;\u0026#39; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. autoFQDN: default # ENUM: default(\u0026lt;svc\u0026gt;.\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), flat (\u0026lt;svc\u0026gt;-\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), \u0026#34;disabled\u0026#34; If the value is disabled then the FQDN generation is disabled. ### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. ControllerSettings: serviceEngineGroupName: Default-Group # Name of the ServiceEngine Group. controllerVersion: \u0026#39;\u0026#39; # The controller API version cloudName: Default-Cloud # The configured cloud name on the Avi controller. controllerHost: \u0026#39;\u0026#39; # IP address or Hostname of Avi Controller tenantName: admin # Name of the tenant where all the AKO objects will be created in AVI. nodePortSelector: # Only applicable if serviceType is NodePort key: \u0026#39;\u0026#39; value: \u0026#39;\u0026#39; resources: limits: cpu: 350m memory: 400Mi requests: cpu: 200m memory: 300Mi securityContext: {} podSecurityContext: {} rbac: # Creates the pod security policy if set to true pspEnable: false avicredentials: username: \u0026#39;\u0026#39; password: \u0026#39;\u0026#39; authtoken: certificateAuthorityData: persistentVolumeClaim: \u0026#39;\u0026#39; mountPath: /log logFile: avi.log More info here\n","date":"26 October 2022","externalUrl":null,"permalink":"/2022/10/26/ako-explained/","section":"Posts","summary":"What is AKO?","title":"AKO Explained","type":"posts"},{"content":"","date":"23 October 2022","externalUrl":null,"permalink":"/tags/amko/","section":"Tags","summary":"","title":"Amko","type":"tags"},{"content":"","date":"23 October 2022","externalUrl":null,"permalink":"/tags/gslb/","section":"Tags","summary":"","title":"Gslb","type":"tags"},{"content":" Global Server LoadBalancing in VMware Tanzu with AMKO # This post will go through how to configure AVI (NSX ALB) with GSLB in vSphere with Tanzu (TKGs) and an upstream k8s cluster in two different physical locations. I have already covered AKO in my previous posts, this post will assume knowledge of AKO (Avi Kubernetes Operator) and extend upon that with the use of AMKO (Avi Multi-Cluster Kubernetes Operator). The goal is to have the ability to scale my k8s applications between my \u0026ldquo;sites\u0026rdquo; and make them geo-redundant. For more information on AVI, AKO and AMKO head over here\nPreparations and diagram over environment used in this post # This post will involve a upstream Ubuntu k8s cluster in my home-lab and a remote vSphere with Tanzu cluster. I have deployed one Avi Controller in my home lab and one Avi controller in the remote site. The k8s cluster in my home-lab is defined as the \u0026ldquo;primary\u0026rdquo; k8s cluster, the same goes for the Avi controller in my home-lab. There are some networking connectivity between the AVI controllers that needs to be in place such as 443 (API) between the controllers, and the AVI SE\u0026rsquo;s needs to reach the GSLB VS vips on their respective side for GSLB health checks. Site A SE\u0026rsquo;s dataplane needs connectivity to the vip that is created for the GSLB service on site B and vice versa. The primary k8s cluster also needs connectivity to the \u0026ldquo;secondary\u0026rdquo; k8s clusters endpoint ip/fqdn, k8s api (port 6443). AMKO needs this connectivity to listen for \u0026ldquo;GSLB\u0026rdquo; enabled services in the remote k8s clusters which triggers AMKO to automatically put them in your GSLB service. More on that later in the article. When all preparations are done the final diagram should look something like this:\n(I will not cover what kind of infrastructure that connects the sites together as that is a completely different topic and can be as much). But there will most likely be a firewall involved between the sites, and the above mentioned connectivity needs to be adjusted in the firewall. In this post the following ip subnets will be used:\nSE Dataplane network home-lab: 10.150.1.0/24 (I only have two se\u0026rsquo;s so there will be two addresses from this subnet) (I am running the all services on the same two SE\u0026rsquo;s which is not recommended, one should atleast have dedicated SE\u0026rsquo;s for the AVI DNS service) SE Dataplane network remote-site: 192.168.102.0/24 (Two SE\u0026rsquo;s here also, in remote site I do have dedicated SE\u0026rsquo;s for the AVI DNS Service but they will not be touched upon in this post only the SE\u0026rsquo;s responsible for the GSLB services being created) VIP subnet for services exposed in home-lab k8s cluster: 10.150.12.0/24 (a dedicated vip subnet for all services exposed from this cluster) VIP subnet for services exposed in remote-site tkgs cluster: 192.168.151.0/24 (a dedicated vip subnet for all services exposed from this cluster) For this network setup to work one needs to have routing in place, either with BGP enabled in AVI or static routes. Explanation: The SE\u0026rsquo;s have their own dataplane network, they are also the ones responsible for creating the VIPs you define for your VS. So, if you want your VIPs to be reachable you have to make sure there are routes in your network to the VIPS where the SEs are next hops either with BGP or static routes. The VIP is what it is, a Virtual IP meaning it dont have its own VLAN and gateway in your infrastructure. It is created and realised by the SE\u0026rsquo;s. The SE\u0026rsquo;s are then the gateways for your VIPS. A VIP address could be anything. At the same time the SEs dataplane network needs connectivity to the backend servers it is supposed to loadbalance, so this dataplane network also needs routes to reach those. In this post that means the SE\u0026rsquo;s dataplane network will need reachability to the k8s worker nodes where your apps are running in the home-lab site and in the remote site it needs reachability to the TKGs workers. On a sidenote I am not running routable pods, they are nat-ed trough my workers, and I am using Antrea as CNI with NodePortLocal configured. I also prefer to have a different network for the SE dataplane, different VIP subnets as it is easier to maintain control, isolation, firewall rules etc.\nThe diagram above is very high level, as it does not go into all networking details, firewall rules etc but it gives an overview of the communication needed.\nWhen one have an clear idea of the connectivity requirements we need to form the GSLB \u0026ldquo;partnership\u0026rdquo; between the AVI controllers. I was thinking back and forth whether I should cover these steps also but instead I will link to a good friends blog site here that does this brilliantly. Its all about saving the environment of unnecessary digital ink \u0026#x1f604;. This also goes for AKO deployment. This is also covered here or from the AVI docs page here\nIt should look like this on both controllers when everything is up and ready for GSLB: It should be reflected on the secondary controller as well, except there will be no option to edit.\nTime to deploy AMKO in K8s # AMKO can be deployed in two ways. It can be sufficient with only one instance of AMKO deployed in your primary k8s cluster, or you can go the federation approach and deploy AMKO in all your clusters that you want to use GSLB on. Then you will end up with one master instance of AMKO and \u0026ldquo;followers\u0026rdquo; or federation member on the others. One of the benefit is that you can promote one of the follower members if the primary is lost. I will go with the simple approach, deploy AMKO once, in my primary k8s cluster in my home-lab.\nAMKO preparations before deploy with Helm # AMKO will be deployed by using Helm, so if Helm is not installed do that. To successfully install AMKO there is a couple of things to be done. First, decide which is your primary cluster (where to deploy AMKO). When you have decided that (the easy step) then you need to prepare a secret that contains the context/clusters/users for all the k8s clusters you want to use GSLB on. An example file can be found here. Create this content in a regular file and name the file gslb-members. The naming of the file is important, if you name it differently AMKO will fail as it cant find the secret. I have tried to find a variable that is able override this in the value.yaml for the Helm chart but has not succeeded, so I went with the default naming. When that is populated with the k8s clusters you want, we need to create a secret in our primary k8s cluster like this: kubectl create secret generic gslb-config-secret --from-file gslb-members -n avi-system. The namespace here is the namespace where AKO is already deployed in.\nThis should give you a secret like this:\ngslb-config-secret Opaque 1 20h A note on kubeconfig for vSphere with Tanzu (TKGs) # When logging into a guest cluster in TKGs we usually do this through the supervisor with either vSphere local users or AD users defined in vSphere and we get a timebased token. Its not possible to use this approach. So what I went with was to grab the admin credentials for my TKGs guest cluster and used that context instead. Here is how to do that. This is not a recommended approach, instead one should create and use a service account. Maybe I will get back to this later and update how.\nBack to the AMKO deployment\u0026hellip;\nThe secret is ready, now we need to get the value.yaml for the AMKO version we will install. I am using AMKO 1.8.1 (same for AKO). The Helm repo for AMKO is already added if AKO has been installed using Helm, the same repo. If not, add the repo:\nhelm repo add ako https://projects.registry.vmware.com/chartrepo/ako Download the value.yaml:\nhelm show values ako/amko --version 1.8.1 \u0026gt; values.yaml (there is a typo in the official doc - it points to just amko) Now edit the values.yaml:\n# This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: projects.registry.vmware.com/ako/amko pullPolicy: IfNotPresent # Configs related to AMKO Federator federation: # image repository image: repository: projects.registry.vmware.com/ako/amko-federator pullPolicy: IfNotPresent # cluster context where AMKO is going to be deployed currentCluster: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name - for your leader/primary cluster # Set to true if AMKO on this cluster is the leader currentClusterIsLeader: true # member clusters to federate the GSLBConfig and GDP objects on, if the # current cluster context is part of this list, the federator will ignore it memberClusters: - \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name - \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name # Configs related to AMKO Service discovery serviceDiscovery: # image repository # image: # repository: projects.registry.vmware.com/ako/amko-service-discovery # pullPolicy: IfNotPresent # Configs related to Multi-cluster ingress. Note: MultiClusterIngress is a tech preview. multiClusterIngress: enable: false configs: gslbLeaderController: \u0026#39;172.18.5.51\u0026#39; ##### MGMT ip leader/primary avi controller controllerVersion: 22.1.1 memberClusters: - clusterContext: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name - clusterContext: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name refreshInterval: 1800 logLevel: INFO # Set the below flag to true if a different GSLB Service fqdn is desired than the ingress/route\u0026#39;s # local fqdns. Note that, this field will use AKO\u0026#39;s HostRule objects\u0026#39; to find out the local to global # fqdn mapping. To configure a mapping between the local to global fqdn, configure the hostrule # object as: # [...] # spec: # virtualhost: # fqdn: foo.avi.com # gslb: # fqdn: gs-foo.avi.com useCustomGlobalFqdn: true ####### set this to true if you want to define custom FQDN for GSLB - I use this gslbLeaderCredentials: username: \u0026#39;admin\u0026#39; ##### username/password AVI Controller password: \u0026#39;password\u0026#39; ##### username/password AVI Controller globalDeploymentPolicy: # appSelector takes the form of: appSelector: label: app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB # Uncomment below and add the required ingress/route/service label # appSelector: # namespaceSelector takes the form of: # namespaceSelector: # label: # ns: gslb \u0026lt;example label key-value for namespace\u0026gt; # Uncomment below and add the reuqired namespace label # namespaceSelector: # list of all clusters that the GDP object will be applied to, can take any/all values # from .configs.memberClusters matchClusters: - cluster: \u0026#39;k8slab-admin@k8slab\u0026#39; ####use the context name - cluster: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; ####use the context name # list of all clusters and their traffic weights, if unspecified, default weights will be # given (optional). Uncomment below to add the required trafficSplit. # trafficSplit: # - cluster: \u0026#34;cluster1-admin\u0026#34; # weight: 8 # - cluster: \u0026#34;cluster2-admin\u0026#34; # weight: 2 # Uncomment below to specify a ttl value in seconds. By default, the value is inherited from # Avi\u0026#39;s DNS VS. # ttl: 10 # Uncomment below to specify custom health monitor refs. By default, HTTP/HTTPS path based health # monitors are applied on the GSs. # healthMonitorRefs: # - hmref1 # - hmref2 # Uncomment below to specify a Site Persistence profile ref. By default, Site Persistence is disabled. # Also, note that, Site Persistence is only applicable on secure ingresses/routes and ignored # for all other cases. Follow https://avinetworks.com/docs/20.1/gslb-site-cookie-persistence/ to create # a Site persistence profile. # sitePersistenceRef: gap-1 # Uncomment below to specify gslb service pool algorithm settings for all gslb services. Applicable # values for lbAlgorithm: # 1. GSLB_ALGORITHM_CONSISTENT_HASH (needs a hashMask field to be set too) # 2. GSLB_ALGORITHM_GEO (needs geoFallback settings to be used for this field) # 3. GSLB_ALGORITHM_ROUND_ROBIN (default) # 4. GSLB_ALGORITHM_TOPOLOGY # # poolAlgorithmSettings: # lbAlgorithm: # hashMask: # required only for lbAlgorithm == GSLB_ALGORITHM_CONSISTENT_HASH # geoFallback: # fallback settings required only for lbAlgorithm == GSLB_ALGORITHM_GEO # lbAlgorithm: # can only have either GSLB_ALGORITHM_ROUND_ROBIN or GSLB_ALGORITHM_CONSISTENT_HASH # hashMask: # required only for fallback lbAlgorithm as GSLB_ALGORITHM_CONSISTENT_HASH serviceAccount: # Specifies whether a service account should be created create: true # Annotations to add to the service account annotations: {} # The name of the service account to use. # If not set and create is true, a name is generated using the fullname template name: resources: limits: cpu: 250m memory: 300Mi requests: cpu: 100m memory: 200Mi service: type: ClusterIP port: 80 rbac: # creates the pod security policy if set to true pspEnable: false persistentVolumeClaim: \u0026#39;\u0026#39; mountPath: /log logFile: amko.log federatorLogFile: amko-federator.log When done, its time to install AMKO like this:\nhelm install ako/amko --generate-name --version 1.8.1 -f /path/to/values.yaml --set configs.gslbLeaderController=\u0026lt;leader_controller_ip\u0026gt; --namespace=avi-system ####There is a typo in the official docs - its pointing to amko only If everything went well you should se a couple of things in your k8s cluster under the namespace avi-system.\nk get pods -n avi-system NAME READY STATUS RESTARTS AGE ako-0 1/1 Running 0 25h amko-0 2/2 Running 0 20h k get amkocluster amkocluster-federation -n avi-system NAME AGE amkocluster-federation 20h k get gc -n avi-system gc-1 NAME AGE gc-1 20h k get gdp -n avi-system NAME AGE global-gdp 20h AMKO is up and running. Time create a GSLB service\nCreate GSLB service # You probably already have a bunch of ingress services running, and to make them GSLB \u0026ldquo;aware\u0026rdquo; there is not much to be done to achieve that. If you noticed in our value.yaml for the AMKO Helm chart we defined this:\nglobalDeploymentPolicy: # appSelector takes the form of: appSelector: label: app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB So what we need to in our ingress service is to add the below, and then a new section where we define our gslb fqdn.\nHere is my sample ingress applied in my primary k8s cluster:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-example labels: #### This is added for GSLB app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml namespace: fruit spec: ingressClassName: avi-lb rules: - host: fruit-global.guzware.net #### Specific for this site (Home Lab) http: paths: - path: /apple pathType: Prefix backend: service: name: apple-service port: number: 5678 - path: /banana pathType: Prefix backend: service: name: banana-service port: number: 5678 --- #### New section to define a host rule apiVersion: ako.vmware.com/v1alpha1 kind: HostRule metadata: namespace: fruit name: gslb-host-rule-fruit spec: virtualhost: fqdn: fruit-global.guzware.net #### Specific for this site (Home Lab) enableVirtualHost: true gslb: fqdn: fruit.gslb.guzware.net ####This is common for both sites As soon as it is applied, and there are no errors in AMKO or AKO, it should be visible in your AVI controller GUI: If you click on the name it should take you to next page where it show the GSLB pool members and the status: Screenshot below is when both sites have applied their GSLB services: \u0026quot;\nNext we need to apply gslb settings on the secondary site also:\nThis is what I have deployed on the secondary site (note the difference in domain names specific for that site)\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-example labels: #### This is added for GSLB app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml namespace: fruit spec: ingressClassName: avi-lb rules: - host: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) http: paths: - path: /apple pathType: Prefix backend: service: name: apple-service port: number: 5678 - path: /banana pathType: Prefix backend: service: name: banana-service port: number: 5678 --- #### New section to define a host rule apiVersion: ako.vmware.com/v1alpha1 kind: HostRule metadata: namespace: fruit name: gslb-host-rule-fruit spec: virtualhost: fqdn: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) enableVirtualHost: true gslb: fqdn: fruit.gslb.guzware.net ##### Common for both sites When this is applied Avi will go ahead and put this into the same GSLB service as above, and the screenshot above will be true.\nNow I have the same application deployed in both sites, but equally available whether I am sitting in my home-lab or at the remote-site. There is a bunch of parameters that can be tuned, which I will not go into now (maybe getting back to this and update with further possibilities with GSLB). But one of them can be LoadBalancing algorithms such as Geo Location Source. Say I want the application to be accessed from clients as close to the application as possible. And should one of the sites become unavailable it will still be accessible from one of the sites that are still online. Very cool indeed. For the sake of the demo I am about to show the only thing I change in the default GSLB settings is the TTL, I am setting it to 2 seconds so I can showcase that the application is being load balanced between both sites. Default algorithm is Round-Robin so it should balance between them regardless of the latency difference (accessing the application from my home network in my home lab vs from my home network in the remote-site which has several ms in distance). Heres where I am setting these settings: With a TTL of 2 seconds it should switch faster so I can see the balancing between the two sites. Let me try to access the application from my browser using the gslb fqdn: fruit.gslb.guzware.net/apple\nA refresh of the page and now: To even illustrate more I will run a curl command against the gslb fqdn: Now a ping against the FQDN to show the ip of the corresponding site that answer on the call: Notice the change in ip address but also the latency in ms\nNow I can go ahead and disable one of the site to simulate failover, and the application is still available on the same FQDN. So many possibilities with GSLB.\nThats it then. NSX ALB, AKO with AMKO between two sites, same application available in two physical location, redundancy, scale-out, availability. Stay tuned for more updates in advanced settings - in the future \u0026#x1f604;\n","date":"23 October 2022","externalUrl":null,"permalink":"/2022/10/23/gslb-with-ako-amko-nsx-advanced-loadbalancer/","section":"Posts","summary":"Global Server LoadBalancing in VMware Tanzu with AMKO # This post will go through how to configure AVI (NSX ALB) with GSLB in vSphere with Tanzu (TKGs) and an upstream k8s cluster in two different physical locations.","title":"GSLB With AKO \u0026 AMKO - NSX Advanced LoadBalancer","type":"posts"},{"content":"","date":"23 October 2022","externalUrl":null,"permalink":"/tags/custom-resource-definitions/","section":"Tags","summary":"","title":"Custom-Resource-Definitions","type":"tags"},{"content":" AKO settings: # What happens if we need to to this\nWhat happens if I need passthrough\nHow does AKO work\n","date":"23 October 2022","externalUrl":null,"permalink":"/2022/10/23/we-take-a-look-at-the-ako-crds/","section":"Posts","summary":"AKO settings: # What happens if we need to to this","title":"We Take a Look at the AKO Crds","type":"posts"},{"content":"","date":"23 October 2022","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"23 October 2022","externalUrl":null,"permalink":"/tags/loadbalancer/","section":"Tags","summary":"","title":"Loadbalancer","type":"tags"},{"content":" TOPICS: # ","date":"23 October 2022","externalUrl":null,"permalink":"/2022/10/23/running-the-unifi-controller-in-kubernetes/","section":"Posts","summary":" TOPICS: # ","title":"Running the Unifi Controller in Kubernetes","type":"posts"},{"content":"","date":"23 October 2022","externalUrl":null,"permalink":"/tags/unifi/","section":"Tags","summary":"","title":"Unifi","type":"tags"},{"content":"","date":"13 October 2022","externalUrl":null,"permalink":"/categories/docker/","section":"Categories","summary":"","title":"Docker","type":"categories"},{"content":"","date":"13 October 2022","externalUrl":null,"permalink":"/tags/harbor/","section":"Tags","summary":"","title":"Harbor","type":"tags"},{"content":"","date":"13 October 2022","externalUrl":null,"permalink":"/categories/harbor/","section":"Categories","summary":"","title":"Harbor","type":"categories"},{"content":"","date":"13 October 2022","externalUrl":null,"permalink":"/categories/helm/","section":"Categories","summary":"","title":"Helm","type":"categories"},{"content":"","date":"13 October 2022","externalUrl":null,"permalink":"/tags/registry/","section":"Tags","summary":"","title":"Registry","type":"tags"},{"content":"This post will briefly go through how to deploy (using Helm), configure and use VMware Harbor registry in Kubernetes.\nQuick introduction to Harbor # Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. link\nI use myself Harbor in many of my own projects, including the images I make for my Hugo blogsite (this).\nDeploy Harbor with Helm # Add helm chart:\nhelm repo add harbor https://helm.goharbor.io helm fetch harbor/harbor --untar Before you perform the default helm install of Harbor you want to grab the helm values for the Harbor charts so you can edit some settings to match your environment:\nhelm show values harbor/harbor \u0026gt; harbor.values.yaml The default values you get from the above command includes all available parameter which can be a bit daunting to go through. In the values file I use I have only picked the parameters I needed to set, here:\nexpose: type: ingress tls: enabled: true certSource: secret secret: secretName: \u0026#34;harbor-tls-prod\u0026#34; # certificates you have created with Cert-Manager notarySecretName: \u0026#34;notary-tls-prod\u0026#34; # certificates you have created with Cert-Manager ingress: hosts: core: registry.example.com notary: notary.example.com annotations: kubernetes.io/ingress.class: \u0026#34;avi-lb\u0026#34; ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; externalURL: https://registry.example.com harborAdminPassword: \u0026#34;PASSWORD\u0026#34; persistence: enabled: true # Setting it to \u0026#34;keep\u0026#34; to avoid removing PVCs during a helm delete # operation. Leaving it empty will delete PVCs after the chart deleted # (this does not apply for PVCs that are created for internal database # and redis components, i.e. they are never deleted automatically) resourcePolicy: \u0026#34;keep\u0026#34; persistentVolumeClaim: registry: # Use the existing PVC which must be created manually before bound, # and specify the \u0026#34;subPath\u0026#34; if the PVC is shared with other components existingClaim: \u0026#34;\u0026#34; # Specify the \u0026#34;storageClass\u0026#34; used to provision the volume. Or the default # StorageClass will be used (the default). # Set it to \u0026#34;-\u0026#34; to disable dynamic provisioning storageClass: \u0026#34;nfs-client\u0026#34; subPath: \u0026#34;\u0026#34; accessMode: ReadWriteOnce size: 50Gi annotations: {} database: existingClaim: \u0026#34;\u0026#34; storageClass: \u0026#34;nfs-client\u0026#34; subPath: \u0026#34;postgres-storage\u0026#34; accessMode: ReadWriteOnce size: 1Gi annotations: {} portal: tls: existingSecret: harbor-tls-prod When you have edited the values file its time to install:\nhelm install -f harbor.values.yaml harbor-deployment harbor/harbor -n harbor Explanation: \u0026ldquo;-f\u0026rdquo; is telling helm to read the values from the specified file after, then the name of your helm installation (here harbor-deployment) then the helm repo and finally the namespace you want it deployed in. A couple of seconds later you should be able to log in to the GUI of Harbor through your webbrowser if everything has been set up right, Ingress, pvc, secrets.\nCertificate # You can either use Cert-manager as explained here or bring your own ca signed certificates.\nHarbor GUI # To log in to the GUI for the first time open your browser and point it to the externalURL you gave it in your values file and the corresponding harborAdminPassword you defined. From there on you create users and projects and start exploring Harbor.\nUsers: Projects: Docker images # To push your images to Harbor execute the following commands:\ndocker login registry.example.com #log in with the user/password you have created in the GUI docker tag image-name:tag registry.example.com/project/image-name:tag docker push registry.example.com/project/image-name:tag ","date":"13 October 2022","externalUrl":null,"permalink":"/2022/10/13/vmware-harbor-registry/","section":"Posts","summary":"This post will briefly go through how to deploy (using Helm), configure and use VMware Harbor registry in Kubernetes.","title":"VMware Harbor Registry","type":"posts"},{"content":"","date":"12 October 2022","externalUrl":null,"permalink":"/categories/hugo/","section":"Categories","summary":"","title":"Hugo","type":"categories"},{"content":"This blog post will cover how I wanted to deploy Hugo to host my blog-page.\nPreparations # To achieve what I wanted, deploy an highly available Hugo hosted blog page, I decided to run Hugo in Kubernetes. For that I needed\nKubernetes cluster, obviously, consisting of several workers for the the \u0026ldquo;hugo\u0026rdquo; pods to run on (already covered here. Persistent storage (NFS in my case, already covered here) An Ingress controller (already covered here) A docker image with Hugo, nginx and go (will be covered here) Docker installed so you can build the image A place to host the docker image (Docker hub or Harbor registry will be covered here) Create the Docker image # Before I can deploy Hugo I need to create an Docker image that contains the necessary bits. I have already created the Dockerfile here:\n#Install the container\u0026#39;s OS. FROM ubuntu:latest as HUGOINSTALL # Install Hugo. RUN apt-get update -y RUN apt-get install wget git ca-certificates golang -y RUN wget https://github.com/gohugoio/hugo/releases/download/v0.104.3/hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ tar -xvzf hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ chmod +x hugo \u0026amp;\u0026amp; \\ mv hugo /usr/local/bin/hugo \u0026amp;\u0026amp; \\ rm -rf hugo_extended_0.104.3_Linux-64bit.tar.gz # Copy the contents of the current working directory to the hugo-site # directory. The directory will be created if it doesn\u0026#39;t exist. COPY . /hugo-site # Use Hugo to build the static site files. RUN hugo -v --source=/hugo-site --destination=/hugo-site/public # Install NGINX and deactivate NGINX\u0026#39;s default index.html file. # Move the static site files to NGINX\u0026#39;s html directory. # This directory is where the static site files will be served from by NGINX. FROM nginx:stable-alpine RUN mv /usr/share/nginx/html/index.html /usr/share/nginx/html/old-index.html COPY --from=HUGOINSTALL /hugo-site/public/ /usr/share/nginx/html/ # The container will listen on port 80 using the TCP protocol. EXPOSE 80 Credits for the Dockerfile as it was initially taken from here. I have updated it, and done some modifications to it.\nBefore building the image with docker, install docker by following this guide.\nBuild the docker image # I need to place myself in the same directory as my Dockerfile and execute the following command (Replace \u0026quot;name-you-want-to-give-the-image:\u0026lt;tag\u0026gt;\u0026quot; with something like \u0026quot;hugo-image:v1\u0026quot;):\ndocker build -t name-you-want-to-give-the-image:\u0026lt;tag\u0026gt; . #Note the \u0026#34;.\u0026#34; important Now the image will be built and hosted locally on my \u0026ldquo;build machine\u0026rdquo;.\nIf anything goes well it should be listed here:\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE hugo-image v1 d43ee98c766a 10 secs ago 70MB nginx stable-alpine 5685937b6bc1 7 days ago 23.5MB ubuntu latest 216c552ea5ba 9 days ago 77.8MB Place the image somewhere easily accessible # Now that I have my image I need to make sure it is easily accessible for my Kubernetes workers so they can download the image and deploy it. For that I can use the local docker registry pr control node and worker node. Meaning I need to load the image into all workers and control plane nodes. Not so smooth way to to do it. This is the approach for such a method:\ndocker save -o \u0026lt;path for generated tar file\u0026gt; \u0026lt;image name\u0026gt; #needs to be done on the machine you built the image. Example: docker save -o /home/username/hugo-image.v1.tar hugo-image:v1 This will \u0026ldquo;download\u0026rdquo; the image from the local docker repository and create tar file. This tar file needs to be copied to all my workers and additional control plane nodes with scp or other methods I find suitable. When that is done I need to upload the tar to each of their local docker repository with the following command:\ndocker -i load /home/username/hugo-image.v1.tar It is ok to know about this process if you are in non-internet environments etc, but even in non-internet environment we can do this with a private registry. And thats where Harbor can come to the rescue link.\nWith Harbor I can have all my images hosted centrally but dont need access to the internet as it is hosted in my own environment.\nI could also use Docker hub. Create an account there, and use it as my repository. I prefer the Harbor registry, as it provides many features. The continuation of this post will use Harbor, the procedure to upload/download images is the same process as with Docker hub but you log in to your own Harbor registry instead of Docker hub.\nUploading my newly created image is done like this:\ndocker login registry.example.com #FQDN to my selfhosted Harbor registry, and the credentials for an account I have created there. docker tag hugo-image:v1 https://registry.example.com/hugo/hugo-image:v1 #\u0026#34;/hugo/\u0026#34; name of project in Harbor docker push registry.example.com/hugo/hugo-image:v1 #upload it Thats it. Now I can go ahead and create my deployment.yaml definition file in my Kubernetes cluster, point it to my image hosted at my local Harbor registry (e.g registry.example.com/hugo/hugo-image:v1). But let me go through how I created my Hugo deployment in Kubernetes, as I am so close to see my newly image in action \u0026#x1f604; (Will it even work).\nDeploy Hugo in Kubernetes # To run my Hugo image in Kubernetes the way I wanted I need to define a Deployment (remember I wanted a highly available Hugo deployment, meaning more than one pod and the ability to scale up/down). The first section of my hugo-deployment.yaml definition file looks like this:\napiVersion: apps/v1 kind: Deployment metadata: name: hugo-site namespace: hugo-site spec: replicas: 3 selector: matchLabels: app: hugo-site tier: web template: metadata: labels: app: hugo-site tier: web spec: containers: - image: registry.example.com/hugo/hugo-image:v1 name: hugo-site imagePullPolicy: Always ports: - containerPort: 80 name: hugo-site volumeMounts: - name: persistent-storage mountPath: /usr/share/nginx/html/ volumes: - name: persistent-storage persistentVolumeClaim: claimName: hugo-pv-claim In the above I define name of deployment, specify number of pods with the replica specification, labels, point to my image hosted in Harbor and then what the container mountPath and the peristent volume claim. mountPath is inside the container, and the files/folders mounted is read from the content it sees in the persistent volume claim \u0026ldquo;hugo-pv-claim\u0026rdquo;. Thats where Hugo will find the content of the Public folder (after the content has been generated).\nI also needed to define a Service so I can reach/expose the containers contents (webpage) on port 80. This is done with this specification:\napiVersion: v1 kind: Service metadata: name: hugo-service namespace: hugo-site labels: svc: hugo-service spec: selector: app: hugo-site tier: web ports: - port: 80 Can be saved as a separate \u0026ldquo;service.yaml\u0026rdquo; file or pasted into one yaml file. But instead of pointing to my workers IP addresses to read the content each time I wanted to expose it with an Ingress by using AKO and Avi LoadBalancer. This is how I done that:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: hugo-ingress namespace: hugo-site labels: app: hugo-ingress annotations: ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; spec: ingressClassName: avi-lb rules: - host: yikes.guzware.net http: paths: - pathType: Prefix path: / backend: service: name: hugo-service port: number: 80 I define my ingressClassName, the hostname for my Ingress controller to listen for requests on and the Service the Ingress should route all the request to yikes.guzware.net to, which is my hugo-service defined earlier. Could also be saved as a separe yaml file. I have chosen to put all three \u0026ldquo;kinds\u0026rdquo; in one yaml file. Which then looks like this:\napiVersion: apps/v1 kind: Deployment metadata: name: hugo-site namespace: hugo-site spec: replicas: 3 selector: matchLabels: app: hugo-site tier: web template: metadata: labels: app: hugo-site tier: web spec: containers: - image: registry.example.com/hugo/hugo-image:v1 name: hugo-site imagePullPolicy: Always ports: - containerPort: 80 name: hugo-site volumeMounts: - name: persistent-storage mountPath: /usr/share/nginx/html/ volumes: - name: persistent-storage persistentVolumeClaim: claimName: hugo-pv-claim --- apiVersion: v1 kind: Service metadata: name: hugo-service namespace: hugo-site labels: svc: hugo-service spec: selector: app: hugo-site tier: web ports: - port: 80 --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: hugo-ingress namespace: hugo-site labels: app: hugo-ingress annotations: ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; spec: ingressClassName: avi-lb rules: - host: yikes.guzware.net http: paths: - pathType: Prefix path: / backend: service: name: hugo-service port: number: 80 Now before my Deployment is ready to be applied I need to create the namespace I have defined in the yaml file above: kubectl create ns hugo-site.\nNow when that is done its time to apply my hugo deployment. kubectl apply -f hugo-deployment.yaml\nI want to check the state of the pods:\n$ kubectl get pod -n hugo-site NAME READY STATUS RESTARTS AGE hugo-site-7f95b4644c-5gtld 1/1 Running 0 10s hugo-site-7f95b4644c-fnrh5 1/1 Running 0 10s hugo-site-7f95b4644c-hc4gw 1/1 Running 0 10s Ok, so far so good. What about my deployment:\n$ kubectl get deployments.apps -n hugo-site NAME READY UP-TO-DATE AVAILABLE AGE hugo-site 3/3 3 3 35s Great news. Lets check the Service, Ingress and persistent volume claim.\nService:\n$ kubectl get service -n hugo-site NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE hugo-service ClusterIP 10.99.25.113 \u0026lt;none\u0026gt; 80/TCP 46s Ingress:\n$ kubectl get ingress -n hugo-site NAME CLASS HOSTS ADDRESS PORTS AGE hugo-ingress avi-lb yikes.guzware.net x.x.x.x 80 54s PVC:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE hugo-pv-claim Bound pvc-b2395264-4500-4d74-8a5c-8d79f9df8d63 10Gi RWO nfs-client 59s Well that looks promising. Will I be able to access my hugo page on yikes.guzware.net \u0026hellip; well yes, otherwise you wouldnt read this.. \u0026#x1f923;\nCreating and updating content # A blog page without content is not so interesting. So just some quick comments on how I create content, and update them.\nI use Typora creating and editing my *.md files. While working with the post (such as now) I run hugo in \u0026ldquo;server-mode\u0026rdquo; whith this command: hugo server. If I run this command on of my linux virtual machines through SSH I want to reach the server from my laptop so I add the parameter --bind=ip-of-linux-vm and I can access the page from my laptop on the ip of the linux VM and port 1313. When I am done with the article/post for the day I generated the web-page with the command hugo -D -v. The updated content of my public folder after I have generated the page is mirrored to the NFS path that is used in my PVC shown above and my containers picks up the updated content instantly. Thats how I do, it works and I find it easy to maintain and operate. And, if one of my workers fails, I have more pods still available on the remaining workers. If a pod fails Kubernetes will just take care of that for me as I have declared a set of pods(replicas) that should run. If I run my Kubernetes environment in Tanzu and one of my workers fails, that will also be automatically taken care of.\n","date":"12 October 2022","externalUrl":null,"permalink":"/2022/10/12/hugo-in-kubernetes/","section":"Posts","summary":"This blog post will cover how I wanted to deploy Hugo to host my blog-page.","title":"Hugo in Kubernetes","type":"posts"},{"content":"","date":"12 October 2022","externalUrl":null,"permalink":"/tags/static-content-generator/","section":"Tags","summary":"","title":"Static-Content-Generator","type":"tags"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/tags/pinniped/","section":"Tags","summary":"","title":"Pinniped","type":"tags"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/categories/pinniped/","section":"Categories","summary":"","title":"Pinniped","type":"categories"},{"content":"How to use Pinniped as the authentication service in Kubernets with OpenLDAP\nGoal: Deploy an authentication service to handle RBAC in Kubernetes Purpose: User/access management in Kubernetes\nPinniped introduction # ","date":"11 October 2022","externalUrl":null,"permalink":"/2022/10/11/pinniped-authentication-service/","section":"Posts","summary":"How to use Pinniped as the authentication service in Kubernets with OpenLDAP","title":"Pinniped Authentication Service","type":"posts"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/tags/rbac/","section":"Tags","summary":"","title":"Rbac","type":"tags"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/categories/rbac/","section":"Categories","summary":"","title":"RBAC","type":"categories"},{"content":"This article will quickly go through how to create wildcard certificates and automatically renew them with Lets Encrypt and Cert-Manager\nCert-Manager # cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\nIt can issue certificates from a variety of supported sources, including Let\u0026rsquo;s Encrypt, HashiCorp Vault, and Venafi as well as private PKI.\nIt will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry. link\nInstall Cert-Manager # I prefer the Helm way so lets add the cert-manager helm chart:\nhelm repo add jetstack https://charts.jetstack.io helm repo update Then we need to deploy cert-manager. This can be done out-of-the-box with the commands given from the official docs (this also installed the necessary CRDs):\nhelm install \\ cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.9.1 \\ --set installCRDs=true Or if you need to customize some settings, as I needed to do, I used this command:\nhelm install -f /path/to/cert-manager.values.yaml cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1 --set installCRDs=true --set \u0026#39;extraArgs={--dns01-recursive-nameservers-only,--dns01-recursive-nameservers=xx.xx.xx.xx:53\\,xx.xx.xx:53}\u0026#39; The above command takes care of the cert-manager installation including the necessary CRDs, but it will also adjust the DNS servers Cert-Manager will use to verify the ownership of my domain.\nDNS01 - Wildcard certificate # In this post I will go with wildcard certificate creation. I find it easier to use instead of having a separate cert for everthing I do, as long as they are within the same subdomain. So if I have my services in *.example.com they can use the same certificate. But if I have services in *.int.example.com I can not use the same certificate as LetsEncrypt certificates dont support that. Then you need to create a separate wildcard cert for each subdomain. But Cert-manager will handle that for you very easy.\nThe offiicial Cert-Manager supported DNS01 providers are:\nACMEDNS Akamai AzureDNS CloudFlare Google Route53 DigitalOcean RFC2136 There is also an option to use Webhooks. I did try that as my previous DNS registrar were not on the DNS01 supported list. I did not succeed with using the webhook approach. It could be an issue with the specific webhooks I used or even with my registrar so I decided to migrate over to CloudFlare which is on the supported list, \u0026ldquo;out of the box\u0026rdquo;.\nIssuer - CloudFlare and LetsEncrypt # The first we need to do is to create a secret for Cert-Manager to use when \u0026ldquo;interacting\u0026rdquo; with CloudFlare. I went with API Token. So head over to your CloudFlare control panel and create a token for Cert Manager like this: Here is the permissions: Now use the tokens to create your secret:\napiVersion: v1 kind: Secret metadata: name: cloudflare-api-token-secret namespace: cert-manager type: Opaque stringData: api-token: Apply it kubect apply -f name.of.yaml\nNow create your issuer. LetsEncrypt have two repos, one called staging and one production. Start out with staging until everything works so you dont hit the LetsEncrypt limit. In regards to this I created two issuers, one for staging and one for production. When everything was working and I have verified the certificates etc I deployed the certs using the prod-issuer.\nIssuer-staging:\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # ACME Server # prod : https://acme-v02.api.letsencrypt.org/directory # staging : https://acme-staging-v02.api.letsencrypt.org/directory server: https://acme-staging-v02.api.letsencrypt.org/directory # ACME Email address email: xxx.xxx@xxx.xxx privateKeySecretRef: name: letsencrypt-key-staging # staging or production solvers: - dns01: cloudflare: apiTokenSecretRef: name: cloudflare-api-token-secret ## created and applied above key: api-token Issuer-production:\napiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: # ACME Server # prod : https://acme-v02.api.letsencrypt.org/directory # staging : https://acme-staging-v02.api.letsencrypt.org/directory server: https://acme-v02.api.letsencrypt.org/directory # ACME Email address email: xxx.xxx@xxx.xxx privateKeySecretRef: name: letsencrypt-key-prod # staging or production solvers: - dns01: cloudflare: apiTokenSecretRef: name: cloudflare-api-token-secret # created and applied above key: api-token Request certificate # Now that the groundwork for cert-manager has been setup, its time to \u0026ldquo;print\u0026rdquo; some certificates. Prepare your yamls for both the staging key and production key.\nWildcard-staging:\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: name-tls-test namespace: namespace-you-want-the-cert-in spec: secretName: name-tls-staging issuerRef: name: letsencrypt-staging kind: ClusterIssuer duration: 2160h # 90d renewBefore: 720h # 30d before SSL will expire, renew it dnsNames: - \u0026#34;*.example.com\u0026#34; Wildcard-production:\napiVersion: cert-manager.io/v1 kind: Certificate metadata: name: name-tls-production namespace: namespace-you-want-the-cert-in spec: secretName: name-tls-prod issuerRef: name: letsencrypt-prod kind: ClusterIssuer duration: 2160h # 90d renewBefore: 720h # 30d before SSL will expire, renew it dnsNames: - \u0026#34;*.example.com\u0026#34; Apply the staging request first. Check your certificate status with this command:\n$ kubectl get certificate -n namespace-you-wrote NAME READY SECRET AGE name-tls-staging True name-tls-staging 8d Please note that it can take a couple of minutes before the certificate is ready. This applies for production also.\nIf everything went well, delete your staging certificate and apply your production certificate with the production yaml. Thats it. Now Cert-Manager will take care of updating your certificate for your, sit back and enjoy your applications with your always up to date certificates.\nTroubleshooting tips, commands # If something should fail there is a couple of commands you can use to figure out whats going on.\n$ kubectl get issuer\r$ kubectl get clusterissuer\r$ kubectl describe issuer $ kubectl describe clusterissuer\r$ kubectl describe certificaterequest\r$ kubectl describe order\r$ kubectl get challenges\r$ kubectl describe challenges For more detailed explanation go here\n","date":"11 October 2022","externalUrl":null,"permalink":"/2022/10/11/cert-manager-and-letsencrypt/","section":"Posts","summary":"This article will quickly go through how to create wildcard certificates and automatically renew them with Lets Encrypt and Cert-Manager","title":"Cert Manager and Letsencrypt","type":"posts"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/tags/certificate/","section":"Tags","summary":"","title":"Certificate","type":"tags"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/categories/certificate/","section":"Categories","summary":"","title":"Certificate","type":"categories"},{"content":"","date":"11 October 2022","externalUrl":null,"permalink":"/tags/network-policies/","section":"Tags","summary":"","title":"Network-Policies","type":"tags"},{"content":" What is the NSX Antrea integration # Assumptions: I assume that the reader is aware of what Antrea is, and what a CNI is and also what NSX is. If not head over here to read more on Antrea and here to read more on NSX.\nFor many years VMware NSX has help many customer secure their workload by using the NSX Distributed Firewall. As NSX has evolved over the years the different platform it supports has also broadened, from virtual machines, bare metal server, cloud workload and kubernetes pods. NSX has had support for security policies in Kubernetes for a long time also with the CNI NCP Read about NCP here. Recently (almost a year ago since I wrote this article, so not so recent in the world of IT) it also got support for using the Antrea CNI. What does that mean then. Well, it mean we can now \u0026ldquo;connect\u0026rdquo; our Antrea CNI enabled clusters to the NSX manager to manage Antrea Native Policies in combination with NSX Distributed Firewall policies. With this integration Antrea will report inventory, such as nodes, pods, services, ip addresses, k8s labels into the NSX manager. This opens up for a clever way of creating and managing security policies from the NSX manager inside the Antrea enabled clusters. Antrea is supported in almost all kinds of Kubernetes platforms, VMware Tanzu solutions, upstream k8s, ARM, public cloud etc so it is very flexible. And with the rich information NSX gets from Antrea we can create more clever security policies by using the native kubernetes labels to form security group membership based on these labels.\nFrom the official VMware NSX documentation:\nBenefits of Integration\nThe integration of Antrea Kubernetes clusters to NSX enables the following capabilities:\nView Antrea Kubernetes cluster resources in the NSX Manager UI (Policy mode). Centrally manage groups and security policies in NSX that reference Antrea Kubernetes clusters and NSX resources (for example, VMs). Distribute the NSX security policies to the Kubernetes clusters for enforcement in the cluster by the Antrea CNI. Extend the NSX network diagnostic and troubleshooting features to the Antrea Kubernetes clusters, such as collecting support bundles, monitoring logs, and performing Traceflow operations. Monitor the runtime state and health status of Antrea Kubernetes cluster components and Antrea Agents in the NSX Manager UI. Antrea NSX integration architecture # To understand a bit more how this works, we need to go through a couple of components that is in involved to get this integration in place.\nThe official documentation has this part covered very well and I will just quote the information here. Or go directly to the source here\nThe integration architecture explains the information exchanged between a Kubernetes cluster that uses Antrea CNI and the NSX Manager Appliance, which is deployed in NSX.\nThis documentation does not explain the functions of Antrea components in a Kubernetes (K8s) cluster. To understand the Antrea architecture and the functions of Antrea components in a Kubernetes cluster, see the Antrea documentation portal at https://antrea.io/docs.\nThis main objective of this documentation is to understand the functions of the Antrea NSX Adapter that integrates a Kubernetes cluster with Antrea CNI to the NSX Manager Appliance.\nAntrea NSX Adapter\nThis component runs as a pod on one of the Kubernetes Control Plane nodes. Antrea NSX Adapter consists of the following two subcomponents:\nManagement Plane Adapter (MP Adapter) Central Control Plane Adapter (CCP Adapter) Management Plane Adapter communicates with the NSX Management Plane (Policy), Kubernetes API Server, and Antrea Controller. Central Control Plane Adapter communicates with the NSX Central Control Plane (CCP) and Kubernetes API Server.\nFunctions of the Management Plane Adapter\nWatches the Kubernetes resource inventory from Kubernetes API and reports the inventory to NSX Manager. Resource inventory of an Antrea Kubernetes cluster includes resources, such as Pods, Ingress, Services, Network Policies, Namespaces, and Nodes. Responds to the policy statistics query from NSX Manager. It receives the statistics from the Antrea Controller API or the statistics that are exported by the Antrea Agent on each K8s worker node, and reports the statistics to NSX Manager. Receives troubleshooting operation requests from NSX Manager, sends the requests to Antrea Controller API server, collects the results, and returns the information to NSX Manager. Examples of troubleshooting operations include Traceflow requests, Support Bundle collection requests, log collection requests. Watches the runtime state and health status of an Antrea Kubernetes cluster from the Antrea Monitoring CustomResourceDefinition (CRD) objects and reports the status to NSX Manager. The status is reported on a per cluster basis. For example, the health status of the following components is reported to the NSX Manager: Management Plane Adapter Central Control Plane Adapter Antrea Controller Antrea Agents Functions of the Central Control Plane Adapter\nReceives the Distributed Firewall (DFW) rules and groups from NSX Central Control Plane, translates them to Antrea policies, and creates Antrea policy CRDs in K8s API. Watches the policy realization status from both K8s network polices and native Antrea policy CRDs and reports the status to NSX Central Control Plane. Stateless Nature of the Central Control Plane Adapter\nThe Central Control Plane Adapter is stateless. Each time the adapter restarts or reconnects to K8s API or NSX Manager, it always synchronizes the state with K8s API and NSX Central Control Plane. Resynchronization of the state ensures the following:\nThe latest Antrea policies are always pushed to K8s API as native Antrea policy CRDs. The stale policy CRDs are removed if the corresponding security policies are deleted in NSX. This post will cover the installation steps of the Antrea/NSX integration.\nManaging Antrea Native Policies from the NSX manager # For more information how to manage Antrea Polcies from the NSX manager I have created this post and this post\n","date":"11 October 2022","externalUrl":null,"permalink":"/2022/10/11/nsx-antrea-integration/","section":"Posts","summary":"What is the NSX Antrea integration # Assumptions: I assume that the reader is aware of what Antrea is, and what a CNI is and also what NSX is.","title":"NSX Antrea Integration","type":"posts"},{"content":"This week I was fortunate to get hold of a VMC on AWS environment and wanted to test out the possibility of managing my K8s security policies from my on-prem NSX manager by utilizing the integration of Antrea in NSX. I haven\u0026rsquo;t covered that specific integration part in a blog yet, but in short: by using Antrea as your CNI and you are running NSX-T 3.2 you can manage all your K8s policies from the NSX manager GUI. Thats a big thing. Manage your k8s policies from the same place where you manage all your other critical security policies. Your K8s clusters does not have to be in the same datacenter as your NSX manager. You can utilize VMC on AWS as your scale-out, prod/test/dev platform and still manage your K8s security policies centrally from the same NSX manager.\nIn this post I will go through how this is done and how it works.\nVMC on AWS # VMC on AWS comes with NSX, but it is not yet on the version that has the NSX-T integration. So what I wanted to do was to use the VMC NSX manager to cover all the vm-level microsegmentation and let my on-prem NSX manager handle the Antrea security policies. To illustrate want I want to achieve:\nVMC on AWS to on-prem connectivity # VMC on AWS supports a variety of connectivity options to your on-prem environment. I have gone with IPSec VPN. Where I configure IPsec on the VMC NSX manager to negotiate with my on-prem firewall to terminate the VPN connection. In VMC I have two networks: Management and Workload. I configured both subnets in my IPsec config as I wanted the flexibility to reach both subnets from my on-prem environment. To get the integration working I had to make sure that the subnet on my on-prem NSX manager resided on also was configured. So the IPsec configurations were done accordingly to support that: Two subnets from VMC and one from on-prem (where my NSX managers resides).\nIPsec config from my VMC NSX manager\nIPsec config from my on-prem firewall\nWhen IPsec tunnel was up I logged on to the VMC NSX manager and configured the \u0026ldquo;North/South\u0026rdquo; security policies allowing my Workload segment to any. I created a NSX Security Group (\u0026ldquo;VMs\u0026rdquo;) with membership criteria in place to grab my Workload Segment VMs (workload). This was just to make it convenient for myself during the test. We can of course (and should) be more granular in making these policies. But we also have the NSX Distributed Firewall which I will come to later.\nNorth/South policies\nNow I had the necessary connectivity and security policies in place for me to log on to the VMC vCenter from my on-prem management jumpbox and deploy my k8s worker nodes.\nVMC on AWS K8s worker nodes # In VMC vCenter I deployed three Ubuntu worker nodes, and configured them to be one master worker and two worker nodes by following my previous blog post covering these steps:\nhere\nThree freshly deployed VMs in VMC to form my k8s cluster\nNAME STATUS ROLES AGE VERSION vmc-k8s-master-01 Ready control-plane,master 2d22h v1.21.8 vmc-k8s-worker-01 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 vmc-k8s-worker-02 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 After the cluster was up I needed to install Antrea as my CNI.\nDownload the Antrea release from here: https://customerconnect.vmware.com/downloads/info/slug/networking_security/vmware_antrea/1_0\nAfter it has been downloaded, unpack it and upload the image to your k8s master and worker nodes by issuing the docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz\nThen apply it by using the manifest antrea-advanced-v1.2.3+vmware.3.yml found under the /antrea-advanced-1.2.3+vmware.3.19009828/manifests folder. Like this: kubectl apply -f antrea-advanced-v1.2.3+vmware.3.yml and Antrea should spin right up and you have a fully working K8s cluster:\nNAME READY STATUS RESTARTS AGE antrea-agent-2pdcr 2/2 Running 0 2d22h antrea-agent-6glpz 2/2 Running 0 2d22h antrea-agent-8zzc4 2/2 Running 0 2d22h antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d20h coredns-558bd4d5db-2j7jf 1/1 Running 0 2d22h coredns-558bd4d5db-kd2db 1/1 Running 0 2d22h etcd-vmc-k8s-master-01 1/1 Running 0 2d22h kube-apiserver-vmc-k8s-master-01 1/1 Running 0 2d22h kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 2d22h kube-proxy-rvzs6 1/1 Running 0 2d22h kube-proxy-tnkxv 1/1 Running 0 2d22h kube-proxy-xv77f 1/1 Running 0 2d22h kube-scheduler-vmc-k8s-master-01 1/1 Running 0 2d22h Antrea NSX integration # Next up is to configure the Antrea NSX integration. This is done by following this guide:\nhttps://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-DFD8033B-22E2-4D7A-BD58-F68814ECDEB1.html\nIts very well described and easy to follow. So instead of me rewriting it here I just point to it. But in general it makes up of a couple of steps needed.\n1. Download the necessary Antrea Interworking parts, which is included in the Antrea-Advanced zip above\n2. Create a certificate to use for the Principal ID User in your on-prem NSX manager.\n3. Import image to your master and workers (interworking-debian-0.2.0.tar)\n4. Edit the bootstrap-config.yaml (https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-1AC65601-8B35-442D-8613-D3C49F37D1CC.html)\n5. Apply the bootstrap-config-yaml and you should end up with this result in your k8s cluster and in your on-prem NSX manager:\nvmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 17 2d22h My VMC k8s cluster: vmc-ant-cluster (in addition to my other on-prem k8s-cluster)\nInventory view from my on-prem NSX manager\nOne can immediately see useful information in the on-prem NSX manager about the \u0026ldquo;remote\u0026rdquo; VMC K8s cluster such as Nodes, Pods and Services in the Inventory view. If I click on the respective numbers I can dive into more useful information. By clicking on \u0026ldquo;Pods\u0026rdquo;\nPod state, ips, nodes they are residing on etc\nEven in this view I can click on the labels links to get the lables NSX gets from kubernetes and Antrea:\nBy clicking on Services I get all the services running in my VMC k8s cluster\nAnd the service labels:\nAll this information is very useful, as they can be used to create the security groups in NSX and use those groups in security policies.\nClicking on the Nodes I also get very useful information:\nVMC on AWS NSX distributed firewall # As I mentioned earlier VMC on AWS also comes with NSX and one should utilize this to segment/create security polices on your worker nodes there. I have just created some simple rules allowing the \u0026ldquo;basic\u0026rdquo; needs for my workers, and then created some specific rules for what they are allowed to where all unspecified traffic is blocked by a default block rule.\nBare in mind that this is a demo environment and not representing any production environment as such, but some rules are in place to showcase that I am utilizing the NSX distributed firewall in VMC to microsegment my workload there.\nThe \u0026ldquo;basic\u0026rdquo; needs rules are the following: Under \u0026ldquo;Infrastructure\u0026rdquo; I am allowing my master and worker nodes to \u0026ldquo;consume\u0026rdquo; NTP and DNS. In this environment I do not have any local DNS and NTP servers as they are all public. DNS I am using Google\u0026rsquo;s public DNS servers 8.8.8.8 and 8.8.4.4 and NTP the workers are using \u0026ldquo;time.ubuntu.com\u0026rdquo;. I have created a security group consisting of the known DNS servers ip, as I know what they are. But the NTP server\u0026rsquo;s IP I do not know so I have created a security group with members only consisting of RFC1918 subnets and created a negated policy indicating that they are only allowed to reach NTP servers if they not reside on any RFC1918 subnet.\nNTP and DNS allow rules under Infrastructure\nUnder \u0026ldquo;Environment\u0026rdquo; I have created a Jump to Application policy that matches my K8s master/worker nodes\nJump to Application\nUnder \u0026ldquo;Application\u0026rdquo; I have a rule that is allowing internet access (could be done on the North/South Gateway Firewall section also) by indication that HTTP/HTTPS is allowed as long as it is not any RFC1918 subnet.\nAllow \u0026ldquo;internet access\u0026rdquo;\nFurther under Application I am specifying a bit more granular rules for what the k8s cluster is allowed in/out. Again, this is just some simple rules restricting the k8s cluster to not allow any-any by utilizing the NSX DFW already in VMC on AWS. One can and should be more granular, but its to give you an idea.\nIn the K8s-Backbone security policy section below I am allowing HTTP in to the k8s cluster as I am planning to run an k8s application there that uses HTTP where I allow a specific IP subnet/range as source and my loadbalancer IP range as the destination.\nThen I allow SSH to the master/workers for management purposes. Then I am creating some specific rules allowing the necessary ports needed for the Antrea control-plane to communicate with my on-prem NSX manager, which are: TCP 443, 1234 and 1235. Then I create an \u0026ldquo;Intra\u0026rdquo; rule allowing the master/workers to talk freely between each other. This should and can also be much more tightened down. When those rules are done processed they will hit a default block rule.\nVMC k8s cluster policy\nDefault drop\nAntrea policies from on-prem NSX manager # Now when the \u0026ldquo;backbone\u0026rdquo; is ready configured and deployed its time to spin up some applications in my \u0026ldquo;VMC K8s cluster\u0026rdquo; and apply some Antrea security policies. Its now time for the magic to begin ;-)\nIn my on-prem environment I already have a couple of Antrea enabled K8s clusters running. On them a couple of demo applications are already running and protected by Antrea security policies created from my on-prem NSX manager. I like to use an application called Yelb (which I have used in previous blog posts here). This application consist of 4 pods. All pods doing their separate thing for the application to work. I have a frontend pod which is hosting the web-page for the application, I have an application pod, db pod and a cache pod. The necessary connectivity between looks like this:\nYelb pods connectivity\nTo make security policy creation easy I make use of all the information I get from Antrea and Kubernetes in form of \u0026ldquo;Labels\u0026rdquo;. These labels are translated into tags in NSX. Which makes it very easy to use, and for \u0026ldquo;non\u0026rdquo; developers to use as \u0026ldquo;human-readable\u0026rdquo; elements instead of IP adresses, pods unique names etc. In this example I want to microsegment the pods that makes up the application \u0026ldquo;Yelb\u0026rdquo;.\nCreating NSX Security Groups for Antrea Security Policy # Before I create the actual Antrea Security Policies I will create a couple of security groups based on the tags I use in K8s for the application Yelb. The Yelb manifest looks like this:\napiVersion: v1 kind: Service metadata: name: redis-server labels: app: redis-server tier: cache namespace: yelb spec: type: ClusterIP ports: - port: 6379 selector: app: redis-server tier: cache --- apiVersion: v1 kind: Service metadata: name: yelb-db labels: app: yelb-db tier: backenddb namespace: yelb spec: type: ClusterIP ports: - port: 5432 selector: app: yelb-db tier: backenddb --- apiVersion: v1 kind: Service metadata: name: yelb-appserver labels: app: yelb-appserver tier: middletier namespace: yelb spec: type: ClusterIP ports: - port: 4567 selector: app: yelb-appserver tier: middletier --- apiVersion: v1 kind: Service metadata: name: yelb-ui labels: app: yelb-ui tier: frontend namespace: yelb spec: type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 80 selector: app: yelb-ui tier: frontend --- apiVersion: v1 kind: ReplicationController metadata: name: yelb-ui namespace: yelb spec: replicas: 1 template: metadata: labels: app: yelb-ui tier: frontend spec: containers: - name: yelb-ui image: mreferre/yelb-ui:0.3 ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: redis-server namespace: yelb spec: selector: matchLabels: app: redis-server replicas: 1 template: metadata: labels: app: redis-server tier: cache spec: containers: - name: redis-server image: redis:4.0.2 ports: - containerPort: 6379 --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-db namespace: yelb spec: selector: matchLabels: app: yelb-db replicas: 1 template: metadata: labels: app: yelb-db tier: backenddb spec: containers: - name: yelb-db image: mreferre/yelb-db:0.3 ports: - containerPort: 5432 --- apiVersion: apps/v1 kind: Deployment metadata: name: yelb-appserver namespace: yelb spec: selector: matchLabels: app: yelb-appserver replicas: 1 template: metadata: labels: app: yelb-appserver tier: middletier spec: containers: - name: yelb-appserver image: mreferre/yelb-appserver:0.3 ports: - containerPort: 4567 As we can see there is a couple of labels that distinguish the different components in the application which I can map to the application topology above. I only want to allow the frontend to talk to the \u0026ldquo;app-server\u0026rdquo;, and the app-server to the \u0026ldquo;db-server\u0026rdquo; and \u0026ldquo;cache-server\u0026rdquo;. And only on the needed ports. All else should be dropped. On my on-prem NSX manager I have created these groups for the already running on-prem Yelb application. I have created four 5 groups for the Yelb application in total. One group for the frontend (\u0026ldquo;ui-server\u0026rdquo;), one for the middletier (app server), one for the backend-db (\u0026ldquo;db-server\u0026rdquo;), one for the cache-tier (\u0026ldquo;cache server\u0026rdquo;) and one last for all the pods in this application:\nSecurity groups filtering out the Yelb pods\nThe membership criteria inside those groups are made up like this, where I am using the labels in my Yelb manifest (these labels are autopopulated so you dont have to guess). Tag equals label frontend and scope equals label dis:k8s:tier:\nGroup definition for the frontend\nThe same goes for the other groups just using their respective labels. The members should then look like this:\nOnly the frontend pod\nThen I have created a security group that selects all pods in my namespace Yelb by using the label Yelb like this:\nWhich then selects all my pods in the namespace Yelb:\nNow I have my security groups and can go on and create my security policies.\nAntrea security policies from NSX manager # Head over to Security, Distributed Firewall section in the on-prem NSX manager to start creating security policies based on your security groups. These are the rules I have created for my application Yelb:\nAntrea Security Policies from the NSX manager\nFirst rule allows traffic from my Avi SE\u0026rsquo;s that are being used to create the service loadbalancer for my application Yelb to the Yelb frontend on HTTP only. Notice that the source part here is in the \u0026ldquo;Applied to field\u0026rdquo; (goes for all rules in this example). Thee second rule allows traffic from the frontend to the middletier (\u0026ldquo;app-server\u0026rdquo;) on port 4567 only. The third rule allows traffic from middletier to backend-db (\u0026ldquo;db-server\u0026rdquo; on port 5432 only. The fourth rule allows traffic from middletier to cache (redis cache) on port 6379 only. All rules according to the topology maps above. Then the last rule is where I am using the namespace selection to select all pods in the namespace Yelb to drop all else not specified above.\nTo verify this I can use the Traceflow feature in Antrea from the NSX manager like this:\n(Head over to Plan \u0026amp; Troubleshoot, Traffic Analysis, Traceflow in your NSX manager)\nChoose Antrea Traceflow, choose the Antrea cluster where your application resides, then select TCP under Protocol type, type in Destination Port (4567) and choose where your pods are from the source and destination. In the screenshot above I want to verify that the needed ports are allowed between Frontend and middletier (application pod).\nClick trace:\nWell that worked, now if I change to port to something else like 4568, am I then still allowed to do that?\nNo, I am not. That is because I have my drop rule in place remember:\nI could go on and test all pod to pod connectivity (I have), but you can trust me their are doing their job. Just to save some screenshots. So that is it, I have microsegmented my Yelb application. But what if I want to scale out this application to my VMC environment. I want to achieve the same thing there. Why not, all our groundwork has already been done so lets head out and spin up the same applicaion on our VMC K8s cluster. Whats going to happen in my on-prem NSX manager. This is cool!\nAntrea security policies in my VMC k8s cluster managed by my on-prem NSX manager # Before I deploy my Yelb application in my VMC K8s cluster I want to refresh the memory by showing what my NSX manager knows about the VMC K8s cluster inventory. Lets take a look again. Head over to Inventory in my on-prem NSX manager and take a look at my VMC-ANT-CLUSTER:\n3 nodes you say, and 18 pods you say\u0026hellip; Are any of them my Yelb pods?\nNo Yelb pods here\u0026hellip;\nNo, there are no yelb pods here. Lets make that a reality. Nothing reported in k8s either:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h\nSpin up the Yelb application in my VMC k8s cluster by using the same manifest:\nkubectl apply -f yelb-lb.yaml\nThe result in my k8s cluster:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h yelb redis-server-74556bbcb7-fk85b 1/1 Running 0 4s yelb yelb-appserver-6b6dbbddc9-9nkd4 1/1 Running 0 4s yelb yelb-db-5444d69cd8-dcfcp 1/1 Running 0 4s yelb yelb-ui-f74hn 1/1 Running 0 4s\nWhat does my on-prem NSX manager reports?\nHmm 22 pods\nAnd a lot of yelb pods\nInstantly my NSX manager shows them in my inventory that they are up.\nNow, are they being protected by any Antrea security policies? Lets us do the same test as above by using Antrea Traceflow from the on-prem NSX manager with same ports as above (frontend to app 4567 and 4568).\nTraceflow from my on-prem NSX manager in the VMC k8s cluster\nNotice the selection I have done, everything is the VMC k8s cluster.\nNeed port is allowed\nThat is allowed, what about 4568 (which is not needed):\nAlso allowed\nThat is also allowed. I cant have that. How can I make use of my already created policy for this application as easy as possible instead of creating all the rules all over?\nWhell, lets test that. Head over to Security, Distributed firewall in my on-prem NSX manager.\nNotice the Applied to field here:\nWhat happens if I click on it?\nIt only shows me my local Antrea k8s cluster. That is also visible if I list the members in the groups being used in the rules:\nOne pod from my local k8s cluster\nWhat if I add the VMC Antrea cluster?\nCick on the pencil and select your remote VMC K8s cluster:\nApply and Publish:\nNow lets have a look inside our groups being used by our security policy:\nMore pods\nThe Yelb namespace group:\nInstantly my security groups are being updated with more members!!!\nRemember how I created the membership criteria of the groups? Labels from my manifest, and labels for the namespace? Antrea is cluster aware, and dont have to specify a specific namespace to select labels from one specific namespace, it can select from all namespaces as long as the label matches. This is really cool.\nNow what about my security policies in my VMC k8s cluster? Is it enforcing anything?\nLets check, doing a traceflow again. Now only on a disallowed port 4568:\nResult:\nDropped\nThe Security policy is in place, enforcing what it is told to do. The only thing I did in my local NSX manager was to add the VMC Antrea cluster in my Security Applied to section\n","date":"13 March 2022","externalUrl":null,"permalink":"/2022/03/13/managing-your-antrea-k8s-clusters-running-in-vmc-from-your-on-prem-nsx-manager/","section":"Posts","summary":"This week I was fortunate to get hold of a VMC on AWS environment and wanted to test out the possibility of managing my K8s security policies from my on-prem NSX manager by utilizing the integration of Antrea in NSX.","title":"Managing your Antrea K8s clusters running in VMC from your on-prem NSX Manager","type":"posts"},{"content":"","date":"13 March 2022","externalUrl":null,"permalink":"/tags/vmconaws/","section":"Tags","summary":"","title":"Vmconaws","type":"tags"},{"content":"","date":"13 March 2022","externalUrl":null,"permalink":"/categories/vmware-cloud/","section":"Categories","summary":"","title":"VMware-Cloud","type":"categories"},{"content":"","date":"18 January 2022","externalUrl":null,"permalink":"/tags/nsx-application-platform/","section":"Tags","summary":"","title":"Nsx-Application-Platform","type":"tags"},{"content":"VMware NSX 3.2 is out and packed with new features. One of them is the NSX Application Platform which runs on Kubernetes to provide the NSX ATP (Advanced Threat Protection) functionality such as NSX Intelligence (covered in a previous post), NSX Network Detection and Response (NDR) and NSX Malware. This post will go through how to spin up a K8s cluster for this specific scenario covering the pre-reqs from start to finish. After that the features itself will be covered in separate posts. Through this post NSX Application Platform will be abbreviated into NAPP.\nGetting started # To get started with NAPP its important that one has read the prerequisites needed to be in place on the K8s cluster that is hosting NAPP. In short NAPP is currently validated to run on upstream K8s version 1.7 all the way up to version 1.21, VMware Tanzu (TKC) versions 1.17.17 to 1.21.2. In addtion to a K8s cluster itself NAPP also needs a Registry supporting images/helm charts. In this walkthrough Harbor will be used. I will also go with an upstream K8s cluster version 1.21.8 running on Ubuntu nodes.\nSources being used to cover this post is mainly from VMware\u0026rsquo;s official documentation. So I list them here as an easy way to reference as they are providing the necessary and important information on how, requirements and all the steps detailed to get NAPP up and running. The purpose of this post is to just go through the steps as a more step by step guide. If more information is needed, head over to our official documentation. Below are the links for the software/tools I have used in this post:\nDeploying and Managing the VMware NSX Application Platform\nHarbor registry\nVMware vSphere Container Storage Plugin\nMetalLB LetsEncrypt\nPrerequisites/Context # First some context. In this post all K8s nodes are running as virtual machines on VMware vSphere. NSX-T is responsible for the underlaying network connectivity for the nodes and the persistent storage volumes in my K8s cluster is VMFS exposed through the vSphere Container Storage Plugin (CSI). The NSX manager cluster consists of 3 NSX managers and a cluster VIP address.\nThe vSphere environment consists of two ESXi hosts with shared storage from a FC SAN and vCenter managing the ESXi hosts.\nA quick summary of what is needed, tools/software and what I have used in my setup:\nA upstream kubernetes cluster running any of the supported versions stated in the official documentation. I am using 1.21.8 A working CNI, I am using Antrea. A registry supporting images/helm charts using a signed trusted certificate (no support for self-signed certificates). I am using Harbor and LetsEncrypt certificates. A load balancer to expose the NAPP endpoint with a static ip. I am using MetalLB Persistent storage in K8s I am using the vSphere Container Storage Plugin (if you are running the nodes on vSphere). NSX 3.2 of course Required resources for the NAPP form factor you want to go with. The required resources for the K8s cluster supporting NAPP is outlined below (from the official docs page): This post will cover the Advanced form factor where I went with the following node configuration:\n1 master worker/control plane node with 4 vCPUs, 8GB RAM and 200GB local disk (ephemeral storage). For the worker nodes: 3 worker nodes with 16vCPUs, 64GB RAM and 200GB (ephemeral storage) for the persistent storage (the 1TB disk requirement) I went with vSphere CSI to expose a VMFS datastore from vSphere for the persistent volume requirement.\nThe next chapters will go trough the installation of Harbor (registry) and the configuration done there, then the K8s configuration specifically the CSI and MetalLB part as I am following the generic K8s installation covered earlier here: Deploy Kubernetes on Ubuntu 20.04 .\nLets get to it.\nHarbor (registry requirement) # Harbor can be run as a pod in Kubernetes or a pod in Docker. I went with the Docker approach and spun up a VM for this sole purpose.\nThe VM is Ubuntu 20.4 with 4vCPU, 8GB RAM and 200GB disk.\nAfter Ubuntu is installed I installed the necessary dependencies to deploy Harbor in Docker by following the Harbor docs here: Harbor Getting Started\nI find it better to just link the steps below instead of copy/paste too much as the steps are very well documented and could also change over time.\nDocker Engine (latest Stable): docker.com/ubuntu Docker Compose (latest Stable): docker.com/linux Prepare the signed cert (if its not already done) NB! The certificate needs to contain the full chain otherwise the Docker client will not accept it. Download the Harbor installer: Harbor installer (I went with 2.4.1) Extract the online installer: tar -zxvf harbor-online-installer-v2.4.1.tgz Edit the harbor.yaml: Go to the folder harbor (result of the extract above) cp the harbor.yml.tmpl to harbor.yml and use your favourite editor and change the following (snippet from the harbor.yml file): Run the installer: sudo ./install.sh --with-chartmuseum The \u0026ndash;with-chartmuseum flag is important (The installer is in the same folder as above and if it is not executable make it executable with chmod +x install.sh check/validate whether you are able to log in to your Harbor registry with the following command: sudo docker login FQDNofHarbor --username admin --password (password defined in harbor.yml). It should not complain about the certificate if the certificate is valid. Log in to the Harbor UI by opening a browser and enter your FQDN of your harbor with the use of admin/password. Create a project: I made a public project\nDownload the NAPP images from your my.vmware.com page: VMware-NSX-Application-Platform-3.2.0.0.0.19067744.tgz upload it to your Harbor VM or to another endpoint where you have a Docker client. \u0026ldquo;Untar\u0026rdquo; the tgz file Find and edit the upload_artifacts_to_private_harbor.sh by changing the following: DOCKER_REPO=harborFQDN/napp (after the / is the project name you created) DOCKER_USERNAME=admin DOCKER_PASSWORD=Password-Defined-In-Harbor.yml\nSave and run it sudo ./upload_artifacts_to_private_harbor.sh (same here make it executable with chmod +x if its not executable. This takes a long time, so sit back and enjoy or go do something useful like taking a 3km run in about 15 minutes. The end result shoul look something like this in the Harbor GUI: Thats it for Harbor, next up the K8s cluster\nThe K8s cluster where NAPP is deployed # As stated in the official documentation for NAPP, K8s needs to be a specific version, it can be upstream K8s, VMware Tanzu (TKC) or other K8s managed platforms such as OpenShift. But the currently validated platforms are at the moment the above two mentioned platforms: upstream K8s and TKC.\nAs I wrote initially I will go with upstream K8s for this. To get this going I prepared 4 VMs where I dedicate one master worker/control-plane node with the above given specifications and 3 worker nodes with the above given specifications. I follow my previous guide for preparing the Ubuntu os and installing K8s here: K8s on Ubuntu so I will not cover this here but just continue from this with the specifics I did to get the CSI driver up, Antrea CNI and MetalLB. First out is the Antrea CNI.\nAssuming the K8s cluster is partially up, due to no CNI is installed. I download the downstream version of Antrea (one can also use the upstream version from the Antrea github repository) from my.vmware.com. One of the reason I want to use the downstream version from my.vmware.com version is that I want to integrate it to my NSX management plane (more on that in an separate post covering NSX-T with Antrea integration). Download the VMware Container Networking with Antrea (Advanced) from your my.vmware.page to your master worker. Unzip the zip file. Copy the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz to all your worker nodes (with scp for example). Found under the folder antrea-advanced-1.2.3+vmware.3.19009828/images (result of the extract previously). Load the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz image on all nodes, including the master worker, with the command sudo docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz Apply the antrea-advanced-v1.2.3+vmware.3.yml found under the folder antrea-advanced-1.2.3+vmware.3.19009828/manifests from your master worker. A second or two later you should have a fully working Antrea CNI in your K8s cluster. Notice that your CoreDNS pods decided to go into a running state. Thats it for the Antrea CNI. Next up is MetalLB When the CNI is up, its time for MetalLB. Installation of MetalLB is easy and well explained here: Install MetalLB. Next is the CSI for persistent volume. For step by step config of vSphere Container Storage Plugin head over the the following link (Getting Started with VMware vSphere Container Storage Plug-in section) and follow the instructions there which are very well described. That is, if you are running the VMs on vSphere and want to utilize VMFS as the underlying storage for your persistent volumes. Works great and is fairly easy to deploy. I might come back later and write up a short summary on this one. When you are done with the step above and have your Storage Class defined its over to NSX for deployment of NAPP - Yes! Deploy NAPP - NSX Application Platform # Its finally time to head over to the NSX manager and start deployment of NAPP\nTo get this show started, I again must refer to the prerequisites page for NAPP. I will paste below and make some comments:\nFirst requirement: NSX version 3.2 (First release with NAPP). Pr now 3.2 is the one and only NSX-T release that supports NAPP. License \u0026mdash;\u0026mdash;\u0026mdash;- Certificate: The first statement with CA-signed certificates is ok to follow. But the second one could be something that needs to be checked. This is valid if you are using NSX-T Self-Signed certificates. Image that you started out with one NSX manager, enabled the VIP cluster address, then it may well be that this cluster IP gets the certificate of your first NSX manager. So its important to verify that all three NSX managers are using their own unique certificate and the VIP uses its own unique certificate. If not, one must update the certificates accordingly. In my environment I had unique certificates on all NSX manager nodes, but my VIP was using NSX manager 1\u0026rsquo;s certificate. So I had to update the certificate on the VIP. I generated a new certificate from the NSX manager here: And followed the instructions here to replace the VIP certificate: Replace Certificates The currently validated platforms NAPP is supported to run on, been through that earlier. Harbor is covered previously with a dedicated section. Harbor is not a strict requirement though. One can BYO registry if it supports image/helm charts When using upstream K8s, the config does not have a default token expiry. If using TKC one must generate a long-lived token so NAPP wont log out from the K8s cluster. Described in the docs Service Name FQDN is a dns record that is mapped to the IP the the endpoint service gets when deployed. Thats were I use MetalLB for this purpose. Just to initiate a type LoadBalancer. If there is a firewall between your NSX manager and the NAPP K8s cluster, one must do firewall openings accordingly. Time sync is important here also. The K8s cluster must be synced to the NSX Manager. Going through the deployment of NAPP from the NSX manager GUI # Log in to the NSX manager GUI. Head over to System and find the new section on the left side called: NSX Application Platform\nFrom there the first thing thats needed to populate is the urls to your Harbor registry (or other BYO registry). The urls goes like this:\nHelm Repository: https://harbor.guzware.net/chartrepo/napp -\u0026gt; FQDN for the Harbor instance, then chartrepo and then the name of the projecy you created in Harbor Docker Registry: harbor.guzware.net/napp/clustering without HTTPS, almost the same url just swap places on napp (project in Harbor) and clustering Click save url and it should validate ok and present you with this and the option to continue in the bottom right corner: Next is the Form factor and kubeconfig:\nThe first thing is to upload your K8s kubeconfig file. Select upload and it will validate. Should you get a warning the the K8s version is newer than the kubectl client onboard the NSX manager upload a newer client from my.vmware.com The Cluster Type is only Standard for now. Storage Class is what you defined in K8s with the CSI (Persistent Volumes) Service Name (FQDN) registered in DNS Form Factor - Here you will have three choices: Standard, Advanced and Evaluation: I have gone with the Advanced form factor. The result should look like this: Now you should be able to click next to do the Precheck Platform for validation:\nFinally Review \u0026amp; Update\nInstallation starts\nAnd after some eager waiting the end result should look like this:\nA brief summary from the K8s cluster:\nA bunch of pods - nice!\nThats it - next time I will continue with the features NAPP brings to the table: NSX Intelligence, Network Detection and Response and NSX Malware Prevention\n","date":"18 January 2022","externalUrl":null,"permalink":"/2022/01/18/vmware-nsx-application-platform/","section":"Posts","summary":"VMware NSX 3.","title":"VMware NSX Application Platform","type":"posts"},{"content":"","date":"19 October 2021","externalUrl":null,"permalink":"/tags/ids/","section":"Tags","summary":"","title":"Ids","type":"tags"},{"content":"","date":"19 October 2021","externalUrl":null,"permalink":"/tags/ips/","section":"Tags","summary":"","title":"Ips","type":"tags"},{"content":"This page will explain my lab environment, which is used in all the examples, tutorials in this blog.\nLab overview/connectivity - physical, logical and hybrid # It is nice to have an overview of how the underlying hardware looks like and when reading my different articles. So I decided to create some diagrams to illustrate this. Which hopefully will help understanding my blog posts further. First out is the physical components (which is relevant for the posts in this blog).\nPhysical hardware # My lab consist of two ESXi hosts, one ToR switch (enterprise dc switch with many capabilities) and a fibrechannel SAN (storage).\nLogical overview # To make possible all the things I want to do in my lab I am running most of the networking and other features virtually on top of my physical hardware. This includes NSX-T, virtual routers (VM based) in additition to the router functionality in NSX-T and VM based \u0026ldquo;perimeter\u0026rdquo; firewall which is based on PfSense. Below is an logical overview of the network topology in my lab.\n","date":"19 October 2021","externalUrl":null,"permalink":"/2021/10/19/my-lab/","section":"Posts","summary":"This page will explain my lab environment, which is used in all the examples, tutorials in this blog.","title":"My LAB","type":"posts"},{"content":"","date":"19 October 2021","externalUrl":null,"permalink":"/categories/netowkring/","section":"Categories","summary":"","title":"Netowkring","type":"categories"},{"content":"This post will go through the IDS/IPS built-in feature of the NSX distributed firewall.\nAbbreviations used in this article:\nIDS = Intrusion Detection System IPS = Intrusion Prevention System Introduction to VMware NSX distributed IDS \u0026amp; IPS # Before we dive into how to configure and use the distributed IDS and IPS feature in NSX let me just go through the basics where I compare the traditional approach with IDS/IPS and the NSX distributed IDS/IPS. This article is a continuation on the article Microsegmentation with VMware NSX\u0026quot; where I talk about east/west and north/south traffic pattern and being in context with the workload its supposed to protect. Where being in context is a key thing, especially when it comes to security policies and IDS/IPS. Know what you are protecting, make the inspection as relevant as possible, inspection done optimal (reduce false positives, maintain performance) and at the right place.\nThe traditional way of using IDS/IPS # In a more traditional infrastructure we have the perimeter firewall that is responsible for the \u0026ldquo;environment\u0026rdquo; policies, enforcing policies between the environments and allowing/blocking different types of the services from each environment to communicate. In such an scenario it is often also the same perimeter firewall that is enabled with IDS/IPS. In a datacenter full of virtualized workload this leads to hairpinning the traffic to a centralized appliance for inspection with the consequence of reducing performance, a lot of unnecessary traffic is sent out to the physical infrastructure to reach the perimeter firewall and sent back again. The appliance is not in context of the workload its analyzing traffic from/to so its hard to be very specific enough when it comes to the right signatures etc. The picture below illustrates this:\nIDS/IPS with a centralized appliance\nNSX Distributed IDS and IPS # To overcome the challenges of hairpinning traffic in an virtualized environment, we need to have the firewall, IDS and IPS enforced where the workload actually resides. This saves unnecessary traffic being sent out on the physical infrastructure if its not meant to go out and it also gives the network logics (firewall/IDS/IPS) to be part of the dataplane where the actual workload its supposed to protect resides and can have much more insight (being in context of) in whats going on. Things as knowing its a Ubuntu 20.04 and MySQL server you are protecting, makes it much easier to create the firewall policies but also much more pinpointed/granular IDS/IPS policies. This leads to very specific IDS/IPS rules, no false positives, better performance. This is where NSX Distributed IDS and IPS comes into play. Both the NSX Distributed Firewall and IDS/IPS runs on the same host as the virtual workload you are protecting. Its not necessary to redirect traffic, no need to change anything in the infrastructure, its as simple as just enabling the feature and create policies. Those policies can be created with an application centric perspective, as we have the ability to know the workload we are protecting as the below illustration:\nIDS/IPS polices with only workload relevant signatures\nIDS/IPS available on each hypervisor host\nHow to use IDS \u0026amp; IPS in VMware NSX-T # To get started with IDPS in NSX is very easy, its already installed on your transport nodes when you have them enabled with NSX. In the following sections I will go through the different parts in the NSX gui that involves the IDPS part and finish up with an example of how to create policies.\nEnable IDPS, settings and signature updates # When one log in to the NSX manager GUI one will see it is divided into different categories such as Networking, Security, and Inventory. IDPS is certainly a security feature of NSX so we will head over there.\nAfter clicking on the Security tab, it will take us to the Security Overview page:\nNSX Security Overview\nAs one can see this gives us a great summarized view over the different security parts in NSX, the IDPS, URL Analysis, the DFW, Anomalies. To see more details in the specific area click on the respective feature on the left side menu. In our case, this is the Distributed IDS/IPS menu.\nWhen inside the Distributed IDS/IPS section, head over to the settings page:\nSettings page of IDPS\nOn this page we can manage the signatures (versions), see the status on the signatures version, whether there is an update on the signature database, update and or adjust whether updates are done automatically. If we want to view the complete list of available signatures click on \u0026ldquo;View and Manage global signature set\u0026rdquo;. It should present us a list of all signatures:\nGlobal signature set\nHere we can search for a specific signature, or signatures based on the filter you choose in the top right corner. Say I want to search for signatures relevant to MySQL, I type in \u0026ldquo;mysql\u0026rdquo;:\nMysql filter\nBut I can also search for a specific CVE ID (one that we have recently been alerted on maybe):\nCVE-2017-12636\nOr a filter based on CVSS score, in this example 7.5:\nCVSS 7.5\nWe can also adjust the Global default action on specific signatures from Alert, Drop and Reject:\nBy hovering over the blue (!) we will be presented with an explanation of how this works:\nInstead of overriding the global setting for a set of signatures here, we will do this in the next section \u0026ldquo;IDPS Profiles\u0026rdquo;.\nFurther down on the same page is where we enable or disable the IDPS feature. It can be enabled on a vSphere cluster (a set of hosts managed by a vCenter) or standalone ESXi hosts. And its just as simple as clicking the enable button on the right side. It should turn green when enabled.\nNow that IDPS is enabled lets head over to Profiles.\nIDPS profiles # The profiles section is where we create our application specific signatures we want to use in our IDPS policies (later). We want to adjust and narrow down the total amount of signatures to be used when we create our idps policies for our workload. If I want to create an IDPS policy for a specific application I should create a profile that matches this to reduce false positives, and maintain an optimal inspection with IDS/IPS as an added security feature on top of the Distributed Firewall. In my demo I am interested in only vulnerabilities affecting product \u0026ldquo;Linux\u0026rdquo;.\nLets start out by clicking \u0026ldquo;Add Profile\u0026rdquo; and create the profile.\nNew profile\nGive the profile a name and start adjusting the signatures we want to use, we start by deciding the Severity Category (Critical, High, Medium and Low). For more information on these categories look here: https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/administration/GUID-4343E565-7AC2-40C2-8B12-5FC14893A607.html\nAfter the severity category has been decided we can go ahead and further adjust the specifics we are looking for, please also make note of the IDS Signatures Included number as we proceed in our selection.\nBefore any selections done:\nDefault\nI will go ahead with Severity Critical \u0026amp; High\nThen I can proceed with a selection based Attack Types, Attack Targets, CVSS and Products Affected. I will just post a screenshot from each four options:\nSearch is possible\nSearch is possible\nI will adjust my profile with only Products Affected (this is done to justify the demo of IDS):\nI am satisfied with my selection from now. Let look at the profile now:\nNow I am down to 217 signatures in this specific profile. I also have an option now to override the default action when/if IDPS detects anything and how to respond. Click on the \u0026ldquo;Manage signatures for this profile\u0026rdquo; and we should be presented with the signatures relevant only for this profile (after the selection is done):\nThere is an important note to take here. If we only have Alert on the signature, and we want to create an IDPS policy with Detect \u0026amp; Prevent one must have the signature action to either Drop or Reject also. Its not sufficient to just create a policy with Detect \u0026amp; Prevent. That is brilliant if we have signatures in the same profile we don\u0026rsquo;t want to be dropped, but we only want to be notified. Then we can have one rule with Detect \u0026amp; Prevent where some traffic is being dropped (Prevent) while the other is just notified (Detect). So if there is one CVE that you really should drop here you have the option to select just the few of those.\nLets apply the profile to a policy/rule in IDPS so it can act upon it.\nIDPS Policy # Under rules in the Distributed IDPS section in NSX:\nClick + Add Policy, this creates a new section:\nName it and then click on the three dots on the left side to create a rule:\nShould now look like this:\nNow we need to fill in source, destination, our IDS Profile created earlier, where to apply the policy and action (mode). In the example below I have already created a couple of security groups (security groups explained in the NSX Distributed firewall article) so I just need to add them to the respective source/destination:\nIDS rule applied to groups used in source and destination\nClick publish and your rule is in effect immediately.\nAlso notice that I did not specify a Service in my rule, here you can be more specific by add the service also, if you know its HTTP for example.\nNow let us check if it detects something\u0026hellip;.\nDistributed IDS/IPS Events # To view detected events, and drill down into the occured events, we must head over to the Events section in our Distributed IDS/IPS section.\nHere we can get a an overview with a timeline on when and where the events have occured.\nIf we look at the overview we can quickly notice if there has been any event in the last 24 hours, 48 hours, last 7 days or 14 days. Lets go through the Events page. Below is a screenshot showing last 24 hours.\nOverview\nTo change the timeline from default 24 hours, take a look at the top right corner and click on the arrow to get the drop down menu.\nNow look at the timeline view using Last 24 hours I can see that there has been 1 event represented by a an orange dot.\nThat tells me very quickly that there has atleast been detected an event. The color code tells me that the event is of severity High.\nIf there are many events in the timeline view, I can choose to filter out the severity I am interested in by unchecking the others.\nLegend color codes\nBy hovering over the orange dot I can get more information on the event:\nI can see what kind of event, and how many attempts of the same kind. If you look closer on the timeline view there are several dots represented. Those represent the other attempts of the same kind. The colored dots will only represent unique occurences within the given timeline. Its also possible to adjust your timeline further if you want to inspect events happening at a certain time within the 24 hours timeline by adjusting the blue sliders:\nAdjusted timeline view\nThen if I adjust it to say just before 18:10 and just after (where there is a dot) the orange dot will appear again as this event suddenly will be unique for this specific time. The bigger timeline view will be updated accordingly. Now I want to know more of this specific event. Look further down, it will be a list (if several unique events has occured represented, again within the timeline give above). The detailed list below will also update according to the adjusted timeline.\nIf one take a look at the below event it says under occurence \u0026ldquo;Single Attempt\u0026rdquo;, but I know there are multiple attempts, as I saw before I adjusted the timeline. I \u0026ldquo;reset\u0026rdquo; the timeline view back to the full 24 hours view it will be updated to multiple attempts.\nSingle attempt at the given timeline\nMultiple attempts over a longer timeline\nNow if one click on the arrow on the left side more information will be revealed.\nIn this view I can see the source (the attacker, where it is initiated from) and the destination (the target, victim of the \u0026ldquo;attack\u0026rdquo;). Intrusion activity, detected and prevented. The number of VMs affected. If one click on the number below VMs affected we will also see a list of VM(s) affected with names:\nIf I now go back to my profile defined earlier, I want to change the signature ID 2023995 to drop and also update my policy to Detect \u0026amp; Prevent. Lets see how this affects the detailed view.\nUpdated the profile\nUpdated the policy\nPrevented events\nWith the profile on this specific signature ID sat do drop and the policy sat to Detect \u0026amp; Prevent it also drops the specific attempt. Meaning I can have a good night sleep, or can I\u0026hellip;.?\nI should probably do something with the source also. But that should be easy now that we know what the source is.\n","date":"19 October 2021","externalUrl":null,"permalink":"/2021/10/19/vmware-nsx-ids-ips/","section":"Posts","summary":"This post will go through the IDS/IPS built-in feature of the NSX distributed firewall.","title":"VMware NSX IDS \u0026 IPS","type":"posts"},{"content":"","date":"14 July 2021","externalUrl":null,"permalink":"/tags/home-assistant/","section":"Tags","summary":"","title":"Home-Assistant","type":"tags"},{"content":"","date":"14 July 2021","externalUrl":null,"permalink":"/categories/home-automation/","section":"Categories","summary":"","title":"Home-Automation","type":"categories"},{"content":"When I finish up the other posts I have started on there will be content coming here also\n","date":"14 July 2021","externalUrl":null,"permalink":"/2021/07/14/the-home-automation-category/","section":"Posts","summary":"When I finish up the other posts I have started on there will be content coming here also","title":"The Home Automation category","type":"posts"},{"content":"","date":"14 July 2021","externalUrl":null,"permalink":"/tags/zwave/","section":"Tags","summary":"","title":"Zwave","type":"tags"},{"content":"NSX ALB has a very useful feature built-in, to function as DNS server for your domains defined in your NSX-ALB environment. Meaning that all host-records will be automatically resolved by fqdn as soon as the service is created.\nIf you have followed my other post about how to configure the AKO (Avi Kubernetes Operator) here you are familiar with creating DNS profiles in NSX-ALB. The first step in configuring NSX-ALB as DNS provider is to configure one or more domain names in NSX-ALB.\nLog in to the NSX-ALB controller GUI: -\u0026gt; Templates -\u0026gt; IPAM/DNS Profiles\nCreate a profile (if you dont already have one) give it a name and add one or more domain names:\nAfter you have configured a DNS profile head over to -\u0026gt; Administration -\u0026gt; Settings -\u0026gt; DNS Service in the controller GUI to create the DNS Virtual Service:\nFrom here one can click \u0026ldquo;Add Virtual Service\u0026rdquo; and configure the DNS VS. Go to the empty drop-down list (if you don\u0026rsquo;t already have DNS VS configured) and click Create Virtual Service. Choose your cloud and VRF context.\nOne can also create a DNS VS directly from the Application menu, but by going this way some fields are automatically decided for the use of DNS Service.\nGive the service a name, and adjust accordingly. I have done some adjustment to the service in my environment such as Service port where I add 53 twice and choose Override TCP/UDP on the last one to get DNS on UDP port 53 also. I have also added my backend DNS servers as a pool to this VS to have them do lookup against those if the record is not found locally (not obligatory). Application-Domain-Name should have the same domain name as defined in your DNS Profile attached to your cloud.\nLeave Policies and Analytics as is. Under Advanced you choose your SE pool where your DNS VS should live. As a best practice the DNS SE should not be shared with other VS\u0026rsquo;es. So create a dedicated pool for the DNS-VS and if resources are scarce you can defined the SE group to only contain one SE (no redundancy for DNS VS though).\nIn my environment I have also created a Conditional forwarder on my backend DNS servers to look for DNS records in my domains defined in the N-ALB environment. Using NSX-ALB DNS provider service is a brilliant feature as I don\u0026rsquo;t have to manually register any applications/services created in N-ALB or from K8s through AKO as this is all handled by the DNS service in N-ALB. My K8s applications can be spun up/down, without having to care about their dns records as this is all handled automatically by the NSX-ALB.\nDemo:\nTake an application created in NSX-ALB\nPing the dns name\nThat\u0026rsquo;s it. Now NSX-ALB handles all your DNS records for you. If you want your backend DNS servers to forward the request to NSX-ALB head over to your DNS servers and either add a Conditional forwarder for your domains or add a Delegated zone as a sub-domain and point to your DNS-VS VIP.\n","date":"12 July 2021","externalUrl":null,"permalink":"/2021/07/12/configure-nsx-advanced-load-balancer-nsx-alb-as-dns-provider/","section":"Posts","summary":"NSX ALB has a very useful feature built-in, to function as DNS server for your domains defined in your NSX-ALB environment.","title":"Configure NSX Advanced Load Balancer (NSX-ALB) as DNS provider","type":"posts"},{"content":"","date":"12 July 2021","externalUrl":null,"permalink":"/tags/dns-service/","section":"Tags","summary":"","title":"Dns-Service","type":"tags"},{"content":" Use NFS for your PVC needs # If you are running vShere with Tanzu, TKG on vSphere or are using vSphere as your hypervisor for your worker-nodes you have the option to use the vSphere CSI plugin here. In Tanzu this is automatically configured and enabled. But if you are not so privileged to have vSphere as your foundation for your environment one have to look at other options. Thats where NFS comes in. To use NFS for your persistent volumes is quite easy to enable in your environment, but there are some pre-reqs that needs to be placed on your workers (including control plane nodes). I will go through the installation steps below.\nPre-reqs # A NFS server available, already configured with shares exported. This could be running on any Linux machine in your environment that has sufficient storage to cover your storage needs for your PVCs. Or any other platform that can export NFS shares.\nInstall and configure # This post is based on Ubuntu 20.04 as operating system for all the workers.\nThe first package that needs to be in place is the nfs-common package in Ubuntu. This is installed with the below command, and on all your workers (control-plane and workers):\nsudo apt install nfs-common -y Now that nfs-common is installed on all workers we are ready deploy the NFS subdir external provisioner link. The below commands is done from your controlplane nodes, if not stated otherwise. I prefer to use Helm. If you dont have Helm installed head over here for how to install Helm. With Helm in place execute the following command to add the NFS Subdir External Provisioner chart:\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ Then we need to install the NFS provisioner like this:\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ --set nfs.server=x.x.x.x \\ --set nfs.path=/exported/path Its quite self-explanatory but I will quickly to through it. --set nfs.server=x.x.x.x \\ needs to be updated with the IP address to your NFS server. --set nfs.path=/exported/path needs to be updated to reflect the path your NFS server exports.\nThats it actually, you know have a storageclass available in your cluster using NFS. The default values for the storageclass deployed without editing the NFS subdir external provisioner helm values looks like this:\n$kubectl get storageclasses.storage.k8s.io NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 51d Values, additional storageclasses # If you want to change the default values get the values file, edit it before you deploy the NFS provisioner or get the file, edit it and update your deployment with helm upgrade To grab the values file run this command:\nhelm show values nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \u0026gt; nfs-prov-values.yaml If you want to add additional storageclasses, say with accessmode to RWX, you deploy NFS provisioner with a value file that has the settings you want. Remember to change the class name.\nname: XXXXX # Allow volume to be expanded dynamically allowVolumeExpansion: true # Method used to reclaim an obsoleted volume reclaimPolicy: Delete # When set to false your PVs will not be archived by the provisioner upon deletion of the PVC. archiveOnDelete: true # If it exists and has \u0026#39;delete\u0026#39; value, delete the directory. If it exists and has \u0026#39;retain\u0026#39; value, save the directory. # Overrides archiveOnDelete. # Ignored if value not set. onDelete: # Specifies a template for creating a directory path via PVC metadata\u0026#39;s such as labels, annotations, name or namespace. # Ignored if value not set. pathPattern: # Set access mode - ReadWriteOnce, ReadOnlyMany or ReadWriteMany accessModes: XXXXXXX Above is a snippet from the values file.\nDeploying pods with special privileges # If you need to deploy pods with special privileges, often mysql containers, you need to prepare your NFS server and filesystem permission for that. Otherwise they will not be able to write correctly to their PV when they are deployed. The example below is a mysql container that needs to create its own permission on the filesystem it writes to:\nspec: securityContext: runAsUser: 999 fsGroup: 999 So what I did to solve this is the following: I changed my NFS export to look like this on my NFS server: /path/to/share -alldirs -maproot=\u0026quot;root\u0026quot;:\u0026quot;wheel\u0026quot; Then I need to update the permissions on the NFS server filesystem with these commands and permissions:\n$chown nobody:nogroup /shared/folder $chmod 777 /shared/folder ","date":"12 July 2021","externalUrl":null,"permalink":"/2021/07/12/kubernetes-persistent-volumes-with-nfs/","section":"Posts","summary":"Use NFS for your PVC needs # If you are running vShere with Tanzu, TKG on vSphere or are using vSphere as your hypervisor for your worker-nodes you have the option to use the vSphere CSI plugin here.","title":"Kubernetes Persistent Volumes with NFS","type":"posts"},{"content":"","date":"12 July 2021","externalUrl":null,"permalink":"/tags/nfs/","section":"Tags","summary":"","title":"Nfs","type":"tags"},{"content":"","date":"12 July 2021","externalUrl":null,"permalink":"/tags/persistent-storage/","section":"Tags","summary":"","title":"Persistent-Storage","type":"tags"},{"content":"","date":"12 July 2021","externalUrl":null,"permalink":"/tags/pvc/","section":"Tags","summary":"","title":"Pvc","type":"tags"},{"content":"","date":"12 July 2021","externalUrl":null,"permalink":"/categories/storage/","section":"Categories","summary":"","title":"Storage","type":"categories"},{"content":"Abbreviations used in this article:\nNSX Advanced Load Balancer = NSX-ALB K8s = Kubernetes (8 letters between the K and s in Kubernetes) SSL = Secure Sockets Layer AKO = Avi Kubernetes Operator (AVI now a VMware product called NSX Advanced Load Balancer) In one of my previous posts I wrote about how to install and configure AKO (Avi Kubernetes Operator) to use as Service type LoadBalancer.\nThis post will try to cover the basics of how to use NSX Advanced LoadBalancer by using AKO to handle our Ingress requests (ingress-controller).\nFor more information on Ingress in Kubernetes\nAn API object that manages external access to the services in a cluster, typically HTTP.\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\nIngress Load Balancer Kubernetes Definition # Within Kubernetes or K8s, a collection of routing rules that control how Kubernetes cluster services are accessed by external users is called ingress. Managing ingress in Kubernetes can take one of several approaches.\nAn application can be exposed to external users via a Kubernetes ingress resource; a Kubernetes NodePort service which exposes the application on a port across each node; or using an ingress load balancer for Kubernetes that points to a service in your cluster.\nAn external load balancer routes external traffic to a Kubernetes service in your cluster and is associated with a specific IP address. Its precise implementation is controlled by which service types the cloud provider supports. Kubernetes deployments on bare metal may require custom load balancer implementations.\nHowever, properly supported ingress load balancing for Kubernetes is the simplest, more secure way to route traffic. link\nGetting AKO ready # While this post assumes AKO is already in place and working in your k8s clusters I will get straight to the parts that involve Ingress. If not head over here to read the official docs how to install Avi Kubernetes Operator (AKO). To verify that you AKO is ready to handle Ingress request, type in this and notice the output:\n$ kubectl get ingressclasses.networking.k8s.io NAME CONTROLLER PARAMETERS AGE avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 50d Default secret for TLS Ingress # As AKO expects all ingresses with TLS termination to have a key and certificate specified, there is a couple of ways this can be done. We can go with a \u0026ldquo;pr service\u0026rdquo;, meaning a dedicated set of keys and certs pr service or a default/common set of keys and certificates that AKO can use if nothing else is specified. To apply the common approach, one common key and certificate for one or more applications we need to add a secret for the AKO. Prepare your router-default.yaml definition file like this (the official docs wants you to put in your cert as is, that does not work so you need to base64 encode both keys and certs and paste in below):\napiVersion: v1 kind: Secret metadata: name: router-certs-default namespace: avi-system type: kubernetes.io/tls data: tls.key: \u0026#34;base64 encoded\u0026#34; tls.crt: \u0026#34;base64 encoded\u0026#34; alt.key: \u0026#34;base64 encoded\u0026#34; alt.crt: \u0026#34;base64 encoded\u0026#34; To base64 encode your keys and certs this can be done like this:\nIf you have the keys and certs in a file, from whatever linux terminal type in:\ncat cert.pem | base64 -w 0 cat key.pem | base64 -w 0 Then paste into the above yaml accordingly (tls.key:key, tls.crt:crt) If you have both ECDSA and RSA certs use the alt.key and alt.crt to apply both. As soon as everything is pasted, apply the yaml file kubectl apply -f router-defaults.yaml\nApply your ingress service # To create an ingress service you need to define this in yaml. An example below:\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: \u0026#34;NameOfIngress-Service\u0026#34; namespace: \u0026#34;Namespaceofwhereyour-service-resides\u0026#34; labels: app: \u0026#34;ifyouwant\u0026#34; annotations: ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; #\u0026#34;Indicates to Avi that you want to use TLS\u0026#34; spec: ingressClassName: avi-lb #\u0026#34;The default class name for AVI\u0026#34; rules: - host: \u0026#34;FQDN\u0026#34; http: paths: - pathType: Prefix path: / backend: service: name: \u0026#34;which-service-to-point-to\u0026#34; port: number: 80 Hostrules and HTTPrules # As I mentioned earlier, you can also define specific rules pr server such as certificates. Here we can use Hostrules and HTTPrules to further adjust granular settings pr service. One Hostrule example below:\napiVersion: ako.vmware.com/v1alpha1 kind: HostRule metadata: name: name-of-your-rule namespace: namespace-of-your-service spec: virtualhost: fqdn: must-match-hostname-above # mandatory fqdnType: Exact enableVirtualHost: true tls: # optional sslKeyCertificate: name: \u0026#34;name-of-certificate\u0026#34; # This must be already defined in your AVI controller type: ref alternateCertificate: name: \u0026#34;name-of-alternate-cert\u0026#34; # This must be already defined in your AVI controller type: ref sslProfile: System-Standard-PFS termination: edge To get all features available head over to the official docs site here\nHostrule example from the official docs:\napiVersion: ako.vmware.com/v1alpha1 kind: HostRule metadata: name: my-host-rule namespace: red spec: virtualhost: fqdn: foo.region1.com # mandatory fqdnType: Exact enableVirtualHost: true tls: # optional sslKeyCertificate: name: avi-ssl-key-cert type: ref alternateCertificate: name: avi-ssl-key-cert2 type: ref sslProfile: avi-ssl-profile termination: edge gslb: fqdn: foo.com includeAliases: false httpPolicy: policySets: - avi-secure-policy-ref overwrite: false datascripts: - avi-datascript-redirect-app1 wafPolicy: avi-waf-policy applicationProfile: avi-app-ref analyticsProfile: avi-analytics-ref errorPageProfile: avi-errorpage-ref analyticsPolicy: # optional fullClientLogs: enabled: true throttle: HIGH logAllHeaders: true tcpSettings: listeners: - port: 8081 - port: 6443 enableSSL: true loadBalancerIP: 10.10.10.1 aliases: # optional - bar.com - baz.com Httprule example from the official docs:\napiVersion: ako.vmware.com/v1alpha1 kind: HTTPRule metadata: name: my-http-rule namespace: purple-l7 spec: fqdn: foo.avi.internal paths: - target: /foo healthMonitors: - my-health-monitor-1 - my-health-monitor-2 loadBalancerPolicy: algorithm: LB_ALGORITHM_CONSISTENT_HASH hash: LB_ALGORITHM_CONSISTENT_HASH_SOURCE_IP_ADDRESS tls: ## This is a re-encrypt to pool type: reencrypt # Mandatory [re-encrypt] sslProfile: avi-ssl-profile destinationCA: |- -----BEGIN CERTIFICATE----- [...] -----END CERTIFICATE----- ","date":"11 July 2021","externalUrl":null,"permalink":"/2021/07/11/k8s-ingress-with-nsx-advanced-load-balancer/","section":"Posts","summary":"Abbreviations used in this article:","title":"K8s Ingress with NSX Advanced Load Balancer","type":"posts"},{"content":" This is an introduction post to Antrea, what it is and which features it has.\nFor more details head over to:\nhttps://antrea.io/ and https://github.com/antrea-io/antrea\nFirst of, Antrea is a CNI. CNI stands for Container Network Interface. As the world moves into Kubernetes more and more, we need a good CNI to support everything from network to security within Kubernetes. Thats where Antrea comes into play.\nAntrea has a rich set of features such as:\nKubernetes-native: Antrea follows best practices to extend the Kubernetes APIs and provide familiar abstractions to users, while also leveraging Kubernetes libraries in its own implementation. Powered by Open vSwitch: Antrea relies on Open vSwitch to implement all networking functions, including Kubernetes Service load-balancing, and to enable hardware offloading in order to support the most demanding workloads. Run everywhere: Run Antrea in private clouds, public clouds and on bare metal, and select the appropriate traffic mode (with or without overlay) based on your infrastructure and use case. Windows Node support: Thanks to the portability of Open vSwitch, Antrea can use the same data plane implementation on both Linux and Windows Kubernetes Nodes. Comprehensive policy model: Antrea provides a comprehensive network policy model, which builds upon Kubernetes Network Policies with new features such as policy tiering, rule priorities and cluster-level policies. Troubleshooting and monitoring tools: Antrea comes with CLI and UI tools which provide visibility and diagnostics capabilities (packet tracing, policy analysis, flow inspection). It exposes Prometheus metrics and supports exporting network flow information which can be visualized in Kibana dashboards. Encryption: Encryption of inter-Node Pod traffic with IPsec tunnels when using an overlay Pod network. Easy deployment: Antrea is deployed by applying a single YAML manifest file. As this blog page evolves, it will cover in more technical posts how to use and configure Antrea with examples. As how this webpage is both handled by Antrea network and security features (yes, this wordpress page is hosted on a native K8s cluster with Antrea as CNI)\n","date":"10 July 2021","externalUrl":null,"permalink":"/2021/07/10/antrea-kubernetes-cni/","section":"Posts","summary":"This is an introduction post to Antrea, what it is and which features it has.","title":"Antrea - Kubernetes CNI","type":"posts"},{"content":"This post will go through the Antrea-native policy resources and how to use them to secure your Kubernetes environment combined with K8s network policies.\nAbbreviations used in this article:\nContainer Network Interface = CNI Antrea Cluster Network Policies = ACNP Antrea Network Policies = ANP Kubernetes Network Policies = K8s policies or KNP When it comes to securing your K8s infrastructure it can be done in several layers in the infrastructure as a whole. This post will focus on the possibilities within the K8s cluster with features in the Antrea CNI. I will go through the Antrea-native policies (Antrea and K8s policies), with examples of when, how and where to use them. As Antrea-native policy resources can be used together with K8s network policies I will show that also.\nThis post will not cover the additional needs of security in your datacenter before reaching your K8s environment. I will cover this in a later post where I go through the use of NSX Distributed Firewall protecting your k8s clusters together with the security policies in Antrea.\nAntrea-native policy resources - short introduction # Antrea comes with a comprehensive policy model. We have the Antrea Cluster Network Policies and Antrea Network Policies. The difference being between those two is that the ACNP applies to all objects on the cluster, where ANP is namespaced meaning its applies to objects within the namespace defined in the policy.\nAntrea policies are tiered, meaning the rules will be following an order of precedence. This makes it very useful to divide the rules into the right categories, having different resources in the organization responsible for the security rules. Sec-ops will have their rules in the beginning setting the \u0026ldquo;ground\u0026rdquo; before the application owners can set their rules and finally some block all rules that rules out all that is left. Antrea-native policy resources is working together with K8s network policies where the latter is placed in the Application tier below the ANP and ACNP policies. Antrea comes with a set of default tiers as of installation of Antrea, but there is also possible to add custom tiers. Read more here. Here are the default tiers:\nEmergency -\u0026gt; Tier name \u0026#34;emergency\u0026#34; with priority \u0026#34;50\u0026#34; SecurityOps -\u0026gt; Tier name \u0026#34;securityops\u0026#34; with priority \u0026#34;100\u0026#34; NetworkOps -\u0026gt; Tier name \u0026#34;networkops\u0026#34; with priority \u0026#34;150\u0026#34; Platform -\u0026gt; Tier name \u0026#34;platform\u0026#34; with priority \u0026#34;200\u0026#34; Application -\u0026gt; Tier name \u0026#34;application\u0026#34; with priority \u0026#34;250\u0026#34; Baseline -\u0026gt; Tier name \u0026#34;baseline\u0026#34; with priority \u0026#34;253\u0026#34; Take a look at the diagram below and imagine your first rules is placed at the first left tier and more rules in the different tiers all the way to the right:\nMaking use of the Antrea-native policy resources # In this section I will describe a demo environment with some namespaces and applications and then go through one way of using Antrea-native policy resources together with K8s network policies. In some of my Antrea-native policies I will use namespace selection based on the actual name of the namespace, but in others I will use selection based on labels. The first example below will make use of labels as selection criteria. I will explain why below. To read more on namespace selection: https://github.com/antrea-io/antrea/blob/main/docs/antrea-network-policy.md#select-namespace-by-name\nMeet the demo environment # I will only use one K8s cluster in this example which is based on one master worker and two worker nodes.\nI will create three \u0026ldquo;environments\u0026rdquo; by using namespaces and label them according to which \u0026ldquo;environment\u0026rdquo; they belong to. The three namespaces will be \u0026ldquo;test-app\u0026rdquo;, \u0026ldquo;dev-app\u0026rdquo; and \u0026ldquo;prod-app\u0026rdquo;. I will then add a label on each namespace with the label env=test, env=dev and env=prod accordingly. Within each namespace I will spin up two pods (an Ubuntu 16.04 and Ubuntu 20.04 pod). What I would like to achieve is that each namespace represents their own environment to simulate scenarios where we do have prod, dev and test environments, where none of the environments are allowed to talk to each other. And by using labels, I can create several namespaces and place them into the correct environment by just \u0026ldquo;tagging\u0026rdquo; them with the correct labels (e.g env=dev).\nNow in the next section I will go through how I can isolate, and control those environments with Antrea-native policy resources.\nAntrea Cluster Network Policies (ACNP) # The first thing I would like to do is to create some kind of basic separation between those environments/namespaces so they cant communicate with each other, and when that is done I can continue to create more granular application policies within each namespace or environment.\nThe first issue I meet is how to create as few rules as possible to just isolate what I know (the tree namespaces which are labeled with three different labels to create my \u0026ldquo;environments\u0026rdquo;) without having to worry about additional namespaces being created and those getting access to the \u0026ldquo;environments\u0026rdquo;. In this example I have already created three namespaces named \u0026ldquo;dev-app\u0026rdquo;, \u0026ldquo;prod-app\u0026rdquo; and \u0026ldquo;test-app\u0026rdquo;. I \u0026ldquo;tag\u0026rdquo; them in Kubernetes with their corresponding \u0026ldquo;env\u0026rdquo; labels: \u0026ldquo;dev\u0026rdquo;, \u0026ldquo;prod\u0026rdquo; and \u0026ldquo;test\u0026rdquo;. The reason I choose that approach is that I then can create several namespaces and choose which environment they belong to instead of doing the selection directly on the name of the namespace. I need to create an Antrea Cluster Network Policy as a \u0026ldquo;default\u0026rdquo; rule for each of my known environments so I can at a minimum guarantee that within each namespace or environment \u0026ldquo;intra-traffic\u0026rdquo; is allowed (traffic within the namespace or namespaces labeled with the same environment label). Meaning that when I do have a complete Antrea-native policy \u0026ldquo;framework\u0026rdquo; in place (with a blocking rule at the end taking care of all that is not specified) I can create new namespaces, but if they are not labeled correctly they will not be allowed to talk to any of my environments. This policy is applied at the SecurityOps tier:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: isolate-dev-env spec: priority: 5 tier: SecurityOps appliedTo: - namespaceSelector: matchLabels: env: dev ingress: - action: Drop from: - namespaceSelector: matchExpressions: - key: env operator: NotIn values: - dev - action: Allow from: - namespaceSelector: matchLabels: env: dev egress: - action: Allow to: - namespaceSelector: matchLabels: env: dev What I am also doing with this ACNP is saying that if you are member of a namespace with the label \u0026ldquo;env=dev\u0026rdquo; you are allowed to ingress the namespace Dev, but not if you are not (\u0026ldquo;operator: NotIn\u0026rdquo; in the ingress namespaceSelector).\nAlso note that I am allowing specifically an Action allow to the dev environment within the same policy, the reason being is that when I apply my block-all-else rule later on it will block intra traffic within the same environment if it is not specifically specified that it is allowed in this rule.\nNow I just have to recreate this policy for my other two namespaces.\nAlso note that in the egress part I am only allowing traffic to namespace with the lavel \u0026ldquo;env=dev\u0026rdquo;. That does not mean right now that I will only allow traffic to anything else, because I don\u0026rsquo;t have any block rules in my cluster yet. Antrea-native policy resources works a bit different than K8s network policies which only supports creating allow policies. In Antrea one can specify both DROP and ALLOW on both INGRESS and EGRESS. I left this with purpose, because I later in will go ahead create a block all rule. Now lets demonstrate this rule:\nBefore applying ACNP namespace isolation rule:\nkubectl get pod -n dev-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-16-04-7f876959c6-p5nxp 1/1 Running 0 9d 10.162.1.57 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ubuntu-20-04-6fb66c64cb-9qg2p 1/1 Running 0 9d 10.162.1.56 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; kubectl get pod -n prod-app -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ubuntu-16-04-7f876959c6-sfdvf 1/1 Running 0 9d 10.162.1.64 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ubuntu-20-04-6fb66c64cb-z528m 1/1 Running 0 9d 10.162.1.65 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Above I list out the pods with IP addresses in my two namespaces \u0026ldquo;dev-app\u0026rdquo; and \u0026ldquo;prod-app\u0026rdquo;\nNow I enter bash of the Ubuntu20.04 pod in \u0026ldquo;dev-app\u0026rdquo; namespace and do a ping to the second pod in the same namespace and then ping another pod in the namespace Prod:\nkubectl exec -it -n dev-app ubuntu-20-04-6fb66c64cb-9qg2p bash root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.57 PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 64 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=0.896 ms 64 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.520 ms 64 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.248 ms root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.64 PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. 64 bytes from 10.162.1.64: icmp_seq=1 ttl=64 time=1.03 ms 64 bytes from 10.162.1.64: icmp_seq=2 ttl=64 time=0.584 ms 64 bytes from 10.162.1.64: icmp_seq=3 ttl=64 time=0.213 ms I have also written about Octant in one of my posts, in Octant there is an Antrea plugin which gives us some graphical features such as traceflow, which is also a powerful tool to showcase/troubleshoot security policies. Below is a screenshot from Octant before the rule is applied:\nAs you can see, this is allowed. Now I apply my \u0026ldquo;isolation\u0026rdquo; ACNP rules \u0026ldquo;prod\u0026rdquo;, \u0026ldquo;dev\u0026rdquo; \u0026amp; \u0026ldquo;test\u0026rdquo;. Also note; to list out the applied ACNP policies the command \u0026ldquo;kubetcl get acnp\u0026rdquo; can be used, without looking in a specific namespace as ACNP is clusterwide.\nkubectl apply -f isolate.environment.prod.negated.yaml clusternetworkpolicy.crd.antrea.io/isolate-prod-env created kubectl apply -f isolate.environment.dev.negated.yaml clusternetworkpolicy.crd.antrea.io/isolate-dev-env created kubectl get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE isolate-dev-env SecurityOps 5 1 1 19s isolate-prod-env SecurityOps 6 1 1 25s After they are applied I will try to do same as above:\nping 10.162.1.57 PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 64 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=3.28 ms 64 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.473 ms 64 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.190 ms 64 bytes from 10.162.1.57: icmp_seq=4 ttl=64 time=0.204 ms ping 10.162.1.64 PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. Pinging within the same namespace works perfect, but to one of the other namespaces (here the Prod namespace) is not allowed. Works as intended.\nDoing the same traceflow with Octant again:\nSo to recap, this is how it looks like now:\nNow that I have created myself some isolated environments, I also need to allow some basic needs from the environments/namespaces to things such as DNS. So I will go ahead and create such a rule. Also have in mind that I haven\u0026rsquo;t yet applied the last block-all-rule (so they can still reach those services as of now). I will make that rule applied when all the necessary rules are in place beforehand. In a greenfield environment those \u0026ldquo;baseline\u0026rdquo; rules would probably be applied as the first thing before the k8s cluster is taken into use.\nGoing down one tier to NetworkOps I will apply this Antrea Policy:\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: allow-core-dns spec: priority: 10 tier: NetworkOps appliedTo: - namespaceSelector: {} egress: - action: Allow to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system ports: - protocol: TCP port: 53 - protocol: UDP port: 53 This policy is probably for some rather \u0026ldquo;wide\u0026rdquo; as it just does a \u0026ldquo;wildcard\u0026rdquo; selection of all namespaces available and gives them access to the backend kube-system (where the coredns pods are located) on protocol TCP and UDP port 53. But again, this post is just to showcase Antrea policies and how they can be used and to give some insights in general.\nDNS allowed showed with Octant:\nOctant Traceflow\nFor now I am finished with the Cluster Policies and will head over to the Network Policies. This is the ACNP policies applied so far:\nkubectl get acnp NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE allow-core-dns NetworkOps 10 3 3 19h isolate-dev-env SecurityOps 5 1 1 22h isolate-prod-env SecurityOps 6 1 1 22h isolate-test-env SecurityOps 7 1 1 19h Antrea Network Policies (ANP) # Antrea Network Policies are namespaced. So one of the use cases for ANP could be to create rules specific for the services running in the namespace. It could be allowing ingress on certain selections (e.g label/frontend) which runs the application I want to expose or which makes sense for clients to talk to which is the frontend part of the application. Everything else is backend services which not necessary to expose to clients, but on the other hand it could be that those services needs access to other backend services or services in other namespaces. So with ANP one can create ingress/egress policies by using the different selection options defining what is allowed in and out of the namespace.\nBefore I continue I have now applied my ACNP block-rule in the \u0026ldquo;Baseline\u0026rdquo; tier which just blocks all else to make sense of the examples used here in this section. Below is the policy (Note that I have excluded some namespaces in this rule) :\napiVersion: crd.antrea.io/v1alpha1 kind: ClusterNetworkPolicy metadata: name: block-all-whitelist spec: priority: 1000 tier: baseline appliedTo: - namespaceSelector: matchExpressions: - key: ns operator: NotIn values: - kube-system - monitoring ingress: - action: Drop from: - namespaceSelector: {} - ipBlock: cidr: 0.0.0.0/0 egress: - action: Drop to: - namespaceSelector: {} - ipBlock: cidr: 0.0.0.0/0 Antrea Network Policies Egress rule # Now that I have applied my \u0026ldquo;whitelist\u0026rdquo; rule I must by now have all my necessary rules in place, otherwise things will stop working (such as access to DNS). I will now apply a policy which is \u0026ldquo;needed\u0026rdquo; by the \u0026ldquo;Prod\u0026rdquo; environment, which is access to SSH on a remote server. So the policy below is allowing Egress on TCP port 22 to this specific remote SSH server. Lets us apply this policy and test how this works out:\napiVersion: crd.antrea.io/v1alpha1 kind: NetworkPolicy metadata: name: allow-prod-env-ssh namespace: prod-app spec: priority: 8 tier: application appliedTo: - podSelector: {} egress: - action: Allow to: - namespaceSelector: matchLabels: kubernetes.io/metadata.name: prod-app - ipBlock: cidr: 10.100.5.10/32 ports: - protocol: TCP port: 22 Just some sanity check before applying the above policy, I am still able to reach all pods within the same namespace due to my \u0026ldquo;isolation ACNP rules\u0026rdquo; even though I have my block all rule applied.\nBut I am not allowed to reach anything outside except what is stated in my DNS rule. If I try to reach my remote SSH server from my \u0026ldquo;Prod\u0026rdquo; namespace I am not allowed. To illustrate this I have entered \u0026ldquo;remoted\u0026rdquo; myself into bash on one of my pods in the Prod namespace and trying to ssh the remote server 10.100.5.10, below is the current result:\nroot@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 ssh: connect to host 10.100.5.10 port 22: Connection timed out Ok, fine. What does my traceflow say about this also:\nNope cant do it also says Traceflow. Great, everything works out as planned. Now I must apply my policy to allow this.\nroot@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 andreasm@10.100.5.10\u0026#39;s password: Now I can finally reach my remote SSH server. To confirm again, lets check with Octant:\nThank you very much, that was very kind of you.\nTo summarize so far what we have done. We have applied the ACNP rules/policies to create environment/namespace isolation\nNAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE allow-core-dns NetworkOps 10 3 3 21h block-all-whitelist baseline 20 2 2 25m isolate-dev-env SecurityOps 5 1 1 24h isolate-prod-env SecurityOps 6 1 1 31m isolate-test-env SecurityOps 7 1 1 21h And we have applied a rule to allow some basic \u0026ldquo;needs\u0026rdquo; such as DNS with the rule allow-core-dns. And the \u0026ldquo;block-all-whitelist\u0026rdquo; policy as a \u0026ldquo;catch all rule\u0026rdquo; to block everything not specified in the tiers.\nAnd then we have applied a more application/namespace specific policy with Antrea Network Policy to allow Prod to egress 10.100.5.10 on port 22/TCP. But I have not specified any ingress rules allow access to any services in the namespace Prod coming from outside the namespace. So it is a very lonely/isolated environment for the moment. This is how it looks like now:\nIn the next example I will create an ingress rule to another application that needs to be accessed from the outside.\nAntrea Network Policies Ingress rule # To make this section a bit more \u0026ldquo;understanding\u0026rdquo; I will use another application as example to easier illustrate the purpose. The example I will be using is a demo application I have been using for several years - Yelb link\nThis application contains of four pods and looks like this:\nYelb diagram\nI already have the application up and running in my environment. But as this application is a bit more complex and contains a frontend which is useless if not exposed or reachable I am exposing this frontend with NSX Advanced Load Balancer. This makes it very easy for me to define the ingress rule as it means I only have to allow the load balancers IPs in my egress rule and not all potential IPs. The load balancers IP\u0026rsquo;s is something I know. Some explanation around the load balancer IP\u0026rsquo;s in my environment is that they are spun up on demand and just pick an IP from a pre-defined IP pool, so instead of pinning the ingress rule to the current IP they have I am bit wide and allow the IP range that is defined. Remember that this is a demo environment and does not represent a production environment. Lets take a look at the policy:\napiVersion: crd.antrea.io/v1alpha1 kind: NetworkPolicy metadata: name: allow-yelb-frontend namespace: yelb spec: priority: 5 tier: application appliedTo: - podSelector: matchLabels: app: yelb-ui ingress: - action: Allow from: - ipBlock: cidr: 10.161.0.0/24 ports: - protocol: TCP port: 80 endPort: 80 name: AllowInYelbFrontend enableLogging: false egress: - action: Allow to: - ipBlock: cidr: 10.161.0.0/24 name: AllowOutYelbFrontend enableLogging: false The CIDR in the rule above is the range my load balancers is \u0026ldquo;living\u0026rdquo; in. So instead to narrow it down too much in this demo I just allow the range 10.161.0.0/24 meaning I dont have to worry too much if they are getting new IP\u0026rsquo;s within this range making my application inaccessible. When I apply this rule it will be placed in the tier \u0026ldquo;Application\u0026rdquo; (See one of the first diagrams in the beginning of this post) with a priority of 5. The basic policies for this application is already in place such as DNS and intra-communication (allowed to talk within the same namespace/environment which in this example is yelb-app/test).\nNow lets see how it is before applying the rule from the perspective of the NSX Advanced Load Balancer which is being asked to expose the frontend of the application:\nFrom NSX Advanced Load Balancer GUI / application showing pool is down\nAs one can see from the above screenshot, the Service Engines (the actual load balancers) are up and running but the application yelb-ui is down because the pool is unreachable. The pool here is the actual pod containing the frontend part of the Yelb app. So I need to apply the Antrea Network Policy to allow the Service Engines to talk to my pod. If I try to access the frontend via the load balancer VIP its also inaccessible:\nLets just apply the rule:\nkubectl apply -f yelb.frontend.allow.yaml networkpolicy.crd.antrea.io/allow-yelb-frontend created kubectl get anp -n yelb NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE allow-yelb-frontend application 5 1 1 11s And now check the NSX Advanced Load Balancer status page and try to access the application through the VIP:\nNSX ALB is showing green\nWell that looks promising, green is a wonderful colour in IT.\nAnd the application is available:\nYelb UI frontend\nThe rule above only gives the NSX Advanced Load Balancers access to the frontend pod on port 80 of the application Yelb. All the other pods are protected by the \u0026ldquo;environment\u0026rdquo; block rule and the default block rule. There is one catch though, we dont have any rules protecting traffic between the pods. Lets say the frontend pod (which is exposed to the outside world) is compromised, there is no rule stopping any traffic coming from this pod to the others within the same namespace/and or environment. That is something we should apply.\nMicrosegmenting the application # What we shall do now is to make sure that the pods that make up the application Yelb is only allowed to talk to each other on the necessary ports and nothing else. Meaning we create a policy that does a selection of the pods and apply specific rules for each pod/service within the application, if one refer to the diagram above over the Yelb application one can also see that there is no need for the fronted pod to be allowed to talk to the redis or db pod at all so that should be completely blocked.\nI will go ahead and apply a rule that does all the selection for me, and only allow what is needed for the application to work. The policy I will make use of here is K8s Native Network Policy kubernetes.io\nHere is the rule:\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-cache namespace: yelb spec: podSelector: matchLabels: tier: cache ingress: - from: - podSelector: matchLabels: tier: middletier - namespaceSelector: matchLabels: tier: middletier ports: - protocol: TCP port: 6379 policyTypes: - Ingress --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-backend namespace: yelb spec: podSelector: matchLabels: tier: backenddb ingress: - from: - podSelector: matchLabels: tier: middletier - namespaceSelector: matchLabels: tier: middletier ports: - protocol: TCP port: 5432 policyTypes: - Ingress --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-middletier namespace: yelb spec: podSelector: matchLabels: tier: middletier ingress: - from: - podSelector: {} - namespaceSelector: matchLabels: tier: frontend ports: - protocol: TCP port: 4567 policyTypes: - Ingress --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: yelb-frontend namespace: yelb spec: podSelector: matchLabels: tier: frontend ingress: - from: ports: - protocol: TCP port: 80 egress: - to: ports: - protocol: TCP port: 30567 policyTypes: - Ingress - Egress As I have already illustrated above I will not go through showing that the pods can talk to each other on all kinds of port, as they can because they do not have any restriction within the same namespace/environment. What I will go through though is how the above policy affects my application.\nThe rule applied:\nkubectl apply -f k8snp_yelb_policy.yaml networkpolicy.networking.k8s.io/yelb-cache created networkpolicy.networking.k8s.io/yelb-backend created networkpolicy.networking.k8s.io/yelb-middletier created networkpolicy.networking.k8s.io/yelb-frontend created kubectl get networkpolicies.networking.k8s.io -n yelb NAME POD-SELECTOR AGE yelb-backend tier=backenddb 80s yelb-cache tier=cache 80s yelb-frontend tier=frontend 80s yelb-middletier tier=middletier 80s So to illustrate I will paste a diagram with the rules applied, and go ahead an see if I am allowed and not allowed to reach pods on ports not specified.\nYelb diagram with policies\nThe first thing I will try is to see if the frontend pod can reach the appserver on the specified port 4567:\nOctant Antrea Traceflow\nAnd the result is in:\nNow, what if I just change the port to something else, say DNS 53\u0026hellip; Will it succeed?\n","date":"10 July 2021","externalUrl":null,"permalink":"/2021/07/10/antrea-network-policies/","section":"Posts","summary":"This post will go through the Antrea-native policy resources and how to use them to secure your Kubernetes environment combined with K8s network policies.","title":"Antrea Network Policies","type":"posts"},{"content":"","date":"10 July 2021","externalUrl":null,"permalink":"/tags/informational/","section":"Tags","summary":"","title":"Informational","type":"tags"},{"content":"This post will go through one way of securing your workloads with VMware NSX. It will cover the different tools and features built into NSX to achieve a robust and automated way of securing your workload. It will go through the use of Security Groups, how they can be utilized, and how to create security policies in the distributed firewall section of NSX-T with the use of the security groups.\nIntroduction to NSX Distributed Firewall # If we take a look inside a modern datacenter we will discover very soon that there is not so much bare metal anymore (physical server with one operating system and often many services to utilize the resources), most workload today is virtualized. From a network perspective the traffic pattern has shifted from being very much north/south to very much east/west. A typical traffic distribution today between north/south and east/west is a 10% (+/-) north/south and 90%(+/-) east/west. When the traffic pattern consisted of a high amount north/south it made sense to have our perimeter firewall regulate and enforce firewall rules in and out of the DC and between server workload. Due to server virtualization a major part of the DC the workload consist of many virtual machine instances with very specific services and \u0026ldquo;intra\u0026rdquo; communication (east/west) is a large part. It is operationally a tough task to manage a perimeter firewall to be the \u0026ldquo;policy enforcer\u0026rdquo; between workload in the east/west \u0026ldquo;zone\u0026rdquo;. It is also very hard for a discrete appliance to be part of the context (it is outside of the dataplane/context of the workload it is trying to protect).. Will delve into this in more detail later in the article. Will also illustrate east/west and north/south traffic pattern.\n","date":"10 July 2021","externalUrl":null,"permalink":"/2021/07/10/microsegmentation-with-vmware-nsx/","section":"Posts","summary":"This post will go through one way of securing your workloads with VMware NSX.","title":"Microsegmentation with VMware NSX","type":"posts"},{"content":"When starting out a microsegmentation journey with VMware NSX it will be very important to have a tool that gives you all the visibility and insight you need. This is crucial if you dont know your applications requirements in detail and to make the right decisions in defining your NSX security policies.\nNSX Intelligence is your tool for that. This post is just to show a couple of screenshots of how it looks, and the next post will go more into detail how it works and how to use it.\n","date":"10 July 2021","externalUrl":null,"permalink":"/2021/07/10/nsx-intelligence-quick-overview/","section":"Posts","summary":"When starting out a microsegmentation journey with VMware NSX it will be very important to have a tool that gives you all the visibility and insight you need.","title":"NSX Intelligence - quick overview","type":"posts"},{"content":"","date":"10 July 2021","externalUrl":null,"permalink":"/categories/nsx-t/","section":"Categories","summary":"","title":"Nsx-T","type":"categories"},{"content":"","date":"10 July 2021","externalUrl":null,"permalink":"/categories/vmare-nsx-intelligence/","section":"Categories","summary":"","title":"Vmare Nsx Intelligence","type":"categories"},{"content":"","date":"10 July 2021","externalUrl":null,"permalink":"/categories/vmware-nsx/","section":"Categories","summary":"","title":"Vmware Nsx","type":"categories"},{"content":"This post will cover the steps to bring up a Kubernetes cluster in Ubuntu, then how to install and configure Antrea as CNI and how to install and configure NSX Advanced Load Balancer to use as a servicetype Load Balancer in the k8s environment with the use of Avi Kubernetes Operator.\nAbbreviations used in this post:\nNSX Advanced Load Balancer = NSX ALB Avi Kubernetes Operator = AKO Kubernetes = k8s Container Network Interface = CNI Load Balancer = lb Introduction to this post # When working with pods in a k8s cluster there is often the use of nodePort, clusterIP and LoadBalancer. In a three tiered application very often only one of the tier is necessary to expose to the \u0026ldquo;outside\u0026rdquo; (outside of the k8s cluster) world so clients can reach the application. There are several ways to do this, and one of them is using the service Load Balancer in k8s. The point of this post is to make use of NSX ALB to be the load balancer when you call for the service load balancer in your application. There are some steps needed to be done to get this up and running, as will be covered in this post, and when done you can enjoy the beauty of automatically provisioned lb\u0026rsquo;s by just stating in your yaml that you want a lb for the specific service/pod/application and NSX ALB will take care of all the configuration needed to make your application available through a VIP. One of the steps involved in making this happen is deploying the AKO in your k8s cluster. AKO runs as a pod and is very easy to deploy and configure.\nDiagram over topology # Deploy Kubernetes on Ubuntu 20.04 # Prepare the Worker and Master nodes. # For this part the initial process was taken from here with modifications from my side.\nThis will be a small 3-node cluster with 1 Master and 2 Worker Nodes.\nInstall 3 Ubuntu VMs, or one and clone from that.\nsudo hostnamectl set-hostname \u0026lt;hostname\u0026gt; By using Ubuntu Server as image make sure openssh is installed.\nDisable Swap\nsudo swapoff -a Edit fstab and comment out the swap entry.\nVerify with the command:\nfree -h Install packages on the master and worker nodes: # On all nodes: # sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl Add Kubernetes repository:\nAdd key to Apt:\nsudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Add repo:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF Install Kubeadm, Kubelet and Kubectl\nsudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl Note, this will install default Kubernetes release in apt repo from your distro, if you want to install a specific version, follow this:\nOverview of differente Kubernets versions and dependices:\n_https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/Packages\n_Lets say we want to install Kubeadmin 1.18.9:\nsudo apt-get install kubelet=1.18.9-00 kubectl kubeadm=1.18.9-00 Check the below link if you can install a later version of kubectl than kubeadmin\nhttps://kubernetes.io/docs/setup/release/version-skew-policy/\nThe latter apt-get install command install kubelet version 1.18.9 and kubeadm 1.18.9 but kubectl will be the default release in apt (1.19.x for me).\nAfter the the three packages have been installed, set them on hold:\nsudo apt-mark hold kubelet kubeadm kubectl kubelet set on hold. kubeadm set on hold. kubectl set on hold. Install Docker container runtime:\nsudo apt-get install docker.io -y On the master-node init the Kubernetes master worker: # sudo kubeadm init --pod-network-cidr=10.162.0.0/16 --apiserver-cert-extra-sans apihost.corp.local Change the CIDR accordingly to match your defined pod-network. It comes down to if you want to do a routable or nat’ed toplogy. And by using the –apiserver-cert-extra variable it will generate the certs to also reflect the dns-name, making it easier to expose this with a name instead of just the ip of the master worker.\nOn the worker-nodes, join the control plane of the master worker: # sudo kubeadm join apihost.corp.local:6443 --token \u0026lt;toke\u0026gt; --discovery-token-ca-cert-hash \u0026lt;hash\u0026gt; The token and hash is presented to you after you have init’d the master-node in the previous step above.\nAfter you have joined all you worker nodes to the control-plane you have a working Kubernetes cluster, but without any pod-networking plugin (CNI), this will be explained a bit later and is also the reason for creating this guide.\nTo make it easier to access your cluster copy the kube config to your $HOME/.kube folder:\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You can now to a kubectl get pods \u0026ndash;all-namespaces and notice that the coredns pods have status pending. That is because there is not pod networking up and running yet.\nInstall Antrea as the CNI # **To read more about Antrea, go here:\n**https://github.com/vmware-tanzu/antrea\nTo get started with Antrea is very easy.\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/getting-started.md\nI will post a dedicated post for Antrea, covering Antrea Policies and other features in Antrea.\nOn the master worker:\nDecide first which version/release of Antrea you want (latest as of this writing is v1.1.1), this could depend on certain dependencies with other solutions you are going to be using. In my case I am using VMware Advanced LoadBalancer AKO v1.4.2 (Previously AVI Networks) as Service LB for my pods/apps. See this for compatibility guide: https://avinetworks.com/docs/ako/1.4/ako-compatibility-guide/\nTag the release you want to be installed of Antrea:\n_TAG=v1.1.1\n_Again, this is all done on the master-worker\nThen apply:\nkubectl apply -f [https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml](https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml)\nThis will automatically create the needed interfaces on all the workers. After a short time check your pods:\nKubectl get pods --all-namespaces kube-system antrea-agent-bzdkx 2/2 Running 0 23h kube-system antrea-agent-lvdxk 2/2 Running 0 23h kube-system antrea-agent-zqp6d 2/2 Running 0 23h kube-system antrea-controller-55946849c-hczkw 1/1 Running 0 23h kube-system antrea-octant-f59b76dd9-6gj82 1/1 Running 0 10h kube-system coredns-66bff467f8-6qz2q 1/1 Running 0 23h kube-system coredns-66bff467f8-cd6dw 1/1 Running 0 23h Coredns is now running and antrea controller/agents have been installed.\nIf you look at your worker nodes, they have also been configured with some extra interfaces:\n4: ovs-system:\n5: genev_sys_6081:\n6: antrea-gw0:\n**You now have a working container network interface, what now?\n**Wouldn’t it be cool to utilize a service load balancer to easily scale and expose your frontends?\nInstall AVI Kubernetes Operator # Here comes AKO (Avi Kubernetes Operator), VMware Advanced LoadBalancer\nTo read more about AKO visit: https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes and https://avinetworks.com/docs/ako/1.4/ako-release-notes/\nTo be able to install AKO there are some prereqs that needs to be done on both your k8s cluster and NSX-ALB controller.\nPrepare NSX-ALB for AKO # Lets start with the pre-reqs on the NSX-ALB controller side by logging into the controller GUI.\nDisclaimer, I have followed some of the official documentation here: https://avinetworks.com/docs/ako/1.2/ako-installation/ but I couldn’t follow everything there as it did not work in my environment.\nThis guide assumes NSX-ALB has been configured with a default-cloud with vCenter and the networks for your VIP subnet, node-network already has been defined as vDS portgroups in vCenter. In additon to basic knowledge of NSX-ALB.\nIn the NSX-ALB Controller\nCreate a dedicated SE group for the K8s cluster you want to use with AVI, define it the way you would like:\nCreate DNS and IPAM profiles\nIn IPAM define this:\nName, cloud and the VIP network for the SEs frontend facing IP. The network that should be reachable outside the node-network.\nGo to infrastructure and select your newly created IPAM/DNS profiles in your default-cloud:\nWhile you are editing the default-cloud also make sure you have configured these settings:\nDHCP is for the management IP interface on the SEs. The two other options “Prefer Static” \u0026amp; Use Static” I have to use otherwise it will not work, the AKO pod refuses to start, crashloopbackoff. And I am not doing BGP on the Avi side, only from the T0 of NSX. One can use the default global routing context as it can be shared, no need to define a custom routing context.\nUnder network select the management network and your SE group created for AKO SEs. And ofcourse enable DHCP:\nNow we are done on the NSX-ALB controller side. Lets jump back to our K8s Master worker to install AKO.\nThe initial steps here I have taken from:\nhttps://www.vrealize.it/2020/09/15/installing-antrea-container-networking-and-avi-kubernets-operator-ako-for-ingress/\nInstall AKO: # AKO is installed with helm, so we need to install helm on our master worker:\nsudo snap install helm --classic\nOr whatever preferred way to install packages in Ubuntu.\n**Create a namespace for the AKO pod:\n**\nkubectl create ns avi-system **Add AKO incubator repository\n**\nhelm repo add ako https://projects.registry.vmware.com/chartrepo/ako **Search for available charts and find the version number:\n**\nhelm search repo NAME CHART VERSION\tAPP VERSION\tDESCRIPTION ako/ako 1.4.2 1.4.2 A helm chart for Avi Kubernetes Operator Download AKO values.yaml\nhelm show values ako/ako --version 1.4.2 \u0026gt; values.yaml This needs to be edited according to your needs. See the comments in the value.yaml and you will se whats needed to be updated.\nI will publish my value.yaml file here and comment what I edited.\nI will remove the default comments and replace them with my own, just refer to de default comments by looking at the original yaml file here:\nhttps://raw.githubusercontent.com/avinetworks/avi-helm-charts/master/charts/stable/ako/values.yaml\n# Default values for ako. # This is a YAML-formatted file. # Declare variables to be passed into your templates. replicaCount: 1 image: repository: avinetworks/ako pullPolicy: IfNotPresent ### This section outlines the generic AKO settings AKOSettings: logLevel: \u0026#34;INFO\u0026#34; #enum: INFO|DEBUG|WARN|ERROR fullSyncFrequency: \u0026#34;1800\u0026#34; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. apiServerPort: 8080 # Specify the port for the API server, default is set as 8080 // EmptyAllowed: false deleteConfig: \u0026#34;false\u0026#34; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI disableStaticRouteSync: \u0026#34;false\u0026#34; # If the POD networks are reachable from the Avi SE, set this knob to true. clusterName: \u0026#34;GuzK8s\u0026#34; # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT cniPlugin: \u0026#34;\u0026#34; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift ### This section outlines the network settings for virtualservices. NetworkSettings: ## This list of network and cidrs are used in pool placement network for vcenter cloud. ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. # nodeNetworkList: [] nodeNetworkList: - networkName: \u0026#34;Native-K8s-cluster\u0026#34; cidrs: - 192.168.0.0/24 # NODE network subnetIP: \u0026#34;10.150.4.0\u0026#34; # Subnet IP of the vip network subnetPrefix: \u0026#34;255.255.255.0\u0026#34; # Subnet Prefix of the vip network networkName: \u0026#34;NativeK8sVIP\u0026#34; # Network Name of the vip network ### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. L7Settings: defaultIngController: \u0026#34;true\u0026#34; l7ShardingScheme: \u0026#34;hostname\u0026#34; serviceType: \u0026#34;ClusterIP\u0026#34; #enum NodePort|ClusterIP shardVSSize: \u0026#34;SMALL\u0026#34; # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL passthroughShardSize: \u0026#34;SMALL\u0026#34; # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL ### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. L4Settings: defaultDomain: \u0026#34;\u0026#34; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. ### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. ControllerSettings: serviceEngineGroupName: \u0026#34;k8s2se\u0026#34; # Name of the ServiceEngine Group. controllerVersion: \u0026#34;20.1.1\u0026#34; # The controller API version cloudName: \u0026#34;Default-Cloud\u0026#34; # The configured cloud name on the Avi controller. controllerIP: \u0026#34;172.18.5.50\u0026#34; nodePortSelector: # Only applicable if serviceType is NodePort key: \u0026#34;\u0026#34; value: \u0026#34;\u0026#34; resources: limits: cpu: 250m memory: 300Mi requests: cpu: 100m memory: 75Mi podSecurityContext: {} # fsGroup: 2000 avicredentials: username: \u0026#34;admin\u0026#34; password: \u0026#34;PASSWORD\u0026#34; service: type: ClusterIP port: 80 persistentVolumeClaim: \u0026#34;\u0026#34; mountPath: \u0026#34;/log\u0026#34; logFile: \u0026#34;avi.log\u0026#34; Remember that YAML’s are indentation sensitive.. so watch your spaces 😉\n**Deploy the AKO controller:\n**helm install ako/ako --generate-name --version 1.4.2 -f avi-values.yaml --namespace=avi-system\nOne can verify that it has been installed with the following command:\nhelm list --namespace=avi-system\nand\nKubectl get pods --namespace=avi-system\nNAME READY STATUS RESTARTS AGE\nako-0 1/1 Running 0 15h\nOne can also check the logs of the pod:\nkubectl logs --namespace=avi-system ako-0\nIf you experience a lot of restarts, go through your config again, I struggled a lot to get it running in my lab the first time after I figured out there were some configs I had to to. I suspected some issues with the pod itself, but the problem was on the NSX-ALB controller side and values.yaml parameters.\nNow to test out the automatic creation of SE and ingress for your frontend install an application and change the service type to use loadBalancer. Everything is automagically created for you. Monitor progress in the NSX-ALB Controller and vCenter.\nNSX-ALB deploys the SE OVAs, as soon as they are up and running they will be automatically configured and you can access your application through the NSX-ALB VIP IP. You can of ofcourse scale the amount of SEs and so on from within the Avi controller.\nThe ips on the right side is the pods, and my frontend/public facing IP is:\nWhich can also be received by using:\nkubectl get service --namespace=app\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\nfrontend-external LoadBalancer 10.96.131.185 10.150.4.2 80:31874/TCP 15h\nNow we have all the power and features from NSX Advanced LoadBalancer, with full visibility and logging. What about monitoring and features of Antra, the CNI plugin?\nInstall Octant - opensource dashboard for K8s # https://github.com/vmware-tanzu/octant\nOctant with Antrea Plugin as a POD\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/octant-plugin-installation.md\nUpgrading the components used in this blog # Upgrade Antrea # Say you are running Antrea version v0.9.1 and want to upgrade to v0.10.1. Do a rolling upgrade like this:\nFirst out is the Antrea-Controller!\nkubectl set image deployments/antrea-controller antrea-controller=antrea/antrea-ubuntu:v0.10.1 \u0026ndash;namespace=kube-system\nCheck status on upgrade:\nkubectl rollout status deployments/antrea-controller \u0026ndash;namespace=kube-system\nThis upgrades the Antrea Controller, next up is the Antrea Agents.\nUpgrading the Antrea-agents:\nkubectl set image daemonset/antrea-agent antrea-agent=antrea/antrea-ubuntu:v0.10.1 \u0026ndash;namespace=kube-system\nAs you are doing the rolling upgrades, one can monitor the process by following the pods or use Octant and get real-time updates when the agents have been upgraded.\nDoing changes in the antrea yaml file, if changes is not updated, do a rollut restart:\nkubectl rollout restart daemonset \u0026ndash;namespace=kube-system antrea-agent\n","date":"8 October 2020","externalUrl":null,"permalink":"/2020/10/08/nsx-advanced-loadbalancer-with-antrea-on-native-k8s/","section":"Posts","summary":"This post will cover the steps to bring up a Kubernetes cluster in Ubuntu, then how to install and configure Antrea as CNI and how to install and configure NSX Advanced Load Balancer to use as a servicetype Load Balancer in the k8s environment with the use of Avi Kubernetes Operator.","title":"NSX Advanced LoadBalancer with Antrea on Native K8s","type":"posts"},{"content":"To be able to deploy an Edge node or nodes in your lab or other environment where you only have 2 physical nic you must be able to deploy it on a N-VDS switch as you have already migrated all your kernels etc to this one N-VDS switch.\nBut trying to do this from the NSX-T 2.4 manager GUI you will only have the option to deploy it to VSS or VDS portgroups, the N-VDS portgroups are not visible at all.\nSo, I followed this blog by Amit Aneja which explains how this works.\nSo after I read this blog post I sat out to try this. I had to write down the api-script he used by hand because I could not find it when I searched for a example I could use. By using PostMan I filled out this:\n{ \u0026#34;resource\\_type\u0026#34;: \u0026#34;EdgeNode\u0026#34;, \u0026#34;display\\_name\u0026#34;: \u0026#34;YourEdgevCenterInventoryName\u0026#34;, \u0026#34;tags\u0026#34;: \\[\\], \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34; (Your edge MGMT IP adress) \\], \u0026#34;deployment\\_config\u0026#34;: { \u0026#34;vm\\_deployment\\_config\u0026#34;: { \u0026#34;placement\\_type\u0026#34;: \u0026#34;VsphereDeploymentConfig\u0026#34;, \u0026#34;vc\\_id\u0026#34;: \u0026#34;YourvCenterIDFromNSXTManager\u0026#34;, \u0026#34;management\\_network\\_id\u0026#34;: \u0026#34;YourLSPortGroupIDFromNSXTManager\u0026#34;, \u0026#34;default\\_gateway\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34; \\], \u0026#34;compute\\_id\u0026#34;: \u0026#34;YourClusterIDFromNSXTManager\u0026#34;, \u0026#34;allow\\_ssh\\_root\\_login\u0026#34;: true, \u0026#34;enable\\_ssh\u0026#34;: true, \u0026#34;hostname\u0026#34;: \u0026#34;yourEdge\\_FQDNName\u0026#34;, \u0026#34;storage\\_id\u0026#34;: \u0026#34;YourDataStoreIDfromNSXTManager\u0026#34;, \u0026#34;management\\_port\\_subnets\u0026#34;: \\[ { \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;YourEdgeIPMGMT\\_AddressAgain\u0026#34; \\], \u0026#34;prefix\\_length\u0026#34;: 24 } \\], \u0026#34;data\\_network\\_ids\u0026#34;: \\[ \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_OverLayVLAN(NotTheHostOverlayVLAN)\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34; \\] }, \u0026#34;form\\_factor\u0026#34;: \u0026#34;SMALL\u0026#34;, \u0026#34;node\\_user\\_settings\u0026#34;: { \u0026#34;cli\\_username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;root\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34;, \u0026#34;cli\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34; } } } Then POST it to your NSX-T manager from Postman and after a short blink, the Edge is deployed, and you have to add it as a transport node in the NSX-T manager. Here it is important that you do this right at once, because (as I found out) this is a one-time config GUI where the first time you will be able to choose the right fp-eth nics. If you try to edit the edge deployment a second time it switches back to only showing the VDS/VSS portgroups. Then you have to redeploy.\nExample screenshots:\nRemember that the Uplink VLANs will belong to their own N-VDS (which you have already defined in their respective Transport Zone) which will not be created on the host, but the Edges. The first N-VDS are already in place on the Hosts. Its only the last two NICs which will be on their own Edge N-VDS switches.\nI am not saying this is a best practice or the right way to do this, but it works in my lab environment so I can fully test out the latest NSX-T 2.4 and continue playing with PKS (when we get CNI plugins for NSX-T 2.4\u0026hellip; are we there yet? are we there yet, are we there yet\u0026hellip;.. its hard to wait ;-) )\n","date":"9 March 2019","externalUrl":null,"permalink":"/2019/03/09/deploy-nsx-t-2.4-edge-nodes-on-a-n-vds-logical-switch/","section":"Posts","summary":"To be able to deploy an Edge node or nodes in your lab or other environment where you only have 2 physical nic you must be able to deploy it on a N-VDS switch as you have already migrated all your kernels etc to this one N-VDS switch.","title":"Deploy NSX-T 2.4 Edge Nodes on a N-VDS Logical Switch","type":"posts"},{"content":"","date":"9 March 2019","externalUrl":null,"permalink":"/categories/troubleshooting/","section":"Categories","summary":"","title":"Troubleshooting","type":"categories"},{"content":"If you have missing objects in the firewall section before upgrading from NSX-T 2.1 to 2.2 you will experience a General Error in the GUI, on the Dashboard, and in the Firewall section of the GUI. You will even get general error when doing API calls to list the DFW sections https://NSXMGRIP/api/v1/firewall/sections: { \u0026quot;module\\_name\u0026quot; : \u0026quot;common-services\u0026quot;, \u0026quot;error\\_message\u0026quot; : \u0026quot;General error has occurred.\u0026quot;, \u0026quot;details\u0026quot; : \u0026quot;java.lang.NullPointerException\u0026quot;, \u0026quot;error\\_code\u0026quot; : \u0026quot;100\u0026quot; }\nIf you have upgraded the fix is straight forward. Go to the following KB and dowload the attached jar file.\nUpload this jar file to the NSX-T manager by logging in with root and do a scp command from where you downloaded it. ex: \u0026quot;scp your\\_username@remotehost:nsx-firewall-1.0.jar /tmp\u0026quot;\nThen replace the existing file with the one from the kb article placed here: /opt/vmware/proton-tomcat/webapps/nsxapi/WEB-INF/lib#\nReboot\n","date":"5 September 2018","externalUrl":null,"permalink":"/2018/09/05/general-error-nsx-t-manager-firewall-section/","section":"Posts","summary":"If you have missing objects in the firewall section before upgrading from NSX-T 2.","title":"\"General Error\" NSX-T Manager Firewall Section","type":"posts"},{"content":"","date":"5 September 2018","externalUrl":null,"permalink":"/tags/distributed-firewall/","section":"Tags","summary":"","title":"Distributed-Firewall","type":"tags"},{"content":"","date":"5 September 2018","externalUrl":null,"permalink":"/tags/troubleshooting/","section":"Tags","summary":"","title":"Troubleshooting","type":"tags"},{"content":"","date":"10 April 2017","externalUrl":null,"permalink":"/categories/virtualization/","section":"Categories","summary":"","title":"Virtualization","type":"categories"},{"content":"After upgrading vCenter (VCSA) to 6.0 U3 and upgrading the vSphere Replication Appliance to 6.1.2 the plugin in vCenter stops working. Follow this KB\nAnd this KB to enable SSH (for those who are unfamiliar with how to enable SSH in a *nix environment):\n","date":"10 April 2017","externalUrl":null,"permalink":"/2017/04/10/vsphere-replication-6.1.2-vcenter-plugin-fails-after-upgrade-vcenter-6.0-u3/","section":"Posts","summary":"After upgrading vCenter (VCSA) to 6.","title":"vSphere Replication 6.1.2 vCenter plugin fails after upgrade (vCenter 6.0 U3)","type":"posts"},{"content":"","date":"10 April 2017","externalUrl":null,"permalink":"/tags/vsphere-replication/","section":"Tags","summary":"","title":"Vsphere-Replication","type":"tags"},{"content":"Welcome to my blog.\nThis is the personal blog of Andreas Marqvardsen. He is currently a Systems Engineer @Arista Networks and have been working in the IT industry since 2001. Joined Arista Networks in 2024, before that he worked for VMware from 2019 as a Solution Engineer in the Networking and Security Business unit focusing on NSX and NSX Advanced LoadBalancer, joined Tanzu Business Unit in 2022 covering everything Kubernetes infrastructure related with focus on network and security.\nAlways interested in new technology, understanding how the technology works and how it can be used. This blog is mostly around what he does in his line of work, but it can also reflect some other stuff he finds interesting from time to time.\nAreas of interest: Kubernetes, Virtulization, Neworking, Security, Linux, OpenSource solutions, DiY projects, Home-Automation, Electronics..\nHe created this blog page many years ago where one of the purposes was to use it as his \u0026ldquo;knowledge\u0026rdquo; base for his own use but also at the same time share it publicly for others if it can provide some useful information.\nAll views expressed on this site is solely his own personal views, not his employer or partners.\nContact:\nE-mail: andreas.marqvardsen[at]gmail.com LinkedIn\n","date":"14 July 2016","externalUrl":null,"permalink":"/about/","section":"blog.andreasm.io","summary":"Welcome to my blog.","title":"About","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]