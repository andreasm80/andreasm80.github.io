[{"body":"","link":"https://blog.andreasm.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.andreasm.io/","section":"","tags":null,"title":"From 0.985mhz... to several Ghz"},{"body":"","link":"https://blog.andreasm.io/categories/kubernetes/","section":"categories","tags":null,"title":"Kubernetes"},{"body":"","link":"https://blog.andreasm.io/tags/kubernetes-api-endpoint/","section":"tags","tags":null,"title":"Kubernetes-api-endpoint"},{"body":"","link":"https://blog.andreasm.io/tags/load-balancing/","section":"tags","tags":null,"title":"load-balancing"},{"body":"","link":"https://blog.andreasm.io/categories/network/","section":"categories","tags":null,"title":"network"},{"body":"","link":"https://blog.andreasm.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://blog.andreasm.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://blog.andreasm.io/tags/tkgs/","section":"tags","tags":null,"title":"TKGs"},{"body":"vSphere with Tanzu and HAProxy Using HAProxy is an option if in need of a an easy and quick way to deploy vSphere with Tanzu. There is no redundancy built into this approach as the HAProxy appliance only consist of one appliance. A support contract is directly with HAProxy not VMware. I would probably recommend this approach for poc, smaller environments, test environments etc. In production environments I would highly recommend NSX Advanced Loadbalancer for many many reasons like resiliency, scalbility and a whole bunch of enterprise features. See my post covering vSphere with Tanzu and NSX-ALB here and here\nBefore I begin this post I will just quickly go through the two different ways HAProxy can be deployed to support a vSphere with Tanzu deployment. The two options are called Default and Frontend. The difference being how the HAProxy network is configured. Lets start with the Default option: The Default option basically means you will deploy the HAProxy appliance using only 2 network interfaces. One network interface is being used for management and the second one will be a combined interface for both VIP(frontend) and data-network(workload network).\nI dont like this approach as it does not give me the option to have a separation between the workload network and the frontend network, meaning there is no distinction networking/firewall wise when restricting traffic from any to the VIP of the kubernetes api. For more information on this topology have a look here in the VMware official documentation.\nSo I will opt for the second option, Frontend, when using HAProxy in vSphere with Tanzu.\nThis option will deploy the HAProxy appliance with three network interfaces. A separate interface for management, workload and frontend (VIP/Kubernetes API). When users hit the VIP or the frontend network assigned it is a separate network interface on the HAProxy appliance, and will forward the traffic (using routes internally in the HAProxy) to forward the requests to the correct kubernetes api in the workload network. It also supports any additional workload networks which I will show later. For more information on the Frontend topology head over here\nThats it for the introduction, lets do the actual implementation of this.\nInstall HAProxy First thing first, prepare the three network in vCenter using either VDS portgroups or NSX backed overlay segments. 1 portgroup for the management interface, one for the default workload network and one for the HAProxy frontend network. In my setup I will be using this IP information:\nManagement network - NSX backed overlay segment: Network = 10.13.10.0/24, GW = 10.13.10.1, HAProxy Interface IP = 10.13.10.199. Routed Workload network - VLAN backed VDS portgroup: Network = 10.13.201.0/24, GW = 10.13.201.1, HAProxy Interface IP = 10.13.201.10. Routed Frontend network - VLAN backed VDS portgroup: Network = 10.13.200.0/24, GW = 10.13.200.1, HAProxy Interface IP = 10.13.200.10. Routed When the networks is ready it is time to deploy the HAProxy appliance/vm itself. To deploy the HAProxy I will use the OVA approach described here. Then it is getting hold of the HAProxy OVA to be used for this purpose which can be found here, or directly to the OVA version 0.2.0 here.\nBefore deploying the OVA prepare the additional information below:\nDNS server = 172.24.3.1 LoadBalancer IP Range = 10.13.200.32/27 - this is just a range I have defined inside the HAProxy frontend network. The Frontend network is 10.13.200.0/24 (where my HAProxy will use 10.13.200.10) and the range I will make available for HAProxy to use is 10.13.200.32/27 which gives me 30 usable addresses x.x.x.33-x.x.x.62. There is also the option to add more ranges e.g 10.13.200.64/27 and so on. To save some time I will skip directly to the actual ova deployment from my vCenter server.\nHAProxy OVA deployment The next step below is where I select the network topology, I will here choose Frontend\nIn the step below I will select my three prepared networks accordingly.\nEnter the root password for the appliance/os itself, check Permit Root Login (useful for troubleshooting and for this demo), leave the TLS sections default blank.\nFill in hostname if wanted, DNS server(s) the appliance management ip in CIDR format 10.13.10.199/24 and the gateway 10.13.10.1.\nThen it is the workload related network settings. Workload IP 10.13.201.10/24 and gateway 10.13.201.1. The Additional Workload Networks can be ignored (it does not seem to have any effect), I will go through that later adding additional workload networks. Then it is the Frontend IP = 10.13.200.10/24. It does say optional, but its only optional if doing the Default setup with only two nics.\nFrontend gateway = 10.13.200.1. Not optional when doing the Frontend setup. Then it is the range you want HAProxy to use for VIP. I have defined a range of 30 usable address by filling in the CIDR 10.13.200.32/27 as below. This can be one of many ranges if you want. Leave the port as default, if not in need of changing it. Then I define a custom API username for the Supervisor to use against the HAProxy, in my case admin.\nThen it is the password for the API admin user. The password I have chosen is ..... .... ..\nThen do a quick review and make sure everything is there. Click finish and wait.\nSSH into the HAProxy appliance When the appliance has been deployed and powered on, ssh into the HAProxy appliance using its IP address in the management network and using root as username and password defined above.\n1ssh root@10.13.10.199 2The authenticity of host \u0026#39;10.13.10.199 (10.13.10.199)\u0026#39; can\u0026#39;t be established. 3ED25519 key fingerprint is SHA256:Asefsdfgsdfsdgsdg5kyKHX0. 4This key is not known by any other names 5Are you sure you want to continue connecting (yes/no/[fingerprint])? yes 6Warning: Permanently added \u0026#39;10.13.10.199\u0026#39; (ED25519) to the list of known hosts. 7(root@10.13.10.199) Password: 8 12:06:48 up 2 min, 0 users, load average: 0.15, 0.12, 0.05 9tdnf update info not available yet! 10root@haproxy [ ~ ]# The reason I want to log in here is because I need the CA.CRT generated so I can tell the Supervisor to use that later on, but also to do some information dump on certain config files that will be good to know about.\nThe most important part, the certificate. When enabling the Supervisor later I will need the certificate. Grab this by using this command:\n1root@haproxy [ ~ ]# cat /etc/haproxy/ca.crt 2-----BEGIN CERTIFICATE----- 3MIIDJ6HD6qCAn3KyQ0KNTJiqT7HgwRvFgZoHcG9vWXjvZy2 4..................................... 5qUOOIbrvQStRTqJl/DjEKi0FyR3lJ1OTOHAbD4YJltSixLkVHRqEH/PY0CbwUNib 6wEkCcibnNLMVLvqvXmyvj0x/Hg== 7-----END CERTIFICATE----- Then there is a couple of config files that is relevant to know about, it would be interesting to have a look at them now before enabling the supervisor and another look after it has been enabled. So lets have a look now.\nI will just run all of them in the same window:\n1########## haproxy.cfg ################ 2root@haproxy [ ~ ]# cat /etc/haproxy/haproxy.cfg 3global 4 log stdout format raw local0 info 5 chroot /var/lib/haproxy 6 stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin expose-fd listeners 7 stats timeout 30s 8 user haproxy 9 group haproxy 10 master-worker 11 # Setting maxconn in the global section is what successfully sets the ulimit on the 12 # host, otherwise we run out of file descriptors (defaulted at 4096). 13 maxconn 60000 14 # This property indicates the number of maximum number of reloads a worker 15 # will survive before being forcefully killed. This number is required to control 16 # the rate at which processes can scale due to the number of reloads outscaling 17 # the rate processes are reaped when all of their connections have been cleaned up. 18 # This number was derived by taking the average virtual memory consumption for a 19 # single HA Proxy process under load, ~28MB, and allocating HA Proxy 3GB out of 4GB 20 # of the total virtual memory space. 21 mworker-max-reloads 100 22 23 # Default SSL material locations 24 ca-base /etc/ssl/certs 25 crt-base /etc/ssl/private 26 27 # Default ciphers to use on SSL-enabled listening sockets. 28 # For more information, see ciphers(1SSL). This list is from: 29 # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ 30 # An alternative list with additional directives can be obtained from 31 # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy 32 ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS 33 ssl-default-bind-options no-sslv3 34 35defaults 36 mode tcp 37 log global 38 option tcplog 39 option dontlognull 40 option tcp-smart-accept 41 timeout check 5s 42 timeout connect 9s 43 timeout client 10s 44 timeout queue 5m 45 timeout server 10s 46 # tunnel timeout needs to be set at a lowish value to deal with the frequent 47 # reloads invoked by dataplaneapi at scale. With a higher value set, established 48 # connections will hang around and prevent haproxy from killing all off older processes 49 # because those old processes won\u0026#39;t terminate those established connections unless 50 # it is told to do so. Having these processes linger for too long can eventually 51 # starve the system of resources as the spawn rate of processes exceeds the death rate. 52 timeout tunnel 5m 53 timeout client-fin 10s 54 55# Stats are disabled by default because enabling them on a non-local IP address 56# would result in allocating a port that could result in a conflict with one 57# of the binds programmed at runtime. 58# 59# To enable stats, uncomment the following section and replace SYSTEM_IP_ADDR 60# with the IP address of the HAProxy host. 61#frontend stats 62# mode http 63# bind SYSTEM_IP_ADDR:8404 64# stats enable 65# stats uri /stats 66# stats refresh 500ms 67# stats hide-version 68# stats show-legends 69 70userlist controller 71 user admin password $dfsdfsdf/mdsfsdfb$6/xsdfsdfsdf379dfsdfMOJ/W1 72##################### haproxy.cfg - no entries yet for any virtual services ############################### 73 74##################### any-ip-routes ########################### 75root@haproxy [ ~ ]# cat /etc/vmware/anyip-routes.cfg 76# 77# Configuration file that contains a line-delimited list of CIDR values 78# that define the network ranges used to bind the load balancer\u0026#39;s frontends 79# to virtual IP addresses. 80# 81# * Lines beginning with a comment character, #, are ignored 82# * This file is used by the anyip-routes service 83# 8410.13.200.32/27 85################### any-ip-routes ######################### 86 87################## route-tables ####################### 88root@haproxy [ ~ ]# cat /etc/vmware/route-tables.cfg 89# 90# Configuration file that contains a line-delimited list of values used to 91# create route tables on which default gateways are defined. This enables 92# the use of IP policy to ensure traffic to interfaces that do not use the 93# default gateway is routed correctly. 94# 95# * Lines beginning with a comment character, #, are ignored 96# * This file is used by the route-tables service 97# 98# Each line that contains a value must adhere to the following, 99# comma-separated format: 100# 101# \u0026lt;TableID\u0026gt;,\u0026lt;TableName\u0026gt;,\u0026lt;MACAddress\u0026gt;,\u0026lt;NetworkCIDR\u0026gt;,\u0026lt;Gateway4\u0026gt; 102# 103# The fields in the above format are as follows: 104# 105# * TableID The route table ID. This value should be an integer between 106# 2-250. Please see /etc/iproute2/rt_tables for a list of the 107# route table IDs currently in use, including reserved IDs. 108# 109# * TableName The name of the route table. This value will be appended 110# to a constant prefix, used to identify route tables managed 111# by the route-tables service. 112# 113# * MACAddress The MAC address of the interface connected to the network 114# specified by NetworkCIDR 115# 116# * NetworkCIDR The CIDR of the network to which the interface by MACAddress 117# is connected 118# 119# * Gateway4 The IPv4 address of the gateway used by the network specified 120# by NetworkCIDR 121# 122# For example, the following lines are valid values: 123# 124# 2,frontend,00:00:00ðŸ†Žcd:ef,192.168.1.0/24,192.168.1.1 125# 3,workload,00:00:00:12:34:56,192.168.2.0/24,192.168.2.1 126# 127 1282,workload,00:50:56:b4:ca:fd,10.13.201.10/24,10.13.201.1 1292,workload,00:50:56:b4:ca:fd,10.13.201.10/24 1303,frontend,00:50:56:b4:ef:d1,10.13.200.10/24,10.13.200.1 1313,frontend,00:50:56:b4:ef:d1,10.13.200.10/24 132#################################### route-tables #################### 133 134########### iproute2 tables ################## 135root@haproxy [ ~ ]# cat /etc/iproute2/rt_tables 136# 137# reserved values 138# 139255\tlocal 140254\tmain 141253\tdefault 1420\tunspec 143# 144# local 145# 146#1\tinr.ruhep 1472\trtctl_workload 1482\trtctl_workload 1493\trtctl_frontend 1503\trtctl_frontend 151################ iproute2 tables ################## One can also view the current configuration of the HAProxy appliance under vApp Settings in vCenter like this:\nThis concludes the deployment of the HAProxy appliance, next step is to enable the Supervisor using this HAProxy appliance for the L4 VIPs.\nEnable Supervisor in vSphere using HAProxy Head over to workload management in vCenter and click get started\nSit back and wait.... and wait.....\nAnd after some waiting it should be green and ready and the Supervisor Control Plane IP should be an IP address from the LoadBalancer range:\nNow I just want to check the HAProxy config (/etc/haproxy/haproxy.cfg) to see if there has been any update there:\n1haproxy.cfg -\u0026gt; 2frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc 3 mode tcp 4 bind 10.13.200.34:443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:nginx 5 bind 10.13.200.34:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:kube-apiserver 6 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc 7 option tcplog 8 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver if { dst_port 6443 } 9 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx if { dst_port 443 } 10 11frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller 12 mode tcp 13 bind 10.13.200.33:2112 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:ctlr 14 bind 10.13.200.33:2113 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:syncer 15 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller 16 option tcplog 17 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer if { dst_port 2113 } 18 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr if { dst_port 2112 } 19 20backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver 21 mode tcp 22 balance roundrobin 23 option tcp-check 24 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver 25 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:6443 10.13.201.20:6443 check weight 100 verify none 26 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:6443 10.13.201.21:6443 check weight 100 verify none 27 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:6443 10.13.201.22:6443 check weight 100 verify none 28 29backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx 30 mode tcp 31 balance roundrobin 32 option tcp-check 33 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx 34 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:443 10.13.201.20:443 check weight 100 verify none 35 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:443 10.13.201.21:443 check weight 100 verify none 36 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:443 10.13.201.22:443 check weight 100 verify none 37 38backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr 39 mode tcp 40 balance roundrobin 41 option tcp-check 42 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr 43 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2112 10.13.201.20:2112 check weight 100 verify none 44 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2112 10.13.201.21:2112 check weight 100 verify none 45 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2112 10.13.201.22:2112 check weight 100 verify none 46 47backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer 48 mode tcp 49 balance roundrobin 50 option tcp-check 51 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer 52 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2113 10.13.201.20:2113 check weight 100 verify none 53 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2113 10.13.201.21:2113 check weight 100 verify none 54 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2113 10.13.201.22:2113 check weight 100 verify none So it seems. All the virtual services for the Supervisor Cluster entries has been added. I should now be able to reach the Kubernetes API endpoint and log-in using the loadbalancer/external ip provided by HAProxy 10.13.200.34\n1vsphere-kubectl login --server=\u0026#34;10.13.200.34\u0026#34; --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-domain.net 2 3KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 4Password: 5Logged in successfully. 6 7You have access to the following contexts: 8 10.13.200.34 9 10If the context you wish to use is not in this list, you may need to try 11logging in again later, or contact your cluster administrator. 12 13To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` Now I can just go ahead and deploy my first workload cluster and get started doing some Kubernetes'ing using the default Workload Network defined. Then it will be like the diagram below:\nBut instead I want to create a new vSphere Namespace in a new subnet/workload network and deploy a workload cluster there. Lets see how this works out.\nAdd additional workload network Assuming I have certain requirement that some Workload Clusters needs to be in separate IP subnets, such as exhausted existing default workload network, ip separation, security zones etc. I need to have the ability to define and add additional workload networks.\nI already have a new workload network defined. Its configured using NSX overlay segment has the following CIDR: 10.13.21.0/24. The only requirement seems to be needed for HAProxy to use this new network I just need to make sure the Workload network can reach the additional networks I add. So routing between the defautl workload network and any additional workload networks added needs to be in place. No other configuration is needed on the HAProxy side. The diagram below tries to illustrate a second workload network:\nWhen a new workload cluster is provisioned in the new workload network the VIP/LoadBalancer range will be the same, but instead HAProxy needs to route the traffic from its Workload network to the new workload network. Its not HAProxy that is responsible for the actual routing, that is something that needs to be in place in the infrastructure. HAProxy receives the traffic on the Frontend/LoadBalancer network interface, forwards it to the workload interface which then uses its configured gateway to forward the traffic to the new workload network where there is a router in between that knows about those network and can forward it to the correct destination and back again.\nLets try to deploy a workload cluster in the new workload network above, creating a new vSphere Namespace with this network.\nClick ADD and populate the necessary fields\nCreate the namespace using and select the new network:\nNow, deploy a cluster in this namespace:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: cluster-2 5 namespace: ns-stc-wld-1 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.26.5---vmware.2-fips.1-tkg.1 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 2 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-small 32 - name: storageClass 33 value: vsan-default-storage-policy 1kubectl apply -f cluster-2.yaml 2cluster.cluster.x-k8s.io/cluster-2 created And here the cluster has been deployed:\nThis looks very promising, as the first indicator of something not working is it only deploys the first control plane and stops. Let me log into the cluster and check the Kubernetes API access also.\n1andreasm@ubuntu02:~$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3cluster-2-8tq8q-mvj5d Ready control-plane 15m v1.26.5+vmware.2-fips.1 10.13.21.20 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 4cluster-2-node-pool-01-jqdms-b8fd8b5bb-7mjkh Ready \u0026lt;none\u0026gt; 11m v1.26.5+vmware.2-fips.1 10.13.21.22 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 5cluster-2-node-pool-01-jqdms-b8fd8b5bb-cxjqn Ready \u0026lt;none\u0026gt; 11m v1.26.5+vmware.2-fips.1 10.13.21.21 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 The nodes are using IP address in the new workload network (10.13.21.0/24) and below we can see the loadbalanced kubernetes api for this workload cluster has been assigned the ip 10.13.200.35 from the HAProxy loadbalancer range.\nAnd a quick look at the haproxy.cfg file again:\n1frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service 2 mode tcp 3 bind 10.13.200.35:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-10.13.200.35:apiserver 4 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service 5 option tcplog 6 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver if { dst_port 6443 } The entry for the new Cluster-2 has been added.\nRestore from a deleted/crashed/killed HAProxy appliance This exercise will involve deleting my existing HAProxy appliance. In the meantime I will loose access to my Supervisor cluster and any other workload cluster that has been deployed. I will have to re-deploy the HAProxy again, following the exact same deployment procedure as described above and update my Supervisor so it can reconfigure it back as it was before I deleted the old HAProxy deployment. So lets start by deleting the existing HAProxy appliance.\nRecent task\nNow that is gone.\nLets check my access to the Supervisor API:\n1andreasm@ubuntu02:~$ k get pods -A 2Unable to connect to the server: dial tcp 10.13.200.34:6443: connect: no route to host Not good. So lets redeploy the HAProxy\nIts deployed Power it on, can I reach the Supervisor API?\n1andreasm@ubuntu02:~$ k get pods -A 2The connection to the server 10.13.200.34:6443 was refused - did you specify the right host or port? No luck yet.\nLog in with SSH and check the HAProxy config:\n1aandreasm@linuxvm02:~/.ssh$ ssh root@10.13.10.199 2The authenticity of host \u0026#39;10.13.10.199 (10.13.10.199)\u0026#39; can\u0026#39;t be established. 3ED25519 key fingerprint is SHA256:BJGSoo5icUWW7+s2FIKBQV+bg33ZOhk10s9+LFoQgXs. 4This key is not known by any other names 5Are you sure you want to continue connecting (yes/no/[fingerprint])? yes 6Warning: Permanently added \u0026#39;10.13.10.199\u0026#39; (ED25519) to the list of known hosts. 7(root@10.13.10.199) Password: 8 08:09:36 up 0 min, 0 users, load average: 0.09, 0.03, 0.01 9tdnf update info not available yet! 10root@haproxy [ ~ ]# 11root@haproxy [ ~ ]# cat /etc/haproxy/haproxy.cfg 12global 13 log stdout format raw local0 info 14 chroot /var/lib/haproxy 15 stats socket /run/haproxy.sock user haproxy group haproxy mode 660 level admin expose-fd listeners 16 stats timeout 30s 17 user haproxy 18 group haproxy 19 master-worker 20 # Setting maxconn in the global section is what successfully sets the ulimit on the 21 # host, otherwise we run out of file descriptors (defaulted at 4096). 22 maxconn 60000 23 # This property indicates the number of maximum number of reloads a worker 24 # will survive before being forcefully killed. This number is required to control 25 # the rate at which processes can scale due to the number of reloads outscaling 26 # the rate processes are reaped when all of their connections have been cleaned up. 27 # This number was derived by taking the average virtual memory consumption for a 28 # single HA Proxy process under load, ~28MB, and allocating HA Proxy 3GB out of 4GB 29 # of the total virtual memory space. 30 mworker-max-reloads 100 31 32 # Default SSL material locations 33 ca-base /etc/ssl/certs 34 crt-base /etc/ssl/private 35 36 # Default ciphers to use on SSL-enabled listening sockets. 37 # For more information, see ciphers(1SSL). This list is from: 38 # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ 39 # An alternative list with additional directives can be obtained from 40 # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy 41 ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS 42 ssl-default-bind-options no-sslv3 43 44defaults 45 mode tcp 46 log global 47 option tcplog 48 option dontlognull 49 option tcp-smart-accept 50 timeout check 5s 51 timeout connect 9s 52 timeout client 10s 53 timeout queue 5m 54 timeout server 10s 55 # tunnel timeout needs to be set at a lowish value to deal with the frequent 56 # reloads invoked by dataplaneapi at scale. With a higher value set, established 57 # connections will hang around and prevent haproxy from killing all off older processes 58 # because those old processes won\u0026#39;t terminate those established connections unless 59 # it is told to do so. Having these processes linger for too long can eventually 60 # starve the system of resources as the spawn rate of processes exceeds the death rate. 61 timeout tunnel 5m 62 timeout client-fin 10s 63 64# Stats are disabled by default because enabling them on a non-local IP address 65# would result in allocating a port that could result in a conflict with one 66# of the binds programmed at runtime. 67# 68# To enable stats, uncomment the following section and replace SYSTEM_IP_ADDR 69# with the IP address of the HAProxy host. 70#frontend stats 71# mode http 72# bind SYSTEM_IP_ADDR:8404 73# stats enable 74# stats uri /stats 75# stats refresh 500ms 76# stats hide-version 77# stats show-legends 78 79# ###No entries..... Certainly a new appliance, not the same authenticity fingerprint. But no entries.. Now I will head over to the Workload Management in vCenter and update the certificate under the LoadBalancer section with the new certificate from the newly deployed HAProxy.\nNote.. If I had copied out the cert and key from the old one I could have imported them back again during deployment.\nClick save. I still cant reach the my kubernetes api endpoints. The next I need to to is just restart the WCP service from the vCenter VAMI.\nScroll all the way down to find the WCP service, select it, scroll all the way up again.\nClick restart.\nAfter the service has been restarted having a look inside tha haproxy.cfg file again now:\n1frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc 2 mode tcp 3 bind 10.13.200.34:443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:nginx 4 bind 10.13.200.34:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.200.34:kube-apiserver 5 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc 6 option tcplog 7 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx if { dst_port 443 } 8 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver if { dst_port 6443 } 9 10frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service 11 mode tcp 12 bind 10.13.200.35:6443 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-10.13.200.35:apiserver 13 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service 14 option tcplog 15 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver if { dst_port 6443 } 16 17frontend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller 18 mode tcp 19 bind 10.13.200.33:2112 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:ctlr 20 bind 10.13.200.33:2113 name domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.200.33:syncer 21 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller 22 option tcplog 23 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer if { dst_port 2113 } 24 use_backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr if { dst_port 2112 } 25 26backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver 27 mode tcp 28 balance roundrobin 29 option tcp-check 30 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-kube-apiserver 31 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:6443 10.13.201.20:6443 check weight 100 verify none 32 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:6443 10.13.201.21:6443 check weight 100 verify none 33 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:6443 10.13.201.22:6443 check weight 100 verify none 34 35backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx 36 mode tcp 37 balance roundrobin 38 option tcp-check 39 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-nginx 40 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.20:443 10.13.201.20:443 check weight 100 verify none 41 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.21:443 10.13.201.21:443 check weight 100 verify none 42 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-kube-system-kube-apiserver-lb-svc-10.13.201.22:443 10.13.201.22:443 check weight 100 verify none 43 44backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver 45 mode tcp 46 balance roundrobin 47 option tcp-check 48 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-apiserver 49 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-ns-stc-wld-1-cluster-2-control-plane-service-10.13.21.20:6443 10.13.21.20:6443 check weight 100 verify none 50 51backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr 52 mode tcp 53 balance roundrobin 54 option tcp-check 55 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-ctlr 56 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2112 10.13.201.20:2112 check weight 100 verify none 57 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2112 10.13.201.21:2112 check weight 100 verify none 58 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2112 10.13.201.22:2112 check weight 100 verify none 59 60backend domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer 61 mode tcp 62 balance roundrobin 63 option tcp-check 64 log-tag domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-syncer 65 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.20:2113 10.13.201.20:2113 check weight 100 verify none 66 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.21:2113 10.13.201.21:2113 check weight 100 verify none 67 server domain-c8:5071d9d4-373d-49aa-a202-4c4ed81adc3b-vmware-system-csi-vsphere-csi-controller-10.13.201.22:2113 10.13.201.22:2113 check weight 100 verify none 68root@haproxy [ ~ ]# All my entries are back, and I can reach my kubernetes cluster again:\n1andreasm@ubuntu02:~$ k get nodes 2NAME STATUS ROLES AGE VERSION 342344c0c03ff6c1cf443328504169cbe Ready control-plane,master 19h v1.25.6+vmware.wcp.2 44234cc01880600b11de8b894be0c2a30 Ready control-plane,master 19h v1.25.6+vmware.wcp.2 54234d1cd346addba50a751b3cbdfd5ed Ready control-plane,master 19h v1.25.6+vmware.wcp.2 6esx01.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c 7esx02.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c 8esx03.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c 9esx04.cpod.domain.net Ready agent 18h v1.25.6-sph-cf2e16c 10 11andreasm@ubuntu02:~$ k config use-context cluster-2 12Switched to context \u0026#34;cluster-2\u0026#34;. 13andreasm@ubuntu02:~$ k get nodes -o wide 14NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 15cluster-2-8tq8q-mvj5d Ready control-plane 17h v1.26.5+vmware.2-fips.1 10.13.21.20 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 16cluster-2-node-pool-01-jqdms-b8fd8b5bb-klqff Ready \u0026lt;none\u0026gt; 5m18s v1.26.5+vmware.2-fips.1 10.13.21.23 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 17cluster-2-node-pool-01-jqdms-b8fd8b5bb-m76rf Ready \u0026lt;none\u0026gt; 5m18s v1.26.5+vmware.2-fips.1 10.13.21.22 \u0026lt;none\u0026gt; Ubuntu 20.04.6 LTS 5.4.0-155-generic containerd://1.6.18-1-gdbc99e5b1 All services back up again.\nThis concludes this post. Thanks for reading.\n","link":"https://blog.andreasm.io/2023/09/19/vsphere-with-tanzu-and-haproxy/","section":"post","tags":["load-balancing","Kubernetes-api-endpoint","TKGs","vSphere-with-tanzu"],"title":"vSphere with Tanzu and HAproxy"},{"body":"","link":"https://blog.andreasm.io/tags/vsphere-with-tanzu/","section":"tags","tags":null,"title":"vSphere-with-tanzu"},{"body":"","link":"https://blog.andreasm.io/tags/autoscaling/","section":"tags","tags":null,"title":"autoscaling"},{"body":"","link":"https://blog.andreasm.io/categories/autoscaling/","section":"categories","tags":null,"title":"Autoscaling"},{"body":"","link":"https://blog.andreasm.io/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"","link":"https://blog.andreasm.io/tags/tanzu/","section":"tags","tags":null,"title":"tanzu"},{"body":"TKG autoscaler From the official TKG documentation page:\nCluster Autoscaler is a Kubernetes program that automatically scales Kubernetes clusters depending on the demands on the workload clusters. Use Cluster Autoscaler only for workload clusters deployed by a standalone management cluster.\nOk, lets try out this then.\nEnable Cluster Autoscaler So one of the pre-requisites is a TKG standalone management cluster. I have that already deployed and running. Then for a workload cluster to be able to use the cluster autoscaler I need to enable this by adding some parameters in the cluster deployment manifest. The following is the autoscaler relevant variables, some variables are required some are optional and only valid for use on a workload cluster deployment manifest. According to the official documentation the only supported way to enable autoscaler is when provisioning a new workload cluster.\nENABLE_AUTOSCALER: \u0026quot;true\u0026quot; #Required if you want to enable the autoscaler\nAUTOSCALER_MAX_NODES_TOTAL: \u0026quot;0\u0026quot; #Optional\nAUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026quot;10m\u0026quot; #Optional\nAUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: \u0026quot;10s\u0026quot; #Optional\nAUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: \u0026quot;3m\u0026quot; #Optional\nAUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: \u0026quot;10m\u0026quot; #Optional\nAUTOSCALER_MAX_NODE_PROVISION_TIME: \u0026quot;15m\u0026quot; #Optional\nAUTOSCALER_MIN_SIZE_0: \u0026quot;1\u0026quot; #Required (if Autoscaler is enabled as above)\nAUTOSCALER_MAX_SIZE_0: \u0026quot;2\u0026quot; #Required (if Autoscaler is enabled as above)\nAUTOSCALER_MIN_SIZE_1: \u0026quot;1\u0026quot; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nAUTOSCALER_MAX_SIZE_1: \u0026quot;3\u0026quot; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nAUTOSCALER_MIN_SIZE_2: \u0026quot;1\u0026quot; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nAUTOSCALER_MAX_SIZE_2: \u0026quot;4\u0026quot; #Required (if Autoscaler is enabled as above, and using prod template and tkg in multi-az )\nEnable Autoscaler upon provisioning of a new workload cluster Start by preparing a class-based yaml for the workload cluster. This procedure involves adding the AUTOSCALER variables (above) to the tkg bootstrap yaml (the one used to deploy the TKG management cluster). Then generate a cluster-class yaml manifest for the new workload cluster. I will make a copy of my existing TKG bootstrap yaml file, name it something relevant to autoscaling. Then in this file I will add these variables:\n1#! --------------- 2#! Workload Cluster Specific 3#! ------------- 4ENABLE_AUTOSCALER: \u0026#34;true\u0026#34; 5AUTOSCALER_MAX_NODES_TOTAL: \u0026#34;0\u0026#34; 6AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026#34;10m\u0026#34; 7AUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: \u0026#34;10s\u0026#34; 8AUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: \u0026#34;3m\u0026#34; 9AUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: \u0026#34;10m\u0026#34; 10AUTOSCALER_MAX_NODE_PROVISION_TIME: \u0026#34;15m\u0026#34; 11AUTOSCALER_MIN_SIZE_0: \u0026#34;1\u0026#34; #This will be used if not using availability zones. If using az this will count as zone 1 - required 12AUTOSCALER_MAX_SIZE_0: \u0026#34;2\u0026#34; ##This will be used if not using availability zones. If using az this will count as zone 1 - required 13AUTOSCALER_MIN_SIZE_1: \u0026#34;1\u0026#34; #This will be used for availability zone 2 14AUTOSCALER_MAX_SIZE_1: \u0026#34;3\u0026#34; #This will be used for availability zone 2 15AUTOSCALER_MIN_SIZE_2: \u0026#34;1\u0026#34; #This will be used for availability zone 3 16AUTOSCALER_MAX_SIZE_2: \u0026#34;4\u0026#34; #This will be used for availability zone 3 Tip! If not using TKG in a multi availability zone deployment, there is no need to add the lines AUTOSCALER_MIN_SIZE_1, AUTOSCALER_MAX_SIZE_1, AUTOSCALER_MIN_SIZE_2, and AUTOSCALER_MAX_SIZE_2 as these are only used for the additional zones you have configured. For a \u0026quot;no AZ\u0026quot; deployment AUTOSCALER_MIN/MAX_SIZE_1 is sufficient.\nAfter the above has been added I will do a \u0026quot;--dry-run\u0026quot; to create my workload cluster class-based yaml file:\n1andreasm@tkg-bootstrap:~$ tanzu cluster create tkg-cluster-3-auto --namespace tkg-ns-3 --file tkg-mgmt-bootstrap-tkg-2.3-autoscaler.yaml --dry-run \u0026gt; tkg-cluster-3-auto.yaml The above command gives the workload cluster the name tkg-cluster-3-auto in the namespace tkg-ns-3 and using the modified tkg bootstrap file containing the autocluster variables. The output is the class-based yaml I will use to create the cluster, like this (if no error during the dry-run command). In my mgmt bootstrap I have defined the autoscaler min_max settings just to reflect the capabilities in differentiating settings pr availability zone. According to the manual this should only be used in AWS, but in 2.3 multi-az is fully supported and the docs has probably not been updated yet. If I take a look at the class-based yaml:\n1 workers: 2 machineDeployments: 3 - class: tkg-worker 4 failureDomain: wdc-zone-2 5 metadata: 6 annotations: 7 cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;2\u0026#34; 8 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; 9 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 10 name: md-0 11 strategy: 12 type: RollingUpdate 13 - class: tkg-worker 14 failureDomain: wdc-zone-3 15 metadata: 16 annotations: 17 cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;3\u0026#34; 18 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; 19 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 20 name: md-1 21 strategy: 22 type: RollingUpdate 23 - class: tkg-worker 24 failureDomain: wdc-zone-3 25 metadata: 26 annotations: 27 cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;4\u0026#34; 28 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; 29 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 30 name: md-2 31 strategy: 32 type: RollingUpdate 33--- I notice that it does take into consideration my different availability zones. Perfect.\nBefore I deploy my workload cluster, I will edit the manifest to only deploy worker nodes in my AZ zone 2 due to resource constraints in my lab and to make the demo a bit better (scaling up from one worker and back again) then I will deploy the workload cluster.\n1andreasm@tkg-bootstrap:~$ tanzu cluster create --file tkg-cluster-3-auto.yaml 2Validating configuration... 3cluster class based input file detected, getting tkr version from input yaml 4input TKR Version: v1.26.5+vmware.2-tkg.1 5TKR Version v1.26.5+vmware.2-tkg.1, Kubernetes Version v1.26.5+vmware.2-tkg.1 configured Now it is all about wating... After the wating period is done it is time for some testing...\nEnable Autoscaler on existing/running workload cluster I have already a TKG workload cluster up and running and I want to \u0026quot;post-enable\u0026quot; autoscaler in this cluster. This cluster has been deployed with the AUTOSCALER_ENABLE=false and below is the class based yaml manifest (no autoscaler variables):\n1 workers: 2 machineDeployments: 3 - class: tkg-worker 4 failureDomain: wdc-zone-2 5 metadata: 6 annotations: 7 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 8 name: md-0 9 replicas: 1 10 strategy: 11 type: RollingUpdate The above class based yaml has been generated from my my mgmt bootstrap yaml with the AUTOSCALER settings like this:\n1#! --------------- 2#! Workload Cluster Specific 3#! ------------- 4ENABLE_AUTOSCALER: \u0026#34;false\u0026#34; 5AUTOSCALER_MAX_NODES_TOTAL: \u0026#34;0\u0026#34; 6AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026#34;10m\u0026#34; 7AUTOSCALER_SCALE_DOWN_DELAY_AFTER_DELETE: \u0026#34;10s\u0026#34; 8AUTOSCALER_SCALE_DOWN_DELAY_AFTER_FAILURE: \u0026#34;3m\u0026#34; 9AUTOSCALER_SCALE_DOWN_UNNEEDED_TIME: \u0026#34;10m\u0026#34; 10AUTOSCALER_MAX_NODE_PROVISION_TIME: \u0026#34;15m\u0026#34; 11AUTOSCALER_MIN_SIZE_0: \u0026#34;1\u0026#34; 12AUTOSCALER_MAX_SIZE_0: \u0026#34;4\u0026#34; 13AUTOSCALER_MIN_SIZE_1: \u0026#34;1\u0026#34; 14AUTOSCALER_MAX_SIZE_1: \u0026#34;4\u0026#34; 15AUTOSCALER_MIN_SIZE_2: \u0026#34;1\u0026#34; 16AUTOSCALER_MAX_SIZE_2: \u0026#34;4\u0026#34; If I check the autoscaler status:\n1andreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status 2Error from server (NotFound): configmaps \u0026#34;cluster-autoscaler-status\u0026#34; not found Now, this cluster is in \u0026quot;serious\u0026quot; need to have autoscaler enabled. So how do I do that? This step is most likely not officially supported. I will now go back to the tkg mgmt bootstrap yaml, enable the autoscaler. Do a dry run of the config and apply the new class-based yaml manifest. This is all done in the TKG mgmt cluster context.\n1andreasm@linuxvm01:~$ tanzu cluster create tkg-cluster-3-auto --namespace tkg-ns-3 --file tkg-mgmt-bootstrap-tkg-2.3-autoscaler-wld-1-zone.yaml --dry-run \u0026gt; tkg-cluster-3-auto-az.yaml Before applying the yaml new class based manifest I will edit out the uneccessary crds, and just keep the updated settings relevant to the autoscaler, it may even be reduced further. Se my yaml below:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 annotations: 5 osInfo: ubuntu,20.04,amd64 6 tkg/plan: dev 7 labels: 8 tkg.tanzu.vmware.com/cluster-name: tkg-cluster-3-auto 9 name: tkg-cluster-3-auto 10 namespace: tkg-ns-3 11spec: 12 clusterNetwork: 13 pods: 14 cidrBlocks: 15 - 100.96.0.0/11 16 services: 17 cidrBlocks: 18 - 100.64.0.0/13 19 topology: 20 class: tkg-vsphere-default-v1.1.0 21 controlPlane: 22 metadata: 23 annotations: 24 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 25 replicas: 1 26 variables: 27 - name: cni 28 value: antrea 29 - name: controlPlaneCertificateRotation 30 value: 31 activate: true 32 daysBefore: 90 33 - name: auditLogging 34 value: 35 enabled: false 36 - name: podSecurityStandard 37 value: 38 audit: restricted 39 deactivated: false 40 warn: restricted 41 - name: apiServerEndpoint 42 value: \u0026#34;\u0026#34; 43 - name: aviAPIServerHAProvider 44 value: true 45 - name: vcenter 46 value: 47 cloneMode: fullClone 48 datacenter: /cPod-NSXAM-WDC 49 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 50 folder: /cPod-NSXAM-WDC/vm/TKGm 51 network: /cPod-NSXAM-WDC/network/ls-tkg-mgmt 52 resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources 53 server: vcsa.FQDN 54 storagePolicyID: \u0026#34;\u0026#34; 55 tlsThumbprint: F8:----:7D 56 - name: user 57 value: 58 sshAuthorizedKeys: 59 - ssh-rsa BBAAB3NzaC1yc2EAAAADAQABA------QgPcxDoOhL6kdBHQY3ZRPE5LIh7RWM33SvsoIgic1OxK8LPaiGEPaOfUvP2ki7TNHLxP78bPxAfbkK7llDSmOIWrm7ukwG4DLHnyriBQahLqv1Wpx4kIRj5LM2UEBx235bVDSve== 60 - name: controlPlane 61 value: 62 machine: 63 diskGiB: 20 64 memoryMiB: 4096 65 numCPUs: 2 66 - name: worker 67 value: 68 machine: 69 diskGiB: 20 70 memoryMiB: 4096 71 numCPUs: 2 72 - name: controlPlaneZoneMatchingLabels 73 value: 74 region: k8s-region 75 tkg-cp: allowed 76 - name: security 77 value: 78 fileIntegrityMonitoring: 79 enabled: false 80 imagePolicy: 81 pullAlways: false 82 webhook: 83 enabled: false 84 spec: 85 allowTTL: 50 86 defaultAllow: true 87 denyTTL: 60 88 retryBackoff: 500 89 kubeletOptions: 90 eventQPS: 50 91 streamConnectionIdleTimeout: 4h0m0s 92 systemCryptoPolicy: default 93 version: v1.26.5+vmware.2-tkg.1 94 workers: 95 machineDeployments: 96 - class: tkg-worker 97 failureDomain: wdc-zone-2 98 metadata: 99 annotations: 100 cluster.x-k8s.io/cluster-api-autoscaler-node-group-max-size: \u0026#34;4\u0026#34; 101 cluster.x-k8s.io/cluster-api-autoscaler-node-group-min-size: \u0026#34;1\u0026#34; 102 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 103 name: md-0 104 strategy: 105 type: RollingUpdate 106--- 107apiVersion: apps/v1 108kind: Deployment 109metadata: 110 labels: 111 app: tkg-cluster-3-auto-cluster-autoscaler 112 name: tkg-cluster-3-auto-cluster-autoscaler 113 namespace: tkg-ns-3 114spec: 115 replicas: 1 116 selector: 117 matchLabels: 118 app: tkg-cluster-3-auto-cluster-autoscaler 119 template: 120 metadata: 121 labels: 122 app: tkg-cluster-3-auto-cluster-autoscaler 123 spec: 124 containers: 125 - args: 126 - --cloud-provider=clusterapi 127 - --v=4 128 - --clusterapi-cloud-config-authoritative 129 - --kubeconfig=/mnt/tkg-cluster-3-auto-kubeconfig/value 130 - --node-group-auto-discovery=clusterapi:clusterName=tkg-cluster-3-auto,namespace=tkg-ns-3 131 - --scale-down-delay-after-add=10m 132 - --scale-down-delay-after-delete=10s 133 - --scale-down-delay-after-failure=3m 134 - --scale-down-unneeded-time=10m 135 - --max-node-provision-time=15m 136 - --max-nodes-total=0 137 command: 138 - /cluster-autoscaler 139 image: projects.registry.vmware.com/tkg/cluster-autoscaler:v1.26.2_vmware.1 140 name: tkg-cluster-3-auto-cluster-autoscaler 141 volumeMounts: 142 - mountPath: /mnt/tkg-cluster-3-auto-kubeconfig 143 name: tkg-cluster-3-auto-cluster-autoscaler-volume 144 readOnly: true 145 serviceAccountName: tkg-cluster-3-auto-autoscaler 146 terminationGracePeriodSeconds: 10 147 tolerations: 148 - effect: NoSchedule 149 key: node-role.kubernetes.io/master 150 - effect: NoSchedule 151 key: node-role.kubernetes.io/control-plane 152 volumes: 153 - name: tkg-cluster-3-auto-cluster-autoscaler-volume 154 secret: 155 secretName: tkg-cluster-3-auto-kubeconfig 156--- 157apiVersion: rbac.authorization.k8s.io/v1 158kind: ClusterRoleBinding 159metadata: 160 creationTimestamp: null 161 name: tkg-cluster-3-auto-autoscaler-workload 162roleRef: 163 apiGroup: rbac.authorization.k8s.io 164 kind: ClusterRole 165 name: cluster-autoscaler-workload 166subjects: 167- kind: ServiceAccount 168 name: tkg-cluster-3-auto-autoscaler 169 namespace: tkg-ns-3 170--- 171apiVersion: rbac.authorization.k8s.io/v1 172kind: ClusterRoleBinding 173metadata: 174 creationTimestamp: null 175 name: tkg-cluster-3-auto-autoscaler-management 176roleRef: 177 apiGroup: rbac.authorization.k8s.io 178 kind: ClusterRole 179 name: cluster-autoscaler-management 180subjects: 181- kind: ServiceAccount 182 name: tkg-cluster-3-auto-autoscaler 183 namespace: tkg-ns-3 184--- 185apiVersion: v1 186kind: ServiceAccount 187metadata: 188 name: tkg-cluster-3-auto-autoscaler 189 namespace: tkg-ns-3 190--- 191apiVersion: rbac.authorization.k8s.io/v1 192kind: ClusterRole 193metadata: 194 name: cluster-autoscaler-workload 195rules: 196- apiGroups: 197 - \u0026#34;\u0026#34; 198 resources: 199 - persistentvolumeclaims 200 - persistentvolumes 201 - pods 202 - replicationcontrollers 203 verbs: 204 - get 205 - list 206 - watch 207- apiGroups: 208 - \u0026#34;\u0026#34; 209 resources: 210 - nodes 211 verbs: 212 - get 213 - list 214 - update 215 - watch 216- apiGroups: 217 - \u0026#34;\u0026#34; 218 resources: 219 - pods/eviction 220 verbs: 221 - create 222- apiGroups: 223 - policy 224 resources: 225 - poddisruptionbudgets 226 verbs: 227 - list 228 - watch 229- apiGroups: 230 - storage.k8s.io 231 resources: 232 - csinodes 233 - storageclasses 234 verbs: 235 - get 236 - list 237 - watch 238- apiGroups: 239 - batch 240 resources: 241 - jobs 242 verbs: 243 - list 244 - watch 245- apiGroups: 246 - apps 247 resources: 248 - daemonsets 249 - replicasets 250 - statefulsets 251 verbs: 252 - list 253 - watch 254- apiGroups: 255 - \u0026#34;\u0026#34; 256 resources: 257 - events 258 verbs: 259 - create 260 - patch 261- apiGroups: 262 - \u0026#34;\u0026#34; 263 resources: 264 - configmaps 265 verbs: 266 - create 267 - delete 268 - get 269 - update 270- apiGroups: 271 - coordination.k8s.io 272 resources: 273 - leases 274 verbs: 275 - create 276 - get 277 - update 278--- 279apiVersion: rbac.authorization.k8s.io/v1 280kind: ClusterRole 281metadata: 282 name: cluster-autoscaler-management 283rules: 284- apiGroups: 285 - cluster.x-k8s.io 286 resources: 287 - machinedeployments 288 - machines 289 - machinesets 290 verbs: 291 - get 292 - list 293 - update 294 - watch 295 - patch 296- apiGroups: 297 - cluster.x-k8s.io 298 resources: 299 - machinedeployments/scale 300 - machinesets/scale 301 verbs: 302 - get 303 - update 304- apiGroups: 305 - infrastructure.cluster.x-k8s.io 306 resources: 307 - \u0026#39;*\u0026#39; 308 verbs: 309 - get 310 - list And now I will apply the above yaml on my running TKG workload cluster using kubectl (done from the mgmt context):\n1andreasm@linuxvm01:~$ kubectl apply -f tkg-cluster-3-enable-only-auto-az.yaml 2cluster.cluster.x-k8s.io/tkg-cluster-3-auto configured 3Warning: would violate PodSecurity \u0026#34;restricted:v1.24\u0026#34;: allowPrivilegeEscalation != false (container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.capabilities.drop=[\u0026#34;ALL\u0026#34;]), runAsNonRoot != true (pod or container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.runAsNonRoot=true), seccompProfile (pod or container \u0026#34;tkg-cluster-3-auto-cluster-autoscaler\u0026#34; must set securityContext.seccompProfile.type to \u0026#34;RuntimeDefault\u0026#34; or \u0026#34;Localhost\u0026#34;) 4deployment.apps/tkg-cluster-3-auto-cluster-autoscaler created 5clusterrolebinding.rbac.authorization.k8s.io/tkg-cluster-3-auto-autoscaler-workload created 6clusterrolebinding.rbac.authorization.k8s.io/tkg-cluster-3-auto-autoscaler-management created 7serviceaccount/tkg-cluster-3-auto-autoscaler created 8clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-workload unchanged 9clusterrole.rbac.authorization.k8s.io/cluster-autoscaler-management unchanged Checking for autoscaler status now shows this:\n1andreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status 2Name: cluster-autoscaler-status 3Namespace: kube-system 4Labels: \u0026lt;none\u0026gt; 5Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-11 10:40:02.369535271 +0000 UTC 6 7Data 8==== 9status: 10---- 11Cluster-autoscaler status at 2023-09-11 10:40:02.369535271 +0000 UTC: 12Cluster-wide: 13 Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) 14 LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 15 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 16 ScaleUp: NoActivity (ready=2 registered=2) 17 LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 18 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 19 ScaleDown: NoCandidates (candidates=0) 20 LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 21 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 22 23NodeGroups: 24 Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-s7d7t 25 Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=1 (minSize=1, maxSize=4)) 26 LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 27 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 28 ScaleUp: NoActivity (ready=1 cloudProviderTarget=1) 29 LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 30 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 31 ScaleDown: NoCandidates (candidates=0) 32 LastProbeTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 33 LastTransitionTime: 2023-09-11 10:40:01.146686706 +0000 UTC m=+26.613355068 34 35 36 37BinaryData 38==== 39 40Events: \u0026lt;none\u0026gt; Thats great.\nAnother way to do it is to edit the cluster directly following this KB article. This KB article can also be used to change/modify existing autoscaler settings.\nTest the autoscaler In the following chapters I will test the scale up and down of my worker nodes, based on load in the cluster. My initial cluster is up and running:\n1NAME STATUS ROLES AGE VERSION 2tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 4m17s v1.26.5+vmware.2 3tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 8m31s v1.26.5+vmware.2 One control-plane node and one worker node. Now I want to check the status of the cluster-scaler:\n1andreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status 2Name: cluster-autoscaler-status 3Namespace: kube-system 4Labels: \u0026lt;none\u0026gt; 5Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 13:30:12.611110965 +0000 UTC 6 7Data 8==== 9status: 10---- 11Cluster-autoscaler status at 2023-09-08 13:30:12.611110965 +0000 UTC: 12Cluster-wide: 13 Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) 14 LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 15 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 16 ScaleUp: NoActivity (ready=2 registered=2) 17 LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 18 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 19 ScaleDown: NoCandidates (candidates=0) 20 LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 21 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC 22 23NodeGroups: 24 Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws 25 Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=1 (minSize=1, maxSize=4)) 26 LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 27 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 28 ScaleUp: NoActivity (ready=1 cloudProviderTarget=1) 29 LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 30 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 31 ScaleDown: NoCandidates (candidates=0) 32 LastProbeTime: 2023-09-08 13:30:11.394021754 +0000 UTC m=+1356.335230920 33 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC 34 35 36 37BinaryData 38==== 39 40Events: \u0026lt;none\u0026gt; Scale-up - amount of worker nodes (horizontally) Now I need to generate some load and see if it will do some magic scaling in the background.\nI have deployed my Yelb app again, the only missing pod is the UI pod:\n1NAME READY STATUS RESTARTS AGE 2redis-server-56d97cc8c-4h54n 1/1 Running 0 6m56s 3yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 6m55s 4yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 6m56s I still have my one cp node and one worker node. I will now deploy the UI pod and scale an insane amount of UI pods for the Yelb application.\n1yelb-ui-5c5b8d8887-9598s 1/1 Running 0 2m35s 1andreasm@linuxvm01:~$ k scale deployment -n yelb yelb-ui --replicas 200 2deployment.apps/yelb-ui scaled Lets check some status after this... A bunch of pods in pending states, waiting for a node to be scheduled on.\n1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 2redis-server-56d97cc8c-4h54n 1/1 Running 0 21m 100.96.1.9 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 3yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 21m 100.96.1.11 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 21m 100.96.1.10 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-ui-5c5b8d8887-22v8p 1/1 Running 0 6m18s 100.96.1.53 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-ui-5c5b8d8887-2587j 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7yelb-ui-5c5b8d8887-2bzcg 0/1 Pending 0 3m51s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8yelb-ui-5c5b8d8887-2gncl 0/1 Pending 0 3m51s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9yelb-ui-5c5b8d8887-2gwp8 1/1 Running 0 3m53s 100.96.1.86 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 10yelb-ui-5c5b8d8887-2gz7r 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 11yelb-ui-5c5b8d8887-2jlvv 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 12yelb-ui-5c5b8d8887-2pfgp 1/1 Running 0 6m18s 100.96.1.36 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 13yelb-ui-5c5b8d8887-2prwf 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 14yelb-ui-5c5b8d8887-2vr4f 0/1 Pending 0 3m53s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 15yelb-ui-5c5b8d8887-2w2t8 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 16yelb-ui-5c5b8d8887-2x6b7 1/1 Running 0 6m18s 100.96.1.34 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 17yelb-ui-5c5b8d8887-2x726 1/1 Running 0 9m40s 100.96.1.23 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 18yelb-ui-5c5b8d8887-452bx 0/1 Pending 0 3m49s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 19yelb-ui-5c5b8d8887-452dd 1/1 Running 0 6m17s 100.96.1.69 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 20yelb-ui-5c5b8d8887-45nmz 0/1 Pending 0 3m48s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 21yelb-ui-5c5b8d8887-4kj69 1/1 Running 0 3m53s 100.96.1.109 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 22yelb-ui-5c5b8d8887-4svbf 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 23yelb-ui-5c5b8d8887-4t6dm 0/1 Pending 0 3m50s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 24yelb-ui-5c5b8d8887-4zlhw 0/1 Pending 0 3m51s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 25yelb-ui-5c5b8d8887-55qzm 1/1 Running 0 9m40s 100.96.1.15 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 26yelb-ui-5c5b8d8887-5fts4 1/1 Running 0 6m18s 100.96.1.55 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; The autoscaler status:\n1andreasm@linuxvm01:~$ k describe cm -n kube-system cluster-autoscaler-status 2Name: cluster-autoscaler-status 3Namespace: kube-system 4Labels: \u0026lt;none\u0026gt; 5Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 14:01:43.794315378 +0000 UTC 6 7Data 8==== 9status: 10---- 11Cluster-autoscaler status at 2023-09-08 14:01:43.794315378 +0000 UTC: 12Cluster-wide: 13 Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) 14 LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 15 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 16 ScaleUp: InProgress (ready=2 registered=2) 17 LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 18 LastTransitionTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 19 ScaleDown: NoCandidates (candidates=0) 20 LastProbeTime: 2023-09-08 14:01:30.091765978 +0000 UTC m=+3235.032975159 21 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC 22 23NodeGroups: 24 Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws 25 Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=4)) 26 LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 27 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 28 ScaleUp: InProgress (ready=1 cloudProviderTarget=2) 29 LastProbeTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 30 LastTransitionTime: 2023-09-08 14:01:41.380962042 +0000 UTC m=+3246.322171235 31 ScaleDown: NoCandidates (candidates=0) 32 LastProbeTime: 2023-09-08 14:01:30.091765978 +0000 UTC m=+3235.032975159 33 LastTransitionTime: 0001-01-01 00:00:00 +0000 UTC 34 35 36 37BinaryData 38==== 39 40Events: 41 Type Reason Age From Message 42 ---- ------ ---- ---- ------- 43 Normal ScaledUpGroup 12s cluster-autoscaler Scale-up: setting group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size to 2 instead of 1 (max: 4) 44 Normal ScaledUpGroup 11s cluster-autoscaler Scale-up: group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size set to 2 instead of 1 (max: 4) O yes, it has triggered a scale up. And in vCenter a new worker node is in the process:\n1NAME STATUS ROLES AGE VERSION 2tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 55m v1.26.5+vmware.2 3tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc NotReady \u0026lt;none\u0026gt; 10s v1.26.5+vmware.2 4tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 59m v1.26.5+vmware.2 Lets check the pods status when the new node has been provisioned and ready..\nThe node is now ready:\n1NAME STATUS ROLES AGE VERSION 2tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 56m v1.26.5+vmware.2 3tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc Ready \u0026lt;none\u0026gt; 101s v1.26.5+vmware.2 4tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 60m v1.26.5+vmware.2 All my 200 UI pods are now scheduled and running across two worker nodes:\n1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 2redis-server-56d97cc8c-4h54n 1/1 Running 0 30m 100.96.1.9 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 3yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 30m 100.96.1.11 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 30m 100.96.1.10 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-ui-5c5b8d8887-22v8p 1/1 Running 0 15m 100.96.1.53 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-ui-5c5b8d8887-2587j 1/1 Running 0 12m 100.96.2.82 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7yelb-ui-5c5b8d8887-2bzcg 1/1 Running 0 12m 100.96.2.9 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8yelb-ui-5c5b8d8887-2gncl 1/1 Running 0 12m 100.96.2.28 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9yelb-ui-5c5b8d8887-2gwp8 1/1 Running 0 12m 100.96.1.86 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 10yelb-ui-5c5b8d8887-2gz7r 1/1 Running 0 12m 100.96.2.38 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 11yelb-ui-5c5b8d8887-2jlvv 1/1 Running 0 12m 100.96.2.58 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 12yelb-ui-5c5b8d8887-2pfgp 1/1 Running 0 15m 100.96.1.36 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 13yelb-ui-5c5b8d8887-2prwf 1/1 Running 0 12m 100.96.2.48 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 14yelb-ui-5c5b8d8887-2vr4f 1/1 Running 0 12m 100.96.2.77 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 15yelb-ui-5c5b8d8887-2w2t8 1/1 Running 0 12m 100.96.2.63 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 16yelb-ui-5c5b8d8887-2x6b7 1/1 Running 0 15m 100.96.1.34 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 17yelb-ui-5c5b8d8887-2x726 1/1 Running 0 18m 100.96.1.23 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 18yelb-ui-5c5b8d8887-452bx 1/1 Running 0 12m 100.96.2.67 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 19yelb-ui-5c5b8d8887-452dd 1/1 Running 0 15m 100.96.1.69 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 20yelb-ui-5c5b8d8887-45nmz 1/1 Running 0 12m 100.96.2.100 tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Scale-down - remove un-needed worker nodes Now that I have seen that the autoscaler is indeed scaling the amount worker nodes automatically, I will like to test whether it is also being capable of scaling down, removing unneccessary worker nodes as the load is not there any more. To test this I will just scale down the amount of UI pods in the Yelb application:\n1andreasm@linuxvm01:~$ k scale deployment -n yelb yelb-ui --replicas 2 2deployment.apps/yelb-ui scaled 3andreasm@linuxvm01:~$ k get pods -n yelb 4NAME READY STATUS RESTARTS AGE 5redis-server-56d97cc8c-4h54n 1/1 Running 0 32m 6yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 32m 7yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 32m 8yelb-ui-5c5b8d8887-22v8p 1/1 Terminating 0 17m 9yelb-ui-5c5b8d8887-2587j 1/1 Terminating 0 14m 10yelb-ui-5c5b8d8887-2bzcg 1/1 Terminating 0 14m 11yelb-ui-5c5b8d8887-2gncl 1/1 Terminating 0 14m 12yelb-ui-5c5b8d8887-2gwp8 1/1 Terminating 0 14m 13yelb-ui-5c5b8d8887-2gz7r 1/1 Terminating 0 14m 14yelb-ui-5c5b8d8887-2jlvv 1/1 Terminating 0 14m 15yelb-ui-5c5b8d8887-2pfgp 1/1 Terminating 0 17m 16yelb-ui-5c5b8d8887-2prwf 1/1 Terminating 0 14m 17yelb-ui-5c5b8d8887-2vr4f 1/1 Terminating 0 14m 18yelb-ui-5c5b8d8887-2w2t8 1/1 Terminating 0 14m When all the unnecessary pods are gone, I need to monitor the removal of the worker nodes. It may take some minutes\nThe Yelb application is back to \u0026quot;normal\u0026quot;\n1NAME READY STATUS RESTARTS AGE 2redis-server-56d97cc8c-4h54n 1/1 Running 0 33m 3yelb-appserver-65855b7ffd-j2bjt 1/1 Running 0 33m 4yelb-db-6f78dc6f8f-rg68q 1/1 Running 0 33m 5yelb-ui-5c5b8d8887-dxlth 1/1 Running 0 21m 6yelb-ui-5c5b8d8887-gv829 1/1 Running 0 21m Checking the autoscaler status now, it has identified a candidate to scale down. But as I have sat this AUTOSCALER_SCALE_DOWN_DELAY_AFTER_ADD: \u0026quot;10m\u0026quot; I will need to wait 10 minutes after LastTransitionTime ...\n1Name: cluster-autoscaler-status 2Namespace: kube-system 3Labels: \u0026lt;none\u0026gt; 4Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 14:19:46.985695728 +0000 UTC 5 6Data 7==== 8status: 9---- 10Cluster-autoscaler status at 2023-09-08 14:19:46.985695728 +0000 UTC: 11Cluster-wide: 12 Health: Healthy (ready=3 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=3 longUnregistered=0) 13 LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 14 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 15 ScaleUp: NoActivity (ready=3 registered=3) 16 LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 17 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 18 ScaleDown: CandidatesPresent (candidates=1) 19 LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 20 LastTransitionTime: 2023-09-08 14:18:26.989571984 +0000 UTC m=+4251.930781291 21 22NodeGroups: 23 Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws 24 Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0 cloudProviderTarget=2 (minSize=1, maxSize=4)) 25 LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 26 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 27 ScaleUp: NoActivity (ready=2 cloudProviderTarget=2) 28 LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 29 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 30 ScaleDown: CandidatesPresent (candidates=1) 31 LastProbeTime: 2023-09-08 14:19:45.772876369 +0000 UTC m=+4330.714085660 32 LastTransitionTime: 2023-09-08 14:18:26.989571984 +0000 UTC m=+4251.930781291 33 34 35 36BinaryData 37==== 38 39Events: 40 Type Reason Age From Message 41 ---- ------ ---- ---- ------- 42 Normal ScaledUpGroup 18m cluster-autoscaler Scale-up: setting group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size to 2 instead of 1 (max: 4) 43 Normal ScaledUpGroup 18m cluster-autoscaler Scale-up: group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size set to 2 instead of 1 (max: 4) After the 10 minutes:\n1NAME STATUS ROLES AGE VERSION 2tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-dcp2q Ready \u0026lt;none\u0026gt; 77m v1.26.5+vmware.2 3tkg-cluster-3-auto-ns4jx-szp69 Ready control-plane 81m v1.26.5+vmware.2 Back to two nodes again, and the VM has been deleted from vCenter.\nThe autoscaler status:\n1Name: cluster-autoscaler-status 2Namespace: kube-system 3Labels: \u0026lt;none\u0026gt; 4Annotations: cluster-autoscaler.kubernetes.io/last-updated: 2023-09-08 14:29:32.692769073 +0000 UTC 5 6Data 7==== 8status: 9---- 10Cluster-autoscaler status at 2023-09-08 14:29:32.692769073 +0000 UTC: 11Cluster-wide: 12 Health: Healthy (ready=2 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=2 longUnregistered=0) 13 LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 14 LastTransitionTime: 2023-09-08 13:07:46.176049718 +0000 UTC m=+11.117258901 15 ScaleUp: NoActivity (ready=2 registered=2) 16 LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 17 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 18 ScaleDown: NoCandidates (candidates=0) 19 LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 20 LastTransitionTime: 2023-09-08 14:28:46.471388976 +0000 UTC m=+4871.412598145 21 22NodeGroups: 23 Name: MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws 24 Health: Healthy (ready=1 unready=0 (resourceUnready=0) notStarted=0 longNotStarted=0 registered=1 longUnregistered=0 cloudProviderTarget=1 (minSize=1, maxSize=4)) 25 LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 26 LastTransitionTime: 2023-09-08 13:12:44.585589045 +0000 UTC m=+309.526798282 27 ScaleUp: NoActivity (ready=1 cloudProviderTarget=1) 28 LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 29 LastTransitionTime: 2023-09-08 14:08:21.539629262 +0000 UTC m=+3646.480838810 30 ScaleDown: NoCandidates (candidates=0) 31 LastProbeTime: 2023-09-08 14:29:31.482497258 +0000 UTC m=+4916.423706440 32 LastTransitionTime: 2023-09-08 14:28:46.471388976 +0000 UTC m=+4871.412598145 33 34 35 36BinaryData 37==== 38 39Events: 40 Type Reason Age From Message 41 ---- ------ ---- ---- ------- 42 Normal ScaledUpGroup 27m cluster-autoscaler Scale-up: setting group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size to 2 instead of 1 (max: 4) 43 Normal ScaledUpGroup 27m cluster-autoscaler Scale-up: group MachineDeployment/tkg-ns-3/tkg-cluster-3-auto-md-0-fhrws size set to 2 instead of 1 (max: 4) 44 Normal ScaleDownEmpty 61s cluster-autoscaler Scale-down: removing empty node \u0026#34;tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc\u0026#34; 45 Normal ScaleDownEmpty 55s cluster-autoscaler Scale-down: empty node tkg-cluster-3-auto-md-0-fhrws-757648f59cxq4hlz-q6fqc removed This works really well. Quite straight forward to enable and a really nice feature to have. And this also concludes this post.\n","link":"https://blog.andreasm.io/2023/09/07/tkg-autoscaler/","section":"post","tags":["autoscaling","kubernetes","tanzu"],"title":"TKG Autoscaler"},{"body":"","link":"https://blog.andreasm.io/tags/lifecycle-management/","section":"tags","tags":null,"title":"lifecycle-management"},{"body":"","link":"https://blog.andreasm.io/categories/lifecycle-management/","section":"categories","tags":null,"title":"Lifecycle-management"},{"body":"","link":"https://blog.andreasm.io/tags/management/","section":"tags","tags":null,"title":"management"},{"body":"","link":"https://blog.andreasm.io/categories/management/","section":"categories","tags":null,"title":"Management"},{"body":"","link":"https://blog.andreasm.io/categories/tanzu/","section":"categories","tags":null,"title":"Tanzu"},{"body":"TMC Self-Managed version 1.0.1 Not that long ago I published an article where I went through how to deploy TMC-SM in my lab, the post can be found here. That post were based on the first release of TMC local, version 1.0. Now version 1.0.1 is out and I figured I wanted to create a post how I upgrade my current TMC local installation to the latest version 1.0.1. And who knows, maybe this will be a short and snappy post from me for a change ðŸ˜„\nWhats new in TMC-SM 1.0.1 Taken from the official documentaion page here where you can find more details, like information about issues that have been resolved.\nTanzu Mission Control Self-Managed now supports deployment to and lifecycle management of the following Tanzu Kubernetes Grid clusters:\nCluster type Environment TKG 2.2.x (Kubernetes 1.25.x) vSphere 8.0 and vSphere 7.0 TKG 2.1.x (Kubernetes 1.24.x) vSphere 8.0 and vSphere 7.0 TKG 1.6.x (Kubernetes 1.23.x) vSphere 7.0 Tanzu Kubernetes Grid Service clusters running in vSphere with Tanzu (Kubernetes 1.24.x and 1.23.x) vSphere 8.0 Update 0 or Update 2 vSphere 7.0 latest update New Features and Improvements\nAdded lifecycle management support for vSphere 8\nYou can now manage Tanzu Kubernetes Grid Service clusters running in vSphere with Tanzu 8u1b. Tanzu Mission Control Self-Managed allows you to register your vSphere with Tanzu Supervisor to perform lifecycle management operations on your Tanzu Kubernetes Grid service clusters.\nAdded Terraform provider support for Tanzu Mission Control Self-Managed\nTanzu Mission Control Self-Managed can now be managed and automated using Hashicorp Terraform platform.\nThe Tanzu Mission Control provider v1.2.1 in Terraform implements support for managing your fleet of Kubernetes clusters by connecting with Tanzu Mission Control Self-Managed.\nYou can use the Tanzu Mission Control provider for Terraform to:\nConnect to Tanzu Mission Control Self-Managed.\nAttach conformant Kubernetes clusters.\nManage the lifecycle of workload clusters.\nManage cluster security using policies - access, image registry, security, network, custom, namespace quota.\nUpgrade TMC-SM to 1.0.1 I am using the steps describing how to upgrade TMC-SM in this chapter from the official TMC documentation page here. Before executing the actual upgrade process there are some necessary steps that needs to be done first. I will go through them here in their own little chapters/sections below. I will reuse the same bootstrap machine and container registry I used in this post in all the steps described.\nDownload the latest packages First I need to download the latest packages from the VMware Customer Connect portal here. The file I will be downloading is this:\nThis file will be landing on my laptop where I will copy it over to my bootstrap machine as soon as it is downloaded.\n1andreasm:~/Downloads/TMC$ scp bundle-1.0.1.tar andreasm@10.101.10.99:/home/andreasm/tmc-sm 2andreasm@10.101.10.99\u0026#39;s password: 3bundle-1.0.1.tar 15% 735MB 5.3MB/s 13:05 ETA Extract and push images to registry From my bootstrap machine I need to extract the newly downloaded bundle-1.0.1.tar file, and put it in a new folder:\n1andreasm@linuxvm01:~/tmc-sm$ mkdir tmc-sm-1.0.1 2andreasm@linuxvm01:~/tmc-sm$ tar -xf bundle-1.0.1.tar -C ./tmc-sm-1.0.1/ Then I will push them to my registry, the same registry and project used in the first installation of TMC-SM.\n1andreasm@linuxvm01:~/tmc-sm/tmc-sm-1.0.1$ ./tmc-sm push-images harbor --project registry.some-domain.net/tmcproject --username username --password password After some waiting, the below should be the output if everything went successfully.\n1INFO[0171] Pushing PackageRepository uri=registry.some-domain.net/tmc-project/package-repository 2Image Staging Complete. Next Steps: 3Setup Kubeconfig (if not already done) to point to cluster: 4export KUBECONFIG={YOUR_KUBECONFIG} 5 6Create \u0026#39;tmc-local\u0026#39; namespace: kubectl create namespace tmc-local 7 8Download Tanzu CLI from Customer Connect (If not already installed) 9 10Update TMC Self Managed Package Repository: 11Run: tanzu package repository add tanzu-mission-control-packages --url \u0026#34;registry.some-domain.net/tmc-project/package-repository:1.0.1\u0026#34; --namespace tmc-local 12 13Create a values based on the TMC Self Managed Package Schema: 14View the Values Schema: tanzu package available get \u0026#34;tmc.tanzu.vmware.com/1.0.1\u0026#34; --namespace tmc-local --values-schema 15Create a Values file named values.yaml matching the schema 16 17Install the TMC Self Managed Package: 18Run: tanzu package install tanzu-mission-control -p tmc.tanzu.vmware.com --version \u0026#34;1.0.1\u0026#34; --values-file values.yaml --namespace tmc-local I should also have a file called pushed-package-repository.json in my tmc-sm-1.0.1 folder:\n1andreasm@linuxvm01:~/tmc-sm/tmc-sm-1.0.1$ ls 2agent-images dependencies packages pushed-package-repository.json tmc-sm The content of this file:\n1andreasm@linuxvm01:~/tmc-sm/tmc-sm-1.0.1$ cat pushed-package-repository.json 2{\u0026#34;repositoryImage\u0026#34;:\u0026#34;registry.some-domain.net/tmc-project/package-repository\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;1.0.1\u0026#34;} This information is needed in the next step.\nUpdate tanzu package repository This step will update the already installed tmc-sm package repository to contain version 1.0.1. Make sure to be logged into the correct context, the kubernetes cluster where the TMC installation is running before doing the below.\n1#Using the information above from the pushed-package-repository file, execute the following command: 2tanzu package repository update tanzu-mission-control-packages --url \u0026#34;registry.some-domain.net/tmc-project/package-repository:1.0.1\u0026#34; --namespace tmc-local 1Waiting for package repository to be updated 2 311:10:20AM: Waiting for package repository reconciliation for \u0026#39;tanzu-mission-control-packages\u0026#39; 411:10:25AM: Waiting for generation 2 to be observed 511:10:29AM: Fetching 6\t| apiVersion: vendir.k14s.io/v1alpha1 7\t| directories: 8\t| - contents: 9\t| - imgpkgBundle: 10\t| image: registry.some-domain.net/tmc-project/package-repository@sha256:89e53c26a9184580c2778a3bf08c45392e1d09773f0e8d1c22052dfb 11\t| tag: 1.0.1 12\t| path: . 13\t| path: \u0026#34;0\u0026#34; 14\t| kind: LockConfig 15\t| 1611:10:29AM: Fetch succeeded 1711:10:30AM: Template succeeded 1811:10:30AM: Deploy started (3s ago) 1911:10:33AM: Deploying 20\t| Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; 21\t| Changes 22\t| Namespace Name Kind Age Op Op st. Wait to Rs Ri 23\t| tmc-local contour.bitnami.com.12.1.0 Package 51d delete - - ok - 24\t| ^ contour.bitnami.com.12.2.6 Package - create ??? - - - 25\t| ^ kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 Package 51d delete - - ok - 26\t| ^ kafka-topic-controller.tmc.tanzu.vmware.com.0.0.22 Package - create ??? - - - 27\t| ^ kafka.bitnami.com.22.1.3 Package 51d delete - - ok - 28\t| ^ kafka.bitnami.com.23.0.7 Package - create ??? - - - 29\t| ^ minio.bitnami.com.12.6.12 Package - create ??? - - - 30\t| ^ minio.bitnami.com.12.6.4 Package 51d delete - - ok - 31\t| ^ monitoring.tmc.tanzu.vmware.com.0.0.13 Package 51d delete - - ok - 32\t| ^ monitoring.tmc.tanzu.vmware.com.0.0.14 Package - create ??? - - - 33\t| ^ pinniped.bitnami.com.1.2.1 Package 51d delete - - ok - 34\t| ^ pinniped.bitnami.com.1.2.8 Package - create ??? - - - 35\t| ^ postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 Package 51d delete - - ok - 36\t| ^ postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.47 Package - create ??? - - - 37\t| ^ s3-access-operator.tmc.tanzu.vmware.com.0.1.22 Package 51d delete - - ok - 38\t| ^ s3-access-operator.tmc.tanzu.vmware.com.0.1.24 Package - create ??? - - - 39\t| ^ tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 Package 51d delete - - ok - 40\t| ^ tmc-local-postgres.tmc.tanzu.vmware.com.0.0.67 Package - create ??? - - - 41\t| ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 Package 51d delete - - ok - 42\t| ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.21880 Package - create ??? - - - 43\t| ^ tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 Package 51d delete - - ok - 44\t| ^ tmc-local-stack.tmc.tanzu.vmware.com.0.0.21880 Package - create ??? - - - 45\t| ^ tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 Package 51d delete - - ok - 46\t| ^ tmc-local-support.tmc.tanzu.vmware.com.0.0.21880 Package - create ??? - - - 47\t| ^ tmc.tanzu.vmware.com.1.0.0 Package 51d delete - - ok - 48\t| ^ tmc.tanzu.vmware.com.1.0.1 Package - create ??? - - - 49\t| Op: 13 create, 13 delete, 0 update, 0 noop, 0 exists 50\t| Wait to: 0 reconcile, 0 delete, 26 noop 51\t| 11:10:32AM: ---- applying 26 changes [0/26 done] ---- 52\t| 11:10:32AM: delete package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 53\t| 11:10:32AM: delete package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 54\t| 11:10:32AM: create package/tmc.tanzu.vmware.com.1.0.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 55\t| 11:10:32AM: delete package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 56\t| 11:10:32AM: delete package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 57\t| 11:10:32AM: delete package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 58\t| 11:10:32AM: delete package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 59\t| 11:10:32AM: delete package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 60\t| 11:10:32AM: delete package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 61\t| 11:10:32AM: delete package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 62\t| 11:10:32AM: delete package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 63\t| 11:10:32AM: delete package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 64\t| 11:10:32AM: delete package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 65\t| 11:10:32AM: create package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 66\t| 11:10:32AM: delete package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 67\t| 11:10:32AM: create package/minio.bitnami.com.12.6.12 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 68\t| 11:10:32AM: create package/pinniped.bitnami.com.1.2.8 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 69\t| 11:10:32AM: create package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.67 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 70\t| 11:10:32AM: create package/contour.bitnami.com.12.2.6 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 71\t| 11:10:32AM: create package/monitoring.tmc.tanzu.vmware.com.0.0.14 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 72\t| 11:10:32AM: create package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.47 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 73\t| 11:10:32AM: create package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 74\t| 11:10:32AM: create package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 75\t| 11:10:32AM: create package/s3-access-operator.tmc.tanzu.vmware.com.0.1.24 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 76\t| 11:10:32AM: create package/tmc-local-support.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 77\t| 11:10:33AM: create package/kafka.bitnami.com.23.0.7 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 78\t| 11:10:33AM: ---- waiting on 26 changes [0/26 done] ---- 79\t| 11:10:33AM: ok: noop package/kafka.bitnami.com.23.0.7 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 80\t| 11:10:33AM: ok: noop package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 81\t| 11:10:33AM: ok: noop package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 82\t| 11:10:33AM: ok: noop package/tmc.tanzu.vmware.com.1.0.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 83\t| 11:10:33AM: ok: noop package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 84\t| 11:10:33AM: ok: noop package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 85\t| 11:10:33AM: ok: noop package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 86\t| 11:10:33AM: ok: noop package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 87\t| 11:10:33AM: ok: noop package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 88\t| 11:10:33AM: ok: noop package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 89\t| 11:10:33AM: ok: noop package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 90\t| 11:10:33AM: ok: noop package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 91\t| 11:10:33AM: ok: noop package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 92\t| 11:10:33AM: ok: noop package/contour.bitnami.com.12.2.6 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 93\t| 11:10:33AM: ok: noop package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 94\t| 11:10:33AM: ok: noop package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 95\t| 11:10:33AM: ok: noop package/minio.bitnami.com.12.6.12 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 96\t| 11:10:33AM: ok: noop package/pinniped.bitnami.com.1.2.8 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 97\t| 11:10:33AM: ok: noop package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.67 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 98\t| 11:10:33AM: ok: noop package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 99\t| 11:10:33AM: ok: noop package/monitoring.tmc.tanzu.vmware.com.0.0.14 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 100\t| 11:10:33AM: ok: noop package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.47 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 101\t| 11:10:33AM: ok: noop package/s3-access-operator.tmc.tanzu.vmware.com.0.1.24 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 102\t| 11:10:33AM: ok: noop package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 103\t| 11:10:33AM: ok: noop package/tmc-local-support.tmc.tanzu.vmware.com.0.0.21880 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 104\t| 11:10:33AM: ok: noop package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 105\t| 11:10:33AM: ---- applying complete [26/26 done] ---- 106\t| 11:10:33AM: ---- waiting complete [26/26 done] ---- 107\t| Succeeded 10811:10:33AM: Deploy succeeded If everything went well, lets check the package version:\n1andreasm@linuxvm01:~/tanzu package repository list --namespace tmc-local 2 3 NAME SOURCE STATUS 4 tanzu-mission-control-packages (imgpkg) registry.some-domain.net/tmc-project/package-repository:1.0.1 Reconcile succeeded After the steps above, we are now ready to start the actual upgrade of the TMC-SM deployment.\nUpgrade TMC-SM deployment To upgrade TMC execute the below command, where values.yaml is the value.yaml file I used in the previous installation:\n1andreasm@linuxvm01:~/tanzu package installed update tanzu-mission-control -p tmc.tanzu.vmware.com --version \u0026#34;1.0.1\u0026#34; --values-file values.yaml --namespace tmc-local Now some output:\n111:19:14AM: Pausing reconciliation for package installation \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; 211:19:15AM: Updating secret \u0026#39;tanzu-mission-control-tmc-local-values\u0026#39; 311:19:15AM: Creating overlay secrets 411:19:15AM: Updating package install for \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; 511:19:15AM: Resuming reconciliation for package installation \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; 611:19:15AM: Waiting for PackageInstall reconciliation for \u0026#39;tanzu-mission-control\u0026#39; 711:19:15AM: Waiting for generation 9 to be observed 811:19:15AM: ReconcileFailed: kapp: Error: waiting on reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local: 9 Finished unsuccessfully (Reconcile failed: (message: Expected to find at least one version, but did not (details: all=1 -\u0026gt; after-prereleases-filter=1 -\u0026gt; after-kapp-controller-version-check=1 -\u0026gt; after-constraints-filter=0))) 1011:19:15AM: Error tailing app: Reconciling app: ReconcileFailed: kapp: Error: waiting on reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local: 11 Finished unsuccessfully (Reconcile failed: (message: Expected to find at least one version, but did not (details: all=1 -\u0026gt; after-prereleases-filter=1 -\u0026gt; after-kapp-controller-version-check=1 -\u0026gt; after-constraints-filter=0))) 12 1311:19:16AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 1411:19:46AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 1511:20:17AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 1611:20:47AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 1711:21:17AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 1811:21:48AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 1911:22:19AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2011:22:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2111:23:19AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2211:23:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2311:24:19AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2411:24:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2511:25:20AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2611:25:50AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2711:26:20AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2811:26:51AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 2911:27:22AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling 3011:27:49AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: ReconcileSucceeded I did experience some error issues like the ones above Error tailing app: Reconciling app: ReconcileFailed: kapp: Error: waiting on reconcile, (Reconcile failed: (message: Expected to find at least one version, but did not\nI monitored the progress with this command:\n1kubectl get pkgi -n tmc-local Which first gave me this:\n1NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE 2contour contour.bitnami.com 12.2.6 Reconciling 51d 3kafka kafka.bitnami.com 22.1.3 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 4kafka-topic-controller kafka-topic-controller.tmc.tanzu.vmware.com 0.0.21 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 5minio minio.bitnami.com 12.6.4 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 6pinniped pinniped.bitnami.com 1.2.1 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 7postgres tmc-local-postgres.tmc.tanzu.vmware.com 0.0.46 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 8postgres-endpoint-controller postgres-endpoint-controller.tmc.tanzu.vmware.com 0.1.43 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 9s3-access-operator s3-access-operator.tmc.tanzu.vmware.com 0.1.22 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 10tanzu-mission-control tmc.tanzu.vmware.com 1.0.1 Reconciling 51d 11tmc-local-monitoring monitoring.tmc.tanzu.vmware.com 0.0.13 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 12tmc-local-stack tmc-local-stack.tmc.tanzu.vmware.com 0.0.17161 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d 13tmc-local-stack-secrets tmc-local-stack-secrets.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d 14tmc-local-support tmc-local-support.tmc.tanzu.vmware.com 0.0.17161 Reconcile failed: Expected to find at least one version, but did not (details: a... 51d But after a little while I tried it again and now it looked much better:\n1NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE 2contour contour.bitnami.com 12.2.6 Reconcile succeeded 51d 3kafka kafka.bitnami.com 23.0.7 Reconcile succeeded 51d 4kafka-topic-controller kafka-topic-controller.tmc.tanzu.vmware.com 0.0.22 Reconcile succeeded 51d 5minio minio.bitnami.com 12.6.12 Reconcile succeeded 51d 6pinniped pinniped.bitnami.com 1.2.8 Reconcile succeeded 51d 7postgres tmc-local-postgres.tmc.tanzu.vmware.com 0.0.67 Reconcile succeeded 51d 8postgres-endpoint-controller postgres-endpoint-controller.tmc.tanzu.vmware.com 0.1.47 Reconcile succeeded 51d 9s3-access-operator s3-access-operator.tmc.tanzu.vmware.com 0.1.24 Reconcile succeeded 51d 10tanzu-mission-control tmc.tanzu.vmware.com 1.0.1 Reconciling 51d 11tmc-local-monitoring monitoring.tmc.tanzu.vmware.com 0.0.14 Reconciling 51d 12tmc-local-stack tmc-local-stack.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d 13tmc-local-stack-secrets tmc-local-stack-secrets.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d 14tmc-local-support tmc-local-support.tmc.tanzu.vmware.com 0.0.21880 Reconcile succeeded 51d And if I look at the pods for the deployment:\n1NAME READY STATUS RESTARTS AGE 2account-manager-server-dd4cb648-mhwsr 1/1 Running 2 (8m35s ago) 51d 3account-manager-server-dd4cb648-s6n52 1/1 Running 2 (8m34s ago) 51d 4agent-gateway-server-ffbd987f9-79p4v 1/1 Running 0 7m48s 5agent-gateway-server-ffbd987f9-7ggg4 1/1 Running 0 7m48s 6alertmanager-tmc-local-monitoring-tmc-local-0 2/2 Running 0 51d 7api-gateway-server-6ccff88f7c-5wjm7 1/1 Running 0 7m48s 8api-gateway-server-6ccff88f7c-c6srd 1/1 Running 0 7m48s 9audit-service-consumer-6665d4968-mq5k7 1/1 Running 0 7m52s 10audit-service-consumer-6665d4968-nbmcp 1/1 Running 0 7m52s 11audit-service-server-58f8cb48b-cwjd2 1/1 Running 0 7m51s 12audit-service-server-58f8cb48b-drgjq 1/1 Running 0 7m51s 13auth-manager-server-777cff744d-9whfl 1/1 Running 1 (51d ago) 51d 14auth-manager-server-777cff744d-hbqrl 1/1 Running 1 (51d ago) 51d 15auth-manager-server-777cff744d-xxq4w 1/1 Running 2 (51d ago) 51d 16authentication-server-555cd5b896-k7lb7 1/1 Running 0 51d 17authentication-server-555cd5b896-nvhtm 1/1 Running 0 51d 18cluster-agent-service-server-596cdb5968-5dnjz 1/1 Running 0 51d 19cluster-agent-service-server-596cdb5968-p2629 1/1 Running 0 51d 20cluster-config-server-7b5c95f48b-rsvv8 1/1 Running 2 (51d ago) 51d 21cluster-config-server-7b5c95f48b-t89bm 1/1 Running 2 (51d ago) 51d 22cluster-object-service-server-844fc87799-9jjcp 1/1 Running 0 51d 23cluster-object-service-server-844fc87799-mkvvv 1/1 Running 0 51d 24cluster-reaper-server-68b94fdcc6-l4nk4 1/1 Running 0 51d 25cluster-secret-server-6cdc68c88c-2ntj2 1/1 Running 1 (51d ago) 51d 26cluster-secret-server-6cdc68c88c-7vld7 1/1 Running 1 (51d ago) 51d 27cluster-service-server-76d9cc4845-mbmxj 1/1 Running 0 7m51s 28cluster-service-server-76d9cc4845-tl6dg 1/1 Running 0 7m51s 29cluster-sync-egest-5946d85c48-5zzfh 1/1 Running 0 51d 30cluster-sync-egest-5946d85c48-jstjl 1/1 Running 0 51d 31cluster-sync-ingest-b8b4b4f7b-b7t2t 1/1 Running 0 51d 32cluster-sync-ingest-b8b4b4f7b-vjrr7 1/1 Running 0 51d 33contour-contour-certgen-9nhxm 0/1 Completed 0 13m 34contour-contour-f99f8c554-hhl45 1/1 Running 0 13m 35contour-envoy-dkgmp 2/2 Running 0 12m 36contour-envoy-knpcp 2/2 Running 0 13m 37contour-envoy-mdw4l 2/2 Running 0 11m 38contour-envoy-w4wl6 2/2 Running 0 10m 39dataprotection-server-7bd8f57c9c-2b6vc 1/1 Running 0 7m51s 40dataprotection-server-7bd8f57c9c-bxzt2 1/1 Running 0 7m51s 41events-service-consumer-75d7bfbc4f-dkdgt 1/1 Running 0 51d 42events-service-consumer-75d7bfbc4f-hmc4c 1/1 Running 0 51d 43events-service-server-57cb555cc6-6tc27 1/1 Running 0 51d 44events-service-server-57cb555cc6-7jp92 1/1 Running 0 51d 45fanout-service-server-5d854fdcb9-shmsm 1/1 Running 0 51d 46fanout-service-server-5d854fdcb9-z4wb9 1/1 Running 0 51d 47feature-flag-service-server-58cb8b8967-bw8nw 1/1 Running 0 7m49s 48inspection-server-84fbb9f554-8kjll 2/2 Running 0 51d 49inspection-server-84fbb9f554-b4kwq 2/2 Running 0 51d 50intent-server-79db6f6cc8-5bb64 1/1 Running 0 51d 51intent-server-79db6f6cc8-wq46l 1/1 Running 0 51d 52kafka-0 1/1 Running 0 9m37s 53kafka-exporter-f665b6bc5-g6bfg 1/1 Running 4 (8m13s ago) 9m40s 54kafka-topic-controller-7745b56c4c-jxfbv 1/1 Running 0 9m57s 55landing-service-server-86987d87b9-rgxtj 1/1 Running 0 7m49s 56minio-676cfff6d6-pk5m4 1/1 Running 0 8m50s 57minio-provisioning-4wx4j 0/1 Completed 0 8m52s 58onboarding-service-server-7dfd944785-6p4qs 1/1 Running 0 7m49s 59onboarding-service-server-7dfd944785-fknhq 1/1 Running 0 7m49s 60package-deployment-server-5446696ff4-l4phd 1/1 Running 0 51d 61package-deployment-server-5446696ff4-w8sl4 1/1 Running 0 51d 62pinniped-supervisor-f44756bc7-bwtz6 1/1 Running 0 10m 63policy-engine-server-6455f7db8f-748mk 1/1 Running 0 51d 64policy-engine-server-6455f7db8f-pnpr8 1/1 Running 0 51d 65policy-insights-server-6cc68b7d7f-5w9c6 1/1 Running 2 (51d ago) 51d 66policy-sync-service-server-8687654cc9-q98bm 1/1 Running 0 7m49s 67policy-view-service-server-7659f84d-qxdkc 1/1 Running 0 51d 68policy-view-service-server-7659f84d-v95w4 1/1 Running 0 51d 69postgres-endpoint-controller-99987dc75-s2xzv 1/1 Running 0 9m15s 70postgres-postgresql-0 2/2 Running 0 9m32s 71prometheus-server-tmc-local-monitoring-tmc-local-0 2/2 Running 0 6m 72provisioner-service-server-85fb5dc6bc-7n7jh 1/1 Running 0 51d 73provisioner-service-server-85fb5dc6bc-lw8pm 1/1 Running 0 51d 74resource-manager-server-5d69d9fd88-5q97d 1/1 Running 0 7m52s 75resource-manager-server-5d69d9fd88-fw75m 1/1 Running 0 7m52s 76s3-access-operator-7ddb9d9695-l5nx4 1/1 Running 0 9m8s 77schema-service-schema-server-7cc9696fc5-mmv5t 1/1 Running 0 7m51s 78telemetry-event-service-consumer-699db98fc7-kfpht 1/1 Running 0 51d 79telemetry-event-service-consumer-699db98fc7-pg2xg 1/1 Running 0 51d 80tenancy-service-server-6db748f79-pqfzx 1/1 Running 0 7m48s 81ui-server-75ccd455b8-55tkg 1/1 Running 0 7m50s 82ui-server-75ccd455b8-nzw6f 1/1 Running 0 7m50s 83wcm-server-6b4f9c6-c6944 1/1 Running 0 7m51s 84wcm-server-6b4f9c6-wbdm9 1/1 Running 0 7m51s It looks good, all pods in a running or completed state. Now the logging into the UI:\nThats it. This concludes this post of how to upgrade TMC Self-Managed\n","link":"https://blog.andreasm.io/2023/09/06/tmc-self-managed-upgrade-to-1.0.1/","section":"post","tags":["management","kubernetes","lifecycle-management"],"title":"TMC Self-Managed Upgrade to 1.0.1"},{"body":"","link":"https://blog.andreasm.io/tags/availability/","section":"tags","tags":null,"title":"availability"},{"body":"","link":"https://blog.andreasm.io/tags/tanzu-kubernetes-grid/","section":"tags","tags":null,"title":"tanzu-kubernetes-grid"},{"body":"","link":"https://blog.andreasm.io/categories/tkg/","section":"categories","tags":null,"title":"TKG"},{"body":"Tanzu Kubernetes Grid 2.3 and availability zones TKG 2.3 brings support for multiple availability zones (AZs) in the stable feature set. So I wanted to explore this possibility and how to configure it. I will go through the configuratuons steps needed before deployment of a new TKG management cluster and TKG workload cluster using different availability zones. This post's primary focus is the multi availability zone feature, so I will not go into details in general TKG configurations such networking, loadbalancing as I already have a post covering a \u0026quot;standard\u0026quot; installation of TKG.\nI will start by deploying the TKG management worker nodes on two of my three vSphere clusters (Cluster-2 and 3) and control-plane nodes in my vSphere Cluster-1, just to illustrate that with TKG 2.3 I can control where the respective type of nodes will be placed. Then I will deploy a TKG workload cluster (tkg-cluster-1) using the same zone-placement as the TKG management cluster. Both the TKG management cluster deployment and first workload cluster will be using vSphere clusters as availability zones. It will end up looking like this: When I have deployed my tkg-cluster-1 (workload cluster) I will apply another zone-config using vSphere DRS host-groups and provision a second TKG workload cluster (tkg-cluster-2-hostgroups) using a zone config configured to use vSphere DRS host-groups where I will define DRS rules on Cluster-3 dividing the four hosts into two zones, something like this:\nThis post will be using a vSphere as the TKG infrastructure provider. The vSphere environment consists of 1 vCenter server, 3 vSphere clusters with 4 hosts in each ( a total of 12 ESXi hosts equally distributed across 3 vSphere clusters). All vSphere clusters are providing their own vSAN datastore local to their vSphere cluster. There is no stretched vSAN nor any datastore replication going on. NSX is the underlaying network infrastructure and NSX-ALB for all loadbalancing needs. To get started there is some steps that needs to be done in vCenter, a prepared linux jumphost/bootstrap client with necessary cli tools. So lets start with the preparations.\nPreparations This section will cover all the needed preparations to get TKG 2.3 up and running in multiple availability zones. First out is the Linux jumphost, then vCenter configurations before doing the deployment. For more details on all requirements I dont cover in this post, head over to the offical documentation here\nLinux jumphost with necessary Tanzu CLI tools The Linux jumphost needs to be configured with the following specifications:\nA Linux, Windows, or macOS operating system running on a physical or virtual machine that has the following hardware: At least 8 GB of RAM. VMware recommends 16 GB of RAM. A disk with 50 GB of available storage. 2 or 4 2-core CPUs. Docker installed and running. When that is sorted, log into the jumphost and start by grabbing the Tanzu CLI. This has become very easy compared to earlier. My Linux jumphost is running Ubuntu, so I just need to add the repository for the Tanzu CLI like this:\n1sudo apt update 2sudo apt install -y ca-certificates curl gpg 3sudo mkdir -p /etc/apt/keyrings 4curl -fsSL https://packages.vmware.com/tools/keys/VMWARE-PACKAGING-GPG-RSA-KEY.pub | sudo gpg --dearmor -o /etc/apt/keyrings/tanzu-archive-keyring.gpg 5echo \u0026#34;deb [signed-by=/etc/apt/keyrings/tanzu-archive-keyring.gpg] https://storage.googleapis.com/tanzu-cli-os-packages/apt tanzu-cli-jessie main\u0026#34; | sudo tee /etc/apt/sources.list.d/tanzu.list 6sudo apt update 7sudo apt install -y tanzu-cli If not using Ubuntu, or you prefer another method of installation, read here for more options.\nThen I need to install the necessary Tanzu CLI plugins like this:\n1andreasm@tkg-bootstrap:~$ tanzu plugin group get vmware-tkg/default:v2.3.0 # to list them 2[i] Reading plugin inventory for \u0026#34;projects.registry.vmware.com/tanzu_cli/plugins/plugin-inventory:latest\u0026#34;, this will take a few seconds. 3Plugins in Group: vmware-tkg/default:v2.3.0 4 NAME TARGET VERSION 5 isolated-cluster global v0.30.1 6 management-cluster kubernetes v0.30.1 7 package kubernetes v0.30.1 8 pinniped-auth global v0.30.1 9 secret kubernetes v0.30.1 10 telemetry kubernetes v0.30.1 11andreasm@tkg-bootstrap:~/.config$ tanzu plugin install --group vmware-tkg/default:v2.3.0 # to install them 12[i] The tanzu cli essential plugins have not been installed and are being installed now. The install may take a few seconds. 13 14[i] Installing plugin \u0026#39;isolated-cluster:v0.30.1\u0026#39; with target \u0026#39;global\u0026#39; 15[i] Installing plugin \u0026#39;management-cluster:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 16[i] Installing plugin \u0026#39;package:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 17[i] Installing plugin \u0026#39;pinniped-auth:v0.30.1\u0026#39; with target \u0026#39;global\u0026#39; 18[i] Installing plugin \u0026#39;secret:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 19[i] Installing plugin \u0026#39;telemetry:v0.30.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 20[ok] successfully installed all plugins from group \u0026#39;vmware-tkg/default:v2.3.0\u0026#39; Then I need the Kubernetes CLI, \u0026quot;kubectl cli v1.26.5 for Linux\u0026quot; for TKG 2.3 which can be found here. After I have downloaded it, I copy it over to the Linux jumphost, extract it and place the binary kubectl in the folder /usr/local/bin so its in my path.\nTo verify the CLI tools and plugins are in place, I will run these commands:\n1# Verify Tanzu CLI version: 2andreasm@tkg-bootstrap:~/.config$ tanzu version 3version: v1.0.0 4buildDate: 2023-08-08 5sha: 006d0429 1# Verify Tanzu CLI plugins: 2andreasm@tkg-bootstrap:~/.config$ tanzu plugin list 3Standalone Plugins 4 NAME DESCRIPTION TARGET VERSION STATUS 5 isolated-cluster Prepopulating images/bundle for internet-restricted environments global v0.30.1 installed 6 pinniped-auth Pinniped authentication operations (usually not directly invoked) global v0.30.1 installed 7 telemetry configure cluster-wide settings for vmware tanzu telemetry global v1.1.0 installed 8 management-cluster Kubernetes management cluster operations kubernetes v0.30.1 installed 9 package Tanzu package management kubernetes v0.30.1 installed 10 secret Tanzu secret management kubernetes v0.30.1 installed 11 telemetry configure cluster-wide settings for vmware tanzu telemetry kubernetes v0.30.1 installed 1# verify kubectl version - look for \u0026#34;Client Version\u0026#34; 2andreasm@tkg-bootstrap:~/.config$ kubectl version 3WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. 4Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;26\u0026#34;, GitVersion:\u0026#34;v1.26.5+vmware.2\u0026#34;, GitCommit:\u0026#34;83112f368344a8ff6d13b89f120d5e646cd3bf19\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-06-26T06:47:19Z\u0026#34;, GoVersion:\u0026#34;go1.19.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 5Kustomize Version: v4.5.7 Next up is some preparations that needs to be done in vCenter.\nvSphere preparations As vSphere is the platform where I deploy TKG there are a couple of things that needs to be done to prepare for the deployment of TKG in general but also how the different availability zones will be configured. Apart from the necessary functions such as networking and storage to support a TKG deployment.\nvCenter TKG Kubernetes OVA template A small but important part is the OVA template to be used for the controlplane/worker nodes. I need to upload the right Kubernetes OVA template, it needs to be version 1.26.5, where I am using the latest Ubuntu 2004 Kubernetes v1.26.5 OVA. This I have downloaded from [here](the Tanzu Kubernetes Grid downloads page), uploaded it to my vCenter server, then converted it to a template, not changing anything on it, name etc.\nvSphere/vCenter availability zones With TKG 2.3 running on vSphere there is two ways to define the availability zones. There is the option to use vSphere clusters as availability zones or vSphere DRS host-groups. This gives flexibility and the possibility to define the availability zones according to how the underlaying vSphere environment has been configured. We can potentially have many availability zones for TKG to consume, some using host-groups, some using vSphere clusters and even different vCenter servers. It all depends on the needs and where it makes sense. Regardless of using vSphere clusters or DRS host-groups we need to define define a region and a zone for TKG to use. In vCenter we need to create tag associated with a category. The category can be whatever you want to call it, they just need to be reflected correctly when defining the vSphereFailureDomain later on. You may end up with several vSphere tag-categories as this depend on the environment and how you want to use the availability zones. These tag and categories are defined a bit different between vSphere clusters and DRS host-groups. When using vSphere clusters the region is defined on the Datacenter object and the zone is the actual vsphere cluster. When using DRS host-groups the vSphere cluster is defined as the the regions and the host-group as the zone.\nvSphere DRS Host-Groups The option to use host-groups (DRS objects) is to create \u0026quot;logical\u0026quot; zones based on host-groups/vm-groups affinity rules to place the TKG nodes in their respective host-groups inside same vSphere Cluster. This done by creating the host-groups, place the corresponding esxi hosts in the respective host-group and use vCenter Tags \u0026amp; Custom Attributes specifying tags on these objects respectively. This can be a good use case if the vSphere hosts are in the same vSphere cluster but spread across several racks. That means I can create a host-group pr rack, and define these host-groups as my availability zones for TKG to place the nodes accordingly. Lets pretend I have 12 ESXi hosts, equally divided and placed in their own rack. I can then create 3 host-groups called rack-1, rack-2 and rack-3.\nvSphere Clusters Using the vSphere clusters option we define the vCenter Datacenter object as the region and the vSphere clusters as the zones. We define that easily by using the vCenter Tags \u0026amp; Custom Attributes specifying tags on these objects respectively. We tag the specific vSphere Datacenter to become a region and we tag the vSphere clusters to be a specific zone. In mye lab I have vSphere hosts in three different vSphere clusters. With that I have defined my vCenter Server's only Datacenter object to be a region, and all my three vSphere clusters as three different zones within that one region. In short that means if I have only one Datacenter object in my vCenter that is a region. In this Datecenter object I have my three vSphere host clusters which will be three different zones for TKG to be aware of for potential placement of the TKG nodes.\nFor more information on multiple availability zones head over to the offical docs here.\nNext up is how configured the AZs in vCenter using vSphere clusters and DRS host-groups\nvCenter Tags - using vSphere cluster and datacenter As I am using vSphere Clusters as my zones and vSphere Datacenter it is very straight forward. The first thing that needs to be done is to create two categories under Tags \u0026amp; Custom Attributes here:\nThe two categories is the region and zone. These two categories are created like this.\nRegion category:\nThen the Zone category:\nThe categories can also be created using a cli tool called govc like this:\n1andreasm@tkg-bootstrap:~$ govc tags.category.create -t Datacenter k8s-region 2urn:vmomi:InventoryServiceCategory:a0248c5d-7050-4891-9635-1b5cbcb89f29:GLOBAL 1andreasm@tkg-bootstrap:~$ govc tags.category.create -t ClusterComputeResource k8s-zone 2urn:vmomi:InventoryServiceCategory:1d13b59d-1d2c-433a-b3ac-4f6528254f98:GLOBAL I should now see the categories like this in my vCenter UI:\nNow when I have created the categories, I need to create the tags using the newly created categories respectively.\nThe k8s-region category is used on the vCenter/vSphere Datacenter object. I will create a tag using the category k8s-region with some kind of meaningful name for the Datacenter object, and then attach this tag to the Datacenter object.\nCreate Datacenter Tag:\nThen attach it to the Datacenter object:\nOr using govc to attach/assign the tag:\n1andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-region wdc-region /cPod-NSXAM-WDC 2# There is no output after execution of this command... Next up is the tags using the k8s-zone category. I am creating three tags for this as I have three vSphere clusters I want to use a three different Availability Zones. The tags are created the same as before only using the category k8s-zone instead.\nI will end up with three tags called wdc-zone-1,wdc-zone-2, and wdc-zone-3.\nAnd here they are:\nNow I need to attach them to my vSphere clusters respectively, Cluster-1 = wdc-zone-1, Cluster-2 = wdc-zone-2 and Cluster-3 = wdc-zone-3.\nAgain, the creation of the tags and attaching them can be done using govc:\n1# Creating the tags using the correct category 2andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone wdc-zone-1 3andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone wdc-zone-2 4andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone wdc-zone-3 5# Attaching the tags to the respective clusters 6andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone wdc-zone-1 /cPod-NSXAM-WDC/host/Cluster-1 7andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone wdc-zone-2 /cPod-NSXAM-WDC/host/Cluster-2 8andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone wdc-zone-3 /cPod-NSXAM-WDC/host/Cluster-3 vCenter Tags - using vSphere DRS host-groups If using vSphere DRS host-groups this how how we can confgure vCenter using DRS host-groups as availability zones for TKG. In this section I will \u0026quot;simulate\u0026quot; that my vSphere Cluster-3 with 4 ESXi hosts is equally divided into two \u0026quot;racks\u0026quot; (2 ESXi hosts in each host-group). So I will create two host-groups, to reflect two availability zones inside Cluster-3.\nFirst I need to create two host-groups where I add my corresponding ESXi hosts. Group-1 will contain ESXi-9 and ESXi-12 and Group-2 will contain ESXi-11 and ESXi-12. From the vCenter UI:\nClick add: I am naming the host-groups rack-1 and rack-2 respectively, adding two hosts in each group.\nThe two host-groups:\nWhen the host-groups have been created and defined with the esxi host membership, I need to define two DRS VM-groups. From the same place as I created the host-groups, I click add and create a VM group instead.\nI need to add a \u0026quot;dummy\u0026quot; vm to be allowed to save and create the group. This is only when creating the group via the vCenter UI.\nBoth vm groups created:\nTo create these groups with cli using govc:\n1# Create Host Groups 2andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-1 -host esx01 esx02 3[31-08-23 08:49:30] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK 4andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-2 -host esx03 esx04 5[31-08-23 08:48:30] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK 6# Create VM groups 7andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-1-vm-group -vm 8[31-08-23 08:52:00] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK 9andreasm@tkg-bootstrap:~$ govc cluster.group.create -cluster=Cluster-3 -name=rack-2-vm-group -vm 10[31-08-23 08:52:04] Reconfigure /cPod-NSXAM-WDC/host/Cluster-3...OK Now I need to create affinity rules restricting the corresponding vm-group to only reside in the correct host-group.\nGroup-1 rule\nand group 2 rule\nNow its just creating the corresponding tag categories k8s-region and k8s-zone and the respective tags pr cluster and host-groups that have been created. Lets start by creating the categories, first from the vCenter UI then later using cli with govc. Note that these DRS host-groups are not the objects being tagged as the actual zones later on, they are just the logical boundary used in vCenter for vm placement. The ESXi hosts themselves will be the ones that are tagged with the zone tag, where the ESXi hosts are part of a host-group with a VM affinity rule.\nCategory k8s-region:\nI already have the k8s-region category from earlier, I just need to update it to also allow cluster.\nCategory k8s-zone:\nI already have the k8s-zone category from earlier, I just need to update it to also allow Host.\nThen I need to create the tag using the correct category, starting with the region tag. Create tag called room1 (in lack of own fantasy)\nThen the two tags pr zone/host-group:\nRack1\nRack2\nNow I need to attach the above tags to the correct objects in vCenter. The region tag will be used on the Cluster-3 object, the k8s-zone tags will be used on the ESXi host objects. The region room1 tag:\nThen the zone tag rack1 and rack2\nRack1\nRack2\nNow I have tagged the region and the zones, and should now have 2 availability zones for TKG to use.\nTo configure the categories, tags and attachment from cli using govc:\n1# Creating the categories if not already created, if already created run the tags.category.update 2andreasm@tkg-bootstrap:~$ govc tags.category.create -t ClusterComputeResource k8s-region 3andreasm@tkg-bootstrap:~$ govc tags.category.create -t HostSystem k8s-zone 4# Create the region tag 5andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-region room1 6# Create the zone tag 7andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone rack1 8andreasm@tkg-bootstrap:~$ govc tags.create -c k8s-zone rack2 9# Attach the region tag to vSphere Cluster-3 10andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-region room1 /cPod-NSXAM-WDC/host/Cluster-3 11# Attach the zone tag to the ESXi hosts 12andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack1 /cPod-NSXAM-WDC/host/Cluster-3/esxi-01.fqdn 13andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack1 /cPod-NSXAM-WDC/host/Cluster-3/esxi-02.fqdn 14andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack2 /cPod-NSXAM-WDC/host/Cluster-3/esxi-03.fqdn 15andreasm@tkg-bootstrap:~$ govc tags.attach -c k8s-zone rack2 /cPod-NSXAM-WDC/host/Cluster-3/esxi-04.fqdn Now that the necessary tags and categories have been created and assigned in vCenter, I can continue to prepare the necessary configs for TKG to use them.\nBe aware If the categories are already in place from previous installations, and you want to add these AZs also - Create new categories as the CSI installation will fail complaining on this error: \u0026quot;plugin registration failed with err: rpc error: code = Internal desc = failed to retrieve topology information for Node: \u0026quot;\u0026quot;. Error: \u0026quot;failed to fetch topology information for the nodeVM \u0026quot;\u0026quot;. Error: duplicate values detected for category k8s-zone as \u0026quot;rack1\u0026quot; and \u0026quot;wdc-zone-3\u0026quot;\u0026quot;, restarting registration container.\u0026quot;\nThese new categories must be updated accordingly in the multi-az.yaml file and tkg-workload cluster manifest before deployment. It can also make sense to have different categories to distinguish the different environments better.\nTKG - Management Cluster Before I can deploy a TKG management cluster I need to prepare a bootstrap yaml file and a multi-zone file so it knows about how the cluster should be configured and the availability zones. For TKG to use the tags created in vCenter we need to define these as Kubernetes FailureDomain and Deployment-Zone objects. This is done by creating a separate yaml file describing this. In this multi-zone file I need to define the region, zone and topology. The category and zone tags created in vCenter and the ones I have used in this post is only to keep it simple. We can have several categories depending on the environment you deploy it on. For more information on this head over here. Here it is also possible to define different networks and storage. A short explanation of the two CRDs in the example below: vSphereFailureDomain is where you provide the necessary information about the region/zones defined in vCenter such as the tags pr region/zone aka Datacenter/Clusters, networks and datastore. The vSphereDeploymentZone is used for placement constraints, using the vSphereFailureDomains and makes it possible mapping them using labels like I am doing below. A bit more on that later when I come to the actual deployment.\nTKG multi-az config file - using vCenter DRS host-groups Below is the yaml file I have prepared to deploy my TKG Management cluster when using vCenter DRS host-groups as availability zones. Comments inline:\n1--- 2apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 3kind: VSphereFailureDomain 4metadata: 5 name: rack1 # A name for this specifc zone, does not have to be the same as the tag used in vCenter 6spec: 7 region: 8 name: room1 # The specific tag created and assigned to the Datacenter object in vCenter 9 type: ComputeCluster 10 tagCategory: k8s-region # The specific tag category created earlier in vCenter 11 zone: 12 name: rack1 # The specific tag created and assigned to the cluster object in vCenter 13 type: HostGroup 14 tagCategory: k8s-zone # The specific tag category created earlier in vCenter 15 topology: 16 datacenter: /cPod-NSXAM-WDC # Specifies which Datacenter in vCenter 17 computeCluster: Cluster-3 # Specifices which Cluster in vCenter 18 hosts: 19 vmGroupName: rack-1-vm-group # The vm group name created earlier in vCenter 20 hostGroupName: rack-1 # The host group name created earlier in vCenter 21 networks: 22 - /cPod-NSXAM-WDC/network/ls-tkg-mgmt # Specify the network the nodes shall use in this region/cluster 23 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 # Specify the datastore the nodes shall use in this region/cluster 24--- 25apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 26kind: VSphereFailureDomain 27metadata: 28 name: rack2 # A name for this specifc zone, does not have to be the same as the tag used in vCenter 29spec: 30 region: 31 name: room1 # The specific tag created and assigned to the Datacenter object in vCenter 32 type: ComputeCluster 33 tagCategory: k8s-region # The specific tag category created earlier in vCenter 34 zone: 35 name: rack2 # The specific tag created and assigned to the cluster object in vCenter 36 type: HostGroup 37 tagCategory: k8s-zone # The specific tag category created earlier in vCenter 38 topology: 39 datacenter: /cPod-NSXAM-WDC # Specifies which Datacenter in vCenter 40 computeCluster: Cluster-3 # Specifices which Cluster in vCenter 41 hosts: 42 vmGroupName: rack-2-vm-group # The vm group name created earlier in vCenter 43 hostGroupName: rack-2 # The host group name created earlier in vCenter 44 networks: 45 - /cPod-NSXAM-WDC/network/ls-tkg-mgmt # Specify the network the nodes shall use in this region/cluster 46 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 # Specify the datastore the nodes shall use in this region/cluster 47--- 48apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 49kind: VSphereDeploymentZone 50metadata: 51 name: rack1 # Give the deploymentzone a name 52 labels: 53 region: room1 # For controlplane placement 54 tkg-cp: allowed # For controlplane placement 55spec: 56 server: vcsa.fqdn 57 failureDomain: rack1 # Calls on the vSphereFailureDomain defined above 58 placementConstraint: 59 resourcePool: /cPod-NSXAM-WDC/host/Cluster-3/Resources 60 folder: /cPod-NSXAM-WDC/vm/TKGm 61--- 62apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 63kind: VSphereDeploymentZone 64metadata: 65 name: rack2 # Give the deploymentzone a name 66 labels: 67 region: room1 68 tkg-cp: allowed 69spec: 70 server: vcsa.fqdn 71 failureDomain: rack2 # Calls on the vSphereFailureDomain defined above 72 placementConstraint: 73 resourcePool: /cPod-NSXAM-WDC/host/Cluster-3/Resources 74 folder: /cPod-NSXAM-WDC/vm/TKGm 75--- TKG multi-az config file - using vSphere cluster and datacenter Below is the yaml file I have prepared to deploy my TKG Management cluster with when using vSphere clusters as availability zones. Comments inline:\n1--- 2apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 3kind: VSphereFailureDomain 4metadata: 5 name: wdc-zone-1 # A name for this specifc zone, does not have to be the same as the tag used in vCenter 6spec: 7 region: 8 name: wdc-region # The specific tag created and assigned to the Datacenter object in vCenter 9 type: Datacenter 10 tagCategory: k8s-region # The specific tag category created earlier in vCenter 11 zone: 12 name: wdc-zone-1 # The specific tag created and assigned to the cluster object in vCenter 13 type: ComputeCluster 14 tagCategory: k8s-zone # The specific tag category created earlier in vCenter 15 topology: 16 datacenter: /cPod-NSXAM-WDC # Specifies which Datacenter in vCenter 17 computeCluster: Cluster-1 # Specifices which Cluster in vCenter 18 networks: 19 - /cPod-NSXAM-WDC/network/ls-tkg-mgmt # Specify the network the nodes shall use in this region/cluster 20 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 # Specify the datastore the nodes shall use in this region/cluster 21--- 22apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 23kind: VSphereFailureDomain 24metadata: 25 name: wdc-zone-2 26spec: 27 region: 28 name: wdc-region 29 type: Datacenter 30 tagCategory: k8s-region 31 zone: 32 name: wdc-zone-2 33 type: ComputeCluster 34 tagCategory: k8s-zone 35 topology: 36 datacenter: /cPod-NSXAM-WDC 37 computeCluster: Cluster-2 38 networks: 39 - /cPod-NSXAM-WDC/network/ls-tkg-mgmt 40 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-02 41--- 42apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 43kind: VSphereFailureDomain 44metadata: 45 name: wdc-zone-3 46spec: 47 region: 48 name: wdc-region 49 type: Datacenter 50 tagCategory: k8s-region 51 zone: 52 name: wdc-zone-3 53 type: ComputeCluster 54 tagCategory: k8s-zone 55 topology: 56 datacenter: /cPod-NSXAM-WDC 57 computeCluster: Cluster-3 58 networks: 59 - /cPod-NSXAM-WDC/network/ls-tkg-mgmt 60 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 61--- 62apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 63kind: VSphereDeploymentZone 64metadata: 65 name: wdc-zone-1 # A name for the DeploymentZone. Does not have to be the same as above 66 labels: 67 region: wdc-region # A specific label to be used for placement restriction, allowing flexibility of node placement. 68 tkg-cp: allowed # A specific label to be used for placement restriction, allowing flexibility of node placement. 69spec: 70 server: vcsa.fqdn # Specifies the vCenter IP or FQDN 71 failureDomain: wdc-zone-1 # Calls on the respective vSphereFailureDomain defined above 72 placementConstraint: 73 resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources # Specify which ResourcePool or Cluster directly 74 folder: /cPod-NSXAM-WDC/vm/TKGm # Specify which folder in vCenter to use for node placement 75--- 76apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 77kind: VSphereDeploymentZone 78metadata: 79 name: wdc-zone-2 80 labels: 81 region: wdc-region 82 tkg-cp: allowed 83 worker: allowed 84spec: 85 server: vcsa.fqdn 86 failureDomain: wdc-zone-2 87 placementConstraint: 88 resourcePool: /cPod-NSXAM-WDC/host/Cluster-2/Resources 89 folder: /cPod-NSXAM-WDC/vm/TKGm 90--- 91apiVersion: infrastructure.cluster.x-k8s.io/v1beta1 92kind: VSphereDeploymentZone 93metadata: 94 name: wdc-zone-3 95 labels: 96 region: wdc-region 97 tkg-cp: allowed 98 worker: allowed 99spec: 100 server: vcsa.fqdn 101 failureDomain: wdc-zone-3 102 placementConstraint: 103 resourcePool: /cPod-NSXAM-WDC/host/Cluster-3/Resources 104 folder: /cPod-NSXAM-WDC/vm/TKGm The multi-az yaml file above is now ready to be used deploying my TKG mgmt cluster in a multi-az environment using vSphere Clusters as the Availability Zones. I have added a label in my multi-az configuration under the vSphereDeploymentZones: tkg-cp: allowed. This custom label is used for the placement of the TKG controlplane nodes. I only want the worker nodes to be placed on AZ-2 and AZ-3 (wdc-zone-2 and wdc-zone-3) where AZ-1 or wdc-zone-1 is only for Control Plane node placement. This is one usecase for the vSphereDeploymentZone, placement constraints. Using these labels to specify the control-plane placement. The worker nodes placement for both TKG management cluster and workload cluster is defined in the bootstrap yaml or the cluster-class manifest for workload cluster.\nTKG bootstrap yaml - common for both vSphere cluster and DRS host-groups In additon to the regular settings that is needed in the bootstrap yaml file I need to add these lines to take into consideration the Availability zones.\n1#! --------------------------------------------------------------------- 2#! Multi-AZ configuration 3#! --------------------------------------------------------------------- 4USE_TOPOLOGY_CATEGORIES: \u0026#34;true\u0026#34; 5VSPHERE_REGION: k8s-region 6VSPHERE_ZONE: k8s-zone 7VSPHERE_AZ_0: wdc-zone-2 # Here I am defining the zone placement for the workers that ends with md-0 8VSPHERE_AZ_1: wdc-zone-3 # Here I am defining the zone placement for the workers that ends with md-1 9VSPHERE_AZ_2: wdc-zone-3 # Here I am defining the zone placement for the workers that ends with md-2 10VSPHERE_AZ_CONTROL_PLANE_MATCHING_LABELS: \u0026#34;region=wdc-region,tkg-cp=allowed\u0026#34; #This defines and uses the vSphereDeploymentsZone labels I have added to instruct the control-plane node placement Note! The Zone names under vSphere_AZ_0-2 needs to reflect the correct zone tag/label used in your corresponding multi-az.yaml file pr vSphereDeploymentZone. The same goes for the VSPHERE_AZ_CONTROL_PLANE_MATCHING_LABELS: the values needs to reflect the labels used/added.\nNow my full bootstrap.yaml below:\n1#! --------------- 2#! Basic config 3#! ------------- 4CLUSTER_NAME: tkg-wdc-az-mgmt 5CLUSTER_PLAN: prod 6INFRASTRUCTURE_PROVIDER: vsphere 7ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; 8ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; 9CLUSTER_CIDR: 100.96.0.0/11 10SERVICE_CIDR: 100.64.0.0/13 11TKG_IP_FAMILY: ipv4 12DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; 13 14#! --------------- 15#! vSphere config 16#! ------------- 17VSPHERE_DATACENTER: /cPod-NSXAM-WDC 18VSPHERE_DATASTORE: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 19VSPHERE_FOLDER: /cPod-NSXAM-WDC/vm/TKGm 20VSPHERE_INSECURE: \u0026#34;false\u0026#34; 21VSPHERE_NETWORK: /cPod-NSXAM-WDC/network/ls-tkg-mgmt 22VSPHERE_CONTROL_PLANE_ENDPOINT: \u0026#34;\u0026#34; 23VSPHERE_PASSWORD: \u0026#34;password\u0026#34; 24VSPHERE_RESOURCE_POOL: /cPod-NSXAM-WDC/host/Cluster-1/Resources 25VSPHERE_SERVER: vcsa.fqdn 26VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa 27VSPHERE_TLS_THUMBPRINT: F:::::::::E 28VSPHERE_USERNAME: andreasm@vsphereSSOdomain.net 29 30#! --------------------------------------------------------------------- 31#! Multi-AZ configuration 32#! --------------------------------------------------------------------- 33USE_TOPOLOGY_CATEGORIES: \u0026#34;true\u0026#34; 34VSPHERE_REGION: k8s-region 35VSPHERE_ZONE: k8s-zone 36VSPHERE_AZ_0: wdc-zone-2 37VSPHERE_AZ_1: wdc-zone-3 38VSPHERE_AZ_2: wdc-zone-3 39VSPHERE_AZ_CONTROL_PLANE_MATCHING_LABELS: \u0026#34;region=wdc-region,tkg-cp=allowed\u0026#34; 40AZ_FILE_PATH: /home/andreasm/tanzu-v-2.3/multi-az/multi-az.yaml 41 42#! --------------- 43#! Node config 44#! ------------- 45OS_ARCH: amd64 46OS_NAME: ubuntu 47OS_VERSION: \u0026#34;20.04\u0026#34; 48VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; 49VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; 50VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; 51VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; 52VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; 53VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; 54#CONTROL_PLANE_MACHINE_COUNT: 3 55#WORKER_MACHINE_COUNT: 3 56 57#! --------------- 58#! Avi config 59#! ------------- 60AVI_CA_DATA_B64: BASE64ENC 61AVI_CLOUD_NAME: wdc-1-nsx 62AVI_CONTROL_PLANE_HA_PROVIDER: \u0026#34;true\u0026#34; 63AVI_CONTROLLER: 172.21.101.50 64# Network used to place workload clusters\u0026#39; endpoint VIPs 65AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 66AVI_CONTROL_PLANE_NETWORK_CIDR: 10.101.114.0/24 67# Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 68AVI_DATA_NETWORK: vip-tkg-wld-l7 69AVI_DATA_NETWORK_CIDR: 10.101.115.0/24 70# Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 71AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.101.113.0/24 72AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 73# Network used to place management clusters\u0026#39; endpoint VIPs 74AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 75AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.101.112.0/24 76AVI_NSXT_T1LR: Tier-1 77AVI_CONTROLLER_VERSION: 22.1.2 78AVI_ENABLE: \u0026#34;true\u0026#34; 79AVI_LABELS: \u0026#34;\u0026#34; 80AVI_PASSWORD: \u0026#34;password\u0026#34; 81AVI_SERVICE_ENGINE_GROUP: nsx-se-generic-group 82AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: nsx-se-generic-group 83AVI_USERNAME: admin 84AVI_DISABLE_STATIC_ROUTE_SYNC: true 85AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true 86AVI_INGRESS_SHARD_VS_SIZE: SMALL 87AVI_INGRESS_SERVICE_TYPE: NodePortLocal 88 89 90#! --------------- 91#! Proxy config 92#! ------------- 93TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; 94 95#! --------------------------------------------------------------------- 96#! Antrea CNI configuration 97#! --------------------------------------------------------------------- 98ANTREA_NODEPORTLOCAL: true 99ANTREA_PROXY: true 100ANTREA_ENDPOINTSLICE: true 101ANTREA_POLICY: true 102ANTREA_TRACEFLOW: true 103ANTREA_NETWORKPOLICY_STATS: false 104ANTREA_EGRESS: true 105ANTREA_IPAM: false 106ANTREA_FLOWEXPORTER: false 107ANTREA_SERVICE_EXTERNALIP: false 108ANTREA_MULTICAST: false 109 110 111#! --------------------------------------------------------------------- 112#! Machine Health Check configuration 113#! --------------------------------------------------------------------- 114ENABLE_MHC: \u0026#34;true\u0026#34; 115ENABLE_MHC_CONTROL_PLANE: true 116ENABLE_MHC_WORKER_NODE: true 117MHC_UNKNOWN_STATUS_TIMEOUT: 5m 118MHC_FALSE_STATUS_TIMEOUT: 12m One last thing to do before heading over to the actual deployment is to add the following to my current Linux jumphost session:\n1andreasm@tkg-bootstrap:~$ export SKIP_MULTI_AZ_VERIFY=\u0026#34;true\u0026#34; This is needed as there is no mgmt cluster yet, and there is no way for anything that's not there to verify anything ðŸ˜º\nTKG Deployment Now that I have done all the needed preparations it is time to do the actual deployment and see if my Availability Zones are being used as I wanted. When the TKG management cluster has been deployed I should end up with all the Control Plane (3) nodes distributed across all my 3 AZs. Then the worker nodes should only be placed in the AZ-2 and AZ-3.\nTKG Mgmt Cluster deployment with multi-availability-zones From my Linux jumphost where I have all the CLI tools in place I am now ready to execute the following command to deploy the mgmt cluster. 3 Control Plane Nodes and 3 Worker Nodes.\n1andreasm@tkg-bootstrap:~$ tanzu mc create -f my-tkg-mgmt-bootstrap.yaml --az-file my-multi-az-file.yaml 1Validating the pre-requisites... 2 3vSphere 8 with Tanzu Detected. 4 5You have connected to a vSphere 8 with Tanzu environment that includes an integrated Tanzu Kubernetes Grid Service which 6turns a vSphere cluster into a platform for running Kubernetes workloads in dedicated resource pools. Configuring Tanzu 7Kubernetes Grid Service is done through the vSphere HTML5 Client. 8 9Tanzu Kubernetes Grid Service is the preferred way to consume Tanzu Kubernetes Grid in vSphere 8 environments. Alternatively you may 10deploy a non-integrated Tanzu Kubernetes Grid instance on vSphere 8. 11Deploying TKG management cluster on vSphere 8 ... 12Identity Provider not configured. Some authentication features won\u0026#39;t work. 13Using default value for CONTROL_PLANE_MACHINE_COUNT = 3. Reason: CONTROL_PLANE_MACHINE_COUNT variable is not set 14Using default value for WORKER_MACHINE_COUNT = 3. Reason: WORKER_MACHINE_COUNT variable is not set 15 16Setting up management cluster... 17Validating configuration... 18Using infrastructure provider vsphere:v1.7.0 19Generating cluster configuration... 20Setting up bootstrapper... Sit back and enjoy while the kind cluster is being deployed locally and hopefully provisioned in your vCenter server..\nWhen you see the below: Start creating management cluster something should start to happen in the vCenter server.\n1Management cluster config file has been generated and stored at: \u0026#39;/home/andreasm/.config/tanzu/tkg/clusterconfigs/tkg-wdc-az-mgmt.yaml\u0026#39; 2Start creating management cluster... And by just clicking on the respective TKG VM so far I can see that they are respecting my zone placement.\nThat is very well. Now just wait for the two last control plane nodes also.\n1You can now access the management cluster tkg-wdc-az-mgmt by running \u0026#39;kubectl config use-context tkg-wdc-az-mgmt-admin@tkg-wdc-az-mgmt\u0026#39; 2 3Management cluster created! 4 5 6You can now create your first workload cluster by running the following: 7 8 tanzu cluster create [name] -f [file] 9 10 11Some addons might be getting installed! Check their status by running the following: 12 13 kubectl get apps -A Exciting, lets have a look at the control plane nodes placement:\nThey have been distributed across my three cluster as wanted. Perfect.\nNow next step is to deploy a workload cluster to achieve the same placement constraints there.\nAdding or adjusting the vSphereFailureDomains and vSphereDeploymentZone In my TKG management cluster deployment above I have used the vSphereFailureDomains and vSphereDeploymentZones for vSphere clusters as my availability zones. If I want to have workload clusters deployed in other availability zones, different zones or even new zones I can add these to the management cluster. In the example below I will add the availability zones configured to use vCenter DRS host-groups using the vsphere-zones yaml config here.\nTo check which zones are available for the management cluster:\n1# vSphereFailureDomains 2andreasm@tkg-bootstrap:~$ k get vspherefailuredomains.infrastructure.cluster.x-k8s.io -A 3NAME AGE 4wdc-zone-1 24h 5wdc-zone-2 24h 6wdc-zone-3 24h 7# vSphereDeploymentZones 8andreasm@tkg-bootstrap:~$ k get vspheredeploymentzones.infrastructure.cluster.x-k8s.io -A 9NAME AGE 10wdc-zone-1 24h 11wdc-zone-2 24h 12wdc-zone-3 24h 13# I have defined both FailureDomains and DeploymentsZone with the same name Now, let me add the DRS host-groups zones.\n1andreasm@tkg-bootstrap:~$ kubectl apply -f multi-az-host-groups.yaml #The file containing host-groups definition 2vspherefailuredomain.infrastructure.cluster.x-k8s.io/rack1 created 3vspherefailuredomain.infrastructure.cluster.x-k8s.io/rack2 created 4vspheredeploymentzone.infrastructure.cluster.x-k8s.io/rack1 created 5vspheredeploymentzone.infrastructure.cluster.x-k8s.io/rack2 created 6# Or 7andreasm@tkg-bootstrap:~$ tanzu mc az set -f multi-az-host-groups.yaml 8# This command actually validate the settings if the export SKIP_MULTI_AZ_VERIFY=\u0026#34;true\u0026#34; is not set ofcourse Now check the failuredomains and placementzones:\n1andreasm@tkg-bootstrap:~$ k get vspherefailuredomains.infrastructure.cluster.x-k8s.io -A 2NAME AGE 3rack1 13s 4rack2 13s 5wdc-zone-1 24h 6wdc-zone-2 24h 7wdc-zone-3 24h 1andreasm@tkg-bootstrap:~$ k get vspheredeploymentzones.infrastructure.cluster.x-k8s.io -A 2NAME AGE 3rack1 2m44s 4rack2 2m44s 5wdc-zone-1 24h 6wdc-zone-2 24h 7wdc-zone-3 24h 1andreasm@tkg-bootstrap:~$ tanzu mc available-zone list -a 2 AZNAME ZONENAME ZONETYPE REGIONNAME REGIONTYPE DATASTORE NETWORK OWNERCLUSTER STATUS 3 rack1 rack1 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt not ready 4 rack2 rack2 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt not ready 5 wdc-zone-1 wdc-zone-1 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready 6 wdc-zone-2 wdc-zone-2 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-02 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready 7 wdc-zone-3 wdc-zone-3 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready I had the variable export SKIP_MULTI_AZ_VERIFY=\u0026quot;true\u0026quot; set so it did not validate my settings and just applied it. Therefore I have the two new zones/AZs in a not ready state. Deleting them, updated the config so it was correct. Sat the export SKIP_MULTI_AZ_VERIFY=\u0026quot;false\u0026quot;. Reapplied using the mc set command it came out ready:\n1andreasm@tkg-bootstrap:~$ tanzu mc az set -f multi-az-host-groups.yaml 2andreasm@tkg-bootstrap:~$ tanzu mc available-zone list -a 3 AZNAME ZONENAME ZONETYPE REGIONNAME REGIONTYPE DATASTORE NETWORK OWNERCLUSTER STATUS 4 rack1 rack1 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready 5 rack2 rack2 HostGroup room1 ComputeCluster /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready 6 wdc-zone-1 wdc-zone-1 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 /cPod-NSXAM-WDC/network/ls-tkg-mgmt ready 7 wdc-zone-2 wdc-zone-2 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-02 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready 8 wdc-zone-3 wdc-zone-3 ComputeCluster wdc-region Datacenter /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-3 /cPod-NSXAM-WDC/network/ls-tkg-mgmt tkg-cluster-1 ready TKG Workload Cluster deployment with multi-availability-zones - using vSphere clusters as AZs At this stage I will use the values already provided earlier, like the multi-az config, network, vCenter folder placements and so on. If I like I could have added a specific multi-az file for the workload cluster, changed the network settings, folder etc. But I am just using the config already in place from the management cluster for now.\nWe have already done the hard work. So the workload cluster deployment is now more or less walk in the park. To generate the necessary workload-cluster yaml definition I execute the following command (this will accommodate the necessary AZs setting, so if you already have a class-based cluster yaml file from previous TKG clusters make sure to add this or just run the command below):\n1andreasm@tkg-bootstrap:~$ tanzu cluster create tkg-cluster-1 --namespace tkg-ns-1 --file tkg-mgmt-bootstrap-for-wld.az.yaml --dry-run \u0026gt; workload-cluster/tkg-cluster-1.yaml 2# tanzu cluster create tkg-cluster-1 gives the cluster the name tkg-cluster-1 3# --namespace is the namespace I have created in my management cluster to place this workload cluster in 4# --file points to the bootstrap.yaml file used to deploy the management cluster 5# --dry-run \u0026gt; generates my workload-cluster.yaml file called tkg-cluster-1.yaml under the folder workload-cluster This process convert the flat bootstrap.yaml file to a cluster-class config-file.\nThe most interesting part in this file is whether the placement constraints have been considered. Lets have a look:\n1 - name: controlPlane 2 value: 3 machine: 4 diskGiB: 20 5 memoryMiB: 4096 6 numCPUs: 2 7 - name: worker 8 value: 9 machine: 10 diskGiB: 20 11 memoryMiB: 4096 12 numCPUs: 2 13 - name: controlPlaneZoneMatchingLabels # check 14 value: 15 region: k8s-region # check - will place my control planes only in the zones with the correct label 16 tkg-cp: allowed # check - will place my control planes only in the zones with the correct label 17 - name: security 18 value: 19 fileIntegrityMonitoring: 20 enabled: false 21 imagePolicy: 22 pullAlways: false 23 webhook: 24 enabled: false 25 spec: 26 allowTTL: 50 27 defaultAllow: true 28 denyTTL: 60 29 retryBackoff: 500 30 kubeletOptions: 31 eventQPS: 50 32 streamConnectionIdleTimeout: 4h0m0s 33 systemCryptoPolicy: default 34 version: v1.26.5+vmware.2-tkg.1 35 workers: 36 machineDeployments: 37 - class: tkg-worker 38 failureDomain: wdc-zone-2 # check - worker in zone-2 39 metadata: 40 annotations: 41 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 42 name: md-0 43 replicas: 1 44 strategy: 45 type: RollingUpdate 46 - class: tkg-worker 47 failureDomain: wdc-zone-3 # check - worker in zone-3 48 metadata: 49 annotations: 50 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 51 name: md-1 52 replicas: 1 53 strategy: 54 type: RollingUpdate 55 - class: tkg-worker 56 failureDomain: wdc-zone-3 # check - worker in zone-3 57 metadata: 58 annotations: 59 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 60 name: md-2 61 replicas: 1 62 strategy: 63 type: RollingUpdate The file looks good. Lets deploy it.\n1andreasm@tkg-bootstrap:~$ tanzu cluster create --file tkg-cluster-1.yaml 2Validating configuration... 3cluster class based input file detected, getting tkr version from input yaml 4input TKR Version: v1.26.5+vmware.2-tkg.1 5TKR Version v1.26.5+vmware.2-tkg.1, Kubernetes Version v1.26.5+vmware.2-tkg.1 configured 6Warning: Pinniped configuration not found; Authentication via Pinniped will not be set up in this cluster. If you wish to set up Pinniped after the cluster is created, please refer to the documentation. 7Skip checking VIP overlap when the VIP is empty. Cluster\u0026#39;s endpoint VIP will be allocated by NSX ALB IPAM. 8creating workload cluster \u0026#39;tkg-cluster-1\u0026#39;... After a couple of minutes or cups of coffee (depends on your environment):\n1Workload cluster \u0026#39;tkg-cluster-1\u0026#39; created Now lets go the same with the nodes here also, where are they placed in my vSphere environment.\nControl plane nodes placement:\nWorker nodes placemement:\nNice, just according to plan.\nTKG Workload Cluster deployment with multi-availability-zones - using vCenter host-groups as AZs In the first workload cluster deployment above I deployed the cluster to use my availability zones configured to use vSphere clusters as AZs. Now I will deploy a second workload cluster using the zones here added after the TKG management cluster was deployed. I will just reuse the workload-cluster.yaml from the first cluster, edit the names, namespaces and zones/regions accordingly.\nLets deploy it:\n1andreasm@tkg-bootstrap:~$ tanzu cluster create --file tkg-cluster-2-host-groups.yaml 2Validating configuration... 3cluster class based input file detected, getting tkr version from input yaml 4input TKR Version: v1.26.5+vmware.2-tkg.1 5TKR Version v1.26.5+vmware.2-tkg.1, Kubernetes Version v1.26.5+vmware.2-tkg.1 configured 6Warning: Pinniped configuration not found; Authentication via Pinniped will not be set up in this cluster. If you wish to set up Pinniped after the cluster is created, please refer to the documentation. 7Skip checking VIP overlap when the VIP is empty. Clusters endpoint VIP will be allocated by NSX ALB IPAM. 8creating workload cluster \u0026#39;tkg-cluster-2-hostgroup\u0026#39;... 9waiting for cluster to be initialized... 10cluster control plane is still being initialized: ScalingUp 11waiting for cluster nodes to be available... 12unable to get the autoscaler deployment, maybe it is not exist 13waiting for addons core packages installation... 14 15Workload cluster \u0026#39;tkg-cluster-2-hostgroup\u0026#39; created This cluster should now only be deployed to Cluster 3, using the DRS host-group based Availability Zones.\nAnd here the cluster has been deployed in Cluster-3, using the AZs rack1 and rack2 (nodes tkg-cluster-2-hostgroup-xxxx)\nNext up is to deploy a test application in the workload cluster utilizing the availability zones.\nDeploy applications on workload cluster in a multi-az environment In my scenario so far I have placed all the control plane nodes evenly distributed across all the vSphere Clusters/AZs. The worker nodes on the other hand is placed only on AZ-2 and AZ-3. Now I want to deploy an application in my workload cluster where I do a placement decision on where the different pods will be placed, according to the available zones the workload cluster is in.\nApplication/pod placement using nodeAffinity When a TKG workload cluster or TKG management cluster has been deployed with availability zones, the nodes will get updated label information that is referencing the availability zone the worker and control-plane nodes have been deployed in. This information can be used if one want to deploy the application in a specific zone. So in this chapter I will do exactly that. Deploy an application consisting of 4 pods, define the placement of the pods using the zone information available on the nodes.\nTo find the labels for the different placements I need to have a look at the nodes. There should be some labels indicating where they reside. I will clean up the output as I am only looking for something that starts with topology and define the zones, and the worker nodes only. So the output below gives me this:\n1andreasm@tkg-bootstrap:~$ k get nodes --show-labels 2NAME STATUS ROLES AGE VERSION LABELS 3tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj Ready \u0026lt;none\u0026gt; 82m v1.26.5+vmware.2 topology.kubernetes.io/zone=wdc-zone-2 4tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df Ready \u0026lt;none\u0026gt; 82m v1.26.5+vmware.2 topology.kubernetes.io/zone=wdc-zone-3 5tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h Ready \u0026lt;none\u0026gt; 82m v1.26.5+vmware.2 topology.kubernetes.io/zone=wdc-zone-3 One can also see the failuredomain placement using the following commands:\n1andreasm@tkg-bootstrap:~$ kubectl get machinedeployment -n tkg-ns-1 -o=custom-columns=NAME:.metadata.name,FAILUREDOMAIN:.spec.template.spec.failureDomain 2NAME FAILUREDOMAIN 3tkg-cluster-1-md-0-b4pfl wdc-zone-2 4tkg-cluster-1-md-1-vfzhk wdc-zone-3 5tkg-cluster-1-md-2-rpk4z wdc-zone-3 1andreasm@tkg-bootstrap:~$ kubectl get machine -n tkg-ns-1 -o=custom-columns=NAME:.metadata.name,FAILUREDOMAIN:.spec.failureDomain 2NAME FAILUREDOMAIN 3tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj wdc-zone-2 4tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df wdc-zone-3 5tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h wdc-zone-3 6tkg-cluster-1-znr8h-4v2tm wdc-zone-2 7tkg-cluster-1-znr8h-d72g5 wdc-zone-1 8tkg-cluster-1-znr8h-j6899 wdc-zone-3 Now I need to update my application deployment adding a section where I can define the placement information, this is done by using affinity.\n1 affinity: 2 nodeAffinity: 3 requiredDuringSchedulingIgnoredDuringExecution: 4 nodeSelectorTerms: 5 - matchExpressions: 6 - key: topology.kubernetes.io/zone 7 operator: In 8 values: 9 - wdc-zone-X My example application has been updated with the relevant information below. With this deployment I am allowing deployment in the wdc-zone-2 and wdc-zone-3 for the yelb-ui deployment, the 3 other deployments are only allowed to be placed in wdc-zone-3.\n1apiVersion: v1 2kind: Service 3metadata: 4 name: redis-server 5 labels: 6 app: redis-server 7 tier: cache 8 namespace: yelb 9spec: 10 type: ClusterIP 11 ports: 12 - port: 6379 13 selector: 14 app: redis-server 15 tier: cache 16--- 17apiVersion: v1 18kind: Service 19metadata: 20 name: yelb-db 21 labels: 22 app: yelb-db 23 tier: backenddb 24 namespace: yelb 25spec: 26 type: ClusterIP 27 ports: 28 - port: 5432 29 selector: 30 app: yelb-db 31 tier: backenddb 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: yelb-appserver 37 labels: 38 app: yelb-appserver 39 tier: middletier 40 namespace: yelb 41spec: 42 type: ClusterIP 43 ports: 44 - port: 4567 45 selector: 46 app: yelb-appserver 47 tier: middletier 48--- 49apiVersion: v1 50kind: Service 51metadata: 52 name: yelb-ui 53 labels: 54 app: yelb-ui 55 tier: frontend 56 namespace: yelb 57spec: 58 loadBalancerClass: ako.vmware.com/avi-lb 59 type: LoadBalancer 60 ports: 61 - port: 80 62 protocol: TCP 63 targetPort: 80 64 selector: 65 app: yelb-ui 66 tier: frontend 67--- 68apiVersion: apps/v1 69kind: Deployment 70metadata: 71 name: yelb-ui 72 namespace: yelb 73spec: 74 selector: 75 matchLabels: 76 app: yelb-ui 77 replicas: 1 78 template: 79 metadata: 80 labels: 81 app: yelb-ui 82 tier: frontend 83 spec: 84 affinity: 85 nodeAffinity: 86 requiredDuringSchedulingIgnoredDuringExecution: 87 nodeSelectorTerms: 88 - matchExpressions: 89 - key: topology.kubernetes.io/zone 90 operator: In 91 values: 92 - wdc-zone-2 93 - wdc-zone-3 94 containers: 95 - name: yelb-ui 96 image: registry.guzware.net/yelb/yelb-ui:0.3 97 imagePullPolicy: Always 98 ports: 99 - containerPort: 80 100--- 101apiVersion: apps/v1 102kind: Deployment 103metadata: 104 name: redis-server 105 namespace: yelb 106spec: 107 selector: 108 matchLabels: 109 app: redis-server 110 replicas: 1 111 template: 112 metadata: 113 labels: 114 app: redis-server 115 tier: cache 116 spec: 117 affinity: 118 nodeAffinity: 119 requiredDuringSchedulingIgnoredDuringExecution: 120 nodeSelectorTerms: 121 - matchExpressions: 122 - key: topology.kubernetes.io/zone 123 operator: In 124 values: 125 - wdc-zone-3 126 containers: 127 - name: redis-server 128 image: registry.guzware.net/yelb/redis:4.0.2 129 ports: 130 - containerPort: 6379 131--- 132apiVersion: apps/v1 133kind: Deployment 134metadata: 135 name: yelb-db 136 namespace: yelb 137spec: 138 selector: 139 matchLabels: 140 app: yelb-db 141 replicas: 1 142 template: 143 metadata: 144 labels: 145 app: yelb-db 146 tier: backenddb 147 spec: 148 affinity: 149 nodeAffinity: 150 requiredDuringSchedulingIgnoredDuringExecution: 151 nodeSelectorTerms: 152 - matchExpressions: 153 - key: topology.kubernetes.io/zone 154 operator: In 155 values: 156 - wdc-zone-3 157 containers: 158 - name: yelb-db 159 image: registry.guzware.net/yelb/yelb-db:0.3 160 ports: 161 - containerPort: 5432 162--- 163apiVersion: apps/v1 164kind: Deployment 165metadata: 166 name: yelb-appserver 167 namespace: yelb 168spec: 169 selector: 170 matchLabels: 171 app: yelb-appserver 172 replicas: 1 173 template: 174 metadata: 175 labels: 176 app: yelb-appserver 177 tier: middletier 178 spec: 179 affinity: 180 nodeAffinity: 181 requiredDuringSchedulingIgnoredDuringExecution: 182 nodeSelectorTerms: 183 - matchExpressions: 184 - key: topology.kubernetes.io/zone 185 operator: In 186 values: 187 - wdc-zone-3 188 containers: 189 - name: yelb-appserver 190 image: registry.guzware.net/yelb/yelb-appserver:0.3 191 ports: 192 - containerPort: 4567 Now to apply it and check the outcome.\n1andreasm@tkg-bootstrap:~$ k get pods -n yelb -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3redis-server-5997cbfdf7-f7wgh 1/1 Running 0 10m 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4yelb-appserver-6d65cc8-xt82g 1/1 Running 0 10m 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-db-7d4c56597f-58zd4 1/1 Running 0 10m 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 10m 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; If I compare this to the nodes below:\n1NAME FAILUREDOMAIN 2tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj wdc-zone-2 3tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df wdc-zone-3 4tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h wdc-zone-3 So far eveything looks good. All pods have been deployed in wdc-zone-3. The ui-pod is also allowed to be placed in wdc-zone-2. What happens if I scale it up with a couple of pods.\n1andreasm@tkg-bootstrap:~$ k scale deployment -n yelb --replicas 5 yelb-ui 2deployment.apps/yelb-ui scaled 3NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 4redis-server-5997cbfdf7-f7wgh 1/1 Running 0 22m 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-appserver-6d65cc8-xt82g 1/1 Running 0 22m 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-db-7d4c56597f-58zd4 1/1 Running 0 22m 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7yelb-ui-6c6fdfc66f-59zqs 1/1 Running 0 5m2s 100.96.1.5 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8yelb-ui-6c6fdfc66f-8w48g 1/1 Running 0 5m2s 100.96.2.4 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9yelb-ui-6c6fdfc66f-mprxx 1/1 Running 0 5m2s 100.96.1.4 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 10yelb-ui-6c6fdfc66f-n9slz 1/1 Running 0 5m2s 100.96.3.19 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 11yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 22m 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Two of the ui-pods has been placed in wdc-zone-2\nNow that I have full control of the app placement, lets test some failure scenarios.\nFailure simulations In this chapter I will quickly simulate an outage of Zone-2 or vSphere Cluster-2 where the 1 of the TKG mgmt control plane is residing, 1 of the workload Cluster-1 is residing plus 1 worker node for both the management cluster and tkg-cluster-1:\n1# TKG management cluster placement in my vSphere environment 2Nodes in the cluster with the \u0026#39;node.cluster.x-k8s.io/esxi-host\u0026#39; label: 3 4Node: tkg-wdc-az-mgmt-hgt2v-hb7vj | ESXi Host: esx04.cpod-nsxam-wdc #controlplane node on Zone-1 5Node: tkg-wdc-az-mgmt-hgt2v-w6r64 | ESXi Host: esx03.cpod-nsxam-wdc-03 #controlplane node on Zone-3 6Node: tkg-wdc-az-mgmt-hgt2v-zl5k9 | ESXi Host: esx04.cpod-nsxam-wdc-02 #controlplane node on Zone-2 7Node: tkg-wdc-az-mgmt-md-0-xn6cg-79f97555c7x45h4b-6ghbg | ESXi Host: esx04.cpod-nsxam-wdc-02 #worker node on Zone 2 8Node: tkg-wdc-az-mgmt-md-1-zmr4d-56ff586997xxndn8-hzs7f | ESXi Host: esx01.cpod-nsxam-wdc-03 #worker node on Zone-3 9Node: tkg-wdc-az-mgmt-md-2-67dm4-64f79b7dd7x6f56s-76qhv | ESXi Host: esx02.cpod-nsxam-wdc-03 #worker node on Zone-3 1# TKG workload cluster (tkg-cluster-1) placement in my vSphere environment 2Nodes in the cluster with the \u0026#39;node.cluster.x-k8s.io/esxi-host\u0026#39; label: 3 4Node: tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj | ESXi Host: esx03.cpod-nsxam-wdc-02 #worker node on Zone-2 5Node: tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df | ESXi Host: esx02.cpod-nsxam-wdc-03 #worker node on Zone-3 6Node: tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h | ESXi Host: esx03.cpod-nsxam-wdc-03 #worker node on Zone-3 7Node: tkg-cluster-1-znr8h-4v2tm | ESXi Host: esx03.cpod-nsxam-wdc-02 #controlplane node on Zone-2 8Node: tkg-cluster-1-znr8h-d72g5 | ESXi Host: esx04.cpod-nsxam-wdc.az-wdc #controlplane node on Zone-1 9Node: tkg-cluster-1-znr8h-j6899 | ESXi Host: esx04.cpod-nsxam-wdc-03.az-wdc #controlplane node on Zone-3 The Yelb application pods placement:\n1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 2redis-server-5997cbfdf7-f7wgh 1/1 Running 0 2d22h 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 3yelb-appserver-6d65cc8-xt82g 1/1 Running 0 2d22h 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4yelb-db-7d4c56597f-58zd4 1/1 Running 0 2d22h 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-ui-6c6fdfc66f-59zqs 1/1 Running 0 2d22h 100.96.1.5 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-ui-6c6fdfc66f-8w48g 1/1 Running 0 2d22h 100.96.2.4 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7yelb-ui-6c6fdfc66f-mprxx 1/1 Running 0 2d22h 100.96.1.4 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8yelb-ui-6c6fdfc66f-n9slz 1/1 Running 0 2d22h 100.96.3.19 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 2d22h 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; What I want to achieve is an available TKG mgmt cluster control plane, TKG workload tkg-cluster-1 control plane, and the Yelb application still up and running. The simple test I will do is to just go into vCenter power off all the nodes for these TKG cluster in Zone-2/vSphere Cluster-2. I could also shutdown the whole ESXi hosts but I have other services running there depending on this cluster also.\nOne last observation to see status of the two control planes, or K8s API endpoints is from my NSX-ALB dashboard for both TKG mgmt cluster and TKG-cluster-1.\nThe TKG mgmt cluster controlplanes/k8s-api endpoint:\nBefore shutting down I have full access to both the mgmt and tkg-cluster-1 k8s api, and the yelb-app ui is accessible.\nNow, powering off the nodes:\nSome observations after power off:\nFrom NSX-ALB\nYelb app is still available:\nIs the k8s api available?\n1NAME STATUS ROLES AGE VERSION 2tkg-wdc-az-mgmt-hgt2v-hb7vj Ready control-plane 4d23h v1.26.5+vmware.2 3tkg-wdc-az-mgmt-hgt2v-w6r64 Ready control-plane 4d23h v1.26.5+vmware.2 4tkg-wdc-az-mgmt-hgt2v-zl5k9 NotReady control-plane 4d23h v1.26.5+vmware.2 5tkg-wdc-az-mgmt-md-0-xn6cg-79f97555c7x45h4b-6ghbg NotReady \u0026lt;none\u0026gt; 4d23h v1.26.5+vmware.2 6tkg-wdc-az-mgmt-md-1-zmr4d-56ff586997xxndn8-hzs7f Ready \u0026lt;none\u0026gt; 4d23h v1.26.5+vmware.2 7tkg-wdc-az-mgmt-md-2-67dm4-64f79b7dd7x6f56s-76qhv Ready \u0026lt;none\u0026gt; 4d23h v1.26.5+vmware.2 The management cluster is, though complaining on the two nodes above as not ready. (They are powered off). The workload cluster k8s api available?\n1NAME STATUS ROLES AGE VERSION 2tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 3tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 4tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 5tkg-cluster-1-znr8h-4v2tm NotReady control-plane 4d21h v1.26.5+vmware.2 6tkg-cluster-1-znr8h-d72g5 Ready control-plane 4d21h v1.26.5+vmware.2 7tkg-cluster-1-znr8h-j6899 Ready control-plane 4d22h v1.26.5+vmware.2 It is. though a control plane node is down.\nThe Yelb pods:\n1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 2redis-server-5997cbfdf7-f7wgh 1/1 Running 0 2d22h 100.96.3.16 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 3yelb-appserver-6d65cc8-xt82g 1/1 Running 0 2d22h 100.96.3.17 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4yelb-db-7d4c56597f-58zd4 1/1 Running 0 2d22h 100.96.2.3 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-ui-6c6fdfc66f-59zqs 1/1 Running 1 (6m24s ago) 2d22h 100.96.1.5 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-ui-6c6fdfc66f-8w48g 1/1 Running 0 2d22h 100.96.2.4 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 7yelb-ui-6c6fdfc66f-mprxx 1/1 Running 1 (6m23s ago) 2d22h 100.96.1.2 tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8yelb-ui-6c6fdfc66f-n9slz 1/1 Running 0 2d22h 100.96.3.19 tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 9yelb-ui-6c6fdfc66f-ngjnm 1/1 Running 0 2d22h 100.96.2.2 tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; So it seems everything is still reacheable still after loosing Zone-2. When I did power off the nodes the first time it took around a minute before they were powered back on. I powered them down again and now it seems they are not being powered on again. Will wait a bit and see, otherwise I will try to power them on manually. After a longer while the tkg-cluster-1 nodes have now this status:\n1NAME STATUS ROLES AGE VERSION 2tkg-cluster-1-md-0-b4pfl-6d66f94fcdxnjnf6-t57dj NotReady,SchedulingDisabled \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 3tkg-cluster-1-md-1-vfzhk-5b6bbbbc5cxqk85x-tj4df Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 4tkg-cluster-1-md-2-rpk4z-78466846fdxkdjsp-vfd9h Ready \u0026lt;none\u0026gt; 4d22h v1.26.5+vmware.2 5tkg-cluster-1-znr8h-4v2tm NotReady,SchedulingDisabled control-plane 4d21h v1.26.5+vmware.2 6tkg-cluster-1-znr8h-d72g5 Ready control-plane 4d22h v1.26.5+vmware.2 7tkg-cluster-1-znr8h-j6899 Ready control-plane 4d22h v1.26.5+vmware.2 I will try to manually power the nodes back on. I did not have the chance to do that, they are now being deleted and recreated! Wow, cool\nUpdate existing TKG cluster to use new Availability Zones For details on how to update existing cluster to use new availabilty zones follow the official documentation here\nWrapping up This finishes the exploration of this useful feature in TKG 2.3. It is very flexible and allows for very robust design of node placement. Opens up for designs with high availability requirements. I do like very much the choice to use vSphere clusters, vCenter servers and DRS host-groups as the different objects in vCenter to use.\n","link":"https://blog.andreasm.io/2023/08/29/tkg-2.3-deployment-in-multiple-availability-zones/","section":"post","tags":["tanzu-kubernetes-grid","availability"],"title":"TKG 2.3 deployment in multiple availability zones"},{"body":"","link":"https://blog.andreasm.io/tags/authentication/","section":"tags","tags":null,"title":"authentication"},{"body":"","link":"https://blog.andreasm.io/tags/cert-manager/","section":"tags","tags":null,"title":"cert-manager"},{"body":"TMC local or TMC-SM TMC, Tanzu Mission Control, has always been a SaaS offering. But now it has also been released as a installable product you can deploy in your own environment. Throughout this post I will most likely refer to it as TMC SM or TMC local. TMC SM stands for Self Managed. For all official documentation and updated content head over here including the installation process.\nPre-requirements There is always some pre-requirements to be in place. Why should it always be pre-requirements? Well there is no need create any cars if there is no roads for them to drive on, will it? Thats enough humour for today. Instead of listing a detailed list of the requirements here, head over to the official page here and get familiar with it. In this post I have already deployed a Kubernetes cluster in my vSphere with Tanzu environment, that meets the requirements. More on that later. Then I will cover the certificate requirement deploying Cert-Manager and configure a ClusterIssuer. The image registry I will not cover as I already have a registry up and running and will be using that. I will not cover the loadbalancer/Ingress installation as I am assuming the following is already in place:\nA working vSphere 8 Environment A working Tanzu with vSphere Supervisor deployment A working NSX-ALB configuration to support both L4 and L7 services (meaning AKO is installed on the cluster for TMC-SM) A working image registry with a valid signed certificate, I will be using Harbor Registry. I will be using NSX ALB in combination with Contour that is being installed with TMC-SM, I will cover the specifics in configuring NSX-ALB, more specifically AKO, to support Keycloak via Ingress. Then I will cover the installation and configuration of Keycloak as the OIDC requirement. Then I will show how I handle my DNS zone for the TMC installation. As a final note, remember that the certificate I going to use needs to be trusted by the components that will be consuming them and DNS is important. Well lets go through it step by step.\nIn this order the following steps will be done:\nAnd, according to the official documentation:\nNote Deploying TMC Self-Managed 1.0 on a Tanzu Kubernetes Grid (TKG) 2.0 workload cluster running in vSphere with Tanzu on vSphere version 8.x is for tech preview only. Initiate deployments only in pre-production environments or production environments where support for the integration is not required. vSphere 8u1 or later is required in order to test the tech preview integration.\nI will use vSphere 8 U1 in this post, and is by no means meant as a guideline to a production ready setup of TMC-SM.\nThe TKG cluster - where TMC will be deployed I have used this configuration to deploy my TKG cluster, I have used the VM class guaranteed-large, it will work with 4CPU and 8GB ram on the nodes also. Oh, and by the way. This installation is done on a vSphere with Tanzu multi-zone setup:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: tmc-sm-cluster #My own name on the cluster 5 namespace: ns-wdc-prod #My vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] #Edited by me 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] #Edited by me 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.24.9+vmware.1-tkg.4 #My latest available TKR version 16 controlPlane: 17 replicas: 1 # only one controlplane (saving resources and time) 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 #muliple node pools are used 23 machineDeployments: 24 - class: node-pool 25 name: node-pool-1 26 replicas: 1 #only 1 worker here 27 metadata: 28 annotations: 29 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 30 #failure domain the machines will be created in 31 #maps to a vSphere Zone; name must match exactly 32 failureDomain: wdc-zone-1 #named after my vSphere zone 33 - class: node-pool 34 name: node-pool-2 35 replicas: 2 #only 1 worker here 36 metadata: 37 annotations: 38 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 39 #failure domain the machines will be created in 40 #maps to a vSphere Zone; name must match exactly 41 failureDomain: wdc-zone-2 #named after my vSphere zone 42 - class: node-pool 43 name: node-pool-3 44 replicas: 1 #only 1 worker here 45 metadata: 46 annotations: 47 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 48 #failure domain the machines will be created in 49 #maps to a vSphere Zone; name must match exactly 50 failureDomain: wdc-zone-3 #named after my vSphere zone 51 variables: 52 - name: vmClass 53 value: guaranteed-large 54 - name: storageClass 55 value: all-vsans #my zonal storageclass 56 - name: defaultStorageClass 57 value: all-vsans 58 - name: controlPlaneVolumes 59 value: 60 - name: etcd 61 capacity: 62 storage: 10Gi 63 mountPath: /var/lib/etcd 64 storageClass: all-vsans 65 - name: nodePoolVolumes 66 value: 67 - name: containerd 68 capacity: 69 storage: 50Gi 70 mountPath: /var/lib/containerd 71 storageClass: all-vsans 72 - name: kubelet 73 capacity: 74 storage: 50Gi 75 mountPath: /var/lib/kubelet 76 storageClass: all-vsans As soon as the cluster is ready and deployed I will log into it and change my context using kubectl vsphere login .... and apply my clusterrole policy:\n1apiVersion: rbac.authorization.k8s.io/v1 2kind: ClusterRole 3metadata: 4 name: psp:privileged 5rules: 6- apiGroups: [\u0026#39;policy\u0026#39;] 7 resources: [\u0026#39;podsecuritypolicies\u0026#39;] 8 verbs: [\u0026#39;use\u0026#39;] 9 resourceNames: 10 - vmware-system-privileged 11--- 12apiVersion: rbac.authorization.k8s.io/v1 13kind: ClusterRoleBinding 14metadata: 15 name: all:psp:privileged 16roleRef: 17 kind: ClusterRole 18 name: psp:privileged 19 apiGroup: rbac.authorization.k8s.io 20subjects: 21- kind: Group 22 name: system:serviceaccounts 23 apiGroup: rbac.authorization.k8s.io ClusterIssuer To support dynamically creating/issuing certificates I will deploy and install Cert-Manager. The approach I am using to deploy Cert-Manager is to use the provided Cert-Manager Packages available in Tanzu.\nTanzu Cert-Manager Package I will have to add the package repository where I can download and install Cert-Manager from and a namespace for the packages themselves. Before I can approach with this I need the Tanzu CLI. The official approach can be found here Download the Tanzu CLI from here\nExtract it:\n1tar -zxvf tanzu-cli-bundle-linux-amd64.tar.gz Enter into the cli folder and copy or move to a folder in your paht:\n1andreasm@linuxvm01:~/tanzu-cli/cli/core/v0.29.0$ cp tanzu-core-linux_amd64 /usr/local/bin/tanzu Run tanzu init and tanzu plugin sync:\n1tanzu init 2tanzu plugin sync When that is done, go ahead dreate the namespace:\n1kubectl create ns tanzu-package-repo-global Then add the the repository:\n1tanzu package repository add tanzu-standard --url projects.registry.vmware.com/tkg/packages/standard/repo:v2.2.0 -n tanzu-package-repo-global Then installing the Cert-Manager package:\n1tanzu package install cert-manager --package cert-manager.tanzu.vmware.com --version 1.7.2+vmware.3-tkg.3 -n tanzu-package-repo-global CA Issuer Now it is time to configure Cert-Manager with a CA certifcate so it can act as a CA ClusterIssuer. To do that lets start by creating a CA certificate.\nCreate the certificate, without passphrase:\n1andreasm@linuxvm01:~/tmc-sm$ openssl req -nodes -x509 -sha256 -days 1825 -newkey rsa:2048 -keyout rootCA.key -out rootCA.crt 2Generating a RSA private key 3..........................................................................+++++ 4.+++++ 5writing new private key to \u0026#39;rootCA.key\u0026#39; 6----- 7You are about to be asked to enter information that will be incorporated 8into your certificate request. 9What you are about to enter is what is called a Distinguished Name or a DN. 10There are quite a few fields but you can leave some blank 11For some fields there will be a default value, 12If you enter \u0026#39;.\u0026#39;, the field will be left blank. 13----- 14Country Name (2 letter code) [AU]:US 15State or Province Name (full name) [Some-State]:punxsutawney 16Locality Name (eg, city) []:Groundhog 17Organization Name (eg, company) [Internet Widgits Pty Ltd]:Day 18Organizational Unit Name (eg, section) []:SameDay 19Common Name (e.g. server FQDN or YOUR name) []:tmc.pretty-awesome-domain.net 20Email Address []: This should give me two files:\n11407 Jul 12 14:21 rootCA.crt 21704 Jul 12 14:19 rootCA.key Then I will go ahead and create a secret for Cert-Manager using these two above files in Base64 format:\n1andreasm@linuxvm01:~/tmc-sm$ cat rootCA.crt | base64 -w0 2LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ0ekNDQXN1Z0F3SUJBZ0lVSFgyak5rbysvdnNlcjc0dGpxS2R3U1ZMQlhVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd2dZQXhDekFKQmdOVkJBWVRBbFZUTVJVd0V3WURWUVFJREF4d2RXNTRjM1YwWVhkdVpYa3hFakFRQmdOVgpCQWNNQ1VkeWIzVnVaR2h2WnpFTU1Bb0dBMVVFQ2d3RFJHRjVNUkF3RGdZRFZRUUxEQWRUWVcxbFJHRjVNU1l3CkpBWURWUVFEREIxMGJX 3andreasm@linuxvm01:~/tmc-sm$ cat rootCA.key | base64 -w0 4LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2Z0lCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktnd2dnU2tBZ0VBQW9JQkFRREFSR2RCSWwreUVUbUsKOGI0N2l4NUNJTDlXNVh2dkZFY0Q3KzZMbkxxQ3ZVTWdyNWxhNGFjUU8vZUsxUFdIV0YvWk9UN0ZyWUY0QVpmYgpFbzB5ejFxL3pGT3AzQS9sMVNqN3lUeHY5WmxYRU9DbWI4dGdQVm9Ld3drUHFiQ0RtNVZ5Ri9HaGUvMDFsbXl6CnEyMlpGM0M4 Put the above content into my secret.yaml file below\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: ca-key-pair 5 namespace: cert-manager 6data: 7 tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQvekNDQ.... 8 tls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2UUlCQUR... Then apply it:\n1andreasm@linuxvm01:~/tmc-sm$ k apply -f secret.yaml 2secret/ca-key-pair configured Now create the ClusterIssuer yaml definition:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: ca-issuer 5spec: 6 ca: 7 secretName: ca-key-pair This points to the secret created in the previous step. And apply it:\n1andreasm@linuxvm01:~/tmc-sm$ k apply -f secret-key-pair.yaml 2clusterissuer.cert-manager.io/ca-issuer configured Now check the status of the clusterissuer. It can take a couple of seconds. If it does not go to a Ready state, check the logs of the cert-manager pod.\n1andreasm@linuxvm01:~/tmc-sm$ k get clusterissuers.cert-manager.io 2NAME READY AGE 3ca-issuer True 20s Now, we have a ClusterIssuer we can use to provide us with self-signed certificates.\nDNS-Zone In my environment I am using dnsmasq as my backend DNS server for all my clients, servers etc to handle dns records and zones. So in my dnsmasq config I will need to create a \u0026quot;forward\u0026quot; zone for my specific tmc.pretty-awesome-domain.net which will forward all requests to the DNS service I have configured in Avi. Here is the dnsmasq.conf:\n1server=/.tmc.pretty-awesome-domain.net/10.101.211.9 The IP 10.101.211.9 is my NSX ALB DNS VS. Now in my NSX ALB DNS service I need to create an entry that points to tmc.pretty-awesome-domain.net where the IP is the Contour IP. In the later stage of this post we need to define a value yaml file. In there we can specify a certain IP the Contour service should get. This IP is being used by the NSX ALB dns to forward all the wildcard requests to the tmc.pretty-awesome-domain.net. To configure that in NSX ALB:\nEdit the the DNS VS, add a static DNS record, point to the ip of the Contour service (not there yet, but will come when we start deploying TMC-SM). Also remeber to check Enable wild-card match:\nSo what is going on now. I have configured my NSX ALB DNS servide to be responsible for a domain called pretty-awesome-domain.net by adding this domain to my DNS Profile template which the NSX ALB Cloud is configured with. Each time a Kubernetes service requests a DNS record in this domain NSX ALB will create this entry with correct fqdn/IP mapping. Then I have also created a static entry for the subdomain tmc.pretty-awesome-domain.net in the NSX ALB provider which will forward all wildcard requests to the Contour service which holds these actual records:\n\u0026lt;my-tmc-dns-zone\u0026gt; alertmanager.\u0026lt;my-tmc-dns-zone\u0026gt; auth.\u0026lt;my-tmc-dns-zone\u0026gt; blob.\u0026lt;my-tmc-dns-zone\u0026gt; console.s3.\u0026lt;my-tmc-dns-zone\u0026gt; gts-rest.\u0026lt;my-tmc-dns-zone\u0026gt; gts.\u0026lt;my-tmc-dns-zone\u0026gt; landing.\u0026lt;my-tmc-dns-zone\u0026gt; pinniped-supervisor.\u0026lt;my-tmc-dns-zone\u0026gt; prometheus.\u0026lt;my-tmc-dns-zone\u0026gt; s3.\u0026lt;my-tmc-dns-zone\u0026gt; tmc-local.s3.\u0026lt;my-tmc-dns-zone\u0026gt; So I dont have to manually create these dns records, they will just happily be handed over to the Contour ingress records. This is how my DNS lookups look like: Keycloak - OIDC/ID provider - using AKO as Ingress controller One of the requirements for TMC local is also an OIDC provider. My colleague Alex gave me the tip to test out Keycloak as it also work as a standalone provider, without any backend ldap service. So this section will be divided into two sub-sections, one section covers the actual installation of Keycloak using Helm, and the other section covers the Keycloak authentication settings that is required for TMC local.\nKeycloak installation I am using Helm to install Keycloak in my cluster. That means we need Helm installed, the Helm repository that contains the Keycloak charts. I will be using the Bitnami repo for this purpose. So first add the Bitnami repo:\n1andreasm@linuxvm01:~$ helm repo add bitnami https://charts.bitnami.com/bitnami 2\u0026#34;bitnami\u0026#34; has been added to your repositories Then do a Helm search repo to see if it has been added (look for a long list of bitnami/xxxx):\n1andreasm@linuxvm01:~$ helm search repo 2NAME CHART VERSION\tAPP VERSION DESCRIPTION 3bitnami/airflow 14.3.1 2.6.3 Apache Airflow is a tool to express and execute... 4bitnami/apache 9.6.4 2.4.57 Apache HTTP Server is an open-source HTTP serve... 5bitnami/apisix 2.0.3 3.3.0 Apache APISIX is high-performance, real-time AP... 6bitnami/appsmith 0.3.9 1.9.25 Appsmith is an open source platform for buildin... 7bitnami/argo-cd 4.7.14 2.7.7 Argo CD is a continuous delivery tool for Kuber... 8bitnami/argo-workflows 5.3.6 3.4.8 Argo Workflows is meant to orchestrate Kubernet... 9bitnami/aspnet-core 4.3.3 7.0.9 ASP.NET Core is an open-source framework for we... 10bitnami/cassandra 10.4.3 4.1.2 Apache Cassandra is an open source distributed ... 11bitnami/cert-manager 0.11.5 1.12.2 cert-manager is a Kubernetes add-on to automate... 12bitnami/clickhouse 3.5.4 23.6.2 ClickHouse is an open-source column-oriented OL... 13bitnami/common 2.6.0 2.6.0 A Library Helm Chart for grouping common logic ... 14bitnami/concourse 2.2.3 7.9.1 Concourse is an automation system written in Go... 15bitnami/consul 10.12.4 1.16.0 HashiCorp Consul is a tool for discovering and ... 16bitnami/contour 12.1.1 1.25.0 Contour is an open source Kubernetes ingress co... 17bitnami/contour-operator 4.2.1 1.24.0 DEPRECATED The Contour Operator extends the Kub... 18bitnami/dataplatform-bp2 12.0.5 1.0.1 DEPRECATED This Helm chart can be used for the ... 19bitnami/discourse 10.3.4 3.0.4 Discourse is an open source discussion platform... 20bitnami/dokuwiki 14.1.4 20230404.1.0 DokuWiki is a standards-compliant wiki optimize... 21bitnami/drupal 14.1.5 10.0.9 Drupal is one of the most versatile open source... 22bitnami/ejbca 7.1.3 7.11.0 EJBCA is an enterprise class PKI Certificate Au... 23bitnami/elasticsearch 19.10.3 8.8.2 Elasticsearch is a distributed search and analy... 24bitnami/etcd 9.0.4 3.5.9 etcd is a distributed key-value store designed ... 25bitnami/external-dns 6.20.4 0.13.4 ExternalDNS is a Kubernetes addon that configur... 26bitnami/flink 0.3.3 1.17.1 Apache Flink is a framework and distributed pro... 27bitnami/fluent-bit 0.4.6 2.1.6 Fluent Bit is a Fast and Lightweight Log Proces... 28bitnami/fluentd 5.8.5 1.16.1 Fluentd collects events from various data sourc... 29bitnami/flux 0.3.5 0.36.1 Flux is a tool for keeping Kubernetes clusters ... 30bitnami/geode 1.1.8 1.15.1 DEPRECATED Apache Geode is a data management pl... 31bitnami/ghost 19.3.23 5.54.0 Ghost is an open source publishing platform des... 32bitnami/gitea 0.3.5 1.19.4 Gitea is a lightweight code hosting solution. W... 33bitnami/grafana 9.0.1 10.0.1 Grafana is an open source metric analytics and ... 34bitnami/grafana-loki 2.10.0 2.8.2 Grafana Loki is a horizontally scalable, highly... 35bitnami/grafana-mimir 0.5.4 2.9.0 Grafana Mimir is an open source, horizontally s... 36bitnami/grafana-operator 3.0.2 5.1.0 Grafana Operator is a Kubernetes operator that ... 37bitnami/grafana-tempo 2.3.4 2.1.1 Grafana Tempo is a distributed tracing system t... 38bitnami/haproxy 0.8.4 2.8.1 HAProxy is a TCP proxy and a HTTP reverse proxy... 39bitnami/haproxy-intel 0.2.11 2.7.1 DEPRECATED HAProxy for Intel is a high-performa... 40bitnami/harbor 16.7.0 2.8.2 Harbor is an open source trusted cloud-native r... 41bitnami/influxdb 5.7.1 2.7.1 InfluxDB(TM) is an open source time-series data... 42bitnami/jaeger 1.2.6 1.47.0 Jaeger is a distributed tracing system. It is u... 43bitnami/jasperreports 15.1.3 8.2.0 JasperReports Server is a stand-alone and embed... 44bitnami/jenkins 12.2.4 2.401.2 Jenkins is an open source Continuous Integratio... 45bitnami/joomla 14.1.5 4.3.3 Joomla! is an award winning open source CMS pla... 46bitnami/jupyterhub 4.1.6 4.0.1 JupyterHub brings the power of notebooks to gro... 47bitnami/kafka 23.0.2 3.5.0 Apache Kafka is a distributed streaming platfor... 48bitnami/keycloak 15.1.6 21.1.2 Keycloak is a high performance Java-based ident... And in the list above we can see the bitnami/keycloak charts. So far so good. Now grab the default keycloak chart values file:\n1helm show values bitnami/keycloak \u0026gt; keycloak-values.yaml This should provide you with a file called keycloak-values.yaml. We need to do some basic changes in here. My values file below is snippets from the full values file where I have edited with comments on what I have changed:\n1## Keycloak authentication parameters 2## ref: https://github.com/bitnami/containers/tree/main/bitnami/keycloak#admin-credentials 3## 4auth: 5 ## @param auth.adminUser Keycloak administrator user 6 ## 7 adminUser: admin # I have changed the user to admin 8 ## @param auth.adminPassword Keycloak administrator password for the new user 9 ## 10 adminPassword: \u0026#34;PASSWORD\u0026#34; # I have entered my password here 11 ## @param auth.existingSecret Existing secret containing Keycloak admin password 12 ## 13 existingSecret: \u0026#34;\u0026#34; 14 ## @param auth.passwordSecretKey Key where the Keycloak admin password is being stored inside the existing secret. 15 ## 16 passwordSecretKey: \u0026#34;\u0026#34; 17 ... 18 ## @param production Run Keycloak in production mode. TLS configuration is required except when using proxy=edge. 19## 20production: false 21## @param proxy reverse Proxy mode edge, reencrypt, passthrough or none 22## ref: https://www.keycloak.org/server/reverseproxy 23## 24proxy: edge # I am using AKO to terminate the SSL cert at the Service Engine side. So set this to edge 25## @param httpRelativePath Set the path relative to \u0026#39;/\u0026#39; for serving resources. Useful if you are migrating from older version which were using \u0026#39;/auth/\u0026#39; 26## ref: https://www.keycloak.org/migration/migrating-to-quarkus#_default_context_path_changed 27## 28... 29postgresql: 30 enabled: true 31 auth: 32 postgresPassword: \u0026#34;PASSWORD\u0026#34; # I have added my own password here 33 username: bn_keycloak 34 password: \u0026#34;PASSWORD\u0026#34; # I have added my own password here 35 database: bitnami_keycloak 36 existingSecret: \u0026#34;\u0026#34; 37 architecture: standalone 38 39 In short, the places I have done changes is adjusting the adminUser, password for the adminUser. Then I changed the proxy setting to edge, and adjusted the PostgreSQL password as I dont want to use the auto-generated passwords.\nThen I can deploy Keycloak with this value yaml file:\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k create ns keycloak 2andreasm@linuxvm01:~/tmc-sm/keycloak$ helm upgrade -i -n keycloak keycloak bitnami/keycloak -f keycloak-values.yaml 3Release \u0026#34;keycloak\u0026#34; has been upgraded. Happy Helming! 4NAME: keycloak 5LAST DEPLOYED: Wed Jul 12 21:34:32 2023 6NAMESPACE: keycloak 7STATUS: deployed 8REVISION: 4 9TEST SUITE: None 10NOTES: 11CHART NAME: keycloak 12CHART VERSION: 15.1.6 13APP VERSION: 21.1.2 14 15** Please be patient while the chart is being deployed ** 16 17Keycloak can be accessed through the following DNS name from within your cluster: 18 19 keycloak.keycloak.svc.cluster.local (port 80) 20 21To access Keycloak from outside the cluster execute the following commands: 22 231. Get the Keycloak URL by running these commands: 24 25 export HTTP_SERVICE_PORT=$(kubectl get --namespace keycloak -o jsonpath=\u0026#34;{.spec.ports[?(@.name==\u0026#39;http\u0026#39;)].port}\u0026#34; services keycloak) 26 kubectl port-forward --namespace keycloak svc/keycloak ${HTTP_SERVICE_PORT}:${HTTP_SERVICE_PORT} \u0026amp; 27 28 echo \u0026#34;http://127.0.0.1:${HTTP_SERVICE_PORT}/\u0026#34; 29 302. Access Keycloak using the obtained URL. 313. Access the Administration Console using the following credentials: 32 33 echo Username: admin 34 echo Password: $(kubectl get secret --namespace keycloak keycloak -o jsonpath=\u0026#34;{.data.admin-password}\u0026#34; | base64 -d) 35 I am using the helm command upgrade -i, which means if it is not installed it will, if it is installed it will upgrade the existing installation with the content in the values yaml file.\nKeeping the values.yaml as default as possible it will not create any serviceType loadBalancer or Ingress. That is something I would like to handle my self after the actual Keycloak deployment is up and running. More on that later.\nAny pods running:\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k get pods -n keycloak 2NAME READY STATUS RESTARTS AGE 3keycloak-0 0/1 Running 0 14s 4keycloak-postgresql-0 1/1 Running 0 11h Almost. Give it a couple of seconds more and it should be ready.\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k get pods -n keycloak 2NAME READY STATUS RESTARTS AGE 3keycloak-0 1/1 Running 0 2m43s 4keycloak-postgresql-0 1/1 Running 0 11h The Keycloak is running. Then I need to expose it with a serviceType loadBalancer or Ingress. I have opted to use Ingress as I feel it is much easier to managed the certificates in NSX-ALB and also let the NSX-ALB SEs handle the TLS termination, instead of in the pod itself. So now I need to confige the Ingress for the ClusterIP service that is automatically created by the Helm chart above. Lets check the service:\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k get svc -n keycloak 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3keycloak ClusterIP 20.10.61.222 \u0026lt;none\u0026gt; 80/TCP 31h 4keycloak-headless ClusterIP None \u0026lt;none\u0026gt; 80/TCP 31h 5keycloak-postgresql ClusterIP 20.10.8.129 \u0026lt;none\u0026gt; 5432/TCP 31h 6keycloak-postgresql-hl ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 31h The one I am interested in is the keycloak ClusterIP service. Next step is to configure the Ingress for this service. I will post the yaml I am using for this Ingress, and explain a bit more below. This step assumes Avi is installed and configured, and AKO has been deployed and ready to provision Ingress requests. For details on how to install AKO in TKG read here and here.\nJust a quick comment before we go through the Ingress, what I want to achieve is an Ingress that is handling the client requests and TLS termination at the \u0026quot;loadbalancer\u0026quot; side. Traffic from the \u0026quot;loadbalancer\u0026quot; (the Avi SEs) to the Keycloak pod is pure http, no SSL. I trust my infra between the SEs and Keycloak pods.\nThe Ingress for Keycloak:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: keycloak 5 namespace: keycloak 6 annotations: 7 cert-manager.io/cluster-issuer: ca-issuer 8 cert-manager.io/common-name: keycloak.tmc.pretty-awesome-domain.net 9# ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 10 11spec: 12 ingressClassName: avi-lb 13 rules: 14 - host: keycloak.tmc.pretty-awesome-domain.net 15 http: 16 paths: 17 - path: / 18 pathType: Prefix 19 backend: 20 service: 21 name: keycloak 22 port: 23 number: 80 24 tls: 25 - hosts: 26 - keycloak.tmc.pretty-awesome-domain.net 27 secretName: keycloak-ingress-secret In the above yaml I am creating the Ingress to expose my Keycloak instance externally. I am also kindly asking my ca-issuer to issue a fresh new certificate for this Ingress to use. This is done by adding the annotation cert-manager.io/cluster-issuer: ca-issuer which would be sufficient enough in other scenarios, but I also needed to add this section:\n1 tls: 2 - hosts: 3 - keycloak.tmc.pretty-awesome-domain.net 4 secretName: keycloak-ingress-secret Now I just need to apply it:\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k apply -f keycloak-ingress.yaml 2ingress.networking.k8s.io/keycloak created Now, what is created on the NSX-ALB side: There is my Ingress for Keycloak. Lets check the certificate it is using:\nIt is using my new freshly created certificate. I will go ahead and open the ui of Keycloak in my browser: Whats this? The certificate is the correct one... Remember that I am using Cert-Manager to issue self-signed certificates? I need to trust the root of the CA in my client to make this certificate trusted. Depending on your client's operating system I will not go through how this is done. But I have now added my rootCA.crt certificate created earlier (the same rootCA.crt I generated for my ClusterIssuer) as a trusted root certificate in my client. Let me try again now.\nNow it is looking much better ðŸ˜„\nLets try to log in: Using the username and password provided in the value yaml file. Seems to be something wrong here.. My login is just \u0026quot;looping\u0026quot; somehow.. Lets check the Keycloak pod logs :\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k logs -n keycloak keycloak-0 2keycloak 21:34:34.96 3keycloak 21:34:34.97 Welcome to the Bitnami keycloak container 4keycloak 21:34:34.97 Subscribe to project updates by watching https://github.com/bitnami/containers 5keycloak 21:34:34.97 Submit issues and feature requests at https://github.com/bitnami/containers/issues 6keycloak 21:34:34.97 7keycloak 21:34:34.97 INFO ==\u0026gt; ** Starting keycloak setup ** 8keycloak 21:34:34.98 INFO ==\u0026gt; Validating settings in KEYCLOAK_* env vars... 9keycloak 21:34:35.00 INFO ==\u0026gt; Trying to connect to PostgreSQL server keycloak-postgresql... 10keycloak 21:34:35.01 INFO ==\u0026gt; Found PostgreSQL server listening at keycloak-postgresql:5432 11keycloak 21:34:35.02 INFO ==\u0026gt; Configuring database settings 12keycloak 21:34:35.05 INFO ==\u0026gt; Enabling statistics 13keycloak 21:34:35.06 INFO ==\u0026gt; Configuring http settings 14keycloak 21:34:35.08 INFO ==\u0026gt; Configuring hostname settings 15keycloak 21:34:35.09 INFO ==\u0026gt; Configuring cache count 16keycloak 21:34:35.10 INFO ==\u0026gt; Configuring log level 17keycloak 21:34:35.11 INFO ==\u0026gt; Configuring proxy 18keycloak 21:34:35.12 INFO ==\u0026gt; ** keycloak setup finished! ** 19 20keycloak 21:34:35.14 INFO ==\u0026gt; ** Starting keycloak ** 21Appending additional Java properties to JAVA_OPTS: -Djgroups.dns.query=keycloak-headless.keycloak.svc.cluster.local 22Updating the configuration and installing your custom providers, if any. Please wait. 232023-07-12 21:34:38,622 WARN [org.keycloak.services] (build-6) KC-SERVICES0047: metrics (org.jboss.aerogear.keycloak.metrics.MetricsEndpointFactory) is implementing the internal SPI realm-restapi-extension. This SPI is internal and may change without notice 242023-07-12 21:34:39,163 WARN [org.keycloak.services] (build-6) KC-SERVICES0047: metrics-listener (org.jboss.aerogear.keycloak.metrics.MetricsEventListenerFactory) is implementing the internal SPI eventsListener. This SPI is internal and may change without notice 252023-07-12 21:34:51,024 INFO [io.quarkus.deployment.QuarkusAugmentor] (main) Quarkus augmentation completed in 14046ms 262023-07-12 21:34:52,578 INFO [org.keycloak.quarkus.runtime.hostname.DefaultHostnameProvider] (main) Hostname settings: Base URL: \u0026lt;unset\u0026gt;, Hostname: \u0026lt;request\u0026gt;, Strict HTTPS: false, Path: \u0026lt;request\u0026gt;, Strict BackChannel: false, Admin URL: \u0026lt;unset\u0026gt;, Admin: \u0026lt;request\u0026gt;, Port: -1, Proxied: true 272023-07-12 21:34:54,013 WARN [io.quarkus.agroal.runtime.DataSources] (main) Datasource \u0026lt;default\u0026gt; enables XA but transaction recovery is not enabled. Please enable transaction recovery by setting quarkus.transaction-manager.enable-recovery=true, otherwise data may be lost if the application is terminated abruptly 282023-07-12 21:34:54,756 INFO [org.infinispan.SERVER] (keycloak-cache-init) ISPN005054: Native IOUring transport not available, using NIO instead: io.netty.incubator.channel.uring.IOUring 292023-07-12 21:34:54,961 WARN [org.infinispan.CONFIG] (keycloak-cache-init) ISPN000569: Unable to persist Infinispan internal caches as no global state enabled 302023-07-12 21:34:54,987 WARN [io.quarkus.vertx.http.runtime.VertxHttpRecorder] (main) The X-Forwarded-* and Forwarded headers will be considered when determining the proxy address. This configuration can cause a security issue as clients can forge requests and send a forwarded header that is not overwritten by the proxy. Please consider use one of these headers just to forward the proxy address in requests. 312023-07-12 21:34:54,990 WARN [org.infinispan.PERSISTENCE] (keycloak-cache-init) ISPN000554: jboss-marshalling is deprecated and planned for removal 322023-07-12 21:34:55,005 INFO [org.infinispan.CONTAINER] (keycloak-cache-init) ISPN000556: Starting user marshaller \u0026#39;org.infinispan.jboss.marshalling.core.JBossUserMarshaller\u0026#39; 332023-07-12 21:34:55,450 INFO [org.infinispan.CLUSTER] (keycloak-cache-init) ISPN000078: Starting JGroups channel `ISPN` 342023-07-12 21:34:55,455 INFO [org.jgroups.JChannel] (keycloak-cache-init) local_addr: 148671ea-e4a4-4b1f-9ead-78c598924c94, name: keycloak-0-45065 352023-07-12 21:34:55,466 INFO [org.jgroups.protocols.FD_SOCK2] (keycloak-cache-init) server listening on *.57800 362023-07-12 21:34:57,471 INFO [org.jgroups.protocols.pbcast.GMS] (keycloak-cache-init) keycloak-0-45065: no members discovered after 2002 ms: creating cluster as coordinator 372023-07-12 21:34:57,480 INFO [org.infinispan.CLUSTER] (keycloak-cache-init) ISPN000094: Received new cluster view for channel ISPN: [keycloak-0-45065|0] (1) [keycloak-0-45065] 382023-07-12 21:34:57,486 INFO [org.infinispan.CLUSTER] (keycloak-cache-init) ISPN000079: Channel `ISPN` local address is `keycloak-0-45065`, physical addresses are `[20.20.2.68:7800]` 392023-07-12 21:34:57,953 INFO [org.keycloak.connections.infinispan.DefaultInfinispanConnectionProviderFactory] (main) Node name: keycloak-0-45065, Site name: null 402023-07-12 21:34:57,962 INFO [org.keycloak.broker.provider.AbstractIdentityProviderMapper] (main) Registering class org.keycloak.broker.provider.mappersync.ConfigSyncEventListener 412023-07-12 21:34:59,149 INFO [io.quarkus] (main) Keycloak 21.1.2 on JVM (powered by Quarkus 2.13.8.Final) started in 7.949s. Listening on: http://0.0.0.0:8080 422023-07-12 21:34:59,150 INFO [io.quarkus] (main) Profile dev activated. 432023-07-12 21:34:59,150 INFO [io.quarkus] (main) Installed features: [agroal, cdi, hibernate-orm, jdbc-h2, jdbc-mariadb, jdbc-mssql, jdbc-mysql, jdbc-oracle, jdbc-postgresql, keycloak, logging-gelf, micrometer, narayana-jta, reactive-routes, resteasy, resteasy-jackson, smallrye-context-propagation, smallrye-health, vertx] 442023-07-12 21:34:59,160 ERROR [org.keycloak.services] (main) KC-SERVICES0010: Failed to add user \u0026#39;admin\u0026#39; to realm \u0026#39;master\u0026#39;: user with username exists 452023-07-12 21:34:59,161 WARN [org.keycloak.quarkus.runtime.KeycloakMain] (main) Running the server in development mode. DO NOT use this configuration in production. 462023-07-12 22:04:22,511 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 472023-07-12 22:04:27,809 WARN [org.keycloak.events] (executor-thread-6) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 482023-07-12 22:04:33,287 WARN [org.keycloak.events] (executor-thread-3) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 492023-07-12 22:04:44,105 WARN [org.keycloak.events] (executor-thread-7) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 502023-07-12 22:04:55,303 WARN [org.keycloak.events] (executor-thread-5) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 512023-07-12 22:05:00,707 WARN [org.keycloak.events] (executor-thread-6) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 522023-07-12 22:05:06,861 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 532023-07-12 22:05:12,484 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 542023-07-12 22:05:18,351 WARN [org.keycloak.events] (executor-thread-6) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 552023-07-12 22:05:28,509 WARN [org.keycloak.events] (executor-thread-4) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 562023-07-12 22:05:37,438 WARN [org.keycloak.events] (executor-thread-7) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 572023-07-12 22:05:42,742 WARN [org.keycloak.events] (executor-thread-5) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 582023-07-12 22:05:47,750 WARN [org.keycloak.events] (executor-thread-5) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 592023-07-12 22:05:53,019 WARN [org.keycloak.events] (executor-thread-3) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret 602023-07-12 22:05:58,020 WARN [org.keycloak.events] (executor-thread-3) type=REFRESH_TOKEN_ERROR, realmId=6944b0b7-3592-4ef3-ad40-4b1a7b64543d, clientId=security-admin-console, userId=null, ipAddress=172.18.6.141, error=invalid_token, grant_type=refresh_token, client_auth_method=client-secret Hmm, error=invalid_token... type=REFRESH_TOKEN_ERROR... Well after some investigating, after some Sherlock Holmsing, I managed to figure out what caused this. I need to deselect a setting in my Avi Application profile selected default for this Ingress. So first I need to create an Application Profile, with most of the setting, but unselect the HTTP-only Cookies. So head over to the NSX-ALB gui, create a new application profile: Click create, select under Type: HTTP: Then scroll down under Security and make these selections: Give it a name at the top and click save at the bottom right corner:\nNow we need to tell our Ingress to use this Application profile. To be able to do that I need to use an AKO crd called HostRule. So I will go ahead and create a yaml using this HostRule crd like this:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: keycloak-host-rule 5 namespace: keycloak 6spec: 7 virtualhost: 8 fqdn: keycloak.tmc.pretty-awesome-domain.net # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: keycloak-ingress-secret 14 type: secret 15 termination: edge 16 applicationProfile: keycloak-http The TLS section is optional, but I have decided to keep it in regardless. The important piece is the applicationProfile where I enter the name of my newly created application profile above. Save it and apply:\n1andreasm@linuxvm01:~/tmc-sm/keycloak$ k apply -f keycloak-hostrule.yaml 2hostrule.ako.vmware.com/keycloak-host-rule created Now, has my application profile changed in my Keycloak Ingress? It has.. So far so good. Will I be able to log in to Keycloak now then?\nSo it seems. Wow, cool. Now lets head over to the section where I configure Keycloak settings to support TMC local authentication.\nKeycloak authentication settings for TMC local One of the recommendations from Keycloak is to create a new realm. So when logged in, head over to the top left corner where you have a dropdown menu: Click Create Realm:\nGive it a name and click CREATE. Select the newly created realm in the top left corners drop-down menu: The first thing I will create is a new Client. Click on Clients in the left menu and click on Create client: Fill in the below information, according to your environment: Click save at the bottom:\nLater on we will need the Client ID and Client Secret, these can be found here: Next head over to the Client scopes section on the left side click Create client scope: Make the following selection as below:\nClick save.\nFind the newly create Client scope called groups and click on its name. From there click on the tab Mappers and click the blue button Add mapper and select From predefined mappers. In the list below select the newly created Client scope named *groups\u0026quot; and add it. Head back to Clients menu again, select your tmc-sm application. In there click on the tab Client scopes and click Add client scope and select the groups mapper. It will be the only available in the list to select from. After it has been added, it shoul be in the list below.\nNext head over to the left menu and click Realm roles, In there click on Create role give it the name tmc:admin and save. Nothing more to be done with this role.\nNow head over to Users in the left menu, and click Add user Here it is important to add an email-address and select Email-verified. Otherwise we will get an error status when trying to log in to TMC later. Click create.\nAfter the user has been created select the Credentials tab and click on Set password Set Temporary to OFF\nNext up and final steps is to create a group and and my user to this group and add the role mapping tmc:admin to the group: Now Keycloak has been configured to work with TMC. Next step is to prepare the packages for TMC local.\nInstalling TMC local The actual Installation of TMC local involves a couple of steps. First its the packages, the source files for the application TMC, they need to be downloaded and uploaded to a registry. A defined value file, the cli tools tanzu and tmc-sm.\nDownload and upload the TMC packages To begin the actuall installation of TMC local we need to download the needed packages from my.vmware.com here\nMove the downloaded tmc-self-managed-1.0.0.tar file to your jumphost, where you also have access to a registry. Create a folder called sourcefiles. Then extract the the tmc-self-managed-1.0.0.tar with the following command enter the dir where files have been extracted. Inside this folder there is a cli called tmc-sm you will use to upload the images to your registry.\n1# create dir 2andreasm@linuxvm01:~/tmc-sm$ mkdir sourcefiles 3# extract the downloaded tmc tar file from my.vmware.com 4andreasm@linuxvm01:~/tmc-sm$ tar -xf tmc-self-managed-1.0.0.tar -C ./tanzumc 5# cd into the folder sourcefiles 6andreasm@linuxvm01:~/tmc-sm$ cd sourcefiles 7# upload the images to your registry 8andreasm@linuxvm01:~/tmc-sm$ tmc-sm push-images harbor --project registry.some-domain.net/project --username \u0026lt;USERNAME\u0026gt; --password \u0026lt;PASSWORD\u0026gt; 9# if using special characters in password use \u0026#39;passw@rd\u0026#39; (single quote) before and after Have a cup of coffee and wait for the images to be uploaded to the registry.\nAdd package repository using the tanzu cli 1# create a new namespace for the tmc-local installation 2andreasm@linuxvm01:~/tmc-sm/sourcefiles$ k create ns tmc-local 3namespace/tmc-local created 4# add the package repo for tmc-local 5andreasm@linuxvm01:~/tmc-sm/sourcefiles$ tanzu package repository add tanzu-mission-control-packages --url \u0026#34;registry.some-domain.net/project/package-repository:1.0.0\u0026#34; --namespace tmc-local 6Waiting for package repository to be added 7 87:22:48AM: Waiting for package repository reconciliation for \u0026#39;tanzu-mission-control-packages\u0026#39; 97:22:48AM: Fetch started (5s ago) 107:22:53AM: Fetching 11\t| apiVersion: vendir.k14s.io/v1alpha1 12\t| directories: 13\t| - contents: 14\t| - imgpkgBundle: 15\t| image: registry.some-domain.net/project/package-repository@sha256:3e19259be2der8d05a342d23dsd3f902c34ffvac4b3c4e61830e27cf0245159e 16\t| tag: 1.0.0 17\t| path: . 18\t| path: \u0026#34;0\u0026#34; 19\t| kind: LockConfig 20\t| 217:22:53AM: Fetch succeeded 227:22:54AM: Template succeeded 237:22:54AM: Deploy started (2s ago) 247:22:56AM: Deploying 25\t| Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; 26\t| Changes 27\t| Namespace Name Kind Age Op Op st. Wait to Rs Ri 28\t| tmc-local contour.bitnami.com PackageMetadata - create ??? - - - 29\t| ^ contour.bitnami.com.12.1.0 Package - create ??? - - - 30\t| ^ kafka-topic-controller.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 31\t| ^ kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 Package - create ??? - - - 32\t| ^ kafka.bitnami.com PackageMetadata - create ??? - - - 33\t| ^ kafka.bitnami.com.22.1.3 Package - create ??? - - - 34\t| ^ minio.bitnami.com PackageMetadata - create ??? - - - 35\t| ^ minio.bitnami.com.12.6.4 Package - create ??? - - - 36\t| ^ monitoring.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 37\t| ^ monitoring.tmc.tanzu.vmware.com.0.0.13 Package - create ??? - - - 38\t| ^ pinniped.bitnami.com PackageMetadata - create ??? - - - 39\t| ^ pinniped.bitnami.com.1.2.1 Package - create ??? - - - 40\t| ^ postgres-endpoint-controller.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 41\t| ^ postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 Package - create ??? - - - 42\t| ^ s3-access-operator.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 43\t| ^ s3-access-operator.tmc.tanzu.vmware.com.0.1.22 Package - create ??? - - - 44\t| ^ tmc-local-postgres.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 45\t| ^ tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 Package - create ??? - - - 46\t| ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 47\t| ^ tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 Package - create ??? - - - 48\t| ^ tmc-local-stack.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 49\t| ^ tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 Package - create ??? - - - 50\t| ^ tmc-local-support.tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 51\t| ^ tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 Package - create ??? - - - 52\t| ^ tmc.tanzu.vmware.com PackageMetadata - create ??? - - - 53\t| ^ tmc.tanzu.vmware.com.1.0.0 Package - create ??? - - - 54\t| Op: 26 create, 0 delete, 0 update, 0 noop, 0 exists 55\t| Wait to: 0 reconcile, 0 delete, 26 noop 56\t| 7:22:55AM: ---- applying 26 changes [0/26 done] ---- 57\t| 7:22:55AM: create packagemetadata/postgres-endpoint-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 58\t| 7:22:55AM: create packagemetadata/tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 59\t| 7:22:55AM: create package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 60\t| 7:22:55AM: create packagemetadata/s3-access-operator.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 61\t| 7:22:55AM: create package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 62\t| 7:22:55AM: create package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 63\t| 7:22:55AM: create packagemetadata/tmc-local-postgres.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 64\t| 7:22:55AM: create packagemetadata/tmc-local-stack-secrets.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 65\t| 7:22:55AM: create package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 66\t| 7:22:55AM: create packagemetadata/tmc-local-stack.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 67\t| 7:22:55AM: create package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 68\t| 7:22:55AM: create package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 69\t| 7:22:55AM: create packagemetadata/tmc-local-support.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 70\t| 7:22:55AM: create package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 71\t| 7:22:55AM: create packagemetadata/contour.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 72\t| 7:22:55AM: create packagemetadata/kafka-topic-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 73\t| 7:22:55AM: create package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 74\t| 7:22:55AM: create packagemetadata/monitoring.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 75\t| 7:22:55AM: create package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 76\t| 7:22:55AM: create packagemetadata/kafka.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 77\t| 7:22:55AM: create package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 78\t| 7:22:55AM: create packagemetadata/minio.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 79\t| 7:22:55AM: create packagemetadata/pinniped.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 80\t| 7:22:55AM: create package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 81\t| 7:22:55AM: create package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 82\t| 7:22:56AM: create package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 83\t| 7:22:56AM: ---- waiting on 26 changes [0/26 done] ---- 84\t| 7:22:56AM: ok: noop package/kafka.bitnami.com.22.1.3 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 85\t| 7:22:56AM: ok: noop packagemetadata/tmc-local-support.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 86\t| 7:22:56AM: ok: noop packagemetadata/kafka.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 87\t| 7:22:56AM: ok: noop packagemetadata/contour.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 88\t| 7:22:56AM: ok: noop packagemetadata/kafka-topic-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 89\t| 7:22:56AM: ok: noop package/contour.bitnami.com.12.1.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 90\t| 7:22:56AM: ok: noop packagemetadata/monitoring.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 91\t| 7:22:56AM: ok: noop package/minio.bitnami.com.12.6.4 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 92\t| 7:22:56AM: ok: noop package/tmc-local-postgres.tmc.tanzu.vmware.com.0.0.46 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 93\t| 7:22:56AM: ok: noop packagemetadata/postgres-endpoint-controller.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 94\t| 7:22:56AM: ok: noop packagemetadata/tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 95\t| 7:22:56AM: ok: noop package/postgres-endpoint-controller.tmc.tanzu.vmware.com.0.1.43 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 96\t| 7:22:56AM: ok: noop packagemetadata/s3-access-operator.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 97\t| 7:22:56AM: ok: noop package/s3-access-operator.tmc.tanzu.vmware.com.0.1.22 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 98\t| 7:22:56AM: ok: noop packagemetadata/pinniped.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 99\t| 7:22:56AM: ok: noop package/kafka-topic-controller.tmc.tanzu.vmware.com.0.0.21 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 100\t| 7:22:56AM: ok: noop packagemetadata/minio.bitnami.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 101\t| 7:22:56AM: ok: noop package/tmc-local-stack-secrets.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 102\t| 7:22:56AM: ok: noop packagemetadata/tmc-local-postgres.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 103\t| 7:22:56AM: ok: noop packagemetadata/tmc-local-stack-secrets.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 104\t| 7:22:56AM: ok: noop package/tmc-local-stack.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 105\t| 7:22:56AM: ok: noop packagemetadata/tmc-local-stack.tmc.tanzu.vmware.com (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 106\t| 7:22:56AM: ok: noop package/tmc-local-support.tmc.tanzu.vmware.com.0.0.17161 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 107\t| 7:22:56AM: ok: noop package/monitoring.tmc.tanzu.vmware.com.0.0.13 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 108\t| 7:22:56AM: ok: noop package/pinniped.bitnami.com.1.2.1 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 109\t| 7:22:56AM: ok: noop package/tmc.tanzu.vmware.com.1.0.0 (data.packaging.carvel.dev/v1alpha1) namespace: tmc-local 110\t| 7:22:56AM: ---- applying complete [26/26 done] ---- 111\t| 7:22:56AM: ---- waiting complete [26/26 done] ---- 112\t| Succeeded 1137:22:56AM: Deploy succeeded Check the status of the package repository added:\n1andreasm@linuxvm01:~/tmc-sm$ k get packagerepositories.packaging.carvel.dev -n tmc-local 2NAME AGE DESCRIPTION 3tanzu-mission-control-packages 31s Reconcile succeeded Install the TMC-SM package Before one can execute the package installation, there is a values-yaml file that needs to be created and edited according to your environment. So I will start with the values-yaml file. Create a file called something like tmc-values.yaml and open with your favourite editor. Below is the content I am using, reflecting the setting in my environment:\n1harborProject: registry.some-domain.net/project # I am using Harbor registry, pointing it to my url/project 2dnsZone: tmc.pretty-awesome-domain.net.net # my tmc DNS zone 3clusterIssuer: ca-issuer # the clusterissuer created earlier 4postgres: 5 userPassword: password # my own password 6 maxConnections: 300 7minio: 8 username: root 9 password: password # my own password 10contourEnvoy: 11 serviceType: LoadBalancer 12# serviceAnnotations: # needed only when specifying load balancer controller specific config like preferred IP 13# ako.vmware.com/load-balancer-ip: \u0026#34;10.12.2.17\u0026#34; 14 # when using an auto-assigned IP instead of a preferred IP, please use the following key instead of the serviceAnnotations above 15 loadBalancerClass: ako.vmware.com/avi-lb # I am using this class as I want NSX ALB to provide me the L4 IP for the Contour Ingress being deployed. 16oidc: 17 issuerType: pinniped 18 issuerURL: https://keycloak.tmc.pretty-awesome-domain.net/realms/tmc-sm # url for my keycloak instance and realm tmc-sm 19 clientID: tmc-sm-application # Id of the client created in keycloak earlier 20 clientSecret: bcwefg3rgrg444ffHH44HHtTTQTnYN # the secret for the client 21trustedCAs: 22 local-ca.pem: | # this is rootCA.crt, created under ClusterIssuer using openssl 23 -----BEGIN CERTIFICATE----- 24 -----END CERTIFICATE----- When the value yaml file has been edited, its time to spin off the installation of TMC-SM.\nExecute the following command:\n1andreasm@linuxvm01:~/tmc-sm$ tanzu package install tanzu-mission-control -p tmc.tanzu.vmware.com --version \u0026#34;1.0.0\u0026#34; --values-file tmc-values.yaml --namespace tmc-local Then you will get a long list of outputs:\n17:38:02AM: Creating service account \u0026#39;tanzu-mission-control-tmc-local-sa\u0026#39; 27:38:02AM: Creating cluster admin role \u0026#39;tanzu-mission-control-tmc-local-cluster-role\u0026#39; 37:38:02AM: Creating cluster role binding \u0026#39;tanzu-mission-control-tmc-local-cluster-rolebinding\u0026#39; 47:38:02AM: Creating secret \u0026#39;tanzu-mission-control-tmc-local-values\u0026#39; 57:38:02AM: Creating overlay secrets 67:38:02AM: Creating package install resource 77:38:02AM: Waiting for PackageInstall reconciliation for \u0026#39;tanzu-mission-control\u0026#39; 87:38:03AM: Fetch started (4s ago) 97:38:07AM: Fetching 10\t| apiVersion: vendir.k14s.io/v1alpha1 11\t| directories: 12\t| - contents: 13\t| - imgpkgBundle: 14\t| image: registry.some-domain.net/project/package-repository@sha256:30ca40e2d5bb63ab5b3ace796c87b5358e85b8fe129d4d145d1bac5633a81cca 15\t| path: . 16\t| path: \u0026#34;0\u0026#34; 17\t| kind: LockConfig 18\t| 197:38:07AM: Fetch succeeded 207:38:07AM: Template succeeded 217:38:07AM: Deploy started (2s ago) 227:38:09AM: Deploying 23\t| Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; (nodes: tmc-sm-cluster-node-pool-3-ctgxg-5f76bd48d8-hzh7h, 4+) 24\t| Changes 25\t| Namespace Name Kind Age Op Op st. Wait to Rs Ri 26\t| (cluster) tmc-install-cluster-admin-role ClusterRole - create - reconcile - - 27\t| ^ tmc-install-cluster-admin-role-binding ClusterRoleBinding - create - reconcile - - 28\t| tmc-local contour PackageInstall - create - reconcile - - 29\t| ^ contour-values-ver-1 Secret - create - reconcile - - 30\t| ^ kafka PackageInstall - create - reconcile - - 31\t| ^ kafka-topic-controller PackageInstall - create - reconcile - - 32\t| ^ kafka-topic-controller-values-ver-1 Secret - create - reconcile - - 33\t| ^ kafka-values-ver-1 Secret - create - reconcile - - 34\t| ^ minio PackageInstall - create - reconcile - - 35\t| ^ minio-values-ver-1 Secret - create - reconcile - - 36\t| ^ monitoring-values-ver-1 Secret - create - reconcile - - 37\t| ^ pinniped PackageInstall - create - reconcile - - 38\t| ^ pinniped-values-ver-1 Secret - create - reconcile - - 39\t| ^ postgres PackageInstall - create - reconcile - - 40\t| ^ postgres-endpoint-controller PackageInstall - create - reconcile - - 41\t| ^ postgres-endpoint-controller-values-ver-1 Secret - create - reconcile - - 42\t| ^ postgres-values-ver-1 Secret - create - reconcile - - 43\t| ^ s3-access-operator PackageInstall - create - reconcile - - 44\t| ^ s3-access-operator-values-ver-1 Secret - create - reconcile - - 45\t| ^ tmc-install-sa ServiceAccount - create - reconcile - - 46\t| ^ tmc-local-monitoring PackageInstall - create - reconcile - - 47\t| ^ tmc-local-stack PackageInstall - create - reconcile - - 48\t| ^ tmc-local-stack-secrets PackageInstall - create - reconcile - - 49\t| ^ tmc-local-stack-values-ver-1 Secret - create - reconcile - - 50\t| ^ tmc-local-support PackageInstall - create - reconcile - - 51\t| ^ tmc-local-support-values-ver-1 Secret - create - reconcile - - 52\t| Op: 26 create, 0 delete, 0 update, 0 noop, 0 exists 53\t| Wait to: 26 reconcile, 0 delete, 0 noop 54\t| 7:38:07AM: ---- applying 13 changes [0/26 done] ---- 55\t| 7:38:08AM: create secret/pinniped-values-ver-1 (v1) namespace: tmc-local 56\t| 7:38:08AM: create secret/minio-values-ver-1 (v1) namespace: tmc-local 57\t| 7:38:08AM: create serviceaccount/tmc-install-sa (v1) namespace: tmc-local 58\t| 7:38:08AM: create secret/kafka-values-ver-1 (v1) namespace: tmc-local 59\t| 7:38:08AM: create secret/contour-values-ver-1 (v1) namespace: tmc-local 60\t| 7:38:08AM: create secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local 61\t| 7:38:08AM: create secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local 62\t| 7:38:08AM: create secret/monitoring-values-ver-1 (v1) namespace: tmc-local 63\t| 7:38:08AM: create secret/postgres-values-ver-1 (v1) namespace: tmc-local 64\t| 7:38:08AM: create secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local 65\t| 7:38:08AM: create secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local 66\t| 7:38:08AM: create secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local 67\t| 7:38:08AM: create clusterrole/tmc-install-cluster-admin-role (rbac.authorization.k8s.io/v1) cluster 68\t| 7:38:08AM: ---- waiting on 13 changes [0/26 done] ---- 69\t| 7:38:08AM: ok: reconcile serviceaccount/tmc-install-sa (v1) namespace: tmc-local 70\t| 7:38:08AM: ok: reconcile secret/pinniped-values-ver-1 (v1) namespace: tmc-local 71\t| 7:38:08AM: ok: reconcile clusterrole/tmc-install-cluster-admin-role (rbac.authorization.k8s.io/v1) cluster 72\t| 7:38:08AM: ok: reconcile secret/contour-values-ver-1 (v1) namespace: tmc-local 73\t| 7:38:08AM: ok: reconcile secret/kafka-values-ver-1 (v1) namespace: tmc-local 74\t| 7:38:08AM: ok: reconcile secret/minio-values-ver-1 (v1) namespace: tmc-local 75\t| 7:38:08AM: ok: reconcile secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local 76\t| 7:38:08AM: ok: reconcile secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local 77\t| 7:38:08AM: ok: reconcile secret/monitoring-values-ver-1 (v1) namespace: tmc-local 78\t| 7:38:08AM: ok: reconcile secret/postgres-values-ver-1 (v1) namespace: tmc-local 79\t| 7:38:08AM: ok: reconcile secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local 80\t| 7:38:08AM: ok: reconcile secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local 81\t| 7:38:08AM: ok: reconcile secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local 82\t| 7:38:08AM: ---- applying 1 changes [13/26 done] ---- 83\t| 7:38:08AM: create clusterrolebinding/tmc-install-cluster-admin-role-binding (rbac.authorization.k8s.io/v1) cluster 84\t| 7:38:08AM: ---- waiting on 1 changes [13/26 done] ---- 85\t| 7:38:08AM: ok: reconcile clusterrolebinding/tmc-install-cluster-admin-role-binding (rbac.authorization.k8s.io/v1) cluster 86\t| 7:38:08AM: ---- applying 2 changes [14/26 done] ---- 87\t| 7:38:08AM: create packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 88\t| 7:38:08AM: create packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 89\t| 7:38:08AM: ---- waiting on 2 changes [14/26 done] ---- 90\t| 7:38:08AM: ongoing: reconcile packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 91\t| 7:38:08AM: ^ Waiting for generation 1 to be observed 92\t| 7:38:08AM: ongoing: reconcile packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 93\t| 7:38:08AM: ^ Waiting for generation 1 to be observed 94\t| 7:38:09AM: ongoing: reconcile packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 95\t| 7:38:09AM: ^ Reconciling 96\t| 7:38:09AM: ongoing: reconcile packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 97\t| 7:38:09AM: ^ Reconciling 98\t| 7:38:14AM: ok: reconcile packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 99\t| 7:38:14AM: ---- waiting on 1 changes [15/26 done] ---- 100\t| 7:38:43AM: ok: reconcile packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 101\t| 7:38:43AM: ---- applying 2 changes [16/26 done] ---- 102\t| 7:38:43AM: create packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 103\t| 7:38:43AM: create packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 104\t| 7:38:43AM: ---- waiting on 2 changes [16/26 done] ---- 105\t| 7:38:43AM: ongoing: reconcile packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 106\t| 7:38:43AM: ^ Waiting for generation 1 to be observed 107\t| 7:38:43AM: ongoing: reconcile packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 108\t| 7:38:43AM: ^ Waiting for generation 1 to be observed 109\t| 7:38:44AM: ongoing: reconcile packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 110\t| 7:38:44AM: ^ Reconciling 111\t| 7:38:44AM: ongoing: reconcile packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 112\t| 7:38:44AM: ^ Reconciling 113\t| 7:38:51AM: ok: reconcile packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 114\t| 7:38:51AM: ---- applying 4 changes [18/26 done] ---- 115\t| 7:38:51AM: create packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 116\t| 7:38:51AM: create packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 117\t| 7:38:51AM: create packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local 118\t| 7:38:51AM: create packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local 119\t| 7:38:51AM: ---- waiting on 5 changes [17/26 done] ---- 120\t| 7:38:51AM: ongoing: reconcile packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local 121\t| 7:38:51AM: ^ Waiting for generation 1 to be observed 122\t| 7:38:51AM: ongoing: reconcile packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local 123\t| 7:38:51AM: ^ Waiting for generation 1 to be observed 124\t| 7:38:51AM: ongoing: reconcile packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 125\t| 7:38:51AM: ^ Waiting for generation 1 to be observed 126\t| 7:38:51AM: ongoing: reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 127\t| 7:38:51AM: ^ Waiting for generation 1 to be observed 128\t| 7:38:52AM: ongoing: reconcile packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local 129\t| 7:38:52AM: ^ Reconciling 130\t| 7:38:52AM: ongoing: reconcile packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local 131\t| 7:38:52AM: ^ Reconciling 132\t| 7:38:52AM: ongoing: reconcile packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 133\t| 7:38:52AM: ^ Reconciling 134\t| 7:38:52AM: ongoing: reconcile packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 135\t| 7:38:52AM: ^ Reconciling You can monitor the progress using this command:\n1andreasm@linuxvm01:~$ k get pods -n tmc-local -w 2NAME READY STATUS RESTARTS AGE 3contour-contour-67b48bff88-fqvwk 1/1 Running 0 107s 4contour-contour-certgen-kt6hk 0/1 Completed 0 108s 5contour-envoy-9r4nm 2/2 Running 0 107s 6contour-envoy-gzkdf 2/2 Running 0 107s 7contour-envoy-hr8lj 2/2 Running 0 108s 8contour-envoy-m95qh 2/2 Running 0 107s 9kafka-0 0/1 ContainerCreating 0 66s 10kafka-exporter-6b4c74b596-k4crf 0/1 CrashLoopBackOff 3 (18s ago) 66s 11kafka-topic-controller-7bc498856b-sj5jw 1/1 Running 0 66s 12minio-7dbcffd86-w4rv9 1/1 Running 0 54s 13minio-provisioning-tsb6q 0/1 Completed 0 54s 14pinniped-supervisor-55c575555-shzjh 1/1 Running 0 74s 15postgres-endpoint-controller-5c784cd44d-gfg55 1/1 Running 0 23s 16postgres-postgresql-0 2/2 Running 0 57s 17s3-access-operator-68b6485c9b-jdbww 0/1 ContainerCreating 0 15s 18s3-access-operator-68b6485c9b-jdbww 1/1 Running 0 16s 19kafka-0 0/1 Running 0 72s There will be stages where several of the pods enters CrashLoopBackOff, Error, etc. Just give it time. If the package reconciliation fails. There is time to do some troubleshooting. And most likely it is DNS, certificate or the OIDC configuration. Check the progress on the package reconciliation:\n1andreasm@linuxvm01:~$ k get pkgi -n tmc-local 2NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE 3contour contour.bitnami.com 12.1.0 Reconcile succeeded 7m20s 4kafka kafka.bitnami.com 22.1.3 Reconcile succeeded 6m37s 5kafka-topic-controller kafka-topic-controller.tmc.tanzu.vmware.com 0.0.21 Reconcile succeeded 6m37s 6minio minio.bitnami.com 12.6.4 Reconcile succeeded 6m37s 7pinniped pinniped.bitnami.com 1.2.1 Reconcile succeeded 6m45s 8postgres tmc-local-postgres.tmc.tanzu.vmware.com 0.0.46 Reconcile succeeded 6m37s 9postgres-endpoint-controller postgres-endpoint-controller.tmc.tanzu.vmware.com 0.1.43 Reconcile succeeded 5m58s 10s3-access-operator s3-access-operator.tmc.tanzu.vmware.com 0.1.22 Reconcile succeeded 5m46s 11tanzu-mission-control tmc.tanzu.vmware.com 1.0.0 Reconciling 7m26s 12tmc-local-stack tmc-local-stack.tmc.tanzu.vmware.com 0.0.17161 Reconciling 5m5s 13tmc-local-stack-secrets tmc-local-stack-secrets.tmc.tanzu.vmware.com 0.0.17161 Reconcile succeeded 7m20s 14tmc-local-support tmc-local-support.tmc.tanzu.vmware.com 0.0.17161 Reconcile succeeded 6m45s In the meantime, also check some of the required dns records such as tmc.pretty-awesome-domain.net and pinniped-supervisor.tmc.pretty-awesome-domain.net if they can be resolved:\n1andreasm@linuxvm01:~$ ping pinniped-supervisor.tmc.pretty-awesome-domain.net If this error:\n1ping: pinniped-supervisor.tmc.pretty-awesome-domain.net: Temporary failure in name resolution I need to troubleshoot my dns-zone.\nIf I get this:\n1andreasm@linuxvm01:~$ ping tmc.pretty-awesome-domain.net 2PING tmc.pretty-awesome-domain.net (10.101.210.12) 56(84) bytes of data. 364 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=13 ttl=61 time=7.31 ms 464 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=14 ttl=61 time=6.47 ms 5andreasm@linuxvm01:~$ ping pinniped-supervisor.tmc.pretty-awesome-domain.net 6PING pinniped-supervisor.tmc.pretty-awesome-domain.net (10.101.210.12) 56(84) bytes of data. 764 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=1 ttl=61 time=3.81 ms 864 bytes from 10.101.210.12 (10.101.210.12): icmp_seq=2 ttl=61 time=9.28 ms I am good ðŸ˜„\nAfter waiting a while, the package installation process finished, either 100% successfully or with errors. In my environment it fails on step 25/26 on the tmc-local-monitoring. This turns out to be the alertmanager. I have a section below that explains how this can be solved.\nHere is the pod that is failing:\n1andreasm@linuxvm01:~$ k get pods -n tmc-local 2NAME READY STATUS RESTARTS AGE 3account-manager-server-84b4758ccd-5zx7n 1/1 Running 0 14m 4account-manager-server-84b4758ccd-zfqlj 1/1 Running 0 14m 5agent-gateway-server-bf4f6c67-mvq2m 1/1 Running 1 (14m ago) 14m 6agent-gateway-server-bf4f6c67-zlj9d 1/1 Running 1 (14m ago) 14m 7alertmanager-tmc-local-monitoring-tmc-local-0 1/2 CrashLoopBackOff 7 (46s ago) 12m 8api-gateway-server-679b8478f9-57ss5 1/1 Running 1 (14m ago) 14m 9api-gateway-server-679b8478f9-t6j9s 1/1 Running 1 (14m ago) 14m 10audit-service-consumer-7bbdd4f55f-bjc5x 1/1 Running 0 14m But its not bad considering all the services and pods being deployed by TMC, one failed out of MANY:\n1andreasm@linuxvm01:~$ k get pods -n tmc-local 2NAME READY STATUS RESTARTS AGE 3account-manager-server-84b4758ccd-5zx7n 1/1 Running 0 14m 4account-manager-server-84b4758ccd-zfqlj 1/1 Running 0 14m 5agent-gateway-server-bf4f6c67-mvq2m 1/1 Running 1 (14m ago) 14m 6agent-gateway-server-bf4f6c67-zlj9d 1/1 Running 1 (14m ago) 14m 7alertmanager-tmc-local-monitoring-tmc-local-0 1/2 CrashLoopBackOff 7 (46s ago) 12m 8api-gateway-server-679b8478f9-57ss5 1/1 Running 1 (14m ago) 14m 9api-gateway-server-679b8478f9-t6j9s 1/1 Running 1 (14m ago) 14m 10audit-service-consumer-7bbdd4f55f-bjc5x 1/1 Running 0 14m 11audit-service-consumer-7bbdd4f55f-h6h8c 1/1 Running 0 14m 12audit-service-server-898c98dc5-97s8l 1/1 Running 0 14m 13audit-service-server-898c98dc5-qvc9k 1/1 Running 0 14m 14auth-manager-server-79d7567986-7699w 1/1 Running 0 14m 15auth-manager-server-79d7567986-bbrg8 1/1 Running 0 14m 16auth-manager-server-79d7567986-tbdww 1/1 Running 0 14m 17authentication-server-695fd77f46-8p67m 1/1 Running 0 14m 18authentication-server-695fd77f46-ttd4l 1/1 Running 0 14m 19cluster-agent-service-server-599cf966f4-4ndkl 1/1 Running 0 14m 20cluster-agent-service-server-599cf966f4-h4g9l 1/1 Running 0 14m 21cluster-config-server-7c5f5f8dc6-99prt 1/1 Running 1 (13m ago) 14m 22cluster-config-server-7c5f5f8dc6-z4rvg 1/1 Running 0 14m 23cluster-object-service-server-7bc8f7c45c-fw97r 1/1 Running 0 14m 24cluster-object-service-server-7bc8f7c45c-k8bwc 1/1 Running 0 14m 25cluster-reaper-server-5f94f8dd6b-k2pxd 1/1 Running 0 14m 26cluster-secret-server-9fc44564f-g5lv5 1/1 Running 1 (14m ago) 14m 27cluster-secret-server-9fc44564f-vnbck 1/1 Running 0 14m 28cluster-service-server-6f7c657d7-ls9t7 1/1 Running 0 14m 29cluster-service-server-6f7c657d7-xvz7z 1/1 Running 0 14m 30cluster-sync-egest-f96d9b6bb-947c2 1/1 Running 0 14m 31cluster-sync-egest-f96d9b6bb-q22sg 1/1 Running 0 14m 32cluster-sync-ingest-798c88467d-c2pgj 1/1 Running 0 14m 33cluster-sync-ingest-798c88467d-pc2z7 1/1 Running 0 14m 34contour-contour-certgen-gdnns 0/1 Completed 0 17m 35contour-contour-ffddc764f-k25pb 1/1 Running 0 17m 36contour-envoy-4ptk4 2/2 Running 0 17m 37contour-envoy-66v8r 2/2 Running 0 17m 38contour-envoy-6shc8 2/2 Running 0 17m 39contour-envoy-br4nk 2/2 Running 0 17m 40dataprotection-server-58c6c9bd8d-dplbs 1/1 Running 0 14m 41dataprotection-server-58c6c9bd8d-hp2nz 1/1 Running 0 14m 42events-service-consumer-76bd756879-49bpb 1/1 Running 0 14m 43events-service-consumer-76bd756879-jnlkw 1/1 Running 0 14m 44events-service-server-694648bcc8-rjg27 1/1 Running 0 14m 45events-service-server-694648bcc8-trtm2 1/1 Running 0 14m 46fanout-service-server-7c6d9559b7-g7mvg 1/1 Running 0 14m 47fanout-service-server-7c6d9559b7-nhcjc 1/1 Running 0 14m 48feature-flag-service-server-855756576c-zltgh 1/1 Running 0 14m 49inspection-server-695b778b48-29s8q 2/2 Running 0 14m 50inspection-server-695b778b48-7hzf4 2/2 Running 0 14m 51intent-server-566dd98b76-dhcrx 1/1 Running 0 14m 52intent-server-566dd98b76-pjdpb 1/1 Running 0 14m 53kafka-0 1/1 Running 0 16m 54kafka-exporter-745d578567-5vhgq 1/1 Running 4 (15m ago) 16m 55kafka-topic-controller-5cf4d8c559-lxpcb 1/1 Running 0 15m 56landing-service-server-7ddd9774f-szx8v 1/1 Running 0 14m 57minio-764b688f5f-p7lrx 1/1 Running 0 16m 58minio-provisioning-5vsqs 0/1 Completed 1 16m 59onboarding-service-server-5ff888758f-bnzp5 1/1 Running 0 14m 60onboarding-service-server-5ff888758f-fq9dg 1/1 Running 0 14m 61package-deployment-server-79dd4b896d-9rv8z 1/1 Running 0 14m 62package-deployment-server-79dd4b896d-txq2x 1/1 Running 0 14m 63pinniped-supervisor-677578c495-jqbq4 1/1 Running 0 16m 64policy-engine-server-6bcbddf747-jks25 1/1 Running 0 14m 65policy-engine-server-6bcbddf747-vhxlm 1/1 Running 0 14m 66policy-insights-server-6878c9c8f-64ggn 1/1 Running 0 14m 67policy-sync-service-server-7699f47d65-scl5f 1/1 Running 0 14m 68policy-view-service-server-86bb698454-bvclh 1/1 Running 0 14m 69policy-view-service-server-86bb698454-zpkg9 1/1 Running 0 14m 70postgres-endpoint-controller-9d4fc9489-kgdf4 1/1 Running 0 15m 71postgres-postgresql-0 2/2 Running 0 16m 72prometheus-server-tmc-local-monitoring-tmc-local-0 2/2 Running 0 12m 73provisioner-service-server-84c4f9dc8f-khv2b 1/1 Running 0 14m 74provisioner-service-server-84c4f9dc8f-xl6gr 1/1 Running 0 14m 75resource-manager-server-8567f7cbbc-pl2fz 1/1 Running 0 14m 76resource-manager-server-8567f7cbbc-pqkxp 1/1 Running 0 14m 77s3-access-operator-7f4d77647b-xnnb2 1/1 Running 0 15m 78schema-service-schema-server-85cb7c7796-prjq7 1/1 Running 0 14m 79telemetry-event-service-consumer-7d6f8cc4b7-ffjcd 1/1 Running 0 14m 80telemetry-event-service-consumer-7d6f8cc4b7-thf44 1/1 Running 0 14m 81tenancy-service-server-57898676cd-9lpjl 1/1 Running 0 14m 82ui-server-6994bc9cd6-gtm6r 1/1 Running 0 14m 83ui-server-6994bc9cd6-xzxbv 1/1 Running 0 14m 84wcm-server-5c95c8d587-7sc9l 1/1 Running 1 (13m ago) 14m 85wcm-server-5c95c8d587-r2kbf 1/1 Running 1 (12m ago) 14m Troubleshooting the Alertmanager pod If your package installation stops at 25/26, and the alertmanager pod is in a crasloopbackoff state: And if you check the logs of the alertmanager container it will throw you this error.\n1k logs -n tmc-local alertmanager-tmc-local-monitoring-tmc-local-0 -c alertmanager 2ts=2023-07-13T14:16:30.239Z caller=main.go:231 level=info msg=\u0026#34;Starting Alertmanager\u0026#34; version=\u0026#34;(version=0.24.0, branch=HEAD, revision=f484b17fa3c583ed1b2c8bbcec20ba1db2aa5f11)\u0026#34; 3ts=2023-07-13T14:16:30.239Z caller=main.go:232 level=info build_context=\u0026#34;(go=go1.17.8, user=root@265f14f5c6fc, date=20220325-09:31:33)\u0026#34; 4ts=2023-07-13T14:16:30.240Z caller=cluster.go:178 level=warn component=cluster err=\u0026#34;couldn\u0026#39;t deduce an advertise address: no private IP found, explicit advertise addr not provided\u0026#34; 5ts=2023-07-13T14:16:30.241Z caller=main.go:263 level=error msg=\u0026#34;unable to initialize gossip mesh\u0026#34; err=\u0026#34;create memberlist: Failed to get final advertise address: No private IP address found, and explicit IP not provided\u0026#34; After some searching around, a workaround is to add the below values to the stateful set (see comments below):\n1 spec: 2 containers: 3 - args: 4 - --volume-dir=/etc/alertmanager 5 - --webhook-url=http://127.0.0.1:9093/-/reload 6 image: registry.domain.net/project/package-repository@sha256:9125ebac75af1eb247de0982ce6d56bc7049a1f384f97c77a7af28de010f20a7 7 imagePullPolicy: IfNotPresent 8 name: configmap-reloader 9 resources: {} 10 terminationMessagePath: /dev/termination-log 11 terminationMessagePolicy: File 12 volumeMounts: 13 - mountPath: /etc/alertmanager/config 14 name: config-volume 15 readOnly: true 16 - args: 17 - --config.file=/etc/alertmanager/config/alertmanager.yaml 18 - --cluster.advertise-address=$(POD_IP):9093 # added from here 19 env: 20 - name: POD_IP 21 valueFrom: 22 fieldRef: 23 fieldPath: status.podIP # To here But setting this directly on the statefulset will be overwritten by the package conciliation.\nSo we need to apply this config using ytt overlay. Create a new yaml file, call it something like alertmanager-overlay.yaml. Below is my ytt config to achieve this:\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: alertmanager-overlay-secret 5 namespace: tmc-local 6stringData: 7 patch.yaml: | 8 #@ load(\u0026#34;@ytt:overlay\u0026#34;, \u0026#34;overlay\u0026#34;) 9 #@overlay/match by=overlay.subset({\u0026#34;kind\u0026#34;:\u0026#34;StatefulSet\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;alertmanager-tmc-local-monitoring-tmc-local\u0026#34;}}) 10 --- 11 spec: 12 template: 13 spec: 14 containers: #@overlay/replace 15 - args: 16 - --volume-dir=/etc/alertmanager 17 - --webhook-url=http://127.0.0.1:9093/-/reload 18 image: registry.domain.net/project/package-repository@sha256:9125ebac75af1eb247de0982ce6d56bc7049a1f384f97c77a7af28de010f20a7 19 imagePullPolicy: IfNotPresent 20 name: configmap-reloader 21 resources: {} 22 terminationMessagePath: /dev/termination-log 23 terminationMessagePolicy: File 24 volumeMounts: 25 - mountPath: /etc/alertmanager/config 26 name: config-volume 27 readOnly: true 28 - args: 29 - --config.file=/etc/alertmanager/config/alertmanager.yaml 30 - --cluster.advertise-address=$(POD_IP):9093 31 env: 32 - name: POD_IP 33 valueFrom: 34 fieldRef: 35 fieldPath: status.podIP 36 image: registry.domain.net/project/package-repository@sha256:74d46d5614791496104479bbf81c041515c5f8c17d9e9fcf1b33fa36e677156f 37 imagePullPolicy: IfNotPresent 38 name: alertmanager 39 ports: 40 - containerPort: 9093 41 name: alertmanager 42 protocol: TCP 43 readinessProbe: 44 failureThreshold: 3 45 httpGet: 46 path: /#/status 47 port: 9093 48 scheme: HTTP 49 initialDelaySeconds: 30 50 periodSeconds: 10 51 successThreshold: 1 52 timeoutSeconds: 30 53 resources: 54 limits: 55 cpu: 300m 56 memory: 100Mi 57 requests: 58 cpu: 100m 59 memory: 70Mi 60 terminationMessagePath: /dev/termination-log 61 terminationMessagePolicy: File 62 volumeMounts: 63 - mountPath: /etc/alertmanager/config 64 name: config-volume 65 readOnly: true 66 - mountPath: /data 67 name: data 68 69--- 70apiVersion: v1 71kind: Secret 72metadata: 73 name: tmc-overlay-override 74 namespace: tmc-local 75stringData: 76 patch-alertmanager.yaml: | 77 #@ load(\u0026#34;@ytt:overlay\u0026#34;, \u0026#34;overlay\u0026#34;) 78 #@overlay/match by=overlay.subset({\u0026#34;kind\u0026#34;:\u0026#34;PackageInstall\u0026#34;, \u0026#34;metadata\u0026#34;: {\u0026#34;name\u0026#34;: \u0026#34;tmc-local-monitoring\u0026#34;}}) 79 --- 80 metadata: 81 annotations: 82 #@overlay/match missing_ok=True 83 ext.packaging.carvel.dev/ytt-paths-from-secret-name.0: alertmanager-overlay-secret This was the only way I managed to get the configs applied correctly. It can probably be done a different way, but it works.\nApply the above yaml:\n1andreasm@linuxvm01:~/tmc-sm/errors$ k apply -f alertmanager-overlay.yaml 2secret/alertmanager-overlay-secret configured 3secret/tmc-overlay-override configured Then I need to annotate the package:\n1kubectl annotate packageinstalls tanzu-mission-control -n tmc-local ext.packaging.carvel.dev/ytt-paths-from-secret-name.0=tmc-overlay-override Pause and unpause the reconciliation (if it is already in a reconciliation state its not always necessary to pause and unpause). But to kick it off immediately, run the commands below.\n1andreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tmc-local-monitoring --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: true}}\u0026#39; 2andreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tmc-local-monitoring --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: false}}\u0026#39; 3packageinstall.packaging.carvel.dev/tmc-local-monitoring patched One can also kick the reconcile by pointing to the package tanzu-mission-control:\n1andreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tanzu-mission-control --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: true}}\u0026#39; 2packageinstall.packaging.carvel.dev/tanzu-mission-control patched 3andreasm@linuxvm01:~/tmc-sm/errors$ kubectl patch -n tmc-local --type merge pkgi tanzu-mission-control --patch \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;paused\u0026#34;: false}}\u0026#39; 4packageinstall.packaging.carvel.dev/tanzu-mission-control patched The end result should give us this in our alertmanager statefulset:\n1andreasm@linuxvm01:~/tmc-sm/errors$ k get statefulsets.apps -n tmc-local alertmanager-tmc-local-monitoring-tmc-local -oyaml 2 #snippet 3 - args: 4 - --config.file=/etc/alertmanager/config/alertmanager.yaml 5 - --cluster.advertise-address=$(POD_IP):9093 6 env: 7 - name: POD_IP 8 valueFrom: 9 fieldRef: 10 apiVersion: v1 11 fieldPath: status.podIP 12 #snippet And the alertmanager pod should start:\n1andreasm@linuxvm01:~/tmc-sm/errors$ k get pod -n tmc-local alertmanager-tmc-local-monitoring-tmc-local-0 2NAME READY STATUS RESTARTS AGE 3alertmanager-tmc-local-monitoring-tmc-local-0 2/2 Running 0 10m If its still in CrashLoopBackOff just delete the pod and it should go into a running state. If not, describe the alermananger statefulset for any additional errors, maybe a typo in the ytt overlay yaml...\nOne can also do this operation while the installation is waiting on the package tmc-local-monitoring re-conciliation. So the package installation will be successful after all.\nInstall the TMC-SM package - continued What about the services created, httpproxies and Ingress?\nGet the Services:\n1andreasm@linuxvm01:~$ k get svc -n tmc-local 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3account-manager-grpc ClusterIP 20.10.134.215 \u0026lt;none\u0026gt; 443/TCP 18m 4account-manager-service ClusterIP 20.10.6.142 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 5agent-gateway-service ClusterIP 20.10.111.64 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 6alertmanager-tmc-local-monitoring-tmc-local ClusterIP 20.10.113.103 \u0026lt;none\u0026gt; 9093/TCP 15m 7api-gateway-service ClusterIP 20.10.241.28 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 8audit-service-consumer ClusterIP 20.10.183.29 \u0026lt;none\u0026gt; 7777/TCP 18m 9audit-service-grpc ClusterIP 20.10.94.221 \u0026lt;none\u0026gt; 443/TCP 18m 10audit-service-rest ClusterIP 20.10.118.27 \u0026lt;none\u0026gt; 443/TCP 18m 11audit-service-service ClusterIP 20.10.193.140 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 12auth-manager-server ClusterIP 20.10.86.230 \u0026lt;none\u0026gt; 443/TCP 18m 13auth-manager-service ClusterIP 20.10.136.164 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 14authentication-grpc ClusterIP 20.10.32.80 \u0026lt;none\u0026gt; 443/TCP 18m 15authentication-service ClusterIP 20.10.69.22 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 16cluster-agent-service-grpc ClusterIP 20.10.55.122 \u0026lt;none\u0026gt; 443/TCP 18m 17cluster-agent-service-installer ClusterIP 20.10.185.105 \u0026lt;none\u0026gt; 80/TCP 18m 18cluster-agent-service-service ClusterIP 20.10.129.243 \u0026lt;none\u0026gt; 443/TCP,80/TCP,7777/TCP 18m 19cluster-config-service ClusterIP 20.10.237.148 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 20cluster-object-service-grpc ClusterIP 20.10.221.128 \u0026lt;none\u0026gt; 443/TCP 18m 21cluster-object-service-service ClusterIP 20.10.238.0 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 22cluster-reaper-grpc ClusterIP 20.10.224.97 \u0026lt;none\u0026gt; 443/TCP 18m 23cluster-reaper-service ClusterIP 20.10.65.179 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 24cluster-secret-service ClusterIP 20.10.17.122 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 25cluster-service-grpc ClusterIP 20.10.152.204 \u0026lt;none\u0026gt; 443/TCP 18m 26cluster-service-rest ClusterIP 20.10.141.159 \u0026lt;none\u0026gt; 443/TCP 18m 27cluster-service-service ClusterIP 20.10.40.169 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 28cluster-sync-egest ClusterIP 20.10.47.77 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 29cluster-sync-egest-grpc ClusterIP 20.10.219.9 \u0026lt;none\u0026gt; 443/TCP 18m 30cluster-sync-ingest ClusterIP 20.10.223.205 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 31cluster-sync-ingest-grpc ClusterIP 20.10.196.7 \u0026lt;none\u0026gt; 443/TCP 18m 32contour ClusterIP 20.10.5.59 \u0026lt;none\u0026gt; 8001/TCP 21m 33contour-envoy LoadBalancer 20.10.72.121 10.101.210.12 80:31964/TCP,443:31350/TCP 21m 34contour-envoy-metrics ClusterIP None \u0026lt;none\u0026gt; 8002/TCP 21m 35dataprotection-grpc ClusterIP 20.10.47.233 \u0026lt;none\u0026gt; 443/TCP 18m 36dataprotection-service ClusterIP 20.10.73.15 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 37events-service-consumer ClusterIP 20.10.38.207 \u0026lt;none\u0026gt; 7777/TCP 18m 38events-service-grpc ClusterIP 20.10.65.181 \u0026lt;none\u0026gt; 443/TCP 18m 39events-service-service ClusterIP 20.10.34.169 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 40fanout-service-grpc ClusterIP 20.10.77.108 \u0026lt;none\u0026gt; 443/TCP 18m 41fanout-service-service ClusterIP 20.10.141.34 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 42feature-flag-service-grpc ClusterIP 20.10.171.161 \u0026lt;none\u0026gt; 443/TCP 18m 43feature-flag-service-service ClusterIP 20.10.112.195 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 44inspection-grpc ClusterIP 20.10.20.119 \u0026lt;none\u0026gt; 443/TCP 18m 45inspection-service ClusterIP 20.10.85.86 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 46intent-grpc ClusterIP 20.10.213.53 \u0026lt;none\u0026gt; 443/TCP 18m 47intent-service ClusterIP 20.10.19.196 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 48kafka ClusterIP 20.10.135.162 \u0026lt;none\u0026gt; 9092/TCP 20m 49kafka-headless ClusterIP None \u0026lt;none\u0026gt; 9092/TCP,9094/TCP,9093/TCP 20m 50kafka-metrics ClusterIP 20.10.175.161 \u0026lt;none\u0026gt; 9308/TCP 20m 51landing-service-metrics ClusterIP None \u0026lt;none\u0026gt; 7777/TCP 18m 52landing-service-rest ClusterIP 20.10.37.157 \u0026lt;none\u0026gt; 443/TCP 18m 53landing-service-server ClusterIP 20.10.28.110 \u0026lt;none\u0026gt; 443/TCP 18m 54minio ClusterIP 20.10.234.32 \u0026lt;none\u0026gt; 9000/TCP,9001/TCP 20m 55onboarding-service-metrics ClusterIP None \u0026lt;none\u0026gt; 7777/TCP 18m 56onboarding-service-rest ClusterIP 20.10.66.85 \u0026lt;none\u0026gt; 443/TCP 18m 57package-deployment-service ClusterIP 20.10.40.90 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 58pinniped-supervisor ClusterIP 20.10.138.177 \u0026lt;none\u0026gt; 443/TCP 20m 59pinniped-supervisor-api ClusterIP 20.10.218.242 \u0026lt;none\u0026gt; 443/TCP 20m 60policy-engine-grpc ClusterIP 20.10.114.38 \u0026lt;none\u0026gt; 443/TCP 18m 61policy-engine-service ClusterIP 20.10.85.191 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 62policy-insights-grpc ClusterIP 20.10.95.196 \u0026lt;none\u0026gt; 443/TCP 18m 63policy-insights-service ClusterIP 20.10.119.38 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 64policy-sync-service-service ClusterIP 20.10.32.72 \u0026lt;none\u0026gt; 7777/TCP 18m 65policy-view-service-grpc ClusterIP 20.10.4.163 \u0026lt;none\u0026gt; 443/TCP 18m 66policy-view-service-service ClusterIP 20.10.41.172 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 67postgres-endpoint-controller ClusterIP 20.10.3.234 \u0026lt;none\u0026gt; 9876/TCP 18m 68postgres-postgresql ClusterIP 20.10.10.197 \u0026lt;none\u0026gt; 5432/TCP 20m 69postgres-postgresql-hl ClusterIP None \u0026lt;none\u0026gt; 5432/TCP 20m 70postgres-postgresql-metrics ClusterIP 20.10.79.247 \u0026lt;none\u0026gt; 9187/TCP 20m 71prometheus-server-tmc-local-monitoring-tmc-local ClusterIP 20.10.152.45 \u0026lt;none\u0026gt; 9090/TCP 15m 72provisioner-service-grpc ClusterIP 20.10.138.198 \u0026lt;none\u0026gt; 443/TCP 18m 73provisioner-service-service ClusterIP 20.10.96.47 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 74resource-manager-grpc ClusterIP 20.10.143.168 \u0026lt;none\u0026gt; 443/TCP 18m 75resource-manager-service ClusterIP 20.10.238.70 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m 76s3-access-operator ClusterIP 20.10.172.230 \u0026lt;none\u0026gt; 443/TCP,8080/TCP 19m 77schema-service-grpc ClusterIP 20.10.237.93 \u0026lt;none\u0026gt; 443/TCP 18m 78schema-service-service ClusterIP 20.10.99.167 \u0026lt;none\u0026gt; 443/TCP,7777/TCP 18m 79telemetry-event-service-consumer ClusterIP 20.10.196.48 \u0026lt;none\u0026gt; 7777/TCP 18m 80tenancy-service-metrics-headless ClusterIP None \u0026lt;none\u0026gt; 7777/TCP 18m 81tenancy-service-tenancy-service ClusterIP 20.10.80.23 \u0026lt;none\u0026gt; 443/TCP 18m 82tenancy-service-tenancy-service-rest ClusterIP 20.10.200.153 \u0026lt;none\u0026gt; 443/TCP 18m 83ui-server ClusterIP 20.10.233.160 \u0026lt;none\u0026gt; 8443/TCP,7777/TCP 18m 84wcm-grpc ClusterIP 20.10.19.188 \u0026lt;none\u0026gt; 443/TCP 18m 85wcm-service ClusterIP 20.10.175.206 \u0026lt;none\u0026gt; 443/TCP,8443/TCP,7777/TCP 18m Get the Ingresses:\n1andreasm@linuxvm01:~$ k get ingress -n tmc-local 2NAME CLASS HOSTS ADDRESS PORTS AGE 3alertmanager-tmc-local-monitoring-tmc-local-ingress tmc-local alertmanager.tmc.pretty-awesome-domain.net 10.101.210.12 80 16m 4landing-service-ingress-global tmc-local landing.tmc.pretty-awesome-domain.net 10.101.210.12 80, 443 19m 5minio tmc-local console.s3.tmc.pretty-awesome-domain.net 10.101.210.12 80 20m 6minio-api tmc-local s3.tmc.pretty-awesome-domain.net 10.101.210.12 80, 443 20m 7prometheus-server-tmc-local-monitoring-tmc-local-ingress tmc-local prometheus.tmc.pretty-awesome-domain.net 10.101.210.12 80 16m Ah, there is my dns records ðŸ˜„\nGet the HTTPProxies\n1andreasm@linuxvm01:~$ k get httpproxies -n tmc-local 2NAME FQDN TLS SECRET STATUS STATUS DESCRIPTION 3auth-manager-server auth.tmc.pretty-awesome-domain.net server-tls valid Valid HTTPProxy 4minio-api-proxy s3.tmc.pretty-awesome-domain.net minio-tls valid Valid HTTPProxy 5minio-bucket-proxy tmc-local.s3.tmc.pretty-awesome-domain.net minio-tls valid Valid HTTPProxy 6minio-console-proxy console.s3.tmc.pretty-awesome-domain.net minio-tls valid Valid HTTPProxy 7pinniped-supervisor pinniped-supervisor.tmc.pretty-awesome-domain.net valid Valid HTTPProxy 8stack-http-proxy tmc.pretty-awesome-domain.net stack-tls valid Valid HTTPProxy 9tenancy-service-http-proxy gts.tmc.pretty-awesome-domain.net valid Valid HTTPProxy 10tenancy-service-http-proxy-rest gts-rest.tmc.pretty-awesome-domain.net valid Valid HTTPProxy Ah.. More DNS records.\nUnfortunately my tmc-sm deployement gave me this error in the end, which can be solved afterwards or during the install process following the section on Alertmanager above:\n1\t| 8:32:35AM: ---- waiting on 1 changes [25/26 done] ---- 2\t| 8:32:36AM: ongoing: reconcile packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local 3\t| 8:32:36AM: ^ Reconciling 48:33:30AM: Deploy failed 5\t| kapp: Error: Timed out waiting after 15m0s for resources: [packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local] 6\t| Deploying: Error (see .status.usefulErrorMessage for details) 78:33:30AM: Error tailing app: Reconciling app: Deploy failed 8 98:33:30AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: ReconcileFailed 10Error: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: Reconciling: kapp: Error: Timed out waiting after 15m0s for resources: [packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local]. Reconcile failed: Error (see .status.usefulErrorMessage for details) Except the Alertmanager pod which can be fixed, it is kind a success. Remember also the note in the official documentation:\nNote Deploying TMC Self-Managed 1.0 on a Tanzu Kubernetes Grid (TKG) 2.0 workload cluster running in vSphere with Tanzu on vSphere version 8.x is for tech preview only. Initiate deployments only in pre-production environments or production environments where support for the integration is not required. vSphere 8u1 or later is required in order to test the tech preview integration.\nNow its time to log in to the TMC-SM UI..\nUninstall tmc-sm packages To uninstall, after a failed deployement or other reasons. Issue this command:\n1andreasm@linuxvm01:~/tmc-sm$ tanzu package installed delete tanzu-mission-control -n tmc-local 2Delete package install \u0026#39;tanzu-mission-control\u0026#39; from namespace \u0026#39;tmc-local\u0026#39; 3 4Continue? [yN]: y 5 6 77:55:19AM: Deleting package install \u0026#39;tanzu-mission-control\u0026#39; from namespace \u0026#39;tmc-local\u0026#39; 87:55:19AM: Waiting for deletion of package install \u0026#39;tanzu-mission-control\u0026#39; from namespace \u0026#39;tmc-local\u0026#39; 97:55:19AM: Waiting for generation 2 to be observed 107:55:19AM: Delete started (2s ago) 117:55:21AM: Deleting 12\t| Target cluster \u0026#39;https://20.10.0.1:443\u0026#39; (nodes: tmc-sm-cluster-node-pool-3-ctgxg-5f76bd48d8-hzh7h, 4+) 13\t| Changes 14\t| Namespace Name Kind Age Op Op st. Wait to Rs Ri 15\t| (cluster) tmc-install-cluster-admin-role ClusterRole 17m delete - delete ok - 16\t| ^ tmc-install-cluster-admin-role-binding ClusterRoleBinding 17m delete - delete ok - 17\t| tmc-local contour PackageInstall 17m delete - delete ok - 18\t| ^ contour-values-ver-1 Secret 17m delete - delete ok - 19\t| ^ kafka PackageInstall 16m delete - delete ok - 20\t| ^ kafka-topic-controller PackageInstall 16m delete - delete ok - 21\t| ^ kafka-topic-controller-values-ver-1 Secret 17m delete - delete ok - 22\t| ^ kafka-values-ver-1 Secret 17m delete - delete ok - 23\t| ^ minio PackageInstall 16m delete - delete ok - 24\t| ^ minio-values-ver-1 Secret 17m delete - delete ok - 25\t| ^ monitoring-values-ver-1 Secret 17m delete - delete ok - 26\t| ^ pinniped PackageInstall 16m delete - delete ok - 27\t| ^ pinniped-values-ver-1 Secret 17m delete - delete ok - 28\t| ^ postgres PackageInstall 16m delete - delete ok - 29\t| ^ postgres-endpoint-controller PackageInstall 15m delete - delete ok - 30\t| ^ postgres-endpoint-controller-values-ver-1 Secret 17m delete - delete ok - 31\t| ^ postgres-values-ver-1 Secret 17m delete - delete ok - 32\t| ^ s3-access-operator PackageInstall 15m delete - delete ok - 33\t| ^ s3-access-operator-values-ver-1 Secret 17m delete - delete ok - 34\t| ^ tmc-install-sa ServiceAccount 17m delete - delete ok - 35\t| ^ tmc-local-monitoring PackageInstall 4m delete - delete ongoing Reconciling 36\t| ^ tmc-local-stack PackageInstall 14m delete - delete fail Reconcile failed: (message: Error 37\t| (see .status.usefulErrorMessage for 38\t| details)) 39\t| ^ tmc-local-stack-secrets PackageInstall 17m delete - delete ok - 40\t| ^ tmc-local-stack-values-ver-1 Secret 17m delete - delete ok - 41\t| ^ tmc-local-support PackageInstall 16m delete - delete ok - 42\t| ^ tmc-local-support-values-ver-1 Secret 17m delete - delete ok - 43\t| Op: 0 create, 26 delete, 0 update, 0 noop, 0 exists 44\t| Wait to: 0 reconcile, 26 delete, 0 noop 45\t| 7:55:19AM: ---- applying 23 changes [0/26 done] ---- 46\t| 7:55:19AM: delete secret/monitoring-values-ver-1 (v1) namespace: tmc-local 47\t| 7:55:19AM: delete secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local 48\t| 7:55:19AM: delete secret/contour-values-ver-1 (v1) namespace: tmc-local 49\t| 7:55:19AM: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local 50\t| 7:55:19AM: delete secret/kafka-values-ver-1 (v1) namespace: tmc-local 51\t| 7:55:19AM: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 52\t| 7:55:19AM: delete secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local 53\t| 7:55:19AM: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 54\t| 7:55:20AM: delete packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 55\t| 7:55:20AM: delete secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local 56\t| 7:55:20AM: delete packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 57\t| 7:55:20AM: delete secret/postgres-values-ver-1 (v1) namespace: tmc-local 58\t| 7:55:20AM: delete secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local 59\t| 7:55:20AM: delete secret/minio-values-ver-1 (v1) namespace: tmc-local 60\t| 7:55:20AM: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 61\t| 7:55:20AM: delete secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local 62\t| 7:55:20AM: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 63\t| 7:55:20AM: delete packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local 64\t| 7:55:20AM: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local 65\t| 7:55:20AM: delete secret/pinniped-values-ver-1 (v1) namespace: tmc-local 66\t| 7:55:20AM: delete packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local 67\t| 7:55:20AM: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 68\t| 7:55:20AM: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local 69\t| 7:55:20AM: ---- waiting on 23 changes [0/26 done] ---- 70\t| 7:55:20AM: ok: delete secret/monitoring-values-ver-1 (v1) namespace: tmc-local 71\t| 7:55:20AM: ok: delete secret/s3-access-operator-values-ver-1 (v1) namespace: tmc-local 72\t| 7:55:20AM: ok: delete secret/contour-values-ver-1 (v1) namespace: tmc-local 73\t| 7:55:20AM: ongoing: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local 74\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 75\t| 7:55:20AM: ongoing: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local 76\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 77\t| 7:55:20AM: ok: delete secret/kafka-topic-controller-values-ver-1 (v1) namespace: tmc-local 78\t| 7:55:20AM: ok: delete secret/kafka-values-ver-1 (v1) namespace: tmc-local 79\t| 7:55:20AM: ongoing: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 80\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 81\t| 7:55:20AM: ongoing: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 82\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 83\t| 7:55:20AM: ok: delete secret/tmc-local-support-values-ver-1 (v1) namespace: tmc-local 84\t| 7:55:20AM: ongoing: delete packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 85\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 86\t| 7:55:20AM: ok: delete secret/postgres-values-ver-1 (v1) namespace: tmc-local 87\t| 7:55:20AM: ongoing: delete packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 88\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 89\t| 7:55:20AM: ok: delete secret/postgres-endpoint-controller-values-ver-1 (v1) namespace: tmc-local 90\t| 7:55:20AM: ok: delete secret/minio-values-ver-1 (v1) namespace: tmc-local 91\t| 7:55:20AM: ongoing: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 92\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 93\t| 7:55:20AM: ongoing: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 94\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 95\t| 7:55:20AM: ok: delete secret/tmc-local-stack-values-ver-1 (v1) namespace: tmc-local 96\t| 7:55:20AM: ok: delete secret/pinniped-values-ver-1 (v1) namespace: tmc-local 97\t| 7:55:20AM: ongoing: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local 98\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 99\t| 7:55:20AM: ongoing: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 100\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 101\t| 7:55:20AM: ongoing: delete packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local 102\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 103\t| 7:55:20AM: ongoing: delete packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local 104\t| 7:55:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 105\t| 7:55:20AM: ---- waiting on 12 changes [11/26 done] ---- 106\t| 7:55:27AM: ok: delete packageinstall/kafka-topic-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 107\t| 7:55:27AM: ---- waiting on 11 changes [12/26 done] ---- 108\t| 7:55:28AM: ok: delete packageinstall/tmc-local-support (packaging.carvel.dev/v1alpha1) namespace: tmc-local 109\t| 7:55:28AM: ---- waiting on 10 changes [13/26 done] ---- 110\t| 7:55:59AM: ok: delete packageinstall/postgres (packaging.carvel.dev/v1alpha1) namespace: tmc-local 111\t| 7:55:59AM: ---- waiting on 9 changes [14/26 done] ---- 112\t| 7:56:03AM: ok: delete packageinstall/minio (packaging.carvel.dev/v1alpha1) namespace: tmc-local 113\t| 7:56:03AM: ---- waiting on 8 changes [15/26 done] ---- 114\t| 7:56:20AM: ongoing: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local 115\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 116\t| 7:56:20AM: ongoing: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 117\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 118\t| 7:56:20AM: ongoing: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local 119\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 120\t| 7:56:20AM: ongoing: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 121\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 122\t| 7:56:20AM: ongoing: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 123\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 124\t| 7:56:20AM: ongoing: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local 125\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 126\t| 7:56:20AM: ongoing: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 127\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 128\t| 7:56:20AM: ongoing: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 129\t| 7:56:20AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 130\t| 7:56:37AM: ok: delete packageinstall/pinniped (packaging.carvel.dev/v1alpha1) namespace: tmc-local 131\t| 7:56:37AM: ---- waiting on 7 changes [16/26 done] ---- 132\t| 7:56:38AM: ok: delete packageinstall/tmc-local-stack-secrets (packaging.carvel.dev/v1alpha1) namespace: tmc-local 133\t| 7:56:38AM: ---- waiting on 6 changes [17/26 done] ---- 134\t| 7:56:40AM: ok: delete packageinstall/tmc-local-monitoring (packaging.carvel.dev/v1alpha1) namespace: tmc-local 135\t| 7:56:40AM: ---- waiting on 5 changes [18/26 done] ---- 136\t| 7:56:43AM: ok: delete packageinstall/s3-access-operator (packaging.carvel.dev/v1alpha1) namespace: tmc-local 137\t| 7:56:43AM: ---- waiting on 4 changes [19/26 done] ---- 138\t| 7:56:48AM: ok: delete packageinstall/kafka (packaging.carvel.dev/v1alpha1) namespace: tmc-local 139\t| 7:56:48AM: ---- waiting on 3 changes [20/26 done] ---- 140\t| 7:56:54AM: ok: delete packageinstall/contour (packaging.carvel.dev/v1alpha1) namespace: tmc-local 141\t| 7:56:54AM: ---- waiting on 2 changes [21/26 done] ---- 142\t| 7:57:21AM: ongoing: delete packageinstall/postgres-endpoint-controller (packaging.carvel.dev/v1alpha1) namespace: tmc-local 143\t| 7:57:21AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 144\t| 7:57:21AM: ongoing: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local 145\t| 7:57:21AM: ^ Waiting on finalizers: finalizers.packageinstall.packaging.carvel.dev/delete 146\t| 7:57:40AM: ok: delete packageinstall/tmc-local-stack (packaging.carvel.dev/v1alpha1) namespace: tmc-local 147\t| 7:57:40AM: ---- waiting on 1 changes [22/26 done] ---- 1487:57:43AM: App \u0026#39;tanzu-mission-control\u0026#39; in namespace \u0026#39;tmc-local\u0026#39; deleted 1497:57:44AM: packageinstall/tanzu-mission-control (packaging.carvel.dev/v1alpha1) namespace: tmc-local: DeletionSucceeded 1507:57:44AM: Deleting \u0026#39;Secret\u0026#39;: tanzu-mission-control-tmc-local-values 1517:57:44AM: Deleting \u0026#39;ServiceAccount\u0026#39;: tanzu-mission-control-tmc-local-sa 1527:57:44AM: Deleting \u0026#39;ClusterRole\u0026#39;: tanzu-mission-control-tmc-local-cluster-role 1537:57:44AM: Deleting \u0026#39;ClusterRoleBinding\u0026#39;: tanzu-mission-control-tmc-local-cluster-rolebinding There may be reasons you need to remove the namespace tmc-local also has it contains a lot of configmaps, secret and pvc volumes. So if you want to completeley and easily remove everything TMC-SM related, delete the namespace. From the official documentation:\nTo remove Tanzu Mission Control Self-Managed and its artifacts from you cluster, use the tanzu cli.\nBack up any data that you do not want to lose.\nRun the following commands:\n1tanzu package installed delete tanzu-mission-control --namespace tmc-local 2tanzu package repository delete tanzu-mission-control-packages --namespace tmc-local If necessary, delete residual resources.\nThe above commands clean up most of the resources that were created by the tanzu-mission-control Tanzu package. However, there are some resources that you have to remove manually. The resources include: - persistent volumes - internal TLS certificates - configmaps\nAlternatively, you can delete the tmc-local namespace. When you delete the tmc-local namespace, the persistent volume claims associated with the namespace are deleted. Make sure you have already backed up any data that you donâ€™t want to lose.\nFirst time login TMC-SM If everything above went after plan (not for me, just a minor issue), I should now be able to login to my TMC-SM console.\nUsing my regular client from a web-browser enter https://tmc.pretty-awesome-domain.net\nAnd ðŸ¥ I am logged into TMC-SM.\nI will end this post here. Will create a second post on working with TMC-SM. Thanks for reading.\nCredits where credit's due In this post its necessary to give credits again for making this post. This time it goes to my manager Antonio and colleague Jose that helped out with the initial configs, then my colleague Alex that helped out with the Keycloak authentication related settings.\n","link":"https://blog.andreasm.io/2023/07/12/installing-tmc-local-on-vsphere-8-with-tanzu-using-keycloak-as-oidc-provider/","section":"post","tags":["cert-manager","Authentication","keycloak","tmc-sm","tmc-local"],"title":"Installing TMC local on vSphere 8 with Tanzu using Keycloak as OIDC provider"},{"body":"","link":"https://blog.andreasm.io/tags/keycloak/","section":"tags","tags":null,"title":"keycloak"},{"body":"","link":"https://blog.andreasm.io/categories/lcm/","section":"categories","tags":null,"title":"LCM"},{"body":"","link":"https://blog.andreasm.io/tags/tmc-local/","section":"tags","tags":null,"title":"tmc-local"},{"body":"","link":"https://blog.andreasm.io/tags/tmc-sm/","section":"tags","tags":null,"title":"tmc-sm"},{"body":"","link":"https://blog.andreasm.io/tags/antrea/","section":"tags","tags":null,"title":"antrea"},{"body":"","link":"https://blog.andreasm.io/categories/antrea/","section":"categories","tags":null,"title":"Antrea"},{"body":"","link":"https://blog.andreasm.io/tags/cni/","section":"tags","tags":null,"title":"cni"},{"body":"","link":"https://blog.andreasm.io/categories/cni/","section":"categories","tags":null,"title":"CNI"},{"body":"Some context... I have written a couple of post previously on the topic Antrea Policies, this time I will try to put it more into a context, how we can use and create Antrea Policies in different scenarios and with some \u0026quot;frameworks\u0026quot; from different perspectives in the organization.\nWhat if, and how, can we deliver a already secured Kubernetes cluster, like an out of the box experience, with policies applied that meets certain guidelines for what is allowed and not allowed in the organization for certain Kubernetes clusters. Whether they are manually provisioned, or provisined on demand. So in this post a will try to be a bit specific on how to achieve this, with a simulated requirement as context, will get back to this context a further down. The following products will be used in this post: vSphere with Tanzu and TKG workload clusters, Antrea as the CNI, Tanzu Mission Control and VMware NSX.\nAs usual, for more details on the above mentioned product head over to the below links\nAntrea for detailed and updated documentation. vSphere with Tanzu for detailed and updated documentation. Tanzu Mission Control for detailed and updated documentation. VMware NSX for detailed and updated documentation. Different layers of security, different personas, different enforcement points This post will mostly be focusing in on the Kubernetes perspective, using specifically Antrea Network policies to restrict traffic inside the Kubernetes cluster. A Kubernetes cluster is just one infrastructure component in the organization, but contains many moving parts with applications and services inside. Even inside a Kubernetes cluster there can be different classifications for what should be allowed and not. Therefore a Kubernetes cluster is also in need to be to be secured with a set of tools and policies to satisfy the security policy guidelines in the organization. A Kubernetes cluster is another layer in the infrastructure that needs to be controlled. In a typical datacenter we have several security mechanisms in place like AV agents, physical firewall, virtual firewall, NSX distributed firewall. All these play an important role in the different layers of the datacenter/organization. Assuming the Kubernetes worker nodes are running as virtual machines on VMware vSphere the below illustration describes two layers of security using NSX distributed firewall securing the VM workers, and Antrea Network Policies securing pods, services inside the Kubernetes cluster.\nWith the illustration above in mind it is fully possible to create a very strict environment with no unwanted lateral movement. Meaning only the strict necessary firewall openings inside the kubernetes cluster between pods, namespaces and services, but also between workers in same subnet and across several several Kubernetes clusters. But the above two layers, VMs in vSphere protected by the NSX distributed firewall and apps running Kubernetes clusters and protected by Antrea Network policies, are often managed by different personas in the organization. We have the vSphere admins, Network admins, Security Admins, App Operators and App Developers. Security is crucial in a modern datacenter, so, again, the correct tools needs to be in place for the organization's security-framework to be implemented all the way down the \u0026quot;stack\u0026quot; to be compliant. Very often there is a decided theoretical security framework/design in place, but that plan is not always so straightforward to implement.\nGoing back to Kubernetes again and Antrea Network policies. Antrea feature several static (and optional custom) Tiers where different types of network policies can be applied. As all the Antrea Network policies are evaluated \u0026quot;top-down\u0026quot; it is very handy to be able to place some strict rules very early in the \u0026quot;chain\u0026quot; of firewall policies to ensure the organization's security compliance is met. Being able to place these rules at the top prohibits the creation of rules further down that contradicts these top rules, they will not be evaluated. Then there is room to create a framework that gives some sense of \u0026quot;flexibility\u0026quot; to support the environment's workload according to the type of classification (prod, dev, test, dmz, trust, untrust). Other policies can be applied to further restrict movement before hitting a default block rule that takes care of anything that is not specified earlier in the \u0026quot;chain\u0026quot; of policies. The illustration below is an example of whom and where these personas can take charge and apply their needed policies.\nThen the next illustration is the default Static Tiers that comes with Antrea. These Tiers makes it easier to categorize the different policies in a Kubernetes cluster, but also provides a great way to delegate responsibility/permissions by using RBAC to control access to the Tiers. This means we can have some admins to apply policies in specific Tiers, and no one else can overwrite these.\nNow, how can the different personas make sure their policies are applied? This is what I will go through next.\nManaging and making sure the required Antrea Policies are applied Lets start out by bringing some light on the simulated requirement I mentioned above. Customer Andreas have some strict security guidelines they need to follow to ensure compliance before anyone can do anything in the Kubernetes platforms. To be compliant according to the strict security guidelines the following must be in place:\nAll Kubernetes workload clusters are considered isolated and not allowed to reach nothing more than themselves, including pods and services (all nodes in the same cluster) Only necessary backend functions such as DNS/NTP are allowed. Certain management tools need access to the clusters All non-system namespaces should be considered \u0026quot;untrusted\u0026quot; and isolated by default. RBAC needs to be in place to ensure no tampering on applied security policies. The above diagram is what customer Andreas needs to have in place. Lets go ahead and apply them. In the next sub-chapters I will show how to apply and manage the policies in three different ways to acheive this. I assume the NSX personas has done their part and applied the correct distributed firewall rules isolating the worker nodes.\nApplying Antrea policies with kubectl This process involves logging into a newly provisioned Kubernetes cluster (TKG cluster in my environment) that someone has provisioned, could be the vSphere admin persona, or via a self-service. Then the security admin will be using kubectl to log in and apply some yaml definitions to acheive the above requirements. This operation will typically be the security admin responsibilities. The definitions the security admin is applying will all be configured in the static Tier \u0026quot;securityops\u0026quot; with different priorities. Here is the demo-environment I will be using in the following chapters: The first requirement is a \u0026quot;no-trust\u0026quot; in any non-system namespaces, where I want to achieve full isolation between namespace. No communication from one namespace to another. In the Antrea homepage there are several examples, and I will use one of the examples that suits my need perfectly. It looks like this:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: strict-ns-isolation-except-system-ns 5spec: 6 priority: 9 7 tier: securityops 8 appliedTo: 9 - namespaceSelector: # Selects all non-system Namespaces in the cluster 10 matchExpressions: 11 - {key: kubernetes.io/metadata.name, operator: NotIn, values: [avi-system,default,kube-node-lease,kube-public,kube-system,secretgen-controller,tanzu-continuousdelivery-resources,tanzu-fluxcd-packageinstalls,tanzu-kustomize-controller,tanzu-source-controller,tkg-system,vmware-system-auth,vmware-system-cloud-provider,vmware-system-csi,vmware-system-tkg,vmware-system-tmc]} 12 ingress: 13 - action: Pass 14 from: 15 - namespaces: 16 match: Self # Skip ACNP evaluation for traffic from Pods in the same Namespace 17 name: PassFromSameNS 18 - action: Drop 19 from: 20 - namespaceSelector: {} # Drop from Pods from all other Namespaces 21 name: DropFromAllOtherNS 22 egress: 23 - action: Pass 24 to: 25 - namespaces: 26 match: Self # Skip ACNP evaluation for traffic to Pods in the same Namespace 27 name: PassToSameNS 28 - action: Drop 29 to: 30 - namespaceSelector: {} # Drop to Pods from all other Namespaces 31 name: DropToAllOtherNS The only modifications I have done is adding all my system-namespaces. Then I will apply it.\n1# Verifying no policies in place: 2andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 3No resources found 4andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f acnp-ns-isolation-except-system-ns.yaml 5clusternetworkpolicy.crd.antrea.io/strict-ns-isolation-except-system-ns created 6andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 7NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 8strict-ns-isolation-except-system-ns securityops 9 0 0 15s Notice the 0 under Desired Nodes and Current Nodes. The reason is that this cluster is completely new, and there is no workload in any non-system namespaces yet. Here are the current namespaces:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get ns 2NAME STATUS AGE 3default Active 28d 4kube-node-lease Active 28d 5kube-public Active 28d 6kube-system Active 28d 7secretgen-controller Active 28d 8tkg-system Active 28d 9vmware-system-auth Active 28d 10vmware-system-cloud-provider Active 28d 11vmware-system-csi Active 28d 12vmware-system-tkg Active 28d Now if I apply a couple of namespaces and deploy some workload in them:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f dev-app.yaml -f dev-app2.yaml 2namespace/dev-app created 3deployment.apps/ubuntu-20-04 created 4namespace/dev-app2 created 5deployment.apps/ubuntu-dev-app2 created How does the policy look like now?\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3strict-ns-isolation-except-system-ns securityops 9 1 1 6s 4# Why only one 5andreasm@linuxvm01:~/antrea/policies/groups$ k get pods -n dev-app -owide 6NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 7ubuntu-20-04-548545fc87-t2lg2 1/1 Running 0 82s 20.20.3.216 three-zone-cluster-1-node-pool-3-6r8c2-6c8d48656c-wntwc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 8andreasm@linuxvm01:~/antrea/policies/groups$ k get pods -n dev-app2 -owide 9NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 10ubuntu-dev-app2-564f46785c-g8vb6 1/1 Running 0 86s 20.20.3.215 three-zone-cluster-1-node-pool-3-6r8c2-6c8d48656c-wntwc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Both workloads ended up on same node...\nSo far so good. Now I need to verify if it is actually enforcing anything. From one of the dev-app pods I will execute into bash and try ping another pod in one for the system-namespaces, a pod in the the other dev-app namespace and try to a dns lookup.\n1andreasm@linuxvm01:~/antrea/policies/groups$ k exec -it -n dev-app ubuntu-20-04-548545fc87-t2lg2 bash 2kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. 3root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.1.7 4PING 20.20.1.7 (20.20.1.7) 56(84) bytes of data. 5^C 6--- 20.20.1.7 ping statistics --- 7170 packets transmitted, 0 received, 100% packet loss, time 173033ms The ping above was from my dev-app pod to the coredns pod in kube-system. Ping to the other dev-app pod in the other dev-app namespace.\n1root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.3.215 2PING 20.20.3.215 (20.20.3.215) 56(84) bytes of data. 3^C 4--- 20.20.3.215 ping statistics --- 59 packets transmitted, 0 received, 100% packet loss, time 8181ms Is also blocked.\nNow DNS lookup:\n1root@ubuntu-20-04-548545fc87-t2lg2:/# ping google.com 2ping: google.com: Temporary failure in name resolution 3#So much empty DNS was also one of the requirements, so I will have to fix this also. I mean, the security admin will have to fix this otherwise going to lunch will not be such a great place to be...\nAs the security admin have applied the above policy in the securityops tier with a priority of 9 he need to open up for DNS with policies in a higher tier or within same tier with a lower priority number (lower equals higher priority).\nThis is the policy he needs to apply:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: allow-all-egress-dns-service 5spec: 6 priority: 8 7 tier: securityops 8 appliedTo: 9 - namespaceSelector: {} 10# matchLabels: 11# k8s-app: kube-dns 12 egress: 13 - action: Allow 14 toServices: 15 - name: kube-dns 16 namespace: kube-system 17 name: \u0026#34;allowdnsegress-service\u0026#34; A simple one, and the requirement is satisfied. Company Andreas allowed necessay functions such as DNS.. This policy will allow any namespace to reach the kube-dns service.\nThe rule applied:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3allow-all-egress-dns-service securityops 8 4 4 2m24s 4strict-ns-isolation-except-system-ns securityops 9 1 1 24m What about DNS lookup now:\n1root@ubuntu-20-04-548545fc87-t2lg2:/# ping google.com 2PING google.com (172.217.12.110) 56(84) bytes of data. 364 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=1 ttl=105 time=33.0 ms 464 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=2 ttl=104 time=29.8 ms 564 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=3 ttl=105 time=30.2 ms 664 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=4 ttl=104 time=30.3 ms 764 bytes from 172.217.12.110 (172.217.12.110): icmp_seq=5 ttl=105 time=30.4 ms 8^C 9--- google.com ping statistics --- 105 packets transmitted, 5 received, 0% packet loss, time 4003ms 11rtt min/avg/max/mdev = 29.763/30.733/32.966/1.138 ms Works.\nThats one more requirement met. Now one of the requirements was also to restrict access to services in other kubernetes clusters. Even though we trust that the NSX admins have created these isolation rules for us we need to make sure we are not allowed from the current kubernetes cluster also.\nSo to acheive this the security admin needs to create ClusterGroup containing the CIDR for its own worker nodes. Then apply a policy using the ClusterGroup. Here is the ClusterGroup definition (containing the cidr for the worker nodes):\n1apiVersion: crd.antrea.io/v1alpha3 2kind: ClusterGroup 3metadata: 4 name: tz-cluster-1-node-cidr 5spec: 6 # ipBlocks cannot be set along with podSelector, namespaceSelector or serviceReference. 7 ipBlocks: 8 - cidr: 10.101.82.32/27 And I also need to define another ClusterGroup for all the RFC1918 subnets I need to block (this will include the cidr above):\n1apiVersion: crd.antrea.io/v1alpha3 2kind: ClusterGroup 3metadata: 4 name: tz-cluster-1-drop-cidr 5spec: 6 # ipBlocks cannot be set along with podSelector, namespaceSelector or serviceReference. 7 ipBlocks: 8 - cidr: 10.0.0.0/8 9 - cidr: 172.16.0.0/12 10 - cidr: 192.168.0.0/16 Apply them:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f tz-cluster-1-group-node-cidr.yaml 2clustergroup.crd.antrea.io/tz-cluster-1-node-cidr created 3andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f tz-cluster-1-drop-cidr.yaml 4clustergroup.crd.antrea.io/tz-cluster-1-drop-cidr created 5andreasm@linuxvm01:~/antrea/policies/groups$ k get clustergroup 6NAME AGE 7tz-cluster-1-drop-cidr 6s 8tz-cluster-1-node-cidr 5s And the policy to deny anything except the own kubernetes worker nodes:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: acnp-drop-except-own-cluster-node-cidr 5spec: 6 priority: 8 7 tier: securityops 8 appliedTo: 9 - namespaceSelector: # Selects all non-system Namespaces in the cluster 10 matchExpressions: 11 - {key: kubernetes.io/metadata.name, operator: NotIn, values: [avi-system,default,kube-node-lease,kube-public,kube-system,secretgen-controller,tanzu-continuousdelivery-resources,tanzu-fluxcd-packageinstalls,tanzu-kustomize-controller,tanzu-source-controller,tkg-system,vmware-system-auth,vmware-system-cloud-provider,vmware-system-csi,vmware-system-tkg,vmware-system-tmc]} 12 egress: 13 - action: Allow 14 to: 15 - group: \u0026#34;tz-cluster-1-node-cidr\u0026#34; 16 - action: Drop 17 to: 18 - group: \u0026#34;tz-cluster-1-drop-cidr\u0026#34; Applied:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k apply -f tz-cluster-1-drop-anything-but-own-nodes.yaml 2clusternetworkpolicy.crd.antrea.io/acnp-drop-except-own-cluster-node-cidr created 3andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5acnp-drop-except-own-cluster-node-cidr securityops 8 1 1 3m39s 6allow-all-egress-dns-service securityops 8 4 4 28m 7strict-ns-isolation-except-system-ns securityops 9 1 1 50m From the dev-app pod again I will verify if I am allowed to SSH to a worker node in \u0026quot;own\u0026quot; Kubernetes cluster, and another Linux machine not in the ClusterGroup cidr I have applied.\n1root@ubuntu-20-04-548545fc87-t2lg2:/# ssh vmware-system-user@10.101.82.34 #A worker node in the current k8s cluster 2vmware-system-user@10.101.82.34\u0026#39;s password: 3#This is allowed 4What about other machines outside the cidr: 5root@ubuntu-20-04-548545fc87-t2lg2:/# ssh 10.101.10.99 6ssh: connect to host 10.101.10.99 port 22: Connection timed out That is very close to achieving this requirement also, but I should be allowed to reach pods inside same namespace regardless of which node they reside on. Here are my dev-app namespace with pods on all three nodes:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get pods -n dev-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-20-04-548545fc87-75nsm 1/1 Running 0 116s 20.20.2.35 three-zone-cluster-1-node-pool-2-kbzvq-6846d5cc5b-6hdmj \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-548545fc87-hhnv2 1/1 Running 0 116s 20.20.1.14 three-zone-cluster-1-node-pool-1-dgcpq-656c75f4f4-nsr2r \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5ubuntu-20-04-548545fc87-t2lg2 1/1 Running 0 66m 20.20.3.216 three-zone-cluster-1-node-pool-3-6r8c2-6c8d48656c-wntwc \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 1root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.1.14 2PING 20.20.1.14 (20.20.1.14) 56(84) bytes of data. 364 bytes from 20.20.1.14: icmp_seq=1 ttl=62 time=20.6 ms 464 bytes from 20.20.1.14: icmp_seq=2 ttl=62 time=2.87 ms 5^C 6--- 20.20.1.14 ping statistics --- 72 packets transmitted, 2 received, 0% packet loss, time 1002ms 8rtt min/avg/max/mdev = 2.869/11.735/20.601/8.866 ms 9root@ubuntu-20-04-548545fc87-t2lg2:/# ping 20.20.2.35 10PING 20.20.2.35 (20.20.2.35) 56(84) bytes of data. 1164 bytes from 20.20.2.35: icmp_seq=1 ttl=62 time=3.49 ms 1264 bytes from 20.20.2.35: icmp_seq=2 ttl=62 time=2.09 ms 1364 bytes from 20.20.2.35: icmp_seq=3 ttl=62 time=1.00 ms 14^C 15--- 20.20.2.35 ping statistics --- 163 packets transmitted, 3 received, 0% packet loss, time 2003ms 17rtt min/avg/max/mdev = 1.000/2.194/3.494/1.020 ms From the Antrea UI, lets do some tests there also:\nNote that I have not created any default-block-all-else-rule. There is always room for improvement, and this was just an excercise to show what is possible, not an final answer on how things should be done. Some of the policies can be even more granular like specifying only ports/protocol/FQDN etc..\nSo just to summarize what I have done:\nThese are the applied rules:\n1NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 2acnp-drop-except-own-cluster-node-cidr securityops 8 3 3 23h 3allow-all-egress-dns-service securityops 8 4 4 23h 4strict-ns-isolation-except-system-ns securityops 9 3 3 23h The first rule is allowing only traffic to the nodes in its own cluster - matches this requirement \u0026quot;All Kubernetes workload clusters are considered isolated and not allowed to reach nothing more than themselves, including pods and services (all nodes in the same cluster)\u0026quot;\nThe second rule is allowing all namespaces to access the kube-dns service in the kube-system namespace - matches this requirement \u0026quot;Only necessary backend functions such as DNS/NTP are allowed\u0026quot;\nThe third rule is dropping all traffic between namespaces, except the \u0026quot;system\u0026quot;-namespaces I have defined. But it allows intra communication inside each namespace - matches this requirement \u0026quot;All non-system namespaces should be considered \u0026quot;untrusted\u0026quot; and isolated by default\u0026quot;\nThen I have not done anything with RBAC yet, will come later in this post. And the requirement: \u0026quot;Certain management tools need access to the clusters\u0026quot; I can assume the NSX admins have covered, as I am not blocking any ingress traffic to the \u0026quot;system\u0026quot;-namespaces, same is true for egress from the system-namespaces. But it could be, if adjusted to allow the necessary traffic from these namespaces to the certain management tools.\nApplying Antrea policies using TMC - Tanzu Mission Control This section will not create any new scenario, it will re-use all the policies created and applied in the above section. The biggest difference is how the policies are being applied.\nNot that I dont think any security admin dont want to log in to a Kubernetes cluster and apply these security policies, but it can be a bit tedious each time a new cluster is applied. What wouldnt be better then if we can auto-deploy them each time a new cluster is being deployed? Like an out-of-the-box experience? Yes, excactly. If we already have defined a policy scope for the different Kubernetes cluster in our environment, we could just apply the correct policies to each cluster respectively each time they are provisioned. This saves a lot of time. We can be sure each time a new cluster is provisioned, it is being applied with the correct set of policies. With the abiltiy to auto apply these required policies on creation or directly after creation will make provisioning out-of-the-box compliant clusters a joy.\nNow this sounds interesting, how can I do that?\n....Into the door comes TMC.... Hello Tanzu Mission Control, short TMC. With TMC we can administer Tanzu with vSphere in addition to a lot of other Kubernetes platforms. From the TMC official docs :\nVMware Tanzu Mission Controlâ„¢ is a centralized management platform for consistently operating and securing your Kubernetes infrastructure and modern applications across multiple teams and clouds.\nAvailable through VMware Cloudâ„¢ services, Tanzu Mission Control provides operators with a single control point to give developers the independence they need to drive business forward, while ensuring consistent management and operations across environments for increased security and governance.\nTanzu Mission Control provides instances of the service in regions around the world, including Australia, Canada, India, Ireland, Japan, and USA. For a list of the regions in which the Tanzu Mission Control is hosted, go to the Cloud Management Services Availability page at https://www.vmware.com/global-infrastructure.html and select VMware Tanzu Mission Control.\nUse Tanzu Mission Control to manage your entire Kubernetes footprint, regardless of where your clusters reside.\nLets cut to the chase and make my cluster compliant with the above rules.\nPreparing TMC In my TMC dashboard I need two thing in place:\nA Git repository where I host my yamls, specifically my Antrea policy yamls. A configured Kustomization using the above Git repo Git repository I will create a dedicated Git repo called tmc-cd-repo, and a folder structure. Here is my Github repo for this purpose: Now push the yamls to this repo's subfolder antrea-baseline-policies:\n1andreasm:~/github_repos/tmc-cd-repo (main)$ git add . 2andreasm:~/github_repos/tmc-cd-repo (main)$ git commit -s -m \u0026#34;ready-to-lockdown\u0026#34; 3[main 4ab93a7] ready-to-lockdown 4 4 files changed, 53 insertions(+) 5 create mode 100644 antrea/antrea-baseline-policies/acnp-allow-egress-all-coredns-service.yaml 6 create mode 100644 antrea/antrea-baseline-policies/tz-cluster-1-drop-anything-but-own-nodes.yaml 7 create mode 100644 antrea/antrea-baseline-policies/tz-cluster-1-drop-cidr.yaml 8 create mode 100644 antrea/antrea-baseline-policies/tz-cluster-1-group-node-cidr.yaml 9andreasm:~/github_repos/tmc-cd-repo (main)$ git push 10Enumerating objects: 11, done. 11Counting objects: 100% (11/11), done. 12Delta compression using up to 16 threads 13Compressing objects: 100% (7/7), done. 14Writing objects: 100% (8/8), 1.43 KiB | 733.00 KiB/s, done. 15Total 8 (delta 1), reused 0 (delta 0), pack-reused 0 16remote: Resolving deltas: 100% (1/1), done. 17To github.com:andreasm80/tmc-cd-repo.git 18 5c9ba04..4ab93a7 main -\u0026gt; main 19andreasm:~/github_repos/tmc-cd-repo (main)$ And here they are:\nTMC Kustomization Now in my TMC dashboard configure Git repo:\nI can choose to add the Git repo per cluster that is managed by TMC or in a cluster group. I will go with adding the Git repo on my cluster called three-zone-cluster-1 for the moment. The benefit with adding it at the group is that it can be shared across multiple clusters. In TMC click Clusters and find your already managed and added cluster then click on it to \u0026quot;enter it\u0026quot;.\nIn your cluster group click on the tab Add-ons Then find Git repositories and Add Git Repository Fill in the needed fields. Make sure to expand advanced settings to update the branch to your branch or main branch. Can also adjust the sync intervall to higher or smaller. Default is 5, I have sat mine to 1. The repository url points to the actual repository, no subfolders. This is because in the Kustomization later we can have multiple pointing to the respective subfolder which can then be unique pr cluster etc. Make sure you also choose \u0026quot;no credentials needed\u0026quot; under Repository Credentials if using a public Git repo as I am. After save you should see a green status: Now, we need to add a Kustomization. This can also be done in either a group or pr cluster. I will start with adding it directly to my specific cluster. In TMC click Cluster and select your cluster. Click Add-ons, Under Continuous Delivery click Installed Kustomizations. Add Kustomization.\nBefore I add my Kustomization, I have made sure I have deleted all the policies and groups in my test-cluster three-zone-cluster-1:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 2No resources found 3andreasm@linuxvm01:~/antrea/policies/groups$ k get clustergroups 4No resources found Then I will continue and add the Kustomization:\nMake sure to point to the correct subfolder in the Git repo. I have enabled the Prune option so I everything deployed via Kustomization will be deleted in my cluster if I decide to remove the Kustomization.\nClick add.\nClick refresh in the top right corner, and it should be green. Lets check the policies and groups in the cluster itself..\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3acnp-drop-except-own-cluster-node-cidr securityops 8 3 3 70s 4allow-all-egress-dns-service securityops 8 4 4 70s 5strict-ns-isolation-except-system-ns securityops 9 3 3 70s 6andreasm@linuxvm01:~/antrea/policies/groups$ k get clustergroups 7NAME AGE 8tz-cluster-1-drop-cidr 73s 9tz-cluster-1-node-cidr 73s The Antrea Policies have been applied.\nDeploy TKC cluster from TMC - auto apply security policies The above section enabled Kustomization on a already managed TKC cluster in TMC. In this section I will apply a TKC cluster from TMC and let the Antrea policies be automatically be applied.\nIn TMC I will create two Cluster Groups, one called andreas-dev-clusters and one called andreas-prod-clusters. After I have added the two cluster groups I will configure Add-ons. Same as in previous section, adding the the Git reop but this time I will point to the different subfolders I created in my Git repo. I have created two different sub-folders in my Git repo called: tmc-cd-repo/antrea/antrea-baseline-policies/dev-clusters and tmc-cd-repo/antrea/antrea-baseline-policies/prod-clusters. The reason I have done that is because I want the option to apply different Antrea policies for certain clusters, different environments different needs.\nBefore adding the Git repo on the two new Cluster groups in TMC I need to enable continuous delivere by clicking on this blue button. The Git repo has been added two both my new cluster groups. Now I just need to add the Kustomization pointing to my new Git repo subfolders dev-clusters and prod-clusters.\nNow the preparations have been done in TMC, it is time to deploy the two TKC clusters from TMC and see if my policies are automatically applied. One \u0026quot;prod-cluster\u0026quot; and one \u0026quot;dev-cluster\u0026quot;.\nLets start with the \u0026quot;prod-cluster\u0026quot; Creating the dev-cluster The clusters are ready: Let us check the sync status of my Kustomizations. Prod-Cluster Group: Dev-Cluster Group: Still applied.\nLets have a look inside the two TKC cluster using kubectl. Prod-Cluster-2:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k config current-context 2prod-cluster-2 3andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5allow-all-egress-dns-service securityops 8 2 2 35m 6prod-clusters-acnp-drop-except-own-cluster-node-cidr securityops 8 0 0 35m 7prod-clusters-strict-ns-isolation-except-system-ns securityops 9 0 0 35m Dev-Cluster-2:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k config current-context 2dev-cluster-2 3andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5dev-clusters-strict-ns-isolation-except-system-ns securityops 9 0 0 45s 6dev-clusters-acnp-drop-except-own-cluster-node-cidr securityops 8 0 0 45s 7dev-clusters-allow-all-egress-dns-service securityops 8 2 2 45s Thats it then, if I need to change the policies I can just edit policies, git add, commit and push and they will be applied to all clusters in the group. By enabling this feature in TMC its just all about adding or attaching your clusters in the respective group in TMC and they will automatically get all the needed yamls applied. Applying Antrea policies with NSX With NSX one can also manage the native Antrea policies inside each TKC cluster (or any other Kubernetes cluster Antrea supports for that matter). I have written about this here. NSX can also create security policies \u0026quot;outside\u0026quot; the TKC cluster by using the inventory information it gets from Antrea and enforce them in the NSX Distributed firewall, a short section on this below.\nApplying Antrea native policies from the NSX manager So in this section I will quickly go through using the same \u0026quot;framework\u0026quot; as above using NSX as the \u0026quot;management-plane\u0026quot;. Just a reminder, we have these three policies:\n1NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 2acnp-drop-except-own-cluster-node-cidr securityops 8 3 3 23h 3allow-all-egress-dns-service securityops 8 4 4 23h 4strict-ns-isolation-except-system-ns securityops 9 3 3 23h The first rule is allowing only traffic to the nodes in its own cluster - matches this requirement \u0026quot;All Kubernetes workload clusters are considered isolated and not allowed to reach nothing more than themselves, including pods and services (all nodes in the same cluster)\u0026quot;\nThe second rule is allowing all namespaces to access the kube-dns service in the kube-system namespace - matches this requirement \u0026quot;Only necessary backend functions such as DNS/NTP are allowed\u0026quot;\nThe third rule is dropping all traffic between namespaces, except the \u0026quot;system\u0026quot;-namespaces I have defined. But it allows intra communication inside each namespace - matches this requirement \u0026quot;All non-system namespaces should be considered \u0026quot;untrusted\u0026quot; and isolated by default\u0026quot;\nIn NSX I will need to create some Security Groups, then use these groups in a Security Policy. So I will start by creating the Security Group for the concerning kube-dns service:\nOne can either define the service kube-dns:\nOr the pods that is responsible for the DNS service (CoreDNS:\nThis depends on how we define the policy in NSX. I have gone with the pod selection group.\nAS the requirement supports all services to access DNS, I dont have to create a security group for the source. Then the policy will look like this in NSX:\nNotice also that I have placed the policy in the Infrastructrue Tier in NSX.\nThis is how it looks like in the Kubernetes clusters:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 933e463e-c061-4e80-80b3-eff3402e41a9 -oyaml 2apiVersion: crd.antrea.io/v1alpha1 3kind: ClusterNetworkPolicy 4metadata: 5 annotations: 6 ccp-adapter.antrea.tanzu.vmware.com/display-name: k8s-core-dns 7 creationTimestamp: \u0026#34;2023-06-27T11:15:30Z\u0026#34; 8 generation: 11 9 labels: 10 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 11 name: 933e463e-c061-4e80-80b3-eff3402e41a9 12 resourceVersion: \u0026#34;2248486\u0026#34; 13 uid: a5d7378d-ede0-4f8c-848b-413c10ce5602 14spec: 15 egress: 16 - action: Allow 17 appliedTo: 18 - podSelector: {} 19 enableLogging: false 20 name: \u0026#34;2025\u0026#34; 21 ports: 22 - port: 53 23 protocol: TCP 24 - port: 53 25 protocol: UDP 26 to: 27 - group: c7e96b35-1961-4659-8a62-688a0e98fe63 28 priority: 1.0000000177635693 29 tier: nsx-category-infrastructure 30status: 31 currentNodesRealized: 4 32 desiredNodesRealized: 4 33 observedGeneration: 11 34 phase: Realized 1andreasm@linuxvm01:~/antrea/policies/groups$ k get tiers 2NAME PRIORITY AGE 3application 250 6d1h 4baseline 253 6d1h 5emergency 50 6d1h 6networkops 150 6d1h 7nsx-category-application 4 6d 8nsx-category-emergency 1 6d 9nsx-category-environment 3 6d 10nsx-category-ethernet 0 6d 11nsx-category-infrastructure 2 6d 12platform 200 6d1h 13securityops 100 6d1h For the next policy, allowing only node in same cluster, I will need to create two groups with \u0026quot;ip-blocks\u0026quot; containing all RFC1918 in one group and the actual node range in the second: The policy in NSX will then look like this: This is how it looks like in the Kubernetes clusters:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 annotations: 5 ccp-adapter.antrea.tanzu.vmware.com/display-name: dev-cluster-1-intra 6 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 7 generation: 2 8 labels: 9 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 10 name: 17dbadce-06cf-4d1e-9747-3e888f0f58e0 11 resourceVersion: \u0026#34;2257468\u0026#34; 12 uid: 73814a58-2da8-44c2-ba85-2522865430d1 13spec: 14 egress: 15 - action: Allow 16 appliedTo: 17 - podSelector: {} 18 enableLogging: false 19 name: \u0026#34;2027\u0026#34; 20 to: 21 - group: 2051f64c-8c65-46a2-8397-61c926c8c4ce 22 - action: Drop 23 appliedTo: 24 - podSelector: {} 25 enableLogging: false 26 name: \u0026#34;2028\u0026#34; 27 to: 28 - group: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 29 priority: 1.000000017763571 30 tier: nsx-category-infrastructure 31status: 32 currentNodesRealized: 4 33 desiredNodesRealized: 4 34 observedGeneration: 2 35 phase: Realized Where the groups contain this:\n1apiVersion: crd.antrea.io/v1alpha3 2kind: ClusterGroup 3metadata: 4 annotations: 5 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 6 ccp-adapter.antrea.tanzu.vmware.com/display-name: 2051f64c-8c65-46a2-8397-61c926c8c4ce 7 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 8 generation: 1 9 labels: 10 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 11 name: 2051f64c-8c65-46a2-8397-61c926c8c4ce 12 resourceVersion: \u0026#34;2257281\u0026#34; 13 uid: 18009c1b-c44f-4c75-a9f2-8a30e2415859 14spec: 15 childGroups: 16 - 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 17status: 18 conditions: 19 - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 20 status: \u0026#34;True\u0026#34; 21 type: GroupMembersComputed 22andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 -oyaml 23apiVersion: crd.antrea.io/v1alpha3 24kind: ClusterGroup 25metadata: 26 annotations: 27 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 28 ccp-adapter.antrea.tanzu.vmware.com/display-name: 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 29 ccp-adapter.antrea.tanzu.vmware.com/parent: 2051f64c-8c65-46a2-8397-61c926c8c4ce 30 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 31 generation: 1 32 labels: 33 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 34 name: 2051f64c-8c65-46a2-8397-61c926c8c4ce-0 35 resourceVersion: \u0026#34;2257278\u0026#34; 36 uid: b1d4a59b-0557-4f6c-a08c-7b76af6bca8c 37spec: 38 ipBlocks: 39 - cidr: 10.101.84.32/27 40status: 41 conditions: 42 - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 43 status: \u0026#34;True\u0026#34; 44 type: GroupMembersComputed 1andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 -oyaml 2apiVersion: crd.antrea.io/v1alpha3 3kind: ClusterGroup 4metadata: 5 annotations: 6 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 7 ccp-adapter.antrea.tanzu.vmware.com/display-name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 8 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 9 generation: 1 10 labels: 11 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 12 name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 13 resourceVersion: \u0026#34;2257282\u0026#34; 14 uid: 6782589e-8488-47df-a750-04432c3c2f18 15spec: 16 childGroups: 17 - 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 18status: 19 conditions: 20 - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 21 status: \u0026#34;True\u0026#34; 22 type: GroupMembersComputed 23andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 -oyaml 24apiVersion: crd.antrea.io/v1alpha3 25kind: ClusterGroup 26metadata: 27 annotations: 28 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 29 ccp-adapter.antrea.tanzu.vmware.com/display-name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 30 ccp-adapter.antrea.tanzu.vmware.com/parent: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1 31 creationTimestamp: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 32 generation: 1 33 labels: 34 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 35 name: 5bfc16b1-08f3-48bd-91f9-fee3d66762b1-0 36 resourceVersion: \u0026#34;2257277\u0026#34; 37 uid: fd2a1c32-1cf8-4ca8-8dad-f5420f57e55c 38spec: 39 ipBlocks: 40 - cidr: 192.168.0.0/16 41 - cidr: 10.0.0.0/8 42 - cidr: 172.16.0.0/12 43status: 44 conditions: 45 - lastTransitionTime: \u0026#34;2023-07-03T12:27:13Z\u0026#34; 46 status: \u0026#34;True\u0026#34; 47 type: GroupMembersComputed Now the last rule is blocking all non-system namespaces to any other namespace than themselves.\nFirst I need to create a Security Group with the namespace as sole member, then a Security Group with the criteria not-equals. Group for the namespace: Negated Security Group, selecting all pods which does not have the same label as any pods in the namespace \u0026quot;dev-app\u0026quot;. Then the Security Policy looks like this: This is how it looks like in the Kubernetes clusters:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 annotations: 5 ccp-adapter.antrea.tanzu.vmware.com/display-name: dev-cluster-strict-ns-islolation 6 creationTimestamp: \u0026#34;2023-07-03T13:00:40Z\u0026#34; 7 generation: 3 8 labels: 9 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 10 name: cfbe3754-c365-4697-b124-5fbaddd87b57 11 resourceVersion: \u0026#34;2267847\u0026#34; 12 uid: 47949441-a69b-47e5-ae9b-1d5760d5c195 13spec: 14 egress: 15 - action: Allow 16 appliedTo: 17 - group: beed7011-4fc7-49e6-b7ed-d521095eb293 18 enableLogging: false 19 name: \u0026#34;2029\u0026#34; 20 to: 21 - group: beed7011-4fc7-49e6-b7ed-d521095eb293 22 - action: Drop 23 appliedTo: 24 - group: beed7011-4fc7-49e6-b7ed-d521095eb293 25 enableLogging: false 26 name: \u0026#34;2030\u0026#34; 27 to: 28 - group: f240efd5-3a95-49d3-9252-058cc80bc0c0 29 priority: 1.0000000177635728 30 tier: nsx-category-infrastructure 31status: 32 currentNodesRealized: 3 33 desiredNodesRealized: 3 34 observedGeneration: 3 35 phase: Realized Where the cluster groups look like this:\n1andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup beed7011-4fc7-49e6-b7ed-d521095eb293 -oyaml 2apiVersion: crd.antrea.io/v1alpha3 3kind: ClusterGroup 4metadata: 5 annotations: 6 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 7 ccp-adapter.antrea.tanzu.vmware.com/display-name: beed7011-4fc7-49e6-b7ed-d521095eb293 8 creationTimestamp: \u0026#34;2023-07-03T13:00:40Z\u0026#34; 9 generation: 1 10 labels: 11 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 12 name: beed7011-4fc7-49e6-b7ed-d521095eb293 13 resourceVersion: \u0026#34;2266125\u0026#34; 14 uid: 7bf8d0f4-d719-47d5-98a9-5fba3b5da7b9 15spec: 16 childGroups: 17 - beed7011-4fc7-49e6-b7ed-d521095eb293-0 18status: 19 conditions: 20 - lastTransitionTime: \u0026#34;2023-07-03T13:00:41Z\u0026#34; 21 status: \u0026#34;True\u0026#34; 22 type: GroupMembersComputed 23andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup beed7011-4fc7-49e6-b7ed-d521095eb293-0 -oyaml 24apiVersion: crd.antrea.io/v1alpha3 25kind: ClusterGroup 26metadata: 27 annotations: 28 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 29 ccp-adapter.antrea.tanzu.vmware.com/display-name: beed7011-4fc7-49e6-b7ed-d521095eb293-0 30 ccp-adapter.antrea.tanzu.vmware.com/parent: beed7011-4fc7-49e6-b7ed-d521095eb293 31 creationTimestamp: \u0026#34;2023-07-03T13:00:40Z\u0026#34; 32 generation: 1 33 labels: 34 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 35 name: beed7011-4fc7-49e6-b7ed-d521095eb293-0 36 resourceVersion: \u0026#34;2266123\u0026#34; 37 uid: 4b393674-981a-488c-a2e2-d794f0b0a312 38spec: 39 namespaceSelector: 40 matchExpressions: 41 - key: kubernetes.io/metadata.name 42 operator: In 43 values: 44 - dev-app 45status: 46 conditions: 47 - lastTransitionTime: \u0026#34;2023-07-03T13:00:41Z\u0026#34; 48 status: \u0026#34;True\u0026#34; 49 type: GroupMembersComputed 1andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup f240efd5-3a95-49d3-9252-058cc80bc0c0 -oyaml 2apiVersion: crd.antrea.io/v1alpha3 3kind: ClusterGroup 4metadata: 5 annotations: 6 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 7 ccp-adapter.antrea.tanzu.vmware.com/display-name: f240efd5-3a95-49d3-9252-058cc80bc0c0 8 creationTimestamp: \u0026#34;2023-07-03T13:06:59Z\u0026#34; 9 generation: 1 10 labels: 11 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 12 name: f240efd5-3a95-49d3-9252-058cc80bc0c0 13 resourceVersion: \u0026#34;2267842\u0026#34; 14 uid: cacd1386-a434-4c42-8739-6813dd1d475b 15spec: 16 childGroups: 17 - f240efd5-3a95-49d3-9252-058cc80bc0c0-0 18status: 19 conditions: 20 - lastTransitionTime: \u0026#34;2023-07-03T13:07:00Z\u0026#34; 21 status: \u0026#34;True\u0026#34; 22 type: GroupMembersComputed 23andreasm@linuxvm01:~/nsx-antrea-integration$ k get clustergroup f240efd5-3a95-49d3-9252-058cc80bc0c0-0 -oyaml 24apiVersion: crd.antrea.io/v1alpha3 25kind: ClusterGroup 26metadata: 27 annotations: 28 ccp-adapter.antrea.tanzu.vmware.com/createdFrom: nestdbGroupMsg 29 ccp-adapter.antrea.tanzu.vmware.com/display-name: f240efd5-3a95-49d3-9252-058cc80bc0c0-0 30 ccp-adapter.antrea.tanzu.vmware.com/parent: f240efd5-3a95-49d3-9252-058cc80bc0c0 31 creationTimestamp: \u0026#34;2023-07-03T13:06:59Z\u0026#34; 32 generation: 5 33 labels: 34 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 35 name: f240efd5-3a95-49d3-9252-058cc80bc0c0-0 36 resourceVersion: \u0026#34;2269597\u0026#34; 37 uid: bd7f4526-2be9-4a4e-860e-0bb85ea30516 38spec: 39 podSelector: 40 matchExpressions: 41 - key: app 42 operator: NotIn 43 values: 44 - ubuntu-20-04 45status: 46 conditions: 47 - lastTransitionTime: \u0026#34;2023-07-03T13:07:00Z\u0026#34; 48 status: \u0026#34;True\u0026#34; 49 type: GroupMembersComputed With all three policies applied, they look like this in the TKC cluster:\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 317dbadce-06cf-4d1e-9747-3e888f0f58e0 nsx-category-infrastructure 1.000000017763571 4 4 18h 4933e463e-c061-4e80-80b3-eff3402e41a9 nsx-category-infrastructure 1.0000000177635702 4 4 18h 5cfbe3754-c365-4697-b124-5fbaddd87b57 nsx-category-infrastructure 1.0000000177635728 3 3 17h By using NSX managing the Antrea policies there is also a very easy way to verify if the policies are working or not by using the Traffic Analysis tool in NSX: This tools will also inform you of any policies applied by using kubectl inside the cluster, in other words it can also show you policies not created or applied from the NSX manager.\nI have applied a Antrea Policy directly in the TKC cluster using kubectl called block-ns-app3-app4.\n1andreasm@linuxvm01:~/antrea/policies/groups$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 317dbadce-06cf-4d1e-9747-3e888f0f58e0 nsx-category-infrastructure 1.000000017763571 4 4 20h 4933e463e-c061-4e80-80b3-eff3402e41a9 nsx-category-infrastructure 1.0000000177635702 4 4 20h 5block-ns-app3-app4 #this securityops 4 1 1 3s 6cfbe3754-c365-4697-b124-5fbaddd87b57 nsx-category-infrastructure 1.0000000177635728 3 3 20h If I do a traceroute from within NSX from a pod in ns Dev-App3 to a pod in ns Dev-App4 and hit this rule, the NSX manager will show me this: Its clearly doing its job and blocking the traffic, but which rule is it? Click on EgressMetric, copy the rule id and paste it in the search field in NSX: Applying Kubernetes related policies using inventory information from Antrea As mentioned above, NSX can also utilize the information from TKC cluster (or any Kubernetes cluster that uses Antrea) to enforce them in the Distributed firewall. The information NSX is currently:\nKubernetes Cluster - Used to create security group containing Kubernetes clusters by name, not used alone but in combination with the below -\u0026gt; Kubernetes Namespace - Used to create security group containing Kubernetes clusters namespace by name or tag, not used alone but in combination from a Kubernetes cluster defined above. Kubernetes Service - Used to create security group containing Kubernetes Services by name or tag, not used alone but in combination with any of the above -\u0026gt; Kubernetes Ingress - Used to create security group containing Kubernetes Ingresses by name or tag, not used alone but in combination with any of the above Kubernetes Cluster or Kubernetes Namespace. Antrea Egress - Used to create security group containing Antrea Egress IP in use by name or tag, not used alone but in combination with only Kubernetes Cluster. Antrea IP Pool - Used to create security group containing Antrea Egress IP Pool by name or tag, not used alone but in combination with only Kubernetes Cluster. Kubernetes Node - Used to create security group containing Kubernetes Node IPs or POD CIDRs by node IP address or POD CIDR, not used alone but in combination with only Kubernetes Cluster. Kubernetes Gateway - Used to create security group containing Kubernetes Gateways by name or tag, not used alone but in combination with only Kubernetes Cluster. An example of a Security group in NSX using the contexts above, Kubernetes Cluster with name dev-cluster and Kubernetes Node IP address: Now, if I want to create a NSX firewall policy isolating two Kubernetes clusters from each other using the constructs above:\nI will simply create two security groups like the one above, selection the two different cluster in each group. Then the policy will be like this:\nNow if I do a traceflow from any node in dev-cluster-1 to any node in dev-cluster-2 it will dropped.\nThe Firewall Rule ID is: With this approach, its very easy to isolate complete clusters from each other with some really simple rules. We could even create a negated rule, saying you are allowed to reach any workers from same cluster but nothing else with one blocking rule (using a negated selection where source is dev-cluster-1 and destination is also dev-cluster-1: The policy: This is just one rule blocing everything except its own Kubernetes nodes.\nRBAC - making sure no one can overwrite/override existing rules. How to manage RBAC, or Tier Entitlement with Antrea I have already covered here\nOutro... I have in this post shown three different ways to manage and apply Antrea Network policies. Three different approaches, the first approach was all manual, the second automatic but the policies still needs to be defined. The last one with the NSX manager a bit different approach as not all the Antrea Network policy features are available and some policies have to be defined different. But, the NSX manager can also be used to automate some of the policies by just adding the clusters to existing policies. Then they will be applied at once.\nThe Antrea policies used and how they are defined in this post is by all means not the final answer or best practice. They were just used as simple examples to have something to \u0026quot;work with\u0026quot; during this post. As I have mentioned, one could utilise the different tiers to delegate administration of the policies to the right set of responsibilities (security admins, vSphere operators, Dev-ops etc). If the target is zero-trust also inside your TKC clusters, this can be achieved by utilizing the tiers and place a drop-all-else rule dead last in the Antrea policy chain (baseline tier e.g).\n","link":"https://blog.andreasm.io/2023/06/21/securing-kubernetes-clusters-with-antrea-network-policies/","section":"post","tags":["security","cni","kubernetes","antrea","tmc"],"title":"Securing Kubernetes clusters with Antrea Network Policies"},{"body":"","link":"https://blog.andreasm.io/tags/security/","section":"tags","tags":null,"title":"security"},{"body":"","link":"https://blog.andreasm.io/categories/security/","section":"categories","tags":null,"title":"Security"},{"body":"","link":"https://blog.andreasm.io/categories/tanzu-mission-control/","section":"categories","tags":null,"title":"Tanzu Mission Control"},{"body":"","link":"https://blog.andreasm.io/tags/tmc/","section":"tags","tags":null,"title":"tmc"},{"body":"","link":"https://blog.andreasm.io/categories/tmc/","section":"categories","tags":null,"title":"TMC"},{"body":"Antrea in vSphere with Tanzu Antrea is the default CNI being used in TKG 2.0 clusters. TKG 2.0 clusters are the workload clusters you deploy with the Supervisor deployed in vSphere 8. Antrea comes in to flavours, we have the open source edition of Antrea which can be found here and then we have the Antrea Advanced (\u0026quot;downstream\u0026quot;) version which is being used in vSphere with Tanzu. This version is also needed when we want to integrate Antrea with NSX-T for policy management. The Antrea Advanced can be found in your VMware customer connect portal here. Both version of Antrea has a very broad support Kubernetes platforms it can be used in. Antrea can be used for Windows worker nodes, Photon, Ubuntu, ARM, x86, VMware TKG, OpenShift, Rancher, AKS, EKS. the list is long see more info here. This post will be focusing on the Antrea Advanced edition and its features like (read more here):\nCentral management of Antrea Security Policies with NSX Central troubleshooting with TraceFlow with NSX FQDN/L7 Security policies RBAC Tiered policies Flow Exporter Egress (Source NAT IP selection of PODs egressing) Managing Antrea settings and Feature Gates in TKG 2 clusters When you deploy a TKG 2 cluster on vSphere with Tanzu and you dont specify a CNI Antrea will be de default CNI. Depending on the TKG version you are on a set of default Antrea features are enabled or disabled. You can easily check which features are enabled after a cluster has been provisioned by issuing the below command: If you know already before you deploy a cluster that a specific feature should be enabled or disabled this can also be handled during bring-up of the cluster so it should come with the settings you want. More on that later.\n1linux-vm:~/from_ubuntu_vm/tkgs/tkgs-stc-cpod$ k get configmaps -n kube-system antrea-config -oyaml 2apiVersion: v1 3data: 4 antrea-agent.conf: | 5 featureGates: 6 AntreaProxy: true 7 EndpointSlice: true 8 Traceflow: true 9 NodePortLocal: true 10 AntreaPolicy: true 11 FlowExporter: false 12 NetworkPolicyStats: false 13 Egress: true 14 AntreaIPAM: false 15 Multicast: false 16 Multicluster: false 17 SecondaryNetwork: false 18 ServiceExternalIP: false 19 TrafficControl: false 20 trafficEncapMode: encap 21 noSNAT: false 22 tunnelType: geneve 23 trafficEncryptionMode: none 24 enableBridgingMode: false 25 disableTXChecksumOffload: false 26 wireGuard: 27 port: 51820 28 egress: 29 exceptCIDRs: [] 30 serviceCIDR: 20.10.0.0/16 31 nodePortLocal: 32 enable: true 33 portRange: 61000-62000 34 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 35 multicast: {} 36 antreaProxy: 37 proxyAll: false 38 nodePortAddresses: [] 39 skipServices: [] 40 proxyLoadBalancerIPs: false 41 multicluster: {} 42 antrea-cni.conflist: | 43 { 44 \u0026#34;cniVersion\u0026#34;:\u0026#34;0.3.0\u0026#34;, 45 \u0026#34;name\u0026#34;: \u0026#34;antrea\u0026#34;, 46 \u0026#34;plugins\u0026#34;: [ 47 { 48 \u0026#34;type\u0026#34;: \u0026#34;antrea\u0026#34;, 49 \u0026#34;ipam\u0026#34;: { 50 \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34; 51 } 52 } 53 , 54 { 55 \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, 56 \u0026#34;capabilities\u0026#34;: {\u0026#34;portMappings\u0026#34;: true} 57 } 58 , 59 { 60 \u0026#34;type\u0026#34;: \u0026#34;bandwidth\u0026#34;, 61 \u0026#34;capabilities\u0026#34;: {\u0026#34;bandwidth\u0026#34;: true} 62 } 63 ] 64 } 65 antrea-controller.conf: | 66 featureGates: 67 Traceflow: true 68 AntreaPolicy: true 69 NetworkPolicyStats: false 70 Multicast: false 71 Egress: true 72 AntreaIPAM: false 73 ServiceExternalIP: false 74 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 75 nodeIPAM: null 76kind: ConfigMap 77metadata: 78 annotations: 79 kapp.k14s.io/identity: v1;kube-system//ConfigMap/antrea-config;v1 80 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;antrea-agent.conf\u0026#34;:\u0026#34;featureGates:\\n AntreaProxy: 81 true\\n EndpointSlice: true\\n Traceflow: true\\n NodePortLocal: true\\n AntreaPolicy: 82 true\\n FlowExporter: false\\n NetworkPolicyStats: false\\n Egress: true\\n AntreaIPAM: 83 false\\n Multicast: false\\n Multicluster: false\\n SecondaryNetwork: false\\n ServiceExternalIP: 84 false\\n TrafficControl: false\\ntrafficEncapMode: encap\\nnoSNAT: false\\ntunnelType: 85 geneve\\ntrafficEncryptionMode: none\\nenableBridgingMode: false\\ndisableTXChecksumOffload: 86 false\\nwireGuard:\\n port: 51820\\negress:\\n exceptCIDRs: []\\nserviceCIDR: 20.10.0.0/16\\nnodePortLocal:\\n enable: 87 true\\n portRange: 61000-62000\\ntlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384\\nmulticast: 88 {}\\nantreaProxy:\\n proxyAll: false\\n nodePortAddresses: []\\n skipServices: 89 []\\n proxyLoadBalancerIPs: false\\nmulticluster: {}\\n\u0026#34;,\u0026#34;antrea-cni.conflist\u0026#34;:\u0026#34;{\\n \\\u0026#34;cniVersion\\\u0026#34;:\\\u0026#34;0.3.0\\\u0026#34;,\\n \\\u0026#34;name\\\u0026#34;: 90 \\\u0026#34;antrea\\\u0026#34;,\\n \\\u0026#34;plugins\\\u0026#34;: [\\n {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;antrea\\\u0026#34;,\\n \\\u0026#34;ipam\\\u0026#34;: 91 {\\n \\\u0026#34;type\\\u0026#34;: \\\u0026#34;host-local\\\u0026#34;\\n }\\n }\\n ,\\n {\\n \\\u0026#34;type\\\u0026#34;: 92 \\\u0026#34;portmap\\\u0026#34;,\\n \\\u0026#34;capabilities\\\u0026#34;: {\\\u0026#34;portMappings\\\u0026#34;: true}\\n }\\n ,\\n {\\n \\\u0026#34;type\\\u0026#34;: 93 \\\u0026#34;bandwidth\\\u0026#34;,\\n \\\u0026#34;capabilities\\\u0026#34;: {\\\u0026#34;bandwidth\\\u0026#34;: true}\\n }\\n ]\\n}\\n\u0026#34;,\u0026#34;antrea-controller.conf\u0026#34;:\u0026#34;featureGates:\\n Traceflow: 94 true\\n AntreaPolicy: true\\n NetworkPolicyStats: false\\n Multicast: false\\n Egress: 95 true\\n AntreaIPAM: false\\n ServiceExternalIP: false\\ntlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384\\nnodeIPAM: 96 null\\n\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685607245932804320\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.c39c4aca919097e50452c3432329dd40\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;antrea-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;kube-system\u0026#34;}}\u0026#39; 97 kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 98 creationTimestamp: \u0026#34;2023-06-01T08:14:14Z\u0026#34; 99 labels: 100 app: antrea 101 kapp.k14s.io/app: \u0026#34;1685607245932804320\u0026#34; 102 kapp.k14s.io/association: v1.c39c4aca919097e50452c3432329dd40 103 name: antrea-config 104 namespace: kube-system 105 resourceVersion: \u0026#34;948\u0026#34; 106 uid: fd18fd20-a82b-4df5-bb1a-686463b86f27 If you want to enable or disable any of these features its a matter of applying an AntreaConfig using the included AntreaConfig CRD in TKG 2.0.\nOne can apply this AntreaConfig on an already provisioned TKG 2.0 cluster or apply before the cluster is provisioned so it will get the features enabled or disabled at creation. Below is an example of AntreaConfig:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: three-zone-cluster-2-antrea-package 5 namespace: ns-three-zone-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: true 14 Egress: true #This needs to be enabled (an example) 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true This example is applied either before or after provisioning of the TKG 2.0 cluster. Just make sure the config has been applied to the correct NS, the same NS as the cluster is deployed in and the name of the config needs to start like this CLUSTER-NAME-antrea-package. In other words the name needs to start with the clustername of the TKG 2.0 cluster and end with -antrea-package.\nIf it is being done after the cluster has provisioned we need to make sure the already running Antrea pods (agents and controller) are restarted so they can read the new configmap.\nIf you need to check which version of Antrea is included in your TKR version (and other components for that sake) just run the following command:\n1linuxvm01:~/three-zones$ k get tkr v1.24.9---vmware.1-tkg.4 -o yaml 2apiVersion: run.tanzu.vmware.com/v1alpha3 3kind: TanzuKubernetesRelease 4metadata: 5 creationTimestamp: \u0026#34;2023-06-01T07:35:28Z\u0026#34; 6 finalizers: 7 - tanzukubernetesrelease.run.tanzu.vmware.com 8 generation: 2 9 labels: 10 os-arch: amd64 11 os-name: photon 12 os-type: linux 13 os-version: \u0026#34;3.0\u0026#34; 14 v1: \u0026#34;\u0026#34; 15 v1.24: \u0026#34;\u0026#34; 16 v1.24.9: \u0026#34;\u0026#34; 17 v1.24.9---vmware: \u0026#34;\u0026#34; 18 v1.24.9---vmware.1: \u0026#34;\u0026#34; 19 v1.24.9---vmware.1-tkg: \u0026#34;\u0026#34; 20 v1.24.9---vmware.1-tkg.4: \u0026#34;\u0026#34; 21 name: v1.24.9---vmware.1-tkg.4 22 ownerReferences: 23 - apiVersion: vmoperator.vmware.com/v1alpha1 24 kind: VirtualMachineImage 25 name: ob-21552850-ubuntu-2004-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 26 uid: 92d3d6af-53f8-4f9a-b262-f70dd33ad19b 27 - apiVersion: vmoperator.vmware.com/v1alpha1 28 kind: VirtualMachineImage 29 name: ob-21554409-photon-3-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 30 uid: 6a0aa87a-63e3-475d-a52d-e63589f454e9 31 resourceVersion: \u0026#34;12111\u0026#34; 32 uid: 54db049e-fdf0-45a2-b4d1-46fa90a22b44 33spec: 34 bootstrapPackages: 35 - name: antrea.tanzu.vmware.com.1.7.2+vmware.1-tkg.1-advanced 36 - name: vsphere-pv-csi.tanzu.vmware.com.2.6.1+vmware.1-tkg.1 37 - name: vsphere-cpi.tanzu.vmware.com.1.24.3+vmware.1-tkg.1 38 - name: kapp-controller.tanzu.vmware.com.0.41.5+vmware.1-tkg.1 39 - name: guest-cluster-auth-service.tanzu.vmware.com.1.1.0+tkg.1 40 - name: metrics-server.tanzu.vmware.com.0.6.2+vmware.1-tkg.1 41 - name: secretgen-controller.tanzu.vmware.com.0.11.2+vmware.1-tkg.1 42 - name: pinniped.tanzu.vmware.com.0.12.1+vmware.3-tkg.3 43 - name: capabilities.tanzu.vmware.com.0.28.0+vmware.2 44 - name: calico.tanzu.vmware.com.3.24.1+vmware.1-tkg.1 45 kubernetes: 46 coredns: 47 imageTag: v1.8.6_vmware.15 48 etcd: 49 imageTag: v3.5.6_vmware.3 50 imageRepository: localhost:5000/vmware.io 51 pause: 52 imageTag: \u0026#34;3.7\u0026#34; 53 version: v1.24.9+vmware.1 54 osImages: 55 - name: ob-21552850-ubuntu-2004-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 56 - name: ob-21554409-photon-3-amd64-vmi-k8s-v1.24.9---vmware.1-tkg.4 57 version: v1.24.9+vmware.1-tkg.4 58status: 59 conditions: 60 - lastTransitionTime: \u0026#34;2023-06-01T07:35:28Z\u0026#34; 61 status: \u0026#34;True\u0026#34; 62 type: Ready 63 - lastTransitionTime: \u0026#34;2023-06-01T07:35:28Z\u0026#34; 64 status: \u0026#34;True\u0026#34; 65 type: Compatible So enabling and disabling Antrea Feature Gates is quite simple. To summarize, the feature gates that can be adjusted is these (as of TKR 1.24.9):\n1spec: 2 antrea: 3 config: 4 defaultMTU: \u0026#34;\u0026#34; 5 disableUdpTunnelOffload: false 6 featureGates: 7 AntreaPolicy: true 8 AntreaProxy: true 9 AntreaTraceflow: true 10 Egress: true 11 EndpointSlice: true 12 FlowExporter: false 13 NetworkPolicyStats: false 14 NodePortLocal: true 15 noSNAT: false 16 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384 17 trafficEncapMode: encap Integrating Antrea with NSX-T To enable the NSX-T Antrea integration there is a couple of steps that needs to be prepared. All the steps can be followed here. I have decided to create a script that automates all these steps. So if you dont want to go through all these steps manually by following the link above you can use this script instead and just enter the necesarry information as prompted, and have the pre-requisities in place before excecuting. Copy and paste the below script into a .sh file on your Linux jumpiest and make it executable with chmod +x.\n1#!/bin/bash 2 3# Echo information 4echo \u0026#34;This script has some dependencies... make sure they are met before continuing. Otherwise click ctrl+c now 51. This script is adjusted for vSphere with Tanzu TKG clusters using Tanzu CLI 62. Have downloaded the antrea-interworking*.zip 73. This script is located in the root of where you have downloaded the zip file above 84. curl is installed 95. Need connectivity to the NSX manager 106. kubectl is installed 117. vsphere with tanzu cli is installed 128. That you are in the correct context of the cluster you want to integrate to NSX 139. If not in the correct context the script will put you in the correct context anyway 1410. A big smile and good mood\u0026#34; 15 16# Prompt the user to press a key to continue 17echo \u0026#34;Press any key to continue...\u0026#34; 18read -n 1 -s 19 20# Continue with the script 21echo \u0026#34;Continuing...\u0026#34; 22 23# Prompt for name 24read -p \u0026#34;Enter the name of the tkg cluster - will be used for certificates and name in NSX: \u0026#34; name 25 26# Prompt for NSX_MGR 27read -p \u0026#34;Enter NSX Manager ip or FQDN: \u0026#34; nsx_mgr 28 29# Prompt for NSX_ADMIN 30read -p \u0026#34;Enter NSX admin username: \u0026#34; nsx_admin 31 32# Prompt for NSX_PASS 33read -p \u0026#34;Enter NSX Password: \u0026#34; nsx_pass 34 35# Prompt for Supervisor Endpoint IP or FQDN 36read -p \u0026#34;Enter Supervisor API IP or FQDN: \u0026#34; svc_api_ip 37 38# Prompt for vSphere Username 39read -p \u0026#34;Enter vSphere Username: \u0026#34; vsphere_username 40 41# Prompt for Tanzu Kubernetes Cluster Namespace 42read -p \u0026#34;Enter Tanzu Kubernetes Cluster Namespace: \u0026#34; tanzu_cluster_namespace 43 44# Prompt for Tanzu Kubernetes Cluster Name 45read -p \u0026#34;Enter Tanzu Kubernetes Cluster Name: \u0026#34; tanzu_cluster_name 46 47# Login to vSphere using kubectl 48kubectl vsphere login --server=\u0026#34;$svc_api_ip\u0026#34; --insecure-skip-tls-verify --vsphere-username=\u0026#34;$vsphere_username\u0026#34; --tanzu-kubernetes-cluster-namespace=\u0026#34;$tanzu_cluster_namespace\u0026#34; --tanzu-kubernetes-cluster-name=\u0026#34;$tanzu_cluster_name\u0026#34; 49 50key_name=\u0026#34;${name}-private.key\u0026#34; 51csr_output=\u0026#34;${name}.csr\u0026#34; 52crt_output=\u0026#34;${name}.crt\u0026#34; 53 54openssl genrsa -out \u0026#34;$key_name\u0026#34; 2048 55openssl req -new -key \u0026#34;$key_name\u0026#34; -out \u0026#34;$csr_output\u0026#34; -subj \u0026#34;/C=US/ST=CA/L=Palo Alto/O=VMware/OU=Antrea Cluster/CN=$name\u0026#34; 56openssl x509 -req -days 3650 -sha256 -in \u0026#34;$csr_output\u0026#34; -signkey \u0026#34;$key_name\u0026#34; -out \u0026#34;$crt_output\u0026#34; 57 58# Convert the certificate file to a one-liner with line breaks 59crt_contents=$(awk \u0026#39;{printf \u0026#34;%s\\\\n\u0026#34;, $0}\u0026#39; \u0026#34;$crt_output\u0026#34;) 60 61# Replace the certificate and name in the curl body 62curl_body=\u0026#39;{ 63 \u0026#34;name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$name\u0026#34;\u0026#39;\u0026#34;, 64 \u0026#34;node_id\u0026#34;: \u0026#34;\u0026#39;\u0026#34;$name\u0026#34;\u0026#39;\u0026#34;, 65 \u0026#34;roles_for_paths\u0026#34;: [ 66 { 67 \u0026#34;path\u0026#34;: \u0026#34;/\u0026#34;, 68 \u0026#34;roles\u0026#34;: [ 69 { 70 \u0026#34;role\u0026#34;: \u0026#34;enterprise_admin\u0026#34; 71 } 72 ] 73 } 74 ], 75 \u0026#34;role\u0026#34;: \u0026#34;enterprise_admin\u0026#34;, 76 \u0026#34;is_protected\u0026#34;: \u0026#34;true\u0026#34;, 77 \u0026#34;certificate_pem\u0026#34; : \u0026#34;\u0026#39;\u0026#34;$crt_contents\u0026#34;\u0026#39;\u0026#34; 78}\u0026#39; 79 80# Make the curl request with the updated body 81# curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;$curl_body\u0026#34; https://example.com/api/endpoint 82curl -ku \u0026#34;$nsx_admin\u0026#34;:\u0026#34;$nsx_pass\u0026#34; -X POST https://\u0026#34;$nsx_mgr\u0026#34;/api/v1/trust-management/principal-identities/with-certificate -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;$curl_body\u0026#34; 83 84# Check if a subfolder starting with \u0026#34;antrea-interworking\u0026#34; exists 85if ls -d antrea-interworking* \u0026amp;\u0026gt;/dev/null; then 86 echo \u0026#34;Subfolder starting with \u0026#39;antrea-interworking\u0026#39; exists. Skipping extraction.\u0026#34; 87else 88 # Extract the zip file starting with \u0026#34;antrea-interworking\u0026#34; 89 unzip \u0026#34;antrea-interworking\u0026#34;*.zip 90fi 91 92# Create a new folder with the name antrea-interworking-\u0026#34;from-name\u0026#34; 93new_folder=\u0026#34;antrea-interworking-$name\u0026#34; 94mkdir \u0026#34;$new_folder\u0026#34; 95 96# Copy all YAML files from the antrea-interworking subfolder to the new folder 97cp antrea-interworking*/{*.yaml,*.yml} \u0026#34;$new_folder/\u0026#34; 98 99# Replace the field after \u0026#34;image: vmware.io/antrea/interworking\u0026#34; with \u0026#34;image: projects.registry.vmware.com/antreainterworking/interworking-debian\u0026#34; in interworking.yaml 100sed -i \u0026#39;s|image: vmware.io/antrea/interworking|image: projects.registry.vmware.com/antreainterworking/interworking-debian|\u0026#39; \u0026#34;$new_folder/interworking.yaml\u0026#34; 101 102# Replace the field after \u0026#34;image: vmware.io/antrea/interworking\u0026#34; with \u0026#34;image: projects.registry.vmware.com/antreainterworking/interworking-debian\u0026#34; in deregisterjob.yaml 103sed -i \u0026#39;s|image: vmware.io/antrea/interworking|image: projects.registry.vmware.com/antreainterworking/interworking-debian|\u0026#39; \u0026#34;$new_folder/deregisterjob.yaml\u0026#34; 104 105# Edit the bootstrap.yaml file in the new folder 106sed -i \u0026#39;s|clusterName:.*|clusterName: \u0026#39;\u0026#34;$name\u0026#34;\u0026#39;|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; 107sed -i \u0026#39;s|NSXManagers:.*|NSXManagers: [\u0026#34;\u0026#39;\u0026#34;$nsx_mgr\u0026#34;\u0026#39;\u0026#34;]|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; 108tls_crt_base64=$(base64 -w 0 \u0026#34;$crt_output\u0026#34;) 109sed -i \u0026#39;s|tls.crt:.*|tls.crt: \u0026#39;\u0026#34;$tls_crt_base64\u0026#34;\u0026#39;|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; 110tls_key_base64=$(base64 -w 0 \u0026#34;$key_name\u0026#34;) 111sed -i \u0026#39;s|tls.key:.*|tls.key: \u0026#39;\u0026#34;$tls_key_base64\u0026#34;\u0026#39;|\u0026#39; \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; 112 113# Interactive prompt to select Kubernetes context 114kubectl config get-contexts 115read -p \u0026#34;Enter the name of the Kubernetes context: \u0026#34; kubectl_context 116kubectl config use-context \u0026#34;$kubectl_context\u0026#34; 117 118# Apply the bootstrap-config.yaml and interworking.yaml files from the new folder 119kubectl apply -f \u0026#34;$new_folder/bootstrap-config.yaml\u0026#34; -f \u0026#34;$new_folder/interworking.yaml\u0026#34; 120 121# Run the last command to verify that something is happening 122kubectl get pods -o wide -n vmware-system-antrea 123 124echo \u0026#34;As it was written each time we ssh\u0026#39;ed into a Suse Linux back in the good old days - Have a lot of fun\u0026#34; As soon as the script has been processed through it should not take long until you have your TKG cluster in the NSX manager:\nThats it for the NSX-T integration, as soon as that have been done its time to look into what we can do with this integration in the following chapters\nAntrea Security Policies Antrea has two sets of security policies, Antrea Network Policies (ANP) and Antrea Cluster Network Policies (ACNP). The difference between these two is that ANP is applied on a Kubernetes Namespace and ACNP is cluster-wide. Both belongs to Antrea Native Policies. Both ANP and ACNP can work together with Kubernetes Network Policies.\nThere are many benefits of using Antrea Native Policies in combination or not in combination with Kubernetes Network Policies.\nSome of the benefits of using Antrea Native Policies:\nCan be tiered Select both ingress and egress Support the following actions: allow, drop, reject and pass Support FQDN filtering in egress (to) with actions allow, drop and reject Tiered policies The benefit of having tiered policies is very useful when for example we have different parts of the organization are responsible for security at different levels/scopes in the platform. Antrea can have policies placed in different tiers where the tiers are evaluated in a given order. If we want some rules to be very early in the policy evaluation and enforced as soon as possible we can place rule in a tier that is considered first, then within the same tier the rules or policies are also being enforced in the order of a given priority, a number. The rule with the lowest number (higher priority) will be evaluated first and then when all rules in a tier has been processed it will go to the next tier. Antrea comes with a set of static tiers already defined. These tier can be shown by running the command:\n1linuxvm01:~$ k get tiers 2NAME PRIORITY AGE 3application 250 3h11m 4baseline 253 3h11m 5emergency 50 3h11m 6networkops 150 3h11m 7platform 200 3h11m 8securityops 100 3h11m Below will show a diagram of how they look, notice also where the Kubernets network policies will be placed:\nThere is also the option to add custom tiers using the following CRD (taken from the offical Antrea docs here:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: Tier 3metadata: 4 name: mytier 5spec: 6 priority: 10 7 description: \u0026#34;my custom tier\u0026#34; When doing the Antrea NSX integration some additional tiers are added automatically (they start with nsx*):\n1linuxvm01:~$ k get tiers 2NAME PRIORITY AGE 3application 250 3h11m 4baseline 253 3h11m 5emergency 50 3h11m 6networkops 150 3h11m 7nsx-category-application 4 87m 8nsx-category-emergency 1 87m 9nsx-category-environment 3 87m 10nsx-category-ethernet 0 87m 11nsx-category-infrastructure 2 87m 12platform 200 3h11m 13securityops 100 3h11m I can quickly show two examples where I create one rule as a \u0026quot;security-admin\u0026quot;, where this security admin has to follow the company's compliance policy to block access to a certain FQDN. This must be enforced all over. So I need to create this policy in the securityops tier. I could have defined it in the emergency tier also but in this tier it makes more sense to have rules applied that are disabled/not-enforced/idle in case of an emergency and we need a way to quickly enable it and override rules later down the hierarchy. So securityops it is:\nLets apply this one:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: acnp-drop-yelb 5spec: 6 priority: 1 7 tier: securityops 8 appliedTo: 9 - podSelector: 10 matchLabels: 11 app: ubuntu-20-04 12 egress: 13 - action: Drop 14 to: 15 - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; 16 ports: 17 - protocol: TCP 18 port: 80 19 - action: Allow #Allow the rest To check if it is applied and in use (notice under desired nodes and current nodes):\n1linuxvm01:~/antrea/policies$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3acnp-drop-yelb securityops 1 1 1 5m33s Now from a test pod I will try to curl the blocked fqdn and another one not in any block rule:\n1root@ubuntu-20-04-548545fc87-kkzbh:/# curl yelb-ui.yelb.cloudburst.somecooldomain.net 2curl: (6) Could not resolve host: yelb-ui.yelb.cloudburst.somecooldomain.net 3 4# Curling a FQDN that is allowed: 5root@ubuntu-20-04-548545fc87-kkzbh:/# curl allowed-yelb.yelb-2.carefor.some-dns.net 6\u0026lt;!doctype html\u0026gt; 7\u0026lt;html\u0026gt; 8\u0026lt;head\u0026gt; 9 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 10 \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; 11 \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; 12 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; 13 \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; 14\u0026lt;/head\u0026gt; 15\u0026lt;body\u0026gt; 16\u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; 17\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; 18\u0026lt;/html\u0026gt; That works as expected. Now what happens then if another use with access to the Kubernetes cluster decide to create a rule later down in the hierarchy, lets go with the application tier, to create an allow rule for this FQDN that is currently being dropped? Lets see what happens\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: acnp-allow-yelb 5spec: 6 priority: 1 7 tier: application 8 appliedTo: 9 - podSelector: 10 matchLabels: 11 app: ubuntu-20-04 12 egress: 13 - action: Allow 14 to: 15 - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; 16 ports: 17 - protocol: TCP 18 port: 80 19 - action: Allow #Allow the rest I will apply this above rule and then try to curl the same fqdn which is supposed to be dropped.\n1linuxvm01:~/antrea/policies$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3acnp-allow-yelb application 1 1 1 4s 4acnp-drop-yelb securityops 1 1 1 5h1m From my test pod again:\n1kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. 2root@ubuntu-20-04-548545fc87-kkzbh:/# curl yelb-ui.yelb.cloudburst.somecooldomain.net 3curl: (6) Could not resolve host: yelb-ui.yelb.cloudburst.somecooldomain.net 4root@ubuntu-20-04-548545fc87-kkzbh:/# curl allowed-yelb.yelb-2.carefor.some-dns.net 5\u0026lt;!doctype html\u0026gt; 6\u0026lt;html\u0026gt; 7\u0026lt;head\u0026gt; 8 \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; 9 \u0026lt;title\u0026gt;Yelb\u0026lt;/title\u0026gt; 10 \u0026lt;base href=\u0026#34;/\u0026#34;\u0026gt; 11 \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1\u0026#34;\u0026gt; 12 \u0026lt;link rel=\u0026#34;icon\u0026#34; type=\u0026#34;image/x-icon\u0026#34; href=\u0026#34;favicon.ico?v=2\u0026#34;\u0026gt; 13\u0026lt;/head\u0026gt; 14\u0026lt;body\u0026gt; 15\u0026lt;yelb\u0026gt;Loading...\u0026lt;/yelb\u0026gt; 16\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;inline.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;styles.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;scripts.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;vendor.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;main.bundle.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; 17\u0026lt;/html\u0026gt; That was expected. It is still being dropped by the first rule placed in the securityops tier. So far so good. But what if this user also has access to the tier where the first rule is applied? Well, then they can override it. That is why I we can now go to the next chapter.\nAntrea RBAC Antrea comes with a couple of CRDs that allow us to configure granular user permissions on the different objects, like the Policy Tiers. So to restrict \u0026quot;normal\u0026quot; users from applying and/or delete security polices created in the higher priority Tiers we need to apply some rolebindings, or to be exact ClusterRoleBindings. Let us see how we can achieve that.\nIn my lab environment I have defined two users, my own admin user (andreasm) that is part of the ClusterRole/cluster-admin and a second user (User1) that is part of the the ClusterRole/view. The ClusterRole View has only read access, not to all objects in the cluster but many. To see what run the following command:\n1linuxvm01:~/antrea/policies$ k get clusterrole view -oyaml 2aggregationRule: 3 clusterRoleSelectors: 4 - matchLabels: 5 rbac.authorization.k8s.io/aggregate-to-view: \u0026#34;true\u0026#34; 6apiVersion: rbac.authorization.k8s.io/v1 7kind: ClusterRole 8metadata: 9 annotations: 10 rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; 11 creationTimestamp: \u0026#34;2023-06-04T09:37:44Z\u0026#34; 12 labels: 13 kubernetes.io/bootstrapping: rbac-defaults 14 rbac.authorization.k8s.io/aggregate-to-edit: \u0026#34;true\u0026#34; 15 name: view 16 resourceVersion: \u0026#34;1052\u0026#34; 17 uid: c4784a81-4451-42af-9134-e141ccf8bc50 18rules: 19- apiGroups: 20 - crd.antrea.io 21 resources: 22 - clustergroups 23 verbs: 24 - get 25 - list 26 - watch 27- apiGroups: 28 - crd.antrea.io 29 resources: 30 - clusternetworkpolicies 31 - networkpolicies 32 verbs: 33 - get 34 - list 35 - watch 36- apiGroups: 37 - crd.antrea.io 38 resources: 39 - traceflows 40 verbs: 41 - get 42 - list 43 - watch 44- apiGroups: 45 - \u0026#34;\u0026#34; 46 resources: 47 - configmaps 48 - endpoints 49 - persistentvolumeclaims 50 - persistentvolumeclaims/status 51 - pods 52 - replicationcontrollers 53 - replicationcontrollers/scale 54 - serviceaccounts 55 - services 56 - services/status 57 verbs: 58 - get 59 - list 60 - watch 61- apiGroups: 62 - \u0026#34;\u0026#34; 63 resources: 64 - bindings 65 - events 66 - limitranges 67 - namespaces/status 68 - pods/log 69 - pods/status 70 - replicationcontrollers/status 71 - resourcequotas 72 - resourcequotas/status 73 verbs: 74 - get 75 - list 76 - watch 77- apiGroups: 78 - \u0026#34;\u0026#34; 79 resources: 80 - namespaces 81 verbs: 82 - get 83 - list 84 - watch 85- apiGroups: 86 - discovery.k8s.io 87 resources: 88 - endpointslices 89 verbs: 90 - get 91 - list 92 - watch 93- apiGroups: 94 - apps 95 resources: 96 - controllerrevisions 97 - daemonsets 98 - daemonsets/status 99 - deployments 100 - deployments/scale 101 - deployments/status 102 - replicasets 103 - replicasets/scale 104 - replicasets/status 105 - statefulsets 106 - statefulsets/scale 107 - statefulsets/status 108 verbs: 109 - get 110 - list 111 - watch 112- apiGroups: 113 - autoscaling 114 resources: 115 - horizontalpodautoscalers 116 - horizontalpodautoscalers/status 117 verbs: 118 - get 119 - list 120 - watch 121- apiGroups: 122 - batch 123 resources: 124 - cronjobs 125 - cronjobs/status 126 - jobs 127 - jobs/status 128 verbs: 129 - get 130 - list 131 - watch 132- apiGroups: 133 - extensions 134 resources: 135 - daemonsets 136 - daemonsets/status 137 - deployments 138 - deployments/scale 139 - deployments/status 140 - ingresses 141 - ingresses/status 142 - networkpolicies 143 - replicasets 144 - replicasets/scale 145 - replicasets/status 146 - replicationcontrollers/scale 147 verbs: 148 - get 149 - list 150 - watch 151- apiGroups: 152 - policy 153 resources: 154 - poddisruptionbudgets 155 - poddisruptionbudgets/status 156 verbs: 157 - get 158 - list 159 - watch 160- apiGroups: 161 - networking.k8s.io 162 resources: 163 - ingresses 164 - ingresses/status 165 - networkpolicies 166 verbs: 167 - get 168 - list 169 - watch 170- apiGroups: 171 - metrics.k8s.io 172 resources: 173 - pods 174 - nodes 175 verbs: 176 - get 177 - list 178 - watch 179- apiGroups: 180 - policy 181 resourceNames: 182 - vmware-system-privileged 183 resources: 184 - podsecuritypolicies 185 verbs: 186 - use On the other hand my own admin user has access to everything, get, list, create, patch, update, delete - the whole shabang. What I would like to demonstrate now is that user1 is a regular user and should only be allowed to create security policies in the Tier application while all other Tiers is restricted to the admins that have the responsibility to create policies there. User1 should also not be allowed to create any custom Tiers.\nSo the first thing I need to create is an Antrea TierEntitlement and TierEntitlementBinding like this:\n1apiVersion: crd.antrea.tanzu.vmware.com/v1alpha1 2kind: TierEntitlement 3metadata: 4 name: secops-edit 5spec: 6 tiers: # Accept list of Tier names. Tier may or may not exist yet. 7 - emergency 8 - securityops 9 - networkops 10 - platform 11 - baseline 12 permission: edit 13--- 14apiVersion: crd.antrea.tanzu.vmware.com/v1alpha1 15kind: TierEntitlementBinding 16metadata: 17 name: secops-bind 18spec: 19 subjects: # List of users to grant this entitlement to 20 - kind: User 21 name: sso:andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net 22 apiGroup: rbac.authorization.k8s.io 23# - kind: Group 24# name: security-admins 25# apiGroup: rbac.authorization.k8s.io 26# - kind: ServiceAccount 27# name: network-admins 28# namespace: kube-system 29 tierEntitlement: secops-edit # Reference to the TierEntitlement Now, notice that I am listing the Tiers that should only be available for the users, groups, or ServiceAccounts in the TierEntitlementBinding (I am only using Kind: User in this example). This means that all unlisted tiers should be allowed for other users to place security policies in.\nNow apply it:\n1linuxvm01:~/antrea/policies$ k apply -f tierentitlement.yaml 2tierentitlement.crd.antrea.tanzu.vmware.com/secops-edit created 3tierentitlementbinding.crd.antrea.tanzu.vmware.com/secops-bind created Next up is to add my User1 to the Antrea CRD \u0026quot;tiers\u0026quot; to be allowed to list and get the tiers:\n1apiVersion: rbac.authorization.k8s.io/v1 2kind: ClusterRole 3metadata: 4 name: tier-placement 5rules: 6- apiGroups: [\u0026#34;crd.antrea.io\u0026#34;] 7 resources: [\u0026#34;tiers\u0026#34;] 8 verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;] 9--- 10apiVersion: rbac.authorization.k8s.io/v1 11kind: ClusterRoleBinding 12metadata: 13 name: tier-bind 14subjects: 15- kind: User 16 name: sso:user1@cpod-nsxam-stc.az-stc.cloud-garage.net # Name is case sensitive 17 apiGroup: rbac.authorization.k8s.io 18roleRef: 19 kind: ClusterRole 20 name: tier-placement 21 apiGroup: rbac.authorization.k8s.io If you want some user to also add/create/delete custom Tiers this can be allowed by adding: \u0026quot;create\u0026quot;,\u0026quot;patch\u0026quot;,\u0026quot;update\u0026quot;,\u0026quot;delete\u0026quot;\nNow apply the above yaml:\n1linuxvm01:~/antrea/policies$ k apply -f antrea-crd-tier-list.yaml 2clusterrole.rbac.authorization.k8s.io/tier-placement created 3clusterrolebinding.rbac.authorization.k8s.io/tier-bind created I will now log in with the User1 and try to apply this network policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: override-rule-allow-yelb 5spec: 6 priority: 1 7 tier: securityops 8 appliedTo: 9 - podSelector: 10 matchLabels: 11 app: ubuntu-20-04 12 egress: 13 - action: Allow 14 to: 15 - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; 16 ports: 17 - protocol: TCP 18 port: 80 19 - action: Allow As User1:\n1linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-secops-tier.test.yaml 2Error from server (Forbidden): error when creating \u0026#34;fqdn-rule-secops-tier.test.yaml\u0026#34;: clusternetworkpolicies.crd.antrea.io is forbidden: User \u0026#34;sso:user1@cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34; cannot create resource \u0026#34;clusternetworkpolicies\u0026#34; in API group \u0026#34;crd.antrea.io\u0026#34; at the cluster scope First bump in the road.. This user is not allowed to create any security policies at all.\nSo I need to use my admin user and apply this ClusterRoleBinding:\n1apiVersion: rbac.authorization.k8s.io/v1 2kind: ClusterRole 3metadata: 4 name: clusternetworkpolicies-edit 5rules: 6- apiGroups: [\u0026#34;crd.antrea.io\u0026#34;] 7 resources: [\u0026#34;clusternetworkpolicies\u0026#34;] 8 verbs: [\u0026#34;get\u0026#34;,\u0026#34;list\u0026#34;,\u0026#34;create\u0026#34;,\u0026#34;patch\u0026#34;,\u0026#34;update\u0026#34;,\u0026#34;delete\u0026#34;] 9--- 10apiVersion: rbac.authorization.k8s.io/v1 11kind: ClusterRoleBinding 12metadata: 13 name: clusternetworkpolicies-bind 14subjects: 15- kind: User 16 name: sso:user1@cpod-nsxam-stc.az-stc.cloud-garage.net # Name is case sensitive 17 apiGroup: rbac.authorization.k8s.io 18roleRef: 19 kind: ClusterRole 20 name: clusternetworkpolicies-edit 21 apiGroup: rbac.authorization.k8s.io Now the user1 has access to create policies... Lets try again:\n1linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-secops-tier.test.yaml 2Error from server: error when creating \u0026#34;fqdn-rule-secops-tier.test.yaml\u0026#34;: admission webhook \u0026#34;acnpvalidator.antrea.io\u0026#34; denied the request: user not authorized to access Tier securityops There it is, I am not allowed to place any security policies in the tier securityops. That is what I wanted to achieve, so thats good. What if user1 tries to apply a policy in the application tier? Lets see:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: override-attempt-failed-allow-yelb 5spec: 6 priority: 1 7 tier: application 8 appliedTo: 9 - podSelector: 10 matchLabels: 11 app: ubuntu-20-04 12 egress: 13 - action: Allow 14 to: 15 - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; 16 ports: 17 - protocol: TCP 18 port: 80 19 - action: Allow 1linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-baseline-tier.test.yaml 2clusternetworkpolicy.crd.antrea.io/override-attempt-failed-allow-yelb created 3linuxvm01:~/antrea/policies$ k get acnp 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5acnp-allow-yelb application 1 1 1 147m 6acnp-drop-yelb securityops 1 1 1 18h 7override-attempt-failed-allow-yelb application 1 1 1 11s That worked, even though the above rule is trying to allow access to yelb it will not allow it due to the Drop rule in the securityops Tier. So how much the User1 tries to get this access it will be blocked.\nThese users....\nWhat if user1 tries to apply the same policy without stating any Tier in in the policy? Lets see:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: override-attempt-failed-allow-yelb 5spec: 6 priority: 1 7 appliedTo: 8 - podSelector: 9 matchLabels: 10 app: ubuntu-20-04 11 egress: 12 - action: Allow 13 to: 14 - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; 15 ports: 16 - protocol: TCP 17 port: 80 18 - action: Allow 1linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-no-tier.yaml 2clusternetworkpolicy.crd.antrea.io/override-attempt-failed-allow-yelb created 3linuxvm01:~/antrea/policies$ k get acnp 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5acnp-allow-yelb application 1 1 1 151m 6acnp-drop-yelb securityops 1 1 1 18h 7override-attempt-failed-allow-yelb application 1 1 1 10s The rule will be placed in the application Tier, even though the user has permission to create clusternetworkpolicies...\nWith this the network or security admins have full control of the network policies before and after the application Tier (ref the Tier diagram above).\nThis example has only shown how to do this on Cluster level, one can also add more granular permission on Namespace level.\nSo far I have gone over how to manage the Antrea FeatureGates in TKG, how to configure the Antrea-NSX integration, Antrea Policies in general and how to manage RBAC. In the the two next chapters I will cover two different ways how we can apply the Antrea Policies. Lets get into it\nHow to manage the Antrea Native Policies As mentioned previously Antrea Native Policies can be applied from inside the Kubernetes cluster using yaml manifests, but there is also another way to manage them using the NSX manager. As not mentioned previously this opens up for a whole new way of managing security policies. Centrally managed across multiple clusters wherever located, easier adoption of roles and responsibilities. If NSX is already in place, chances are that NSX security policies are already in place and being managed by the network or security admins. Now they can continue doing that but also take into consideration pod network security across the different TKG/Kubernetes clusters.\nAntrea Security policies from the NSX manager After you have connected your TKG clusters to the NSX manager (as shown earlier in this post) you will see the status of these connections in the NSX manager under System -\u0026gt; Fabric -\u0026gt; Nodes:\nThe status indicator is also a benefit of this integration as it will show you the status of Antrea Controller, and the components responsible for the Antrea-NSX integration.\nUnder inventory we can get all the relevant info from the TKG clusters:\nWhere in the screenshot above stc-tkg-cluster 1 and 2 are my TKG Antrea clusters. I can get all kinds of information like namespaces, pods, labels, ip addresses, names, services. This informaton is relevant as I can use them in my policy creation, but it also gives me status on whether pods, services are up.\nAntrea Cluster Network Policies - Applied from the NSX manager With the NSX manager we can create and manage the Antrea Native Policies from the NSX graphical user interface instead of CLI. Using NSX security groups and labels make it also much more fun, but also very easy to maintain know what we do as we can see the policies.\nLets create some policies from the NSX manager microsegmenting my demo application Yelb. This is my demo application, it consists of four pods, and a service called yelb-ui where the webpage is exposed. I know the different parts of the application (e.g pods) are using labels so I will use them. First let us list them from cli and then get them from the NSX manager.\n1linuxvm01:~/antrea/policies$ k get pods -n yelb --show-labels 2NAME READY STATUS RESTARTS AGE LABELS 3redis-server-69846b4888-5m757 1/1 Running 0 22h app=redis-server,pod-template-hash=69846b4888,tier=cache 4yelb-appserver-857c5c76d5-4cgbq 1/1 Running 0 22h app=yelb-appserver,pod-template-hash=857c5c76d5,tier=middletier 5yelb-db-6bd4fc5d9b-92rkf 1/1 Running 0 22h app=yelb-db,pod-template-hash=6bd4fc5d9b,tier=backenddb 6yelb-ui-6df49457d6-4bktw 1/1 Running 0 20h app=yelb-ui,pod-template-hash=6df49457d6,tier=frontend Ok, there I have the labels. Fine, just for the sake of it I will find the same labels in the NSX manager also:\nNow I need to create some security groups in NSX using these labels.\nFirst group is called acnp-yelb-frontend-ui and are using these membership criterias: (I am also adding the namespace criteria, to exclude any other applications using the same labels in other namespaces).\nNow hurry back to the security group and check whether there are any members.... Disappointment. Just empty:\nFear not, let us quickly create a policy with this group:\nCreate a new policy and set Antrea Container Clusters in the applied to field:\nThe actual rule:\nThe rule above allows my AVI Service Engines to reach the web-port on my yelb-ui pod on port 80 (http) as they are the loadbalancer for my application.\nAny members in the group now? Yes ðŸ˜ƒ\nNow go ahead and create similar groups and rules (except the ports) for the other pods using their respective label.\nEnd result:\nDo they work? Let us find that out a bit later as I need something to put in my TraceFlow chapter ðŸ˜„\nThe rules I have added above was just for the application in the namespace Yelb. If I wanted to extend this ruleset to also include the same application from other clusters its just adding the Kubernetes cluster in the Applied field like this: NSX Distributed Firewall - Kubernetes objects Policies In additon to managing the Antrea Native Policies from the NSX manager as above, in the recent NSX release additional features have been added to support security policies enforced in the NSX Distributed Firewall to also cover these components:\nWith this we can create security policies in NSX using the distributed firewall to cover the above components using security groups. With this feature its no longer necessary to investigate to get the information about the above components as they are already reported into the NSX manager. Let is do an example of how such a rule can be created and work.\nI will create a security policy based on this feature where I will use Kubernetes Service in my example. I will create a security group as above, but this time I will do some different selections. First grab the labels from the service, I will use the yelb-ui service in my example:\n1linuxvm01:~/antrea/policies$ k get svc -n yelb --show-labels 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE LABELS 3redis-server ClusterIP 20.10.102.8 \u0026lt;none\u0026gt; 6379/TCP 23h app=redis-server,tier=cache 4yelb-appserver ClusterIP 20.10.247.130 \u0026lt;none\u0026gt; 4567/TCP 23h app=yelb-appserver,tier=middletier 5yelb-db ClusterIP 20.10.44.17 \u0026lt;none\u0026gt; 5432/TCP 23h app=yelb-db,tier=backenddb 6yelb-ui LoadBalancer 20.10.194.179 10.13.210.10 80:30912/TCP 21h app=yelb-ui,tier=frontend I can either decide to use app=yelb-ui or tier=frontend. Now that I have my labels I will create my security group like this:\nI used the name of the service itself and the name of the namespace. This gives me this member: Which is right...\nNow create a security policy using this group where source is another group where I have defined a VM running in the same NSX environment. I have also created a any group which contains just 0.0.0.0/0. Remember that this policy is enforced in the DFW, so there must be something that is running in NSX for this to work, which in my environments is not only the the TKG cluster, but also the Avi Service Engines which acts as LoadBalancer and Ingress for my exposed services. This is kind of important to think of, as the Avi Service Engines communicates with the TKG cluster nodes using NodePortLocal in the default portrange 61000-62000 (if not changed in the Antrea configmap).\nLets see if the below rule works then:\nI will adjust it to Action Drop:\nTest Yelb ui access from my linux vm via curl and my physical laptop's browser, and the results are in:\n1ubuntu02:~$ curl yelb-ui.yelb.carefor.some-dns.net 2curl: (28) Failed to connect to yelb-ui.yelb.carefor.some-dns.net port 80: Connection timed out From my physical laptop browser:\nThis will be dropped even though I still have these rules in place from earlier (remember): Now, what about the Avi Service Engines?\nIf we just look at the rules above, the NSX Kubernetes Service rule and the Antrea Policies rules we are doing the firewall enforcing at two different levels. When creating policies with the Antrea Native Policies, like the one just above, we are applying and enforcing inside the Kubernetes cluster, with the NSX Kubernetes Service rule we are applying and enforcing on the DFW layer. So the Avi Service Engines will first need a policy that is allowing them to communicate to the TKG worker nodes on specific ports/protocol, in my exampe above with Yelb it is port 61002 and TCP. We can see that by looking in Avi UI:\nRegardless of the Avi SE's are using the same DFW as the worker nodes, we need to create this policy for the SE to reach the worker nodes to allow this connection. These policies can either be very \u0026quot;lazy\u0026quot; allowing the SEs on everyting TCP with a range of 61000-62000 to the worker nodes or can be made very granual pr service. The Avi SEs are automatically being grouped in NSX security groups if using Avi with NSX Cloud, explore that.\nIf we are not allowing the SEs this traffic, we will se this in the Avi UI:\nWhy is that though, I dont have a default block-all rule in my NSX environment... Well this is because of a set of default rules being created by NCP from TKG. Have a look at this rule:\nWhat is the membership in the group used in this Drop rule? That is all my TKG nodes including the Supervisor Control Plane nodes (the workload interface).\nNow in the Antrea Policies, we need to allow the IP addresses the SEs are using to reach the yelb-ui, as its not the actual client-ip that is being used, it is the SEs dataplane network.\nThe above diagram tries to explain the traffic flow and how it will be enforced. First the user want to access the VIP of the Yelb UI service. This is allowed by the NSX Firewall saying, yes Port 80 on IP 10.13.210.10 is OK to pass. As this VIP is realized by the Avi SEs, and are on NSX this rule will be enforced by the NSX firewall. Then the Avi SEs will forward (loadbalance) the traffic to the worker node(s) using NodePortLocal ranges between 61000-62000(default) where the worker nodes are also on the same NSX DFW, so we need to allow the SEs to forward this traffic. When all above is allowed, we will get \u0026quot;into\u0026quot; the actual TKG (Kubernetes) cluster and need to negiotiate the Antrea Native Policies that have been applied. These rules remember are allowing the SE dataplane IPs to reach the pod yelb-ui on port 80. And thats it.\nJust before we end up this chapter and head over to the next, let us quickly see how the policies created from the NSX manager look like inside the TKG cluster:\n1linuxvm01:~/antrea/policies$ k get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3823fca6f-88ee-4032-8150-ac8cf22f1c93 nsx-category-infrastructure 1.000000017763571 3 3 23h 49ae2599a-3bd3-4413-849e-06f53f467559 nsx-category-application 1.0000000532916369 2 2 24h The policies will be placed according to the NSX tiers from the UI:\nIf I describe one of the policies I will get the actual yaml manifest:\n1linuxvm01:~/antrea/policies$ k get acnp 9ae2599a-3bd3-4413-849e-06f53f467559 -oyaml 2apiVersion: crd.antrea.io/v1alpha1 3kind: ClusterNetworkPolicy 4metadata: 5 annotations: 6 ccp-adapter.antrea.tanzu.vmware.com/display-name: Yelb-Zero-Trust 7 creationTimestamp: \u0026#34;2023-06-05T12:12:14Z\u0026#34; 8 generation: 6 9 labels: 10 ccp-adapter.antrea.tanzu.vmware.com/managedBy: ccp-adapter 11 name: 9ae2599a-3bd3-4413-849e-06f53f467559 12 resourceVersion: \u0026#34;404591\u0026#34; 13 uid: 6477e785-fde4-46ba-b0a1-5ff5f784db8c 14spec: 15 ingress: 16 - action: Allow 17 appliedTo: 18 - group: 6f39fadf-04e8-4f49-be77-da0d4005ff37 19 enableLogging: false 20 from: 21 - ipBlock: 22 cidr: 10.13.11.101/32 23 - ipBlock: 24 cidr: 10.13.11.100/32 25 name: \u0026#34;4084\u0026#34; 26 ports: 27 - port: 80 28 protocol: TCP 29 - action: Allow 30 appliedTo: 31 - group: 31cf5eab-8bcd-4305-b72d-f1a44843fd8e 32 enableLogging: false 33 from: 34 - group: 6f39fadf-04e8-4f49-be77-da0d4005ff37 35 name: \u0026#34;4085\u0026#34; 36 ports: 37 - port: 4567 38 protocol: TCP 39 - action: Allow 40 appliedTo: 41 - group: 672f4d75-c83b-4fa1-b0ab-ae414c2e8e8c 42 enableLogging: false 43 from: 44 - group: 31cf5eab-8bcd-4305-b72d-f1a44843fd8e 45 name: \u0026#34;4087\u0026#34; 46 ports: 47 - port: 5432 48 protocol: TCP 49 - action: Allow 50 appliedTo: 51 - group: 52c3548b-4758-427f-bcde-b25d36613de6 52 enableLogging: false 53 from: 54 - group: 31cf5eab-8bcd-4305-b72d-f1a44843fd8e 55 name: \u0026#34;4088\u0026#34; 56 ports: 57 - port: 6379 58 protocol: TCP 59 - action: Drop 60 appliedTo: 61 - group: d250b7d7-3041-4f7f-8fdf-c7360eee9615 62 enableLogging: false 63 from: 64 - group: d250b7d7-3041-4f7f-8fdf-c7360eee9615 65 name: \u0026#34;4089\u0026#34; 66 priority: 1.0000000532916369 67 tier: nsx-category-application 68status: 69 currentNodesRealized: 2 70 desiredNodesRealized: 2 71 observedGeneration: 6 72 phase: Realized Antrea Security policies from kubernetes api I have already covered this topic in another post here. Head over and have look, also its worth reading the official documentation page from Antrea here as it contains examples and is updated on new features.\nOne thing I would like to use this chapter for though is trying to apply a policy on the NSX added Tiers when doing the integration (explained above). Remember the Tiers?\n1linuxvm01:~/antrea/policies$ k get tiers 2NAME PRIORITY AGE 3application 250 2d2h 4baseline 253 2d2h 5emergency 50 2d2h 6networkops 150 2d2h 7nsx-category-application 4 2d 8nsx-category-emergency 1 2d 9nsx-category-environment 3 2d 10nsx-category-ethernet 0 2d 11nsx-category-infrastructure 2 2d 12platform 200 2d2h 13securityops 100 2d2h These nsx* tiers are coming from the NSX manager, but can I as a cluster-owner/editor place rules in here by default? If you look at the PRIORITY of these, they are pretty low.\nLet us apply the same rule as used earlier in this post, by just editing in the tier placement:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: acnp-nsx-tier-from-kubectl 5spec: 6 priority: 1 7 tier: nsx-category-environment 8 appliedTo: 9 - podSelector: 10 matchLabels: 11 app: ubuntu-20-04 12 egress: 13 - action: Allow 14 to: 15 - fqdn: \u0026#34;yelb-ui.yelb.carefor.some-dns.net\u0026#34; 16 ports: 17 - protocol: TCP 18 port: 80 19 - action: Allow 1linuxvm01:~/antrea/policies$ k apply -f fqdn-rule-nsx-tier.yaml 2Error from server: error when creating \u0026#34;fqdn-rule-nsx-tier.yaml\u0026#34;: admission webhook \u0026#34;acnpvalidator.antrea.io\u0026#34; denied the request: user not authorized to access Tier nsx-category-environment Even though I am the cluster-owner/admin/superuser I am not allowed to place any rules in these nsx tiers. So this just gives us further control and mechanisms to support both NSX created Antrea policies and Antrea policies from kubectl. This allows for a good control of security enforcement by roles in the organization.\nAntrea Dashboard As the Octant dashboard is no more, Antrea now has its own dashboard. Its very easy to deploy. Let me quickly go through it. Read more about it here\n1# Add the helm charts 2helm repo add antrea https://charts.antrea.io 3helm repo update Install it:\n1helm install antrea-ui antrea/antrea-ui --namespace kube-system 1linuxvm01:~/antrea/policies$ helm repo add antrea https://charts.antrea.io 2\u0026#34;antrea\u0026#34; has been added to your repositories 3linuxvm01:~/antrea/policies$ helm repo update 4Hang tight while we grab the latest from your chart repositories... 5...Successfully got an update from the \u0026#34;ako\u0026#34; chart repository 6...Successfully got an update from the \u0026#34;antrea\u0026#34; chart repository 7...Successfully got an update from the \u0026#34;bitnami\u0026#34; chart repository 8Update Complete. âŽˆHappy Helming!âŽˆ 9linuxvm01:~/antrea/policies$ helm install antrea-ui antrea/antrea-ui --namespace kube-system 10NAME: antrea-ui 11LAST DEPLOYED: Tue Jun 6 12:56:21 2023 12NAMESPACE: kube-system 13STATUS: deployed 14REVISION: 1 15TEST SUITE: None 16NOTES: 17The Antrea UI has been successfully installed 18 19You are using version 0.1.1 20 21To access the UI, forward a local port to the antrea-ui service, and connect to 22that port locally with your browser: 23 24 $ kubectl -n kube-system port-forward service/antrea-ui 3000:3000 25 26After running the command above, access \u0026#34;http://localhost:3000\u0026#34; in your browser.For the Antrea documentation, please visit https://antrea.io This will spin up a new pod, and a clusterip service.\n1linuxvm01:~/antrea/policies$ k get pods -n kube-system 2NAME READY STATUS RESTARTS AGE 3antrea-agent-9rvqc 2/2 Running 0 2d16h 4antrea-agent-m7rg7 2/2 Running 0 2d16h 5antrea-agent-wvpp8 2/2 Running 0 2d16h 6antrea-controller-6d56b6d664-vlmh2 1/1 Running 0 2d16h 7antrea-ui-9c89486f4-msw6m 2/2 Running 0 62s 1linuxvm01:~/antrea/policies$ k get svc -n kube-system 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3antrea ClusterIP 20.10.96.45 \u0026lt;none\u0026gt; 443/TCP 2d16h 4antrea-ui ClusterIP 20.10.228.144 \u0026lt;none\u0026gt; 3000/TCP 95s Now instead of exposing the service as nodeport, I am just creating a serviceType loadBalancer for it like this:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: antrea-dashboard-ui 5 labels: 6 app: antrea-ui 7 namespace: kube-system 8spec: 9 loadBalancerClass: ako.vmware.com/avi-lb 10 type: LoadBalancer 11 ports: 12 - port: 80 13 protocol: TCP 14 targetPort: 3000 15 selector: 16 app: antrea-ui Apply it:\n1linuxvm01:~/antrea$ k apply -f antrea-dashboard-lb-yaml 2service/antrea-dashboard-ui created 3linuxvm01:~/antrea$ k get svc -n kube-system 4NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 5antrea ClusterIP 20.10.96.45 \u0026lt;none\u0026gt; 443/TCP 2d16h 6antrea-dashboard-ui LoadBalancer 20.10.76.243 10.13.210.12 80:31334/TCP 7s 7antrea-ui ClusterIP 20.10.228.144 \u0026lt;none\u0026gt; 3000/TCP 8m47s Now access it through my browser:\nDefault password is admin\nGood overview:\nThe option to do Traceflow:\nOops, dropped by a NetworkPolicy... Where does that come from ðŸ¤” ... More on this later.\nAntrea Network Monitoring Being able to know what's going on is crucial when planning security policies, but also to know if the policies are working and being enforced. With that information available we can know if we are compliant with the policies apllied. Without any network flow information we are kind of in the blind. Luckily Antrea is fully capable of report full flow information, and export it. To be able to export the flow information we need to enable the FeatureGate FlowExporter:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: stc-tkg-cluster-1-antrea-package 5 namespace: ns-stc-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: true #This needs to be enabled 14 Egress: true 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Flow-Exporter - IPFIX From the offical Antrea documentation:\nAntrea is a Kubernetes network plugin that provides network connectivity and security features for Pod workloads. Considering the scale and dynamism of Kubernetes workloads in a cluster, Network Flow Visibility helps in the management and configuration of Kubernetes resources such as Network Policy, Services, Pods etc., and thereby provides opportunities to enhance the performance and security aspects of Pod workloads.\nFor visualizing the network flows, Antrea monitors the flows in Linux conntrack module. These flows are converted to flow records, and then flow records are post-processed before they are sent to the configured external flow collector. High-level design is given below:\nFrom the Antrea official documentation again:\nFlow Exporter\nIn Antrea, the basic building block for the Network Flow Visibility is the Flow Exporter. Flow Exporter operates within Antrea Agent; it builds and maintains a connection store by polling and dumping flows from conntrack module periodically. Connections from the connection store are exported to the Flow Aggregator Service using the IPFIX protocol, and for this purpose we use the IPFIX exporter process from the go-ipfix library.\nRead more Network Flow Visibility in Antrea here.\nTraceflow When troubleshooting network issues or even firewall rules (is my traffic being blocked or allowed?) it is very handy to have the option to do Traceflow. Antrea supports Traceflow. To be able to use Traceflow, the AntreaTraceFlow FeatureGate needs to be enabled if not already.\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: stc-tkg-cluster-1-antrea-package 5 namespace: ns-stc-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: true 14 Egress: true 15 NodePortLocal: true 16 AntreaTraceflow: true #This needs to be enabled 17 NetworkPolicyStats: true Now that it is enabled, how can we perform Traceflow?\nWe can do Traceflow using kubectl, Antrea UI or even from the NSX manager if using the NSX/Antrea integration.\nTraceflow in Antrea supports the following:\nSource: pod, protocol (TCP/UDP/ICMP) and port numbers Destination: pod, service, ip, protocol (TCP/UDP/ICMP) and port numbers One time Traceflow or live Now to get back to my Antrea policies created earlier I want to test if they are actually being in use and enforced. So let me do a Traceflow form my famous Yelb-ui pod and see if it can reach the application pod on its allowed port. Remember that the UI pod needed to communicate with the appserver pod on TCP 4567 and that I created a rule that only allows this, all else is blocked.\nIf I want to do Traceflow from kubectl, this is an example to test if port 4567 is allowed from ui pod to appserver pod:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: Traceflow 3metadata: 4 name: tf-test 5spec: 6 source: 7 namespace: yelb 8 pod: yelb-ui-6df49457d6-m5clv 9 destination: 10 namespace: yelb 11 pod: yelb-appserver-857c5c76d5-4cd86 12 # destination can also be an IP address (\u0026#39;ip\u0026#39; field) or a Service name (\u0026#39;service\u0026#39; field); the 3 choices are mutually exclusive. 13 packet: 14 ipHeader: # If ipHeader/ipv6Header is not set, the default value is IPv4+ICMP. 15 protocol: 6 # Protocol here can be 6 (TCP), 17 (UDP) or 1 (ICMP), default value is 1 (ICMP) 16 transportHeader: 17 tcp: 18 srcPort: 0 # Source port needs to be set when Protocol is TCP/UDP. 19 dstPort: 4567 # Destination port needs to be set when Protocol is TCP/UDP. 20 flags: 2 # Construct a SYN packet: 2 is also the default value when the flags field is omitted. Now apply it and get the output:\n1linuxvm01:~/antrea/policies$ k apply -f traceflow.yaml 2traceflow.crd.antrea.io/tf-test created 1linuxvm01:~/antrea/policies$ k get traceflows.crd.antrea.io -n yelb tf-test -oyaml 2apiVersion: crd.antrea.io/v1alpha1 3kind: Traceflow 4metadata: 5 annotations: 6 kubectl.kubernetes.io/last-applied-configuration: | 7 {\u0026#34;apiVersion\u0026#34;:\u0026#34;crd.antrea.io/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Traceflow\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;tf-test\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;destination\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-appserver-857c5c76d5-4cd86\u0026#34;},\u0026#34;packet\u0026#34;:{\u0026#34;ipHeader\u0026#34;:{\u0026#34;protocol\u0026#34;:6},\u0026#34;transportHeader\u0026#34;:{\u0026#34;tcp\u0026#34;:{\u0026#34;dstPort\u0026#34;:4567,\u0026#34;flags\u0026#34;:2,\u0026#34;srcPort\u0026#34;:0}}},\u0026#34;source\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-ui-6df49457d6-m5clv\u0026#34;}}} 8 creationTimestamp: \u0026#34;2023-06-07T12:47:14Z\u0026#34; 9 generation: 1 10 name: tf-test 11 resourceVersion: \u0026#34;904386\u0026#34; 12 uid: c550596b-ed43-4bab-a6f1-d23e90d35f84 13spec: 14 destination: 15 namespace: yelb 16 pod: yelb-appserver-857c5c76d5-4cd86 17 packet: 18 ipHeader: 19 protocol: 6 20 transportHeader: 21 tcp: 22 dstPort: 4567 23 flags: 2 24 srcPort: 0 25 source: 26 namespace: yelb 27 pod: yelb-ui-6df49457d6-m5clv 28status: 29 phase: Succeeded 30 results: 31 - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-5r8gj 32 observations: 33 - action: Received 34 component: Forwarding 35 - action: Forwarded 36 component: NetworkPolicy 37 componentInfo: IngressRule 38 networkPolicy: AntreaClusterNetworkPolicy:9ae2599a-3bd3-4413-849e-06f53f467559 39 - action: Delivered 40 component: Forwarding 41 componentInfo: Output 42 timestamp: 1686142036 43 - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-bpx7s 44 observations: 45 - action: Forwarded 46 component: SpoofGuard 47 - action: Forwarded 48 component: Forwarding 49 componentInfo: Output 50 tunnelDstIP: 10.13.82.39 51 timestamp: 1686142036 52 startTime: \u0026#34;2023-06-07T12:47:14Z\u0026#34; That was a success. - action: Forwarded\nNow I want to run it again but with another port. So I change the above yaml to use port 4568 (which should not be allowed):\n1linuxvm01:~/antrea/policies$ k get traceflows.crd.antrea.io -n yelb tf-test -oyaml 2apiVersion: crd.antrea.io/v1alpha1 3kind: Traceflow 4metadata: 5 annotations: 6 kubectl.kubernetes.io/last-applied-configuration: | 7 {\u0026#34;apiVersion\u0026#34;:\u0026#34;crd.antrea.io/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Traceflow\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;tf-test\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;destination\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-appserver-857c5c76d5-4cd86\u0026#34;},\u0026#34;packet\u0026#34;:{\u0026#34;ipHeader\u0026#34;:{\u0026#34;protocol\u0026#34;:6},\u0026#34;transportHeader\u0026#34;:{\u0026#34;tcp\u0026#34;:{\u0026#34;dstPort\u0026#34;:4568,\u0026#34;flags\u0026#34;:2,\u0026#34;srcPort\u0026#34;:0}}},\u0026#34;source\u0026#34;:{\u0026#34;namespace\u0026#34;:\u0026#34;yelb\u0026#34;,\u0026#34;pod\u0026#34;:\u0026#34;yelb-ui-6df49457d6-m5clv\u0026#34;}}} 8 creationTimestamp: \u0026#34;2023-06-07T12:53:59Z\u0026#34; 9 generation: 1 10 name: tf-test 11 resourceVersion: \u0026#34;905571\u0026#34; 12 uid: d76ec419-3272-4595-98a5-72a49adce9d3 13spec: 14 destination: 15 namespace: yelb 16 pod: yelb-appserver-857c5c76d5-4cd86 17 packet: 18 ipHeader: 19 protocol: 6 20 transportHeader: 21 tcp: 22 dstPort: 4568 23 flags: 2 24 srcPort: 0 25 source: 26 namespace: yelb 27 pod: yelb-ui-6df49457d6-m5clv 28status: 29 phase: Succeeded 30 results: 31 - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-bpx7s 32 observations: 33 - action: Forwarded 34 component: SpoofGuard 35 - action: Forwarded 36 component: Forwarding 37 componentInfo: Output 38 tunnelDstIP: 10.13.82.39 39 timestamp: 1686142441 40 - node: stc-tkg-cluster-1-node-pool-01-p6nms-84c55d4574-5r8gj 41 observations: 42 - action: Received 43 component: Forwarding 44 - action: Dropped 45 component: NetworkPolicy 46 componentInfo: IngressMetric 47 networkPolicy: AntreaClusterNetworkPolicy:9ae2599a-3bd3-4413-849e-06f53f467559 48 timestamp: 1686142441 49 startTime: \u0026#34;2023-06-07T12:53:59Z\u0026#34; That was also a success, as it was dropped by design: - action: Dropped\nIts great being able to do this from kubectl, if one quickly need to check this before starting to look somewhere else and create a support ticket ðŸ˜ƒ or one dont have access to other tools like the Antrea UI or even the NSX manager, speaking of NSX manager. Let us do the exact same trace from the NSX manager gui:\nHead over Plan\u0026amp;Troubleshoot -\u0026gt; Traffic Analysis:\nResults:\nNow I change it to another port again and test it again:\nDropped again.\nThe same procedure can also be done from the Antrea UI as shown above, now with a port that is allowed:\nTo read more on Traceflow in Antrea, head over here.\nTheia Now that we have know it's possible to export all flows using IPFIX, I thought it would be interesting to just showcase how the flow information can be presented with a solution called Theia. From the official docs:\nTheia is a network observability and analytics platform for Kubernetes. It is built on top of Antrea, and consumes network flows exported by Antrea to provide fine-grained visibility into the communication and NetworkPolicies among Pods and Services in a Kubernetes cluster.\nTo install Theia I have followed the instructions from here which is also a greate place to read more about Theia.\nTheia is installed using Helm, start by adding the charts, do an update and deploy:\n1linuxvm01:~/antrea$ helm repo add antrea https://charts.antrea.io 2\u0026#34;antrea\u0026#34; already exists with the same configuration, skipping 3linuxvm01:~/antrea$ helm repo update 4Hang tight while we grab the latest from your chart repositories... 5...Successfully got an update from the \u0026#34;antrea\u0026#34; chart repository 6Update Complete. âŽˆHappy Helming!âŽˆ Make sure that FlowExporter has been enabled, if not apply an AntreaConfig that enables it:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: stc-tkg-cluster-1-antrea-package 5 namespace: ns-stc-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: true #Enable this! 14 Egress: true 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true After the config has been enabled, delete the Antrea agents and controller so these will read the new configMap:\n1linuxvm01:~/antrea/theia$ k delete pod -n kube-system -l app=antrea 2pod \u0026#34;antrea-agent-58nn2\u0026#34; deleted 3pod \u0026#34;antrea-agent-cnq9p\u0026#34; deleted 4pod \u0026#34;antrea-agent-sx6vr\u0026#34; deleted 5pod \u0026#34;antrea-controller-6d56b6d664-km64t\u0026#34; deleted After the Helm charts have been added, I start by installing the Flow Aggregator\n1helm install flow-aggregator antrea/flow-aggregator --set clickHouse.enable=true,recordContents.podLabels=true -n flow-aggregator --create-namespace As usual with Helm charts, if there is any specific settings you would like to change get the helm chart values for your specific charts first and refer to them by using -f values.yaml..\n1linuxvm01:~/antrea/theia$ helm show values antrea/flow-aggregator \u0026gt; flow-agg-values.yaml I dont have any specifics I need to change for this one, so I will just deploy using the defaults:\n1linuxvm01:~/antrea/theia$ helm install flow-aggregator antrea/flow-aggregator --set clickHouse.enable=true,recordContents.podLabels=true -n flow-aggregator --create-namespace 2NAME: flow-aggregator 3LAST DEPLOYED: Tue Jun 6 21:28:49 2023 4NAMESPACE: flow-aggregator 5STATUS: deployed 6REVISION: 1 7TEST SUITE: None 8NOTES: 9The Antrea Flow Aggregator has been successfully installed 10 11You are using version 1.12.0 12 13For the Antrea documentation, please visit https://antrea.io Now what has happened in my TKG cluster:\n1linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator 2NAME READY STATUS RESTARTS AGE 3flow-aggregator-5b4c69885f-mklm5 1/1 Running 1 (10s ago) 22s 4linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator 5NAME READY STATUS RESTARTS AGE 6flow-aggregator-5b4c69885f-mklm5 1/1 Running 1 (13s ago) 25s 7linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator 8NAME READY STATUS RESTARTS AGE 9flow-aggregator-5b4c69885f-mklm5 0/1 Error 1 (14s ago) 26s 10linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator 11NAME READY STATUS RESTARTS AGE 12flow-aggregator-5b4c69885f-mklm5 0/1 CrashLoopBackOff 3 (50s ago) 60s Well, that did'nt go so well...\nThe issue is that Flow Aggregator is looking for a service that is not created yet and will just fail until this is deployed. This is our next step.\n1linuxvm01:~/antrea/theia$ helm install theia antrea/theia --set sparkOperator.enable=true,theiaManager.enable=true -n flow-visibility --create-namespace 2 3NAME: theia 4LAST DEPLOYED: Tue Jun 6 22:02:37 2023 5NAMESPACE: flow-visibility 6STATUS: deployed 7REVISION: 1 8TEST SUITE: None 9NOTES: 10Theia has been successfully installed 11 12You are using version 0.6.0 13 14For the Antrea documentation, please visit https://antrea.io What has been created now:\n1linuxvm01:~/antrea/theia$ k get pods -n flow-visibility 2NAME READY STATUS RESTARTS AGE 3chi-clickhouse-clickhouse-0-0-0 2/2 Running 0 8m52s 4grafana-684d8948b-c6wzn 1/1 Running 0 8m56s 5theia-manager-5d8d6b86b7-cbxrz 1/1 Running 0 8m56s 6theia-spark-operator-54d9ddd544-nqhqd 1/1 Running 0 8m56s 7zookeeper-0 1/1 Running 0 8m56s Now flow-aggreator should also be in a runing state, if not just delete the pod and it should get back on its feet.\n1linuxvm01:~/antrea/theia$ k get pods -n flow-aggregator 2NAME READY STATUS RESTARTS AGE 3flow-aggregator-5b4c69885f-xhdkx 1/1 Running 0 5m2s So, now its all about getting access to the Grafana dashboard. I will just expose this with serviceType loadBalancer as it \u0026quot;out-of-the-box\u0026quot; is only exposed with NodePort:\n1linuxvm01:~/antrea/theia$ k get svc -n flow-visibility 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3chi-clickhouse-clickhouse-0-0 ClusterIP None \u0026lt;none\u0026gt; 8123/TCP,9000/TCP,9009/TCP 8m43s 4clickhouse-clickhouse ClusterIP 20.10.136.211 \u0026lt;none\u0026gt; 8123/TCP,9000/TCP 10m 5grafana NodePort 20.10.172.165 \u0026lt;none\u0026gt; 3000:30096/TCP 10m 6theia-manager ClusterIP 20.10.156.217 \u0026lt;none\u0026gt; 11347/TCP 10m 7zookeeper ClusterIP 20.10.219.137 \u0026lt;none\u0026gt; 2181/TCP,7000/TCP 10m 8zookeepers ClusterIP None \u0026lt;none\u0026gt; 2888/TCP,3888/TCP 10m So let us create a LoadBalancer service for this:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: theia-dashboard-ui 5 labels: 6 app: grafana 7 namespace: flow-visibility 8spec: 9 loadBalancerClass: ako.vmware.com/avi-lb 10 type: LoadBalancer 11 ports: 12 - port: 80 13 protocol: TCP 14 targetPort: 3000 15 selector: 16 app: grafana 1linuxvm01:~/antrea/theia$ k get svc -n flow-visibility 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) grafana NodePort 20.10.172.165 \u0026lt;none\u0026gt; 3000:30096/TCP 15m 3theia-dashboard-ui LoadBalancer 20.10.24.174 10.13.210.13 80:32075/TCP 13s Lets try to access it through the browser:\nGreat. Theia comes with a couple of predefined dashobards that is interesting to start out with. So let me list some of the screenshots from the predefined dashboards below:\nThe homepage:\nList of dashboards:\nFlow_Records_Dashboard:\nNetwork_Topology_Dashboard:\nNetwork Policy Recommendation From the official docs:\nTheia NetworkPolicy Recommendation recommends the NetworkPolicy configuration to secure Kubernetes network and applications. It analyzes the network flows collected by Grafana Flow Collector to generate Kubernetes NetworkPolicies or Antrea NetworkPolicies. This feature assists cluster administrators and app developers in securing their applications according to Zero Trust principles.\nI like the sound of that. Let us try it out.\nThe first I need to install inst the Theia CLI, this can be found and the instructions from here\nTheia CLI\n1curl -Lo ./theia \u0026#34;https://github.com/antrea-io/theia/releases/download/v0.6.0/theia-$(uname)-x86_64\u0026#34; 2chmod +x ./theia 3mv ./theia /usr/local/bin/theia 4theia help 1linuxvm01:~/antrea/theia$ curl -Lo ./theia \u0026#34;https://github.com/antrea-io/theia/releases/download/v0.6.0/theia-$(uname)-x86_64\u0026#34; 2 % Total % Received % Xferd Average Speed Time Time Time Current 3 Dload Upload Total Spent Left Speed 4 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 5100 37.9M 100 37.9M 0 0 11.6M 0 0:00:03 0:00:03 --:--:-- 17.2M 6linuxvm01:~/antrea/theia$ chmod +x ./theia 7linuxvm01:~/antrea/theia$ sudo cp theia /usr/local/bin/theia 8linuxvm01:~/antrea/theia$ theia help 9theia is the command line tool for Theia which provides access 10to Theia network flow visibility capabilities 11 12Usage: 13 theia [command] 14 15Available Commands: 16 clickhouse Commands of Theia ClickHouse feature 17 completion Generate the autocompletion script for the specified shell 18 help Help about any command 19 policy-recommendation Commands of Theia policy recommendation feature 20 supportbundle Generate support bundle 21 throughput-anomaly-detection Commands of Theia throughput anomaly detection feature 22 version Show Theia CLI version 23 24Flags: 25 -h, --help help for theia 26 -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified 27 -v, --verbose int set verbose level 28 29Use \u0026#34;theia [command] --help\u0026#34; for more information about a command. These are the following commands when running the policy-recommendation option:\n1andreasm@linuxvm01:~/antrea/theia$ theia policy-recommendation --help 2Command group of Theia policy recommendation feature. 3Must specify a subcommand like run, status or retrieve. 4 5Usage: 6 theia policy-recommendation [flags] 7 theia policy-recommendation [command] 8 9Aliases: 10 policy-recommendation, pr 11 12Available Commands: 13 delete Delete a policy recommendation job 14 list List all policy recommendation jobs 15 retrieve Get the recommendation result of a policy recommendation job 16 run Run a new policy recommendation job 17 status Check the status of a policy recommendation job 18 19 20Use \u0026#34;theia policy-recommendation [command] --help\u0026#34; for more information about a command. These are the options for the run command:\n1linuxvm01:~/antrea/theia$ theia policy-recommendation run --help 2Run a new policy recommendation job. 3Must finish the deployment of Theia first 4 5Usage: 6 theia policy-recommendation run [flags] 7 8Examples: 9Run a policy recommendation job with default configuration 10$ theia policy-recommendation run 11Run an initial policy recommendation job with policy type anp-deny-applied and limit on last 10k flow records 12$ theia policy-recommendation run --type initial --policy-type anp-deny-applied --limit 10000 13Run an initial policy recommendation job with policy type anp-deny-applied and limit on flow records from 2022-01-01 00:00:00 to 2022-01-31 23:59:59. 14$ theia policy-recommendation run --type initial --policy-type anp-deny-applied --start-time \u0026#39;2022-01-01 00:00:00\u0026#39; --end-time \u0026#39;2022-01-31 23:59:59\u0026#39; 15Run a policy recommendation job with default configuration but doesn\u0026#39;t recommend toServices ANPs 16$ theia policy-recommendation run --to-services=false 17 18 19Flags: 20 --driver-core-request string Specify the CPU request for the driver Pod. Values conform to the Kubernetes resource quantity convention. 21 Example values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;) 22 --driver-memory string Specify the memory request for the driver Pod. Values conform to the Kubernetes resource quantity convention. 23 Example values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;) 24 -e, --end-time string The end time of the flow records considered for the policy recommendation. 25 Format is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the end time of flow records by default. 26 --exclude-labels Enable this option will exclude automatically generated Pod labels including \u0026#39;pod-template-hash\u0026#39;, 27 \u0026#39;controller-revision-hash\u0026#39;, \u0026#39;pod-template-generation\u0026#39; during policy recommendation. (default true) 28 --executor-core-request string Specify the CPU request for the executor Pod. Values conform to the Kubernetes resource quantity convention. 29 Example values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;) 30 --executor-instances int32 Specify the number of executors for the Spark application. Example values include 1, 2, 8, etc. (default 1) 31 --executor-memory string Specify the memory request for the executor Pod. Values conform to the Kubernetes resource quantity convention. 32 Example values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;) 33 -f, --file string The file path where you want to save the result. It can only be used when wait is enabled. 34 -h, --help help for run 35 -l, --limit int The limit on the number of flow records read from the database. 0 means no limit. 36 -n, --ns-allow-list string List of default allow Namespaces. 37 If no Namespaces provided, Traffic inside Antrea CNI related Namespaces: [\u0026#39;kube-system\u0026#39;, \u0026#39;flow-aggregator\u0026#39;, 38 \u0026#39;flow-visibility\u0026#39;] will be allowed by default. 39 -p, --policy-type string Types of generated NetworkPolicy. 40 Currently we have 3 generated NetworkPolicy types: 41 anp-deny-applied: Recommending allow ANP/ACNP policies, with default deny rules only on Pods which have an allow rule applied. 42 anp-deny-all: Recommending allow ANP/ACNP policies, with default deny rules for whole cluster. 43 k8s-np: Recommending allow K8s NetworkPolicies. (default \u0026#34;anp-deny-applied\u0026#34;) 44 -s, --start-time string The start time of the flow records considered for the policy recommendation. 45 Format is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the start time of flow records by default. 46 --to-services Use the toServices feature in ANP and recommendation toServices rules for Pod-to-Service flows, 47 only works when option is anp-deny-applied or anp-deny-all. (default true) 48 -t, --type string {initial|subsequent} Indicates this recommendation is an initial recommendion or a subsequent recommendation job. (default \u0026#34;initial\u0026#34;) 49 --wait Enable this option will hold and wait the whole policy recommendation job finishes. 50 51Global Flags: 52 -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified 53 --use-cluster-ip Enable this option will use ClusterIP instead of port forwarding when connecting to the Theia 54 Manager Service. It can only be used when running in cluster. 55 -v, --verbose int set verbose level I will just run the following theia policy-recommendation run --type initial --policy-type anp-deny-applied --limit 10000 to generate some output.\n1linuxvm01:~/antrea/theia$ theia policy-recommendation run --type initial --policy-type anp-deny-applied --limit 10000 2Successfully created policy recommendation job with name pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 Lets check the status:\n1# First I will list all the runs to get the name 2linuxvm01:~/antrea/theia$ theia policy-recommendation list 3CreationTime CompletionTime Name Status 42023-06-08 07:50:28 N/A pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 SCHEDULED 5# Then I will check the status on the specific run 6linuxvm01:~/antrea/theia$ theia policy-recommendation status pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 7Status of this policy recommendation job is SCHEDULED Seems like I have to wait some, time to grab a coffee.\nJust poured my coffee, wanted to check again:\n1linuxvm01:~/antrea/theia$ theia policy-recommendation status pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 2Status of this policy recommendation job is RUNNING: 0/1 (0%) stages completed Alright, it is running.\nNow time to drink the coffee.\nLets check in on it again:\n1linuxvm01:~/antrea/theia$ theia policy-recommendation status pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 2Status of this policy recommendation job is COMPLETED Oh yes, now I am excited which policies it recommends:\n1linuxvm01:~/antrea/theia$ theia policy-recommendation retrieve pr-e81a42e4-013a-4cf6-be43-b1ee48ea9a18 2apiVersion: crd.antrea.io/v1alpha1 3kind: ClusterNetworkPolicy 4metadata: 5 name: recommend-reject-acnp-9np4b 6spec: 7 appliedTo: 8 - namespaceSelector: 9 matchLabels: 10 kubernetes.io/metadata.name: yelb 11 podSelector: 12 matchLabels: 13 app: traffic-generator 14 egress: 15 - action: Reject 16 to: 17 - podSelector: {} 18 ingress: 19 - action: Reject 20 from: 21 - podSelector: {} 22 priority: 5 23 tier: Baseline 24--- 25apiVersion: crd.antrea.io/v1alpha1 26kind: ClusterNetworkPolicy 27metadata: 28 name: recommend-reject-acnp-ega4b 29spec: 30 appliedTo: 31 - namespaceSelector: 32 matchLabels: 33 kubernetes.io/metadata.name: avi-system 34 podSelector: 35 matchLabels: 36 app.kubernetes.io/instance: ako-1685884771 37 app.kubernetes.io/name: ako 38 statefulset.kubernetes.io/pod-name: ako-0 39 egress: 40 - action: Reject 41 to: 42 - podSelector: {} 43 ingress: 44 - action: Reject 45 from: 46 - podSelector: {} 47 priority: 5 48 tier: Baseline 49--- 50apiVersion: crd.antrea.io/v1alpha1 51kind: NetworkPolicy 52metadata: 53 name: recommend-allow-anp-nl6re 54 namespace: yelb 55spec: 56 appliedTo: 57 - podSelector: 58 matchLabels: 59 app: traffic-generator 60 egress: 61 - action: Allow 62 ports: 63 - port: 80 64 protocol: TCP 65 to: 66 - ipBlock: 67 cidr: 10.13.210.10/32 68 ingress: [] 69 priority: 5 70 tier: Application 71--- 72apiVersion: crd.antrea.io/v1alpha1 73kind: NetworkPolicy 74metadata: 75 name: recommend-allow-anp-2ifjo 76 namespace: avi-system 77spec: 78 appliedTo: 79 - podSelector: 80 matchLabels: 81 app.kubernetes.io/instance: ako-1685884771 82 app.kubernetes.io/name: ako 83 statefulset.kubernetes.io/pod-name: ako-0 84 egress: 85 - action: Allow 86 ports: 87 - port: 443 88 protocol: TCP 89 to: 90 - ipBlock: 91 cidr: 172.24.3.50/32 92 ingress: [] 93 priority: 5 94 tier: Application 95--- 96apiVersion: crd.antrea.io/v1alpha1 97kind: ClusterNetworkPolicy 98metadata: 99 name: recommend-allow-acnp-kube-system-kaoh6 100spec: 101 appliedTo: 102 - namespaceSelector: 103 matchLabels: 104 kubernetes.io/metadata.name: kube-system 105 egress: 106 - action: Allow 107 to: 108 - podSelector: {} 109 ingress: 110 - action: Allow 111 from: 112 - podSelector: {} 113 priority: 5 114 tier: Platform 115--- 116apiVersion: crd.antrea.io/v1alpha1 117kind: ClusterNetworkPolicy 118metadata: 119 name: recommend-allow-acnp-flow-aggregator-dnvhc 120spec: 121 appliedTo: 122 - namespaceSelector: 123 matchLabels: 124 kubernetes.io/metadata.name: flow-aggregator 125 egress: 126 - action: Allow 127 to: 128 - podSelector: {} 129 ingress: 130 - action: Allow 131 from: 132 - podSelector: {} 133 priority: 5 134 tier: Platform 135--- 136apiVersion: crd.antrea.io/v1alpha1 137kind: ClusterNetworkPolicy 138metadata: 139 name: recommend-allow-acnp-flow-visibility-sqjwf 140spec: 141 appliedTo: 142 - namespaceSelector: 143 matchLabels: 144 kubernetes.io/metadata.name: flow-visibility 145 egress: 146 - action: Allow 147 to: 148 - podSelector: {} 149 ingress: 150 - action: Allow 151 from: 152 - podSelector: {} 153 priority: 5 154 tier: Platform 155--- 156apiVersion: crd.antrea.io/v1alpha1 157kind: ClusterNetworkPolicy 158metadata: 159 name: recommend-reject-acnp-hmjt8 160spec: 161 appliedTo: 162 - namespaceSelector: 163 matchLabels: 164 kubernetes.io/metadata.name: yelb-2 165 podSelector: 166 matchLabels: 167 app: yelb-ui 168 tier: frontend 169 egress: 170 - action: Reject 171 to: 172 - podSelector: {} 173 ingress: 174 - action: Reject 175 from: 176 - podSelector: {} 177 priority: 5 178 tier: Baseline Ok, well. I appreciate the output, but I would need to do some modifications to it before I would apply it. As my lab is not generating that much traffic, it does not create all the flows needed to generate a better recommendation. For it to generate better recommendations, the flows also needs to be there. My traffic-generator is not doing a good job to achieve this. I will need to generate some more activity for the recommendation engine to get enough flows to consider.\nThroughput Anomaly Detection From the offical docs:\nFrom Theia v0.5, Theia supports Throughput Anomaly Detection. Throughput Anomaly Detection (TAD) is a technique for understanding and reporting the throughput abnormalities in the network traffic. It analyzes the network flows collected by Grafana Flow Collector to report anomalies in the network. TAD uses three algorithms to find the anomalies in network flows such as ARIMA, EWMA, and DBSCAN. These anomaly analyses help the user to find threats if present.\nLets try it out. I already have the dependencies and Theia CLI installed.\nWhat is the different commands available:\n1linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection --help 2Command group of Theia throughput anomaly detection feature. 3\tMust specify a subcommand like run, list, delete, status or retrieve 4 5Usage: 6 theia throughput-anomaly-detection [flags] 7 theia throughput-anomaly-detection [command] 8 9Aliases: 10 throughput-anomaly-detection, tad 11 12Available Commands: 13 delete Delete a anomaly detection job 14 list List all anomaly detection jobs 15 retrieve Get the result of an anomaly detection job 16 run throughput anomaly detection using Algo 17 status Check the status of a anomaly detection job 18 19Flags: 20 -h, --help help for throughput-anomaly-detection 21 --use-cluster-ip Enable this option will use ClusterIP instead of port forwarding when connecting to the Theia 22 Manager Service. It can only be used when running in cluster. 23 24Global Flags: 25 -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified 26 -v, --verbose int set verbose level 27 28Use \u0026#34;theia throughput-anomaly-detection [command] --help\u0026#34; for more information about a command. 1linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection run --help 2throughput anomaly detection using algorithms, currently supported algorithms are EWMA, ARIMA and DBSCAN 3 4Usage: 5 theia throughput-anomaly-detection run [flags] 6 7Examples: 8Run the specific algorithm for throughput anomaly detection 9\t$ theia throughput-anomaly-detection run --algo ARIMA --start-time 2022-01-01T00:00:00 --end-time 2022-01-31T23:59:59 10\tRun throughput anomaly detection algorithm of type ARIMA and limit on flow records from \u0026#39;2022-01-01 00:00:00\u0026#39; to \u0026#39;2022-01-31 23:59:59\u0026#39; 11\tPlease note, algo is a mandatory argument\u0026#39; 12 13Flags: 14 -a, --algo string The algorithm used by throughput anomaly detection. 15 Currently supported Algorithms are EWMA, ARIMA and DBSCAN. 16 --driver-core-request string Specify the CPU request for the driver Pod. Values conform to the Kubernetes resource quantity convention. 17 Example values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;) 18 --driver-memory string Specify the memory request for the driver Pod. Values conform to the Kubernetes resource quantity convention. 19 Example values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;) 20 -e, --end-time string The end time of the flow records considered for the anomaly detection. 21 Format is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the end time of flow records by default. 22 --executor-core-request string Specify the CPU request for the executor Pod. Values conform to the Kubernetes resource quantity convention. 23 Example values include 0.1, 500m, 1.5, 5, etc. (default \u0026#34;200m\u0026#34;) 24 --executor-instances int32 Specify the number of executors for the Spark application. Example values include 1, 2, 8, etc. (default 1) 25 --executor-memory string Specify the memory request for the executor Pod. Values conform to the Kubernetes resource quantity convention. 26 Example values include 512M, 1G, 8G, etc. (default \u0026#34;512M\u0026#34;) 27 -h, --help help for run 28 -n, --ns-ignore-list string List of default drop Namespaces. Use this to ignore traffic from selected namespaces 29 If no Namespaces provided, Traffic from all namespaces present in flows table will be allowed by default. 30 -s, --start-time string The start time of the flow records considered for the anomaly detection. 31 Format is YYYY-MM-DD hh:mm:ss in UTC timezone. No limit of the start time of flow records by default. 32 33Global Flags: 34 -k, --kubeconfig string absolute path to the k8s config file, will use $KUBECONFIG if not specified 35 --use-cluster-ip Enable this option will use ClusterIP instead of port forwarding when connecting to the Theia 36 Manager Service. It can only be used when running in cluster. 37 -v, --verbose int set verbose level I will use the example above:\n1linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection run --algo ARIMA --start-time 2023-06-06T00:00:00 --end-time 2023-06-08T09:00:00 2Successfully started Throughput Anomaly Detection job with name: tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 1linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection list 2CreationTime CompletionTime Name Status 32023-06-08 08:25:10 N/A tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 RUNNING 4linuxvm01:~/antrea/theia$ theia throughput-anomaly-detection status tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 5Status of this anomaly detection job is RUNNING: 0/0 (0%) stages completed Lets wait for it to finish...\n1linuxvm01:~$ theia throughput-anomaly-detection list 2CreationTime CompletionTime Name Status 32023-06-08 08:25:10 2023-06-08 08:40:03 tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 COMPLETED Now check the output:\n1# It is a long list so I am piping it to a text file 2linuxvm01:~$ theia throughput-anomaly-detection retrieve tad-2ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 \u0026gt; anomaly-detection-1.txt A snippet from the output:\n1id sourceIP sourceTransportPort destinationIP destinationTransportPort flowStartSeconds flowEndSeconds throughput algoCalc anomaly 22ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T21:41:38Z 54204 65355.16485680155 true 32ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T07:09:32Z 49901 54713.50251767502 true 42ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T03:33:48Z 50000 53550.532983008845 true 52ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T00:52:48Z 59725 52206.69079880149 true 62ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T18:49:03Z 48544 53287.107990749006 true 72ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T22:06:43Z 61832 53100.99541753638 true 82ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T20:57:28Z 58295 54168.70924924757 true 92ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T07:36:38Z 47309 53688.236655529385 true 102ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T08:05:43Z 59227 52623.71668244673 true 112ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T05:22:12Z 58217 53709.42205164235 true 122ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T02:19:03Z 48508 55649.8819138477 true 132ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-07T23:27:28Z 53846 48125.33491950862 true 142ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T00:09:38Z 59562 52143.367660610136 true 152ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T02:44:08Z 50966 57119.323329628125 true 162ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T08:24:17Z 55553 50480.7443391562 true 172ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T00:02:38Z 44172 53694.11880964807 true 182ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T04:07:57Z 53612 49714.00885995446 true 192ecb054a-8c0d-4ae1-8444-c3493e7bb6d9 20.20.3.12 59448 20.20.0.12 2181 2023-06-06T22:04:43Z 2023-06-08T01:54:58Z 59089 51972.42465384903 true Antrea Egress This chapter is also covered in another post I have done here with some tweaks ðŸ˜ƒ\nFrom the offical docs\nEgress is a CRD API that manages external access from the Pods in a cluster. It supports specifying which egress (SNAT) IP the traffic from the selected Pods to the external network should use. When a selected Pod accesses the external network, the egress traffic will be tunneled to the Node that hosts the egress IP if itâ€™s different from the Node that the Pod runs on and will be SNATed to the egress IP when leaving that Node.\nYou may be interested in using this capability if any of the following apply:\nA consistent IP address is desired when specific Pods connect to services outside of the cluster, for source tracing in audit logs, or for filtering by source IP in external firewall, etc. You want to force outgoing external connections to leave the cluster via certain Nodes, for security controls, or due to network topology restrictions. ","link":"https://blog.andreasm.io/2023/06/01/managing-antrea-in-vsphere-with-tanzu/","section":"post","tags":["antrea","tanzu","nsx-t"],"title":"Managing Antrea in vSphere with Tanzu"},{"body":"","link":"https://blog.andreasm.io/tags/nsx-t/","section":"tags","tags":null,"title":"nsx-t"},{"body":"","link":"https://blog.andreasm.io/categories/loadbalancing/","section":"categories","tags":null,"title":"Loadbalancing"},{"body":"","link":"https://blog.andreasm.io/tags/multi-dc/","section":"tags","tags":null,"title":"multi-dc"},{"body":"","link":"https://blog.andreasm.io/categories/networking/","section":"categories","tags":null,"title":"Networking"},{"body":"","link":"https://blog.andreasm.io/tags/nsx-advanced-loadbalancer/","section":"tags","tags":null,"title":"nsx advanced loadbalancer"},{"body":"Overview of what this post is about In this post my goal is to use TKG (Tanzu Kubernetes Grid here) to manage and deploy workload clusters in remote datacenter's. The reason for such needs can be many like easier lifecycle management of workload clusters in environments where we have several datacenters, with different physical locations. To lower the management overhead and simplify lifecycle management, like updates, being able to centrally manage these operations is sometimes key.\nSo in this post I have two datacenters, with vSphere clusters each being managed by their own vCenter Server. NSX is installed in both datacenters managing the network in their respective datacenter. NSX Advanced Loadbalancer (Avi) is deployed in datacenter 1 and is managing both datacenters (this post will not cover GSLB and having Avi controllers deployed in both datacenters and why this also could be a smart thing to consider). This Avi installation will be responsible for creating SE's (Service Engines) and virtual services for the datacenter it is deployed in as well as the remote datacenter (datacenter 2). This will be the only controller/manager that is being shared across the two datacenters.\nSo trying to illustrate the above with the following diagram:\nI will not go into how connectivity between these sites are established, I just assume that relevant connectivity between the sites are in place (as that is a requirement for this to work).\nMeet the \u0026quot;moving parts\u0026quot; involved This section will quickly go through the components being used in their different datacenters. First out is the components used in the datacenter 1 environment.\nDatacenter 1 In datacenter 1 I have one vSphere cluster consisting of four ESXi hosts being managed by a vCenter server. This datacenter will be my managment datacenter where I will deploy my TKG management cluster. It is placed in physical location 1 with its own physical network and storage (using vSAN).\nThis is vSphere cluster in datacenter 1:\nIn datacenter 1 I have also installed NSX-T to handle all the networking needs in this datacenter. There is no stretching of networks between the datacenters, the NSX environment is only responsible for the same datacenter it is installed in as you can see below:\nIt also has a 1:1 relationship to the vCenter Server in datacenter 1:\nThis NSX environment has created the following networks to support my TKG managment cluster deployment:\nAnd from the vCenter:\nI will quickly just describe the different networks:\nls-avi-dns-se-data: This is where I place the dataplane of my SE's used for the DNS service in Avi ls-avi-se-data: This is where I place the dataplane of my SE's used for my other virtual services (regular application needs, or if I happen to deploy services in my TKG mgmt cluster or a workload cluster in the same datacenter.) This network will not be used in this post. ls-mgmt: is where I place the management interface of my SE's. ls-tkg-mgmt: This will be used in this post and is where my TKG management cluster nodes will be placed. The ls-tkg-mgmt network has also been configured with DHCP on the segment in NSX:\nAnd last but not least component, the Avi controller.\nThis is the component in this post that has been configured to handle requests from both datacenters as a shared resource, whether it is regular layer-4 services like servicetype loadBalancer or layer-7 services like Ingress. As both datacenters are being managed by their own NSX-T I have configured the Avi controller to use both NSX-T environments as two different clouds:\nEach cloud depicted above is reflecting the two different datacenters and have been configured accordingly to support the network settings in each datacenter respectively.\nEach cloud is a NSX-T cloud and have its own unique configurations that matches the configuration for the respective datacenter the cloud is in. Networks, IPAM/DNS profiles, routing contexts, service engine groups. Below is some screenshots from the Avi controller:\nService engine groups in the stc-nsx-cloud:\nThe above SE groups have been configured for placement in their respective vSphere clusters, folder, naming, datastore etc.\nThe above networks have been configured to provision IP addresses using Avi IPAM to automate SE's dataplane creation.\nThe below networks are the vip networks configured in the stc-nsx-cloud:\nThen the routing context (or VRF context) for the SE's to reach the backend\nThe same has been done for the wdc-nsx-cloud. I will not print them here, but just show that there's is also a wdc-cloud configured in these sections also:\nNotice the difference in IP subnets.\nThen its the IPAM DNS profiles for both clouds:\nInstead of going into too many details how to configure Avi, its all about to configure it to support the infrastructure settings in each datacenter. Then when the requests for virtual services come to the Avi controller it knows how to handle the requests and create the virtual services, the service engines and do the ip addressing correctly. Then this part will just work butter smooth.\nAn overview of the components in datacenter 1: Datacenter 2 In datacenter 2 I have also one vSphere cluster consisting of four ESXi hosts being managed by a vCenter server. This datacenter will be my remote/edge datacenter where I will deploy my TKG workload clusters. It is placed in physical location 2 with its own physical network and storage (using vSAN).\nThis is vSphere cluster in datacenter 2:\nIn datacenter 2 I have also installed NSX-T to handle all the networking needs in this datacenter. As mentioned above, there is no stretching of networks between the datacenters, the NSX environments is only responsible for the same datacenter it is installed in as you can see below:\nIt also has a 1:1 relationship to the vCenter Server in datacenter 1:\nThis NSX environment has created the following networks to support my TKG managment cluster deployment:\nAnd from the vCenter:\nI will quickly just describe the different networks:\nls-avi-dns-se-data: This is where I place the dataplane of my SE's used for the DNS service in Avi ls-avi-generic-se-data: This is where I place the dataplane of my SE's used for the virtual services created when I expose services from the workload clusters. This network will be used in this post. ls-mgmt: is where I place the management interface of my SE's. ls-tkg-wdc-wlc-1: This will be used as placement for my TKG workload cluster nodes in this datacenter. The ls-tkg-wdc-wlc-1 network has also been configured with DHCP on the segment in NSX:\nAn overview again of the components in DC2:\nThats it for the \u0026quot;moving parts\u0026quot; involved in both datacenters for this practice.\nTKG management cluster deployment Now finally for the fun parts. Deployment. As I have mentioned in the previous chapters, I will deploy the TKG management cluster in datacenter 1. But before I do the actual deployment I will need to explain a little around how a TKG cluster is reached, whether its the management cluster or the workload clusters.\nKubernetes API endpoint - exposing services inside the kubernetes clusters (tkg clusters) A Kubernetes cluster consist usually of 1 or 3 controlplane nodes. This is where the Kubernetes API endpoint lives. When interacting with Kubernetes we are using the exposed Kubernetes APIs to tell it declaratively (some say in a nice way) to realise something we want it to do. This api endpoint is usually exposed on port 6443, and will always be available on the control plane nodes, not on the worker nodes. So the first criteria to be met is connectivity to the control plane nodes on port 6443 (or ssh into the controlplane nodes themselves on port 22 and work with the kube-api from there, but not ideal). We want to reach the api from a remote workstation to be more flexible and effective in how we interact with the Kubernetes API. When having just 1 controlplane node it is probably just ok to reach this one controlplane node and send our api calls directly but with just one controlplane node this can create some issues down the road when we want to replace/upgrade this one node, it can change (most likely will) IP address. Meaning our kubeconfig context/automation tool needs to be updated accordingly. So what we want is a virtual ip address that will stay consistent across the lifetime of the Kubernetes cluster. The same is also when we have more than one controlplane node, 3 is a common number of controlplane nodes in production. We cant have an even number of controlplane nodes as we want quorum. We want to have 1 consistent IP address to reach either just the one controlplane node's Kubernetes API or 1 consistent IP address loadbalanced across all three controlplane nodes. To achieve that we need some kind of loadbalancer that can create this virtual ip address for us to expose the Kubernetes API consistently. In TKG we can use NSX Advanced Loabalancer for this purpose, or a simpler approach like Kube-VIP. I dont want to go into a big writeup on the difference between these two other than they are not comparable to each other. Kube-VIP will not loadbalance the Kubernetes API between the 3 control-plane nodes, it will just create a virtual ip in the same subnet as the controlplane nodes and be placed on one of the controlplane nodes, stay there until the node fails and move over to the other control-plane nodes. While NSX ALB will loadbalance the Kuberntes API endpoint between all three control-plane nodes and the IP address is automatically allocated on provisioning. Kube-VIP is statically assigned.\nInfo Why I am mentioning this? Why could I not just focus on NSX Advanced Loadbalancer that can cover all my needs? That is because in this specific post I am hitting a special use-case where I have my TKG management cluster placed in one datacenter managed by its own NSX-T, while I want to deploy and manage TKG workload clusters in a completely different datacenter also managed by its own NSX-T. By using NSX Advanced Loabalancer as my API endpoint VIP provider in combination with NSX-T Clouds (Avi Clouds) I am currently not allowed to override the control-plane network (API endpoint). It is currently not possible to override or select a different NSX-T Tier1 for the the control-plane network, as these are different due to two different NSX-T environments, I can name the Tier-1 routers identically in both datacenters, but its not so easily fooled ðŸ˜„ So my option to work around this is to use Kube-VIP. Kube-VIP allows me to configure manually the API endpoint IP for my workload clusters. I will try to explain a bit more how the NSX ALB integration works in TKG below.\nWhat about the services I want to expose from the different workload-clusters like servicetype loadBalancer and Ingress? That is a different story, there we can use NSX Advanced Loadbalancer as much as we want and in a very flexible way too. The reason for that is that the Kubernetes API endpoint VIP or controlplane network is something that is managed and controlled by the TKG management cluster while whats coming from the inside of a working TKG workload cluster is completely different. Using NSX Advanced Loadbalancer in TKG or in any other Kubernetes platform like native upstream Kubernetes we use a component called AKO (Avi Kubernetes Operator) that handles all the standard Kubernetes requests like servicetype loadBalancer and Ingress creation and forwards them to the NSX ALB controller to realize them. In TKG we have AKO running in the management cluster that is responsible for the services being exposed from inside the TKG management cluster, but also assigning the VIP for the workload clusters Kubernetes API (controlplane network). As soon as we have our first TKG workload cluster, this comes with its own AKO that is responsible for all the services in the workload cluster it runs in, it will not have anything to do with the controlplane network and the AKO running in the TKG management cluster. So we can actually adjust this AKO instance to match our needs there without being restricted to what the AKO instance in the TKG management cluster is configured with.\nIn a TKG workload cluster there is a couple of ways to get AKO installed. One option is to use the AKO Operator running in the TKG management cluster to deploy it automatically on TKG workload cluster provisioning. This approach is best if you want TKG to handle the lifecycle of the AKO instance, like upgrades and it is very hands-off. We need to define an AkoDeploymentConfig in the TKG management cluster that defines the AKO settings for the respective TKG workload cluster or clusters if they can share the same settings. This is based on labels so its very easy to create the ADC for a series of clusters or specific cluster by applying the correct label on the cluster. The other option is to install AKO via Helm, this gives you full flexibility but is a manual process that needs to be done on all TKG workload clusters that needs AKO installed. I tend to lean on the ADC approach as I cant see any limitation this approach has compared to the AKO via Helm approach. ADC also supports AviInfraSettings which gives you further flexibility and options.\nWith that out of the way let us get this TKG management cluster deployed already...\nTKG management cluster deployment - continued I will not cover any of the pre-reqs to deploy TKG, for that have a look here, I will just go straight to it. My TKG managment cluster bootstrap yaml manifest. Below I will paste my yaml for the TKG mgmt cluster with some comments that I have done to make use of Kube-VIP for the controlplane, aka Kubernetes API endpoint.\n1#! --------------- 2#! Basic config 3#! ------------- 4CLUSTER_NAME: tkg-stc-mgmt-cluster 5CLUSTER_PLAN: dev 6INFRASTRUCTURE_PROVIDER: vsphere 7ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; 8ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; 9CLUSTER_CIDR: 100.96.0.0/11 10SERVICE_CIDR: 100.64.0.0/13 11TKG_IP_FAMILY: ipv4 12DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; 13CLUSTER_API_SERVER_PORT: 6443 #Added for Kube-VIP 14VSPHERE_CONTROL_PLANE_ENDPOINT: 10.13.20.100 #Added for Kube-VIP - specify a static IP in same subnet as nodes 15VSPHERE_CONTROL_PLANE_ENDPOINT_PORT: 6443 #Added for Kube-VIP 16VIP_NETWORK_INTERFACE: \u0026#34;eth0\u0026#34; #Added for Kube-VIP 17# VSPHERE_ADDITIONAL_FQDN: 18AVI_CONTROL_PLANE_HA_PROVIDER: false #Set to false to use Kube-VIP instead 19AVI_ENABLE: \u0026#34;true\u0026#34; #I still want AKO to be installed, but not used for controplane endpoint 20 21#! --------------- 22#! vSphere config 23#! ------------- 24VSPHERE_DATACENTER: /cPod-NSXAM-STC 25VSPHERE_DATASTORE: /cPod-NSXAM-STC/datastore/vsanDatastore 26VSPHERE_FOLDER: /cPod-NSXAM-STC/vm/TKGm 27VSPHERE_INSECURE: \u0026#34;false\u0026#34; 28VSPHERE_NETWORK: /cPod-NSXAM-STC/network/ls-tkg-mgmt 29VSPHERE_PASSWORD: \u0026#34;password\u0026#34; 30VSPHERE_RESOURCE_POOL: /cPod-NSXAM-STC/host/Cluster/Resources 31#VSPHERE_TEMPLATE: /Datacenter/vm/TKGm/ubuntu-2004-kube-v1.23.8+vmware.2 32VSPHERE_SERVER: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net 33VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa ssh-public key 34VSPHERE_TLS_THUMBPRINT: vcenter SHA1 35VSPHERE_USERNAME: username@domain.net 36 37#! --------------- 38#! Node config 39#! ------------- 40OS_ARCH: amd64 41OS_NAME: ubuntu 42OS_VERSION: \u0026#34;20.04\u0026#34; 43VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; 44VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; 45VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; 46VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; 47VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; 48VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; 49CONTROL_PLANE_MACHINE_COUNT: 1 50WORKER_MACHINE_COUNT: 2 51 52#! --------------- 53#! Avi config 54#! ------------- 55AVI_CA_DATA_B64: AVI Controller Base64 Certificate 56AVI_CLOUD_NAME: stc-nsx-cloud 57AVI_CONTROLLER: 172.24.3.50 58# Network used to place workload clusters\u0026#39; endpoint VIPs 59#AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 60#AVI_CONTROL_PLANE_NETWORK_CIDR: 10.13.102.0/24 61# Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 62AVI_DATA_NETWORK: vip-tkg-wld-l7 63AVI_DATA_NETWORK_CIDR: 10.13.103.0/24 64# Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 65AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.13.101.0/24 66AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 67# Network used to place management clusters\u0026#39; endpoint VIPs 68#AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 69#AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.13.100.0/24 70AVI_NSXT_T1LR: Tier-1 71AVI_CONTROLLER_VERSION: 22.1.2 72AVI_LABELS: \u0026#34;{adc-enabled: \u0026#39;true\u0026#39;}\u0026#34; #Added so I can select easily which workload cluster that will use this AKO config 73AVI_PASSWORD: \u0026#34;password\u0026#34; 74AVI_SERVICE_ENGINE_GROUP: stc-nsx 75AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: tkgm-se-group 76AVI_USERNAME: admin 77AVI_DISABLE_STATIC_ROUTE_SYNC: false 78AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true 79AVI_INGRESS_SHARD_VS_SIZE: SMALL 80AVI_INGRESS_SERVICE_TYPE: NodePortLocal 81 82#! --------------- 83#! Proxy config 84#! ------------- 85TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; 86 87#! --------------------------------------------------------------------- 88#! Antrea CNI configuration 89#! --------------------------------------------------------------------- 90# ANTREA_NO_SNAT: false 91# ANTREA_TRAFFIC_ENCAP_MODE: \u0026#34;encap\u0026#34; 92# ANTREA_PROXY: false 93# ANTREA_POLICY: true 94# ANTREA_TRACEFLOW: false 95ANTREA_NODEPORTLOCAL: true 96ANTREA_PROXY: true 97ANTREA_ENDPOINTSLICE: true 98ANTREA_POLICY: true 99ANTREA_TRACEFLOW: true 100ANTREA_NETWORKPOLICY_STATS: false 101ANTREA_EGRESS: true 102ANTREA_IPAM: false 103ANTREA_FLOWEXPORTER: false 104ANTREA_SERVICE_EXTERNALIP: false 105ANTREA_MULTICAST: false 106 107#! --------------------------------------------------------------------- 108#! Machine Health Check configuration 109#! --------------------------------------------------------------------- 110ENABLE_MHC: \u0026#34;true\u0026#34; 111ENABLE_MHC_CONTROL_PLANE: true 112ENABLE_MHC_WORKER_NODE: true 113MHC_UNKNOWN_STATUS_TIMEOUT: 5m 114MHC_FALSE_STATUS_TIMEOUT: 12m 115 116#! --------------------------------------------------------------------- 117#! Identity management configuration 118#! --------------------------------------------------------------------- 119 120IDENTITY_MANAGEMENT_TYPE: none All the configs above should match the datacenter 1 environment so the TKG management cluster can be deployed. Lets deploy it using Tanzu CLI from my TKG bootstrap client:\n1tanzu mc create -f tkg-mgmt-bootstrap.yaml As soon as it is deployed grab the k8s config and add it to your context:\n1tanzu mc kubeconfig get --admin --export-file stc-tkgm-mgmt-cluster.yaml The IP address used for the Kubernetes API endpoint is the controlplane IP defined above:\n1VSPHERE_CONTROL_PLANE_ENDPOINT: 10.13.20.100 We can also see this IP being assigned to my one controlplane node in the vCenter view:\nNow just have a quick look inside the TKG mgmt cluster and specifically after AKO and eventual ADC:\n1tkg-bootstrap-vm:~/Kubernetes-library/examples/ingress$ k get pods -A 2NAMESPACE NAME READY STATUS RESTARTS AGE 3avi-system ako-0 1/1 Running 0 8h 4capi-kubeadm-bootstrap-system capi-kubeadm-bootstrap-controller-manager-5fb8fbc6c7-rqkzf 1/1 Running 0 8h 5capi-kubeadm-control-plane-system capi-kubeadm-control-plane-controller-manager-78c559f48c-cj2dm 1/1 Running 0 8h 6capi-system capi-controller-manager-84fbb669c-bhk4j 1/1 Running 0 8h 7capv-system capv-controller-manager-5f46567b86-pccf5 1/1 Running 0 8h 8cert-manager cert-manager-5d8d7b4dfb-gj6h2 1/1 Running 0 9h 9cert-manager cert-manager-cainjector-7797ff666f-zxh5l 1/1 Running 0 9h 10cert-manager cert-manager-webhook-59969cbb8c-vpsgr 1/1 Running 0 9h 11kube-system antrea-agent-6xzvh 2/2 Running 0 8h 12kube-system antrea-agent-gsfhc 2/2 Running 0 8h 13kube-system antrea-agent-t5gzb 2/2 Running 0 8h 14kube-system antrea-controller-74b468c659-hcrgp 1/1 Running 0 8h 15kube-system coredns-5d4666ccfb-qx5qt 1/1 Running 0 9h 16kube-system coredns-5d4666ccfb-xj47b 1/1 Running 0 9h 17kube-system etcd-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h 18kube-system kube-apiserver-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h 19kube-system kube-controller-manager-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h 20kube-system kube-proxy-9d7b9 1/1 Running 0 9h 21kube-system kube-proxy-kd8h8 1/1 Running 0 9h 22kube-system kube-proxy-n7zwx 1/1 Running 0 9h 23kube-system kube-scheduler-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h 24kube-system kube-vip-tkg-stc-mgmt-cluster-sbptz-lkn58 1/1 Running 0 9h 25kube-system metrics-server-b468f4d5f-hvtbg 1/1 Running 0 8h 26kube-system vsphere-cloud-controller-manager-fnsvh 1/1 Running 0 8h 27secretgen-controller secretgen-controller-697cb6c657-lh9rr 1/1 Running 0 8h 28tanzu-auth tanzu-auth-controller-manager-d75d85899-d8699 1/1 Running 0 8h 29tkg-system-networking ako-operator-controller-manager-5bbb9d4c4b-2bjsk 1/1 Running 0 8h 30tkg-system kapp-controller-9f9f578c7-dpzgk 2/2 Running 0 9h 31tkg-system object-propagation-controller-manager-5cbb94894f-k56w5 1/1 Running 0 8h 32tkg-system tanzu-addons-controller-manager-79f656b4c7-m72xw 1/1 Running 0 8h 33tkg-system tanzu-capabilities-controller-manager-5868c5f789-nbkgm 1/1 Running 0 8h 34tkg-system tanzu-featuregates-controller-manager-6d567fffd6-647s5 1/1 Running 0 8h 35tkg-system tkr-conversion-webhook-manager-6977bfc965-gjjbt 1/1 Running 0 8h 36tkg-system tkr-resolver-cluster-webhook-manager-5c8484ffd8-8xc8n 1/1 Running 0 8h 37tkg-system tkr-source-controller-manager-57c56d55d9-x6vsz 1/1 Running 0 8h 38tkg-system tkr-status-controller-manager-55b4b845b9-77snb 1/1 Running 0 8h 39tkg-system tkr-vsphere-resolver-webhook-manager-6476749d5d-5pxlk 1/1 Running 0 8h 40vmware-system-csi vsphere-csi-controller-585bf4dc75-wtlw2 7/7 Running 0 8h 41vmware-system-csi vsphere-csi-node-ldrs6 3/3 Running 2 (8h ago) 8h 42vmware-system-csi vsphere-csi-node-rwgpw 3/3 Running 4 (8h ago) 8h 43vmware-system-csi vsphere-csi-node-rx8f6 3/3 Running 4 (8h ago) 8h There is an AKO pod running. Are there any ADCs created?\n1tkg-bootstrap-vm:~/Kubernetes-library/examples/ingress$ k get adc 2NAME AGE 3install-ako-for-all 8h 4install-ako-for-management-cluster 8h Lets have a look inside both of them, first out install-aka-for-all and then install-ako-for-management-cluster\n1# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, 2# and an empty file will abort the edit. If an error occurs while saving this file will be 3# reopened with the relevant failures. 4# 5apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 6kind: AKODeploymentConfig 7metadata: 8 annotations: 9 kapp.k14s.io/identity: v1;/networking.tkg.tanzu.vmware.com/AKODeploymentConfig/install-ako-for-all;networking.tkg.tanzu.vmware.com/v1alpha1 10 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.tkg.tanzu.vmware.com/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;AKODeploymentConfig\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685446688101132090\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.8329c3602ed02133e324fc22d58dcf28\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;install-ako-for-all\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;adminCredentialRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-credentials\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;certificateAuthorityRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-ca\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterSelector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;adc-enabled\u0026#34;:\u0026#34;true\u0026#34;}},\u0026#34;controlPlaneNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.101.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-mgmt-l7\u0026#34;},\u0026#34;controller\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;dataNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.103.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-wld-l7\u0026#34;},\u0026#34;extraConfigs\u0026#34;:{\u0026#34;disableStaticRouteSync\u0026#34;:false,\u0026#34;ingress\u0026#34;:{\u0026#34;defaultIngressController\u0026#34;:false,\u0026#34;disableIngressClass\u0026#34;:true,\u0026#34;nodeNetworkList\u0026#34;:[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-mgmt\u0026#34;}]},\u0026#34;networksConfig\u0026#34;:{\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;Tier-1\u0026#34;}},\u0026#34;serviceEngineGroup\u0026#34;:\u0026#34;stc-nsx\u0026#34;}}\u0026#39; 11 kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 12 creationTimestamp: \u0026#34;2023-05-30T11:38:45Z\u0026#34; 13 finalizers: 14 - ako-operator.networking.tkg.tanzu.vmware.com 15 generation: 2 16 labels: 17 kapp.k14s.io/app: \u0026#34;1685446688101132090\u0026#34; 18 kapp.k14s.io/association: v1.8329c3602ed02133e324fc22d58dcf28 19 name: install-ako-for-all 20 resourceVersion: \u0026#34;4686\u0026#34; 21 uid: 0cf0dd57-b193-40d5-bb03-347879157377 22spec: 23 adminCredentialRef: 24 name: avi-controller-credentials 25 namespace: tkg-system-networking 26 certificateAuthorityRef: 27 name: avi-controller-ca 28 namespace: tkg-system-networking 29 cloudName: stc-nsx-cloud 30 clusterSelector: 31 matchLabels: 32 adc-enabled: \u0026#34;true\u0026#34; 33 controlPlaneNetwork: 34 cidr: 10.13.101.0/24 35 name: vip-tkg-mgmt-l7 36 controller: 172.24.3.50 37 controllerVersion: 22.1.3 38 dataNetwork: 39 cidr: 10.13.103.0/24 40 name: vip-tkg-wld-l7 41 extraConfigs: 42 disableStaticRouteSync: false 43 ingress: 44 defaultIngressController: false 45 disableIngressClass: true 46 nodeNetworkList: 47 - networkName: ls-tkg-mgmt 48 networksConfig: 49 nsxtT1LR: Tier-1 50 serviceEngineGroup: stc-nsx This is clearly configured for my datacenter 1, and will not match my datacenter 2 environment. Also notice the label, if I do create cluster and apply this label I will get the \u0026quot;default\u0026quot; ADC applied which will not match what I have to use in datacenter 2.\nLets have a look at the last one:\n1# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, 2# and an empty file will abort the edit. If an error occurs while saving this file will be 3# reopened with the relevant failures. 4# 5apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 6kind: AKODeploymentConfig 7metadata: 8 annotations: 9 kapp.k14s.io/identity: v1;/networking.tkg.tanzu.vmware.com/AKODeploymentConfig/install-ako-for-management-cluster;networking.tkg.tanzu.vmware.com/v1alpha1 10 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;networking.tkg.tanzu.vmware.com/v1alpha1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;AKODeploymentConfig\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685446688101132090\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.3012c3c8e0fa37b13f4916c7baca1863\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;install-ako-for-management-cluster\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;adminCredentialRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-credentials\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;certificateAuthorityRef\u0026#34;:{\u0026#34;name\u0026#34;:\u0026#34;avi-controller-ca\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;tkg-system-networking\u0026#34;},\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterSelector\u0026#34;:{\u0026#34;matchLabels\u0026#34;:{\u0026#34;cluster-role.tkg.tanzu.vmware.com/management\u0026#34;:\u0026#34;\u0026#34;}},\u0026#34;controlPlaneNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.101.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-mgmt-l7\u0026#34;},\u0026#34;controller\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;dataNetwork\u0026#34;:{\u0026#34;cidr\u0026#34;:\u0026#34;10.13.101.0/24\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;vip-tkg-mgmt-l7\u0026#34;},\u0026#34;extraConfigs\u0026#34;:{\u0026#34;disableStaticRouteSync\u0026#34;:false,\u0026#34;ingress\u0026#34;:{\u0026#34;defaultIngressController\u0026#34;:false,\u0026#34;disableIngressClass\u0026#34;:true,\u0026#34;nodeNetworkList\u0026#34;:[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-mgmt\u0026#34;}]},\u0026#34;networksConfig\u0026#34;:{\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;Tier-1\u0026#34;}},\u0026#34;serviceEngineGroup\u0026#34;:\u0026#34;tkgm-se-group\u0026#34;}}\u0026#39; 11 kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 12 creationTimestamp: \u0026#34;2023-05-30T11:38:45Z\u0026#34; 13 finalizers: 14 - ako-operator.networking.tkg.tanzu.vmware.com 15 generation: 2 16 labels: 17 kapp.k14s.io/app: \u0026#34;1685446688101132090\u0026#34; 18 kapp.k14s.io/association: v1.3012c3c8e0fa37b13f4916c7baca1863 19 name: install-ako-for-management-cluster 20 resourceVersion: \u0026#34;4670\u0026#34; 21 uid: c41e6e39-2b0f-4fa4-9245-0eec1bcf6b5d 22spec: 23 adminCredentialRef: 24 name: avi-controller-credentials 25 namespace: tkg-system-networking 26 certificateAuthorityRef: 27 name: avi-controller-ca 28 namespace: tkg-system-networking 29 cloudName: stc-nsx-cloud 30 clusterSelector: 31 matchLabels: 32 cluster-role.tkg.tanzu.vmware.com/management: \u0026#34;\u0026#34; 33 controlPlaneNetwork: 34 cidr: 10.13.101.0/24 35 name: vip-tkg-mgmt-l7 36 controller: 172.24.3.50 37 controllerVersion: 22.1.3 38 dataNetwork: 39 cidr: 10.13.101.0/24 40 name: vip-tkg-mgmt-l7 41 extraConfigs: 42 disableStaticRouteSync: false 43 ingress: 44 defaultIngressController: false 45 disableIngressClass: true 46 nodeNetworkList: 47 - networkName: ls-tkg-mgmt 48 networksConfig: 49 nsxtT1LR: Tier-1 50 serviceEngineGroup: tkgm-se-group The same is true for this one. Configured for my datacenter 1, only major difference being a different dataNetwork. So if I decide to deploy a workload cluster in the same datacenter it would be fine, but I dont want that I want my workload cluster to be in a different datacenter.. Lets do that..\nTKG workload cluster deployment - with corresponding ADC Before I deploy my workload cluster I will create a \u0026quot;custom\u0026quot; ADC specific for the datacenter 2 where I will deploy the workload cluster. Lets paste my ADC for the dc 2 workload cluster:\n1apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 2kind: AKODeploymentConfig 3metadata: 4 name: ako-tkg-wdc-cloud 5spec: 6 adminCredentialRef: 7 name: avi-controller-credentials 8 namespace: tkg-system-networking 9 certificateAuthorityRef: 10 name: avi-controller-ca 11 namespace: tkg-system-networking 12 cloudName: wdc-nsx-cloud 13 clusterSelector: 14 matchLabels: 15 avi-cloud: \u0026#34;wdc-nsx-cloud\u0026#34; 16 controller: 172.24.3.50 17 dataNetwork: 18 cidr: 10.101.221.0/24 19 name: tkg-wld-1-apps 20 extraConfigs: 21 cniPlugin: antrea 22 disableStaticRouteSync: false # required 23 ingress: 24 defaultIngressController: true 25 disableIngressClass: false # required 26 nodeNetworkList: # required 27 - cidrs: 28 - 10.101.13.0/24 29 networkName: ls-tkg-wdc-wld-1 30 serviceType: NodePortLocal # required 31 shardVSSize: SMALL # required 32 l4Config: 33 autoFQDN: default 34 networksConfig: 35 nsxtT1LR: /infra/tier-1s/Tier-1 36 serviceEngineGroup: wdc-se-group In this ADC I will configure the dataNetwork for the VIP network I have defined in the NSX ALB DC 2 cloud, pointing to the NSX-T Tier1 (yes they have the same name as in DC1, but they are not the same), nodeNetworkList matching where my workload cluster nodes will be placed in DC 2. And also notice the label, for my workload cluster to use this ADC I will need to apply this label either during provisioning or label it after creation. Apply the ADC:\n1k apply -f ako-wld-cluster-1.wdc.cloud.yaml Is it there:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm$ k get adc 2NAME AGE 3ako-tkg-wdc-cloud 20s 4install-ako-for-all 9h 5install-ako-for-management-cluster 9h Yes it is.\nNow prepare the TKG workload cluster manifest to match the DC 2 environment and apply it, also making sure aviAPIServerHAProvider is set to false.\n1apiVersion: cpi.tanzu.vmware.com/v1alpha1 2kind: VSphereCPIConfig 3metadata: 4 name: wdc-tkgm-wld-cluster-1 5 namespace: ns-wdc-1 6spec: 7 vsphereCPI: 8 ipFamily: ipv4 9 mode: vsphereCPI 10 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 11 vmNetwork: 12 excludeExternalSubnetCidr: 10.101.13.100/32 13 excludeInternalSubnetCidr: 10.101.13.100/32 14--- 15apiVersion: csi.tanzu.vmware.com/v1alpha1 16kind: VSphereCSIConfig 17metadata: 18 name: wdc-tkgm-wld-cluster-1 19 namespace: ns-wdc-1 20spec: 21 vsphereCSI: 22 config: 23 datacenter: /cPod-NSXAM-WDC 24 httpProxy: \u0026#34;\u0026#34; 25 httpsProxy: \u0026#34;\u0026#34; 26 noProxy: \u0026#34;\u0026#34; 27 region: null 28 tlsThumbprint: vCenter SHA-1 of DC2 29 useTopologyCategories: false 30 zone: null 31 mode: vsphereCSI 32--- 33apiVersion: run.tanzu.vmware.com/v1alpha3 34kind: ClusterBootstrap 35metadata: 36 annotations: 37 tkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.25.7---vmware.2-tkg.1 38 name: wdc-tkgm-wld-cluster-1 39 namespace: ns-wdc-1 40spec: 41 additionalPackages: 42 - refName: metrics-server* 43 - refName: secretgen-controller* 44 - refName: pinniped* 45 cpi: 46 refName: vsphere-cpi* 47 valuesFrom: 48 providerRef: 49 apiGroup: cpi.tanzu.vmware.com 50 kind: VSphereCPIConfig 51 name: wdc-tkgm-wld-cluster-1 52 csi: 53 refName: vsphere-csi* 54 valuesFrom: 55 providerRef: 56 apiGroup: csi.tanzu.vmware.com 57 kind: VSphereCSIConfig 58 name: wdc-tkgm-wld-cluster-1 59 kapp: 60 refName: kapp-controller* 61--- 62apiVersion: v1 63kind: Secret 64metadata: 65 name: wdc-tkgm-wld-cluster-1 66 namespace: ns-wdc-1 67stringData: 68 password: password vCenter User 69 username: user@vcenter.net 70--- 71apiVersion: cluster.x-k8s.io/v1beta1 72kind: Cluster 73metadata: 74 annotations: 75 osInfo: ubuntu,20.04,amd64 76 tkg.tanzu.vmware.com/cluster-controlplane-endpoint: 10.101.13.100 #here is the VIP for the workload k8s API - by Kube-VIP 77 tkg/plan: dev 78 labels: 79 tkg.tanzu.vmware.com/cluster-name: wdc-tkgm-wld-cluster-1 80 avi-cloud: \u0026#34;wdc-nsx-cloud\u0026#34; 81 name: wdc-tkgm-wld-cluster-1 82 namespace: ns-wdc-1 83spec: 84 clusterNetwork: 85 pods: 86 cidrBlocks: 87 - 20.10.0.0/16 88 services: 89 cidrBlocks: 90 - 20.20.0.0/16 91 topology: 92 class: tkg-vsphere-default-v1.0.0 93 controlPlane: 94 metadata: 95 annotations: 96 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 97 replicas: 1 98 variables: 99 - name: cni 100 value: antrea 101 - name: controlPlaneCertificateRotation 102 value: 103 activate: true 104 daysBefore: 90 105 - name: auditLogging 106 value: 107 enabled: false 108 - name: apiServerPort 109 value: 6443 110 - name: podSecurityStandard 111 value: 112 audit: baseline 113 deactivated: false 114 warn: baseline 115 - name: apiServerEndpoint 116 value: 10.101.13.100 #Here is the K8s API endpoint provided by Kube-VIP 117 - name: aviAPIServerHAProvider 118 value: false 119 - name: vcenter 120 value: 121 cloneMode: fullClone 122 datacenter: /cPod-NSXAM-WDC 123 datastore: /cPod-NSXAM-WDC/datastore/vsanDatastore-wdc-01 124 folder: /cPod-NSXAM-WDC/vm/TKGm 125 network: /cPod-NSXAM-WDC/network/ls-tkg-wdc-wld-1 126 resourcePool: /cPod-NSXAM-WDC/host/Cluster-1/Resources 127 server: vcsa.cpod-nsxam-wdc.az-wdc.cloud-garage.net 128 storagePolicyID: \u0026#34;\u0026#34; 129 template: /cPod-NSXAM-WDC/vm/ubuntu-2004-efi-kube-v1.25.7+vmware.2 130 tlsThumbprint: vCenter SHA1 DC2 131 - name: user 132 value: 133 sshAuthorizedKeys: 134 - ssh-rsa public key 135 - name: controlPlane 136 value: 137 machine: 138 diskGiB: 20 139 memoryMiB: 4096 140 numCPUs: 2 141 - name: worker 142 value: 143 count: 2 144 machine: 145 diskGiB: 20 146 memoryMiB: 4096 147 numCPUs: 2 148 version: v1.25.7+vmware.2-tkg.1 149 workers: 150 machineDeployments: 151 - class: tkg-worker 152 metadata: 153 annotations: 154 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 155 name: md-0 156 replicas: 2 Notice the label, it should match what you defined in the ADC.. avi-cloud: \u0026quot;wdc-nsx-cloud\u0026quot;\nNow apply the cluster.\n1tanzu cluster create -f wdc-tkg-wld-cluster-1.yaml After a cup of coffee it should be ready.\nLets check the cluster from the mgmt cluster context:\n1linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get cluster -n ns-wdc-1 wdc-tkgm-wld-cluster-1 2NAME PHASE AGE VERSION 3wdc-tkgm-wld-cluster-1 Provisioned 13m v1.25.7+vmware.2 Grab the k8s config and switch to the workload cluster context.\n1tanzu cluster kubeconfig get wdc-tkgm-wld-cluster-1 --namespace ns-wdc-1 --admin --export-file wdc-tkgm-wld-cluster-1-k8s-config.yaml Check the nodes:\n1linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-tkgm-wld-cluster-1-md-0-jkcjw-d7795fbb5-gxnpc Ready \u0026lt;none\u0026gt; 9h v1.25.7+vmware.2 10.101.13.26 10.101.13.26 Ubuntu 20.04.6 LTS 5.4.0-144-generic containerd://1.6.18-1-gdbc99e5b1 4wdc-tkgm-wld-cluster-1-md-0-jkcjw-d7795fbb5-ph5j4 Ready \u0026lt;none\u0026gt; 9h v1.25.7+vmware.2 10.101.13.42 10.101.13.42 Ubuntu 20.04.6 LTS 5.4.0-144-generic containerd://1.6.18-1-gdbc99e5b1 5wdc-tkgm-wld-cluster-1-s49w2-mz85r Ready control-plane 9h v1.25.7+vmware.2 10.101.13.41 10.101.13.41 Ubuntu 20.04.6 LTS 5.4.0-144-generic containerd://1.6.18-1-gdbc99e5b1 In my vCenter in DC 2?\nLets see them running pods:\n1linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get pods -A 2NAMESPACE NAME READY STATUS RESTARTS AGE 3avi-system ako-0 1/1 Running 0 8h 4kube-system antrea-agent-rhmsn 2/2 Running 0 8h 5kube-system antrea-agent-ttk6f 2/2 Running 0 8h 6kube-system antrea-agent-tw6t7 2/2 Running 0 8h 7kube-system antrea-controller-787994578b-7v2cl 1/1 Running 0 8h 8kube-system coredns-5d4666ccfb-b2j85 1/1 Running 0 8h 9kube-system coredns-5d4666ccfb-lr97g 1/1 Running 0 8h 10kube-system etcd-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h 11kube-system kube-apiserver-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h 12kube-system kube-controller-manager-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h 13kube-system kube-proxy-9m2zh 1/1 Running 0 8h 14kube-system kube-proxy-rntv8 1/1 Running 0 8h 15kube-system kube-proxy-t7z49 1/1 Running 0 8h 16kube-system kube-scheduler-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h 17kube-system kube-vip-wdc-tkgm-wld-cluster-1-s49w2-mz85r 1/1 Running 0 8h 18kube-system metrics-server-c6d9969cb-7h5l7 1/1 Running 0 8h 19kube-system vsphere-cloud-controller-manager-b2rkl 1/1 Running 0 8h 20secretgen-controller secretgen-controller-cd678b84c-cdntv 1/1 Running 0 8h 21tkg-system kapp-controller-6c5dfccc45-7nhl5 2/2 Running 0 8h 22tkg-system tanzu-capabilities-controller-manager-5bf587dcd5-fp6t9 1/1 Running 0 8h 23vmware-system-csi vsphere-csi-controller-5459886d8c-5jzlz 7/7 Running 0 8h 24vmware-system-csi vsphere-csi-node-6pfbj 3/3 Running 4 (8h ago) 8h 25vmware-system-csi vsphere-csi-node-cbcpm 3/3 Running 4 (8h ago) 8h 26vmware-system-csi vsphere-csi-node-knk8q 3/3 Running 2 (8h ago) 8h There is an AKO pod running. So far so good. How does the AKO configmap look like?\n1# Please edit the object below. Lines beginning with a \u0026#39;#\u0026#39; will be ignored, 2# and an empty file will abort the edit. If an error occurs while saving this file will be 3# reopened with the relevant failures. 4# 5apiVersion: v1 6data: 7 apiServerPort: \u0026#34;8080\u0026#34; 8 autoFQDN: default 9 cloudName: wdc-nsx-cloud 10 clusterName: ns-wdc-1-wdc-tkgm-wld-cluster-1 11 cniPlugin: antrea 12 controllerIP: 172.24.3.50 13 controllerVersion: 22.1.3 14 defaultIngController: \u0026#34;true\u0026#34; 15 deleteConfig: \u0026#34;false\u0026#34; 16 disableStaticRouteSync: \u0026#34;false\u0026#34; 17 fullSyncFrequency: \u0026#34;1800\u0026#34; 18 logLevel: INFO 19 nodeNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-wdc-wld-1\u0026#34;,\u0026#34;cidrs\u0026#34;:[\u0026#34;10.101.13.0/24\u0026#34;]}]\u0026#39; 20 nsxtT1LR: /infra/tier-1s/Tier-1 21 serviceEngineGroupName: wdc-se-group 22 serviceType: NodePortLocal 23 shardVSSize: SMALL 24 useDefaultSecretsOnly: \u0026#34;false\u0026#34; 25 vipNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;tkg-wld-1-apps\u0026#34;,\u0026#34;cidr\u0026#34;:\u0026#34;10.101.221.0/24\u0026#34;}]\u0026#39; 26kind: ConfigMap 27metadata: 28 annotations: 29 kapp.k14s.io/identity: v1;avi-system//ConfigMap/avi-k8s-config;v1 30 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;apiServerPort\u0026#34;:\u0026#34;8080\u0026#34;,\u0026#34;autoFQDN\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;cloudName\u0026#34;:\u0026#34;wdc-nsx-cloud\u0026#34;,\u0026#34;clusterName\u0026#34;:\u0026#34;ns-wdc-1-wdc-tkgm-wld-cluster-1\u0026#34;,\u0026#34;cniPlugin\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;controllerIP\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;controllerVersion\u0026#34;:\u0026#34;22.1.3\u0026#34;,\u0026#34;defaultIngController\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;deleteConfig\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;disableStaticRouteSync\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;fullSyncFrequency\u0026#34;:\u0026#34;1800\u0026#34;,\u0026#34;logLevel\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;nodeNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;ls-tkg-wdc-wld-1\\\u0026#34;,\\\u0026#34;cidrs\\\u0026#34;:[\\\u0026#34;10.101.13.0/24\\\u0026#34;]}]\u0026#34;,\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;/infra/tier-1s/Tier-1\u0026#34;,\u0026#34;serviceEngineGroupName\u0026#34;:\u0026#34;wdc-se-group\u0026#34;,\u0026#34;serviceType\u0026#34;:\u0026#34;NodePortLocal\u0026#34;,\u0026#34;shardVSSize\u0026#34;:\u0026#34;SMALL\u0026#34;,\u0026#34;useDefaultSecretsOnly\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;vipNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;tkg-wld-1-apps\\\u0026#34;,\\\u0026#34;cidr\\\u0026#34;:\\\u0026#34;10.101.221.0/24\\\u0026#34;}]\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1685448627039212099\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.ae838cced3b6caccc5a03bfb3ae65cd7\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;avi-k8s-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;avi-system\u0026#34;}}\u0026#39; 31 kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 32 creationTimestamp: \u0026#34;2023-05-30T12:10:34Z\u0026#34; 33 labels: 34 kapp.k14s.io/app: \u0026#34;1685448627039212099\u0026#34; 35 kapp.k14s.io/association: v1.ae838cced3b6caccc5a03bfb3ae65cd7 36 name: avi-k8s-config 37 namespace: avi-system 38 resourceVersion: \u0026#34;2456\u0026#34; 39 uid: 81bbe809-f5a1-45b8-aef2-a83ff36a3dd1 That looks good, the question now will it blend?\nSo far I dont have anything in my NSX ALB dashboard. What happens then if I create some servicetype loadBalancer or Ingresses? Lets have a look. First check, do I have an IngressClass?\n1linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get ingressclasses.networking.k8s.io 2NAME CONTROLLER PARAMETERS AGE 3avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 10m I certainly do. I will now deploy an application with serviceType loadBalancer and an application using Ingress.\nHere they are:\n1linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get svc -n yelb 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3redis-server ClusterIP 20.20.103.179 \u0026lt;none\u0026gt; 6379/TCP 10s 4yelb-appserver ClusterIP 20.20.242.204 \u0026lt;none\u0026gt; 4567/TCP 10s 5yelb-db ClusterIP 20.20.202.153 \u0026lt;none\u0026gt; 5432/TCP 10s 6yelb-ui LoadBalancer 20.20.123.18 10.101.221.10 80:30119/TCP 10s 1linux-vm:~/Kubernetes-library/tkgm/stc-tkgm$ k get ingress -n fruit 2NAME CLASS HOSTS ADDRESS PORTS AGE 3ingress-example avi-lb fruit-tkg.you-have.your-domain.here 10.101.221.11 80 50s Looking at the Ingres I can also see NSX ALB has been so kind to register DNS record for it also.\nHow does it look inside my NSX ALB in DC1?\nAnd where are the above NSX ALB Service Engines deployed?\nIn my vCenter in DC-2.\nWell that was it. Thanks for reading.\n","link":"https://blog.andreasm.io/2023/05/30/tanzu-kubernetes-grid-2.2-remote-workload-clusters/","section":"post","tags":["tkg","multi-dc","nsx-t","tanzu","nsx advanced loadbalancer"],"title":"Tanzu Kubernetes Grid 2.2 \u0026 Remote Workload Clusters"},{"body":"","link":"https://blog.andreasm.io/tags/tkg/","section":"tags","tags":null,"title":"tkg"},{"body":"","link":"https://blog.andreasm.io/tags/nsx/","section":"tags","tags":null,"title":"nsx"},{"body":"This post will explore a bit further how we can utilize the three-zone setup configured here for TKG cluster and application placement. It will not be a very long post, but showing how it can be done and which values to use. In the \u0026quot;part 1\u0026quot; post I enabled a three-zone Supervisor deployment, meaning the three Supervisor Control Plane VMs will be distributed evenly across my three vSphere Clusters. To use a three-zone deployment we need to have three vSphere Zones defined. Each of these vSphere zones is described as a \u0026quot;Failure Domain\u0026quot; and becomes a value we can use when we decide the placement of both TKG clusters and applications inside our TKG clusters. So basically this post will describe how I can take advantage of this when I deploy my workload clusters and how can I decide where my applications will be placed.\nWorkload cluster placement My Supervisor cluster is already deployed in my three vSphere Zones, just waiting for me to give it something to do. I have created a vSphere Namespace for my TKG cluster called ns-three-zone-1. I want to use a different workload network than my Supervisor workload network is placed on, that is a benefit when using NSX-T.\nTo give some context, this is how my environment is looking before deploying the TKG cluster:\nNow I just need to log into the supervisor, prepare my TKG yaml manifest and deploy my TKG cluster.\nLog in to Supervisor\n1linuxvm01:~/$ kubectl vsphere login --server=10.101.90.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.101.90.2 10 11If the context you wish to use is not in this list, you may need to try 12logging in again later, or contact your cluster administrator. 13 14To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` 15linuxvm01:~/$ Now that I am logged in, I will check if my vSphere Zones are available by issuing the following command:\n1linuxvm01:~/three-zones$ k get vspherezones.topology.tanzu.vmware.com 2NAME AGE 3wdc-zone-1 19d 4wdc-zone-2 19d 5wdc-zone-3 19d 6linuxvm01:~/three-zones$ So it seems, now I need to use these names/labels in my TKG yaml manifest. In my first deployment I will use the example from the official VMware documentation here with some additions from my side like run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu I will be using the API v1beta1 with kubectl not Tanzu CLI.\nLet us have a look at it and edit it accordingly:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: three-zone-cluster-1 #My own name on the cluster 5 namespace: ns-three-zone-1 #My vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] #Edited by me 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] #Edited by me 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.24.9+vmware.1-tkg.4 #My latest available TKR version 16 controlPlane: 17 replicas: 1 # only one controlplane (saving resources and time) 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 #muliple node pools are used 23 machineDeployments: 24 - class: node-pool 25 name: node-pool-1 26 replicas: 1 #only 1 worker here 27 metadata: 28 annotations: 29 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 30 #failure domain the machines will be created in 31 #maps to a vSphere Zone; name must match exactly 32 failureDomain: wdc-zone-1 #named after my vSphere zone 33 - class: node-pool 34 name: node-pool-2 35 replicas: 1 #only 1 worker here 36 metadata: 37 annotations: 38 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 39 #failure domain the machines will be created in 40 #maps to a vSphere Zone; name must match exactly 41 failureDomain: wdc-zone-2 #named after my vSphere zone 42 - class: node-pool 43 name: node-pool-3 44 replicas: 1 #only 1 worker here 45 metadata: 46 annotations: 47 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 48 #failure domain the machines will be created in 49 #maps to a vSphere Zone; name must match exactly 50 failureDomain: wdc-zone-3 #named after my vSphere zone 51 variables: 52 - name: vmClass 53 value: best-effort-small 54 - name: storageClass 55 value: all-vsans #my zonal storageclass Lets apply and see what happens. What I am expecting is the worker nodes should be placed according to the plan above, 1 worker pr vSphere cluster. The control plane node will be placed random.\n1linuxvm01:~/three-zones$ k apply -f three-zone-cluster-1.yaml 2cluster.cluster.x-k8s.io/three-zone-cluster-1 created And the results are in:\nAll three worker nodes were placed in their respective vSphere Zones (vSphere clusters) as configured in the yaml. The control plane node was just randomly placed in vSphere zone 3.\nThats it for this task. Now I want to deploy nearly the same, but with 3 control plane nodes. Where will they be placed?\nHere is the cluster definition:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: three-zone-cluster-1 #My own name on the cluster 5 namespace: ns-three-zone-1 #My vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] #Edited by me 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] #Edited by me 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.24.9+vmware.1-tkg.4 #My latest available TKR version 16 controlPlane: 17 replicas: 3 # should be spread evenly across zones 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 #muliple node pools are used 23 machineDeployments: 24 - class: node-pool 25 name: node-pool-1 26 replicas: 1 #only 1 worker here 27 metadata: 28 annotations: 29 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 30 #failure domain the machines will be created in 31 #maps to a vSphere Zone; name must match exactly 32 failureDomain: wdc-zone-1 #named after my vSphere zone 33 - class: node-pool 34 name: node-pool-2 35 replicas: 1 #only 1 worker here 36 metadata: 37 annotations: 38 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 39 #failure domain the machines will be created in 40 #maps to a vSphere Zone; name must match exactly 41 failureDomain: wdc-zone-2 #named after my vSphere zone 42 - class: node-pool 43 name: node-pool-3 44 replicas: 1 #only 1 worker here 45 metadata: 46 annotations: 47 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 48 #failure domain the machines will be created in 49 #maps to a vSphere Zone; name must match exactly 50 failureDomain: wdc-zone-3 #named after my vSphere zone 51 variables: 52 - name: vmClass 53 value: best-effort-small 54 - name: storageClass 55 value: all-vsans #my zonal storageclass And in vCenter where is the control plane nodes placed:\nApplication placement After your workload cluster has been deployed as specified above you also want to utilize the different vSphere Zones for application placement. Start by switching into the context of your workload cluster:\n1linuxvm01:~/three-zones$ kubectl vsphere login --server=10.101.90.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-namespace ns-three-zone-1 --tanzu-ku 2bernetes-cluster-name three-zone-cluster-1 1linuxvm01:~/three-zones$ k config current-context 2three-zone-cluster-1 Now that I am in the correct context, lets check the nodes status, and specifically if there are any labels of interest or relevance to the vSphere Zones.\n1linuxvm01:~/three-zones$ k get nodes --show-labels 2NAME STATUS ROLES AGE VERSION LABELS 3three-zone-cluster-1-h9nxh-9xsvp Ready control-plane 4m25s v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-2,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-h9nxh-9xsvp,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-2 4three-zone-cluster-1-h9nxh-bqqzd Ready control-plane 16m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-3,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-h9nxh-bqqzd,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-3 5three-zone-cluster-1-h9nxh-kkvkz Ready control-plane 10m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-1,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-h9nxh-kkvkz,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers=,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-1 6three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz Ready \u0026lt;none\u0026gt; 13m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-1,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz,kubernetes.io/os=linux,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-1 7three-zone-cluster-1-node-pool-2-prg6m-84d45c4bd-vwhns Ready \u0026lt;none\u0026gt; 11m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-2,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-node-pool-2-prg6m-84d45c4bd-vwhns,kubernetes.io/os=linux,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-2 8three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 Ready \u0026lt;none\u0026gt; 11m v1.24.9+vmware.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/zone=wdc-zone-3,kubernetes.io/arch=amd64,kubernetes.io/hostname=three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6,kubernetes.io/os=linux,run.tanzu.vmware.com/kubernetesDistributionVersion=v1.24.9---vmware.1-tkg.4,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.4,topology.kubernetes.io/zone=wdc-zone-3 In both the worker nodes and control plane nodes we have the labels:\n1linuxvm01:~/three-zones$ k get nodes --show-labels 2NAME LABELS 3three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz topology.kubernetes.io/zone=wdc-zone-1 4three-zone-cluster-1-node-pool-2-prg6m-84d45c4bd-vwhns topology.kubernetes.io/zone=wdc-zone-2 5three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 topology.kubernetes.io/zone=wdc-zone-3 These labels can then be used when we deploy our applications. I will be using node affinity in my example below. For more information on pod to node placement see here.\nNow I want to deploy an application called Yelb that consist of four pods.\nI will define the yelp-db, yelp-appserver and yelb-cache pod to be placed in my vSphere Zone 3. The yelb-ui I will define to be placed in my vSphere Zone 1.\nBelow is my yelp application yaml manifest.\n1apiVersion: v1 2kind: Service 3metadata: 4 name: redis-server 5 labels: 6 app: redis-server 7 tier: cache 8 namespace: yelb 9spec: 10 type: ClusterIP 11 ports: 12 - port: 6379 13 selector: 14 app: redis-server 15 tier: cache 16--- 17apiVersion: v1 18kind: Service 19metadata: 20 name: yelb-db 21 labels: 22 app: yelb-db 23 tier: backenddb 24 namespace: yelb 25spec: 26 type: ClusterIP 27 ports: 28 - port: 5432 29 selector: 30 app: yelb-db 31 tier: backenddb 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: yelb-appserver 37 labels: 38 app: yelb-appserver 39 tier: middletier 40 namespace: yelb 41spec: 42 type: ClusterIP 43 ports: 44 - port: 4567 45 selector: 46 app: yelb-appserver 47 tier: middletier 48--- 49apiVersion: v1 50kind: Service 51metadata: 52 name: yelb-ui 53 labels: 54 app: yelb-ui 55 tier: frontend 56 namespace: yelb 57spec: 58 type: LoadBalancer 59 ports: 60 - port: 80 61 protocol: TCP 62 targetPort: 80 63 selector: 64 app: yelb-ui 65 tier: frontend 66--- 67apiVersion: apps/v1 68kind: Deployment 69metadata: 70 name: yelb-ui 71 namespace: yelb 72spec: 73 selector: 74 matchLabels: 75 app: yelb-ui 76 replicas: 1 77 template: 78 metadata: 79 labels: 80 app: yelb-ui 81 tier: frontend 82 spec: 83 affinity: 84 nodeAffinity: 85 requiredDuringSchedulingIgnoredDuringExecution: 86 nodeSelectorTerms: 87 - matchExpressions: 88 - key: topology.kubernetes.io/zone 89 operator: In 90 values: 91 - wdc-zone-1 92 containers: 93 - name: yelb-ui 94 image: registry.guzware.net/yelb/yelb-ui:0.3 95 imagePullPolicy: Always 96 ports: 97 - containerPort: 80 98--- 99apiVersion: apps/v1 100kind: Deployment 101metadata: 102 name: redis-server 103 namespace: yelb 104spec: 105 selector: 106 matchLabels: 107 app: redis-server 108 replicas: 1 109 template: 110 metadata: 111 labels: 112 app: redis-server 113 tier: cache 114 spec: 115 affinity: 116 nodeAffinity: 117 requiredDuringSchedulingIgnoredDuringExecution: 118 nodeSelectorTerms: 119 - matchExpressions: 120 - key: topology.kubernetes.io/zone 121 operator: In 122 values: 123 - wdc-zone-3 124 containers: 125 - name: redis-server 126 image: registry.guzware.net/yelb/redis:4.0.2 127 ports: 128 - containerPort: 6379 129--- 130apiVersion: apps/v1 131kind: Deployment 132metadata: 133 name: yelb-db 134 namespace: yelb 135spec: 136 selector: 137 matchLabels: 138 app: yelb-db 139 replicas: 1 140 template: 141 metadata: 142 labels: 143 app: yelb-db 144 tier: backenddb 145 spec: 146 affinity: 147 nodeAffinity: 148 requiredDuringSchedulingIgnoredDuringExecution: 149 nodeSelectorTerms: 150 - matchExpressions: 151 - key: topology.kubernetes.io/zone 152 operator: In 153 values: 154 - wdc-zone-3 155 containers: 156 - name: yelb-db 157 image: registry.guzware.net/yelb/yelb-db:0.3 158 ports: 159 - containerPort: 5432 160--- 161apiVersion: apps/v1 162kind: Deployment 163metadata: 164 name: yelb-appserver 165 namespace: yelb 166spec: 167 selector: 168 matchLabels: 169 app: yelb-appserver 170 replicas: 1 171 template: 172 metadata: 173 labels: 174 app: yelb-appserver 175 tier: middletier 176 spec: 177 affinity: 178 nodeAffinity: 179 requiredDuringSchedulingIgnoredDuringExecution: 180 nodeSelectorTerms: 181 - matchExpressions: 182 - key: topology.kubernetes.io/zone 183 operator: In 184 values: 185 - wdc-zone-3 186 containers: 187 - name: yelb-appserver 188 image: registry.guzware.net/yelb/yelb-appserver:0.3 189 ports: 190 - containerPort: 4567 This is the section I define where the deployments should be placed:\n1 affinity: 2 nodeAffinity: 3 requiredDuringSchedulingIgnoredDuringExecution: 4 nodeSelectorTerms: 5 - matchExpressions: 6 - key: topology.kubernetes.io/zone 7 operator: In 8 values: 9 - wdc-zone-X And now I apply my application, and where will the different pods be placed:\n1linuxvm01:~/three-zones$ k apply -f yelb-lb-zone-affinity.yaml 2service/redis-server created 3service/yelb-db created 4service/yelb-appserver created 5service/yelb-ui created 6deployment.apps/yelb-ui created 7deployment.apps/redis-server created 8deployment.apps/yelb-db created 9deployment.apps/yelb-appserver created Check pod information with -o wide\n1linuxvm01:~/three-zones$ k get pods -n yelb -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3redis-server-6cc65b47bd-sndht 1/1 Running 0 70s 20.40.3.8 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4yelb-appserver-84d4784595-jw7m5 1/1 Running 0 70s 20.40.3.10 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 5yelb-db-7f888657dd-nt427 1/1 Running 0 70s 20.40.3.9 three-zone-cluster-1-node-pool-3-b6hkw-df698b86d-8hdd6 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 6yelb-ui-6597db5d9b-972h7 1/1 Running 0 70s 20.40.1.6 three-zone-cluster-1-node-pool-1-7xsnp-75994d44d8-zxzsz \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; As I can see from this output all pods execpt my frontend-ui pod has been placed in vSphere Zone 3. Now, if you read the official documentation from Kubernets.io pod placemement can be done in different ways according to different needs. Worth reading.\nThis concludes this post.\n","link":"https://blog.andreasm.io/2023/05/03/vsphere-with-tanzu-and-multi-zones-part2/","section":"post","tags":["availability","nsx","tanzu"],"title":"vSphere with Tanzu and Multi Zones part2"},{"body":"vSphere with Tanzu and Multi-Zone This post will be a brief introduction of how we can deploy and configure vSphere with Tanzu on three vSphere clusters (vSphere Zones) to achieve better availability of the TKG clusters and Supervisor controlplane nodes. This feature came with Sphere 8, not available in vSphere 7. There are some requirements that needs to be addressed for this to work in addition to the pre-reqs for deploying vSphere with Tanzu on a single vSphere cluster. I will list them below later.\nFrom the official VMware documentation:\nYou can deploy a Supervisor on three vSphere Zones to provide cluster-level high-availability that protects your Kubernetes workloads against cluster-level failure. A vSphere Zone maps to one vSphere cluster that you can setup as an independent failure domain. In a three-zone deployment, all three vSphere clusters become one Supervisor. You can also deploy a Supervisor on one vSphere cluster, which will automatically create a vSphere Zone and map it to the cluster, unless you use a vSphere cluster that is already mapped to a zone. In a single cluster deployment, the Supervisor only has high availability on host level that is provided by vSphere HA.\nOn a three-zone Supervisor you can run Kubernetes workloads on Tanzu Kubernetes Grid clusters and VMs created by using the VM service. A three-zone Supervisor has the following components:\nSupervisor control plane VM. Three Supervisor control plane VMs in total are created on the Supervisor. In a three-zone deployment, one control plane VM resides on each zone. The three Supervisor control plane VMs are load balanced as each one of them has its own IP address. Additionally, a floating IP address is assigned to one of the VMs and a 5th IP address is reserved for patching purposes. vSphere DRS determines the exact placement of the control plane VMs on the ESXi hosts part of the Supervisor and migrates them when needed. Tanzu Kubernetes Grid and Cluster API. Modules running on the Supervisor and enable the provisioning and management of Tanzu Kubernetes Grid clusters. Virtual Machine Service. A module that is responsible for deploying and running stand-alone VMs and VMs that make up Tanzu Kubernetes Grid clusters. Planning and deployment of a three-zone Supervisor Before heading into the actual deployment, some pre-requirements needs to be in place. I will go through the important ones below such as storage, network and user-roles.\nvSphere ESXi clusters A deployment of a three-zone Supervisor may also be referred to as multi-zone. This can maybe lead to the understanding that we may deploy vSphere with Tanzu multi-zone on 2 vSphere ESXi clusters or even 4 or higher number of vSphere clusters. The only number of vSphere clusters supported in a multi-zone Supervisor deployment (three-zone) is 3 vSphere clusters.\nFrom the official documentation:\nCreate three vSphere clusters with at least 2 hosts. For storage with vSAN, the cluster must have 3 or 4 hosts. Configure storage with vSAN or other shared storage solution for each cluster. Enable vSphere HA and vSphere DRS on Fully Automate or Partially Automate mode. Configure each cluster as an independent failure domain. Configure networking with NSX or vSphere Distributed Switch (vDS) networking for the clusters. Network To be able to deploy a three-zone Supervisor an important requirement on the networking side is that all vSphere clusters to be used are connected to the same VDS. One VDS shared across all three vSphere clusters. From the official documentation:\nIn a three-zone Supervisor configured with NSX as the networking stack, all hosts from all three vSphere clusters mapped to the zones must use be connected to the same VDS and participate in the same NSX Overlay Transport Zone. All hosts must be connected to the same L2 physical device.\nNot sure what is meant with connecting to the same L2 physical device though... But anyhow, this constraint can be a limiting factor and something to be aware of. Some explanations:\nIf you have an NSX environment with one common Overlay transportzone across all vSphere clusters, but configured with three individual VDS switches (1 specific VDS pr vSphere cluster) it is currently not supported. Having individual VDS switches per cluster allows you to have only relevant portgroups in your vSphere clusters that are configured specifically for their respective cluster's network, different vlans, placed in different racks and there is no L2 between them, only L3. The different vSphere clusters can be on different racks with different ToR switches, in a Spine-Leaf topology, different subnets/vlans configured for mgmt, vmotion, vSAN, Geneve tunnel VLAN, VM network and so on.\nAn example when having a dedicated VDS pr vSphere cluster:\nThe illustration above indicates that we have portgroups defined that is valid for the VLAN trunks configured to the attached ToR switches with the corresponding VLAN trunks. So there is no risk of using any of the portgroups defined in the respective VDS in any of the above two vSphere clusters to use VLAN portgoups that has not been defined in the ToR switches trunk-ports. That means you will not end up with any unrelevant portgroups not valid for the respective cluster they are being used in. All this depends on the vlan trunks configured in the ToR switches. There is no need to have portgroups spanning across different vSphere clusters with unrelevant vlan tags. They will just end up not getting anywhere, and create a lot of \u0026quot;noise\u0026quot; for the admin managing these portgroups.\nThe example below illustrates one shared VDS across all vSphere clusters, where we have differentiating ToR VLAN trunks, configured per rack.\nAs we are using the same VDS across all clusters, all portgroups defined in this shared VDS is accessible and visible for all ESXi hosts participating in this VDS. This means they can easily be used, but they may not be the correct portgroups to be used for this cluster as the underlaying ToR switches may not have the correct vlan trunks for the ESXi hosts uplinks (pNICs).\nThat is just something to be aware of. If your ESXi hosts have been configured with more than two pNICs it is also possible to have one dedicated VDS per cluster for services specificially for the respective ESXi clusters and one shared for services that are configured identically across the clusters.\nStorage From the official documentation:\nWhen you prepare storage resources for the three-zone Supervisor, keep in mind the following considerations:\nStorage in all three zones does not need to be of the same type. However, having uniform storage in all three clusters provides a consistent performance. For the namespace on the three-zone Supervisor, use a storage policy that is compliant with the shared storage in each of the clusters. The storage policy must be topology aware. Do not remove topology constraints from the storage policy after assigning it to the namespace. Do not mount zonal datastores on other zones. A three-zone Supervisor does not support the following items: Cross-zonal volumes vSAN File volumes (ReadWriteMany Volumes) Static volume provisioning using Register Volume API Workloads that use vSAN Data Persistence platform vSphere Pod vSAN Stretched Clusters VMs with vGPU and instance storage Permissions From the official documentation:\nvSphere Namespaces required privileges:\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Allows disk decommission operations Allows for decommissioning operations of data stores. Data stores Namespaces.ManageDisks Backup Workloads component files Allows for backing up the contents of the etcd cluster (used only in VMware Cloud on AWS). Clusters Namespaces.Backup List accessible namespaces Allows listing the accessible vSphere Namespaces. Clusters Namespaces.ListAccess Modify cluster-wide configuration Allows modifying the cluster-wide configuration, and activating and deactivating cluster namespaces. Clusters Namespaces.ManageCapabilities Modify cluster-wide namespace self-service configuration Allows modifying the namespace self-service configuration. Clusters (for activating and deactivating)Templates(for modifying the configuration)vCenter Server(for creating a template) Namespaces.SelfServiceManage Modify namespace configuration Allows modifying namespace configuration options such as resource allocation and user permissions. Clusters Namespaces.Manage Toggle cluster capabilities Allows manipulating the state of cluster capabilities (used internally only for VMware Cloud on AWS). Clusters NA Upgrade clusters to newer versions Allows initiation of the cluster upgrade. Clusters Namespaces.Upgrade vSphere Zones Privileges\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Attach and Detach vSphere objects for vSphere Zones Allows for adding and removing clusters from a vSphere Zone. Clusters Zone.ObjectAttachable Create, Update and Delete vSphere Zones and their associations Allows for creating and deleting a vSphere Zone. Clusters Zone.Manage Supervisor Services Privileges\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Manage Supervisor Services Allows for creating, updating, or deleting a Supervisor Service. Also allows for installing a Supervisor Service on a Supervisor, and creating or deleting a Supervisor Service version. Clusters SupervisorServices.Manage Deployment time Now that we have gone through the requirements I will start the actual installation/deployment steps of vSphere with Tanzu in a three-zone setup. This post is assuming an already configured and working vSphere environment and NSX installation. Will only focus on the actual deployment of vSphere with Tanzu in a three-zone setup.\nThis is how my environment looks like on a vSphere level before enabling the three-zone Supervisor:\nMy vSphere cluster consists of three vSphere cluster with 4 ESXi hosts each. They are all connected to the same Distributed Virtual Switch, but with different portgroups as their backend vlans is different between the clusters. The only common portgroups they share is all the overlay segments created from NSX (they are all part of the same Overlay TransportZone). Each cluster has its own VSAN datastore, not shared or stretched between the clusters. VSAN local to every vSphere cluster.\nEnable three-zone Supervisor vSphere Zones Before I can go ahead and enable a three-zone Supervisor I need to create the vSphere Zones which is done here:\nClick on the vCenter in the inventory tree -\u0026gt; Configure \u0026gt; vSphere Zones. From there I need to create three zones representing my three vSphere clusters. Click Add New vSphere Zone:\nThen select the cluster it should apply to and finish. The end result looks like this:\nStorage policy Now I need to create a storage policy for the Supervisor. This policy needs to be a zonal policy. Head over to Policies and Profiles here:\nClick on VM Storage Policies:\nAnd click Create:\nGive it a name:\nI am using VSAN so in the next step (2.) I select rules for VSAN storage and the important bit needed for three-zone deployment is the Enable consumption domain\nThen on step 3. I will leave it default unless I have some special VSAN policies I want to apply.\nUnder step 4 the Storage topology type is Zonal\nIn step 5 all the vSAN storages should be listed as compatible\nThen it is just the review and finish\nNow that the storage policy is in place next up is the actual deployment of the three-zone Supervisor.\nDeploy the three-zone Supervisor When all the prerequisites have been done, the actual enablement of the Supervisor is not so different from a regual Supervisor enablement, but let us go through the steps anyway.\nHead over to Workload Management\nStart the deployment wizard, I am using NSX so I am selecting NSX as the network stack\nIn step 2 I select vSphere Zone Deployment, select all my vSphere Zones, the give the Supervisor a name.\nNotice Step 2 is the only step that is done different from a single-zone deployment.\nIn step 3 I select my newly created \u0026quot;zonal\u0026quot; policy, notice that we only have to select the Control Plane Storage Policy\nThen in step 4 it is the Supervisor management network configurations, here I am using my ls-mgmt NSX overlay segment which is common/shared across all esxi hosts/vSphere cluster.\nIn step 5 its the Supervisor Workload network configurations.\nThen in the last step its review and finish time, or put in a dns name for the supervisor k8s api endpoint.\nIt should start to deploy as soon as clicking the finish button:\nNow let us head back to vCenter inventory view and check whats going on there.\nAs you can see from the screenshot above, the three supervisors will be distributed across my three vSphere clusters, one Supervisor per vSphere cluster.\nThis concludes the enabling of a three-zone Supervisor cluster. Next step is to deploy your TKC or guest clusters.\nDeploy a TKC/guest cluster in a three-zone Deploying a TKC cluster in a three-zone is not any different than a single zone, but we need to create storage policy for the workload cluster to be used for deployment.\nThe storage policy I have created look like this and is applied on the vSphere Namespace I create. I will quickly go through the step below:\nThats it, the same policy configuration used for the Supervisor deployment. Then it is all about creating a vSphere Namespace and apply a cluster to it.\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-cluster-small-cidr 5 namespace: test-small-subnet 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 2 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-small #machineclass, get the available classes by running \u0026#39;k get virtualmachineclass\u0026#39; in vSphere ns context 32 - name: storageClass 33 value: all-vsans And the result should the same as for the Supervisor cluster, the workload clusters control plane and worker nodes should be distributed across your vSphere cluster.\nHappy deployment\n","link":"https://blog.andreasm.io/2023/05/03/vsphere-with-tanzu-multi-three-zones-and-nsx/","section":"post","tags":["availability","nsx","tanzu"],"title":"vSphere with Tanzu, Multi (three) Zones and NSX"},{"body":"","link":"https://blog.andreasm.io/tags/network/","section":"tags","tags":null,"title":"network"},{"body":"Tanzu with vSphere using NSX with multiple T0s In this post I will go through how to configure Tanzu with different NSX T0 routers for IP separation use cases, network isolation and multi-tenancy. The first part will involve spinning up dedicated NSX Tier-0 routers by utlizing several NSX Edges and NSX Edge Clusters. The second part will involve using NSX VRF-Tier0. Same needs, two different approaches, and some different configs in NSX.\nSome background to why this is a useful feature: In vSphere with Tanzu with NSX we have the option to override network setting pr vSphere Namespace. That means we can place TKC/Workload clusters on different subnets/segments for ip separation and easy NSX Distributed Firewall policy creation (separation by environments DEV, TEST, PROD etc), but we can also override and define separate NSX Tier-0 routers for separation all the way out to the physical infrastructure. In some environments this is needed as there is guidelines/policies for certain network classifications to be separated, and filtered in physical firewall/security perimeters. Although NSX comes with a powerful and advanced distributed firewall, including Gateway firewall (on Tier1 and Tier0) there is nothing in the way for NSX to be combined in such environments, it just allows for more granular firewall policies before the traffic is eventually shipped out to the perimeter firewalls.\nThe end-goal would be something like this (high level):\nBefore jumping into the actual configuration of this setup I will need to explain a couple of things, as there are some steps that needs to be involved in getting this to work. Lets continue this discussion/monologue in the chapter below.\nRouting considerations Tanzu with vSphere consists of a Supervisor Controlplane VM cluster (3 nodes). These three are configured with two network interfaces, interface ETH0 is placed in the management network. This management network can be NSX overlay segments, or regular VLAN backed segments or VDS/vSwitch portgroups. Its main responsibility is to communicate with the vCenter server API for vm/node creation etc. The second interface (ETH1) on these Supervisor Controlpane VMs is the Workload network. This network's responsibilities are among a couple of things the Kubernetes API endpoint where we can interact with the Kubernetes API to create workload clusters, and here is an important note to remember: It is also used to communicate with the workload clusters being created. If there is no communication from this network to the workload cluster being created, workload cluster creation will fail. So needles to say, it is important that this communication works. When we deploy or enable the Workload Control Plane/Supervisor cluster on a fresh vCenter server and with NSX as the networking stack we are presented with a choice how the workload network can be configured, and the option I am interested in here is the NAT or no NAT option.\nThis option is very easy to select (selected by default) and very easy to deselect, but the impact it does is big. Especially when we in this post are discussing TKG with multi-Tier-0/VRF-T0.\nWhat this option does is deciding whether the workload network in your vSphere Namespace will be routed or not. Meaning that the controlplane and worker nodes actual ip addresses will be directly reachable and exposed (routed) in your network or if it will be masked by NAT rules so they will not be exposed with their real ip addresses in the network. The same approach as a POD in Kubernetes, default it will be Source NAT'ed through its worker node it is running on when it is doing egress traffic (going out and wants to make contact with something). So with NAT enabled on the workload network in a vSphere Namespace the workers will not be using their real ip addresses when they are communicating out, they will be masked by a NAT rule created in NSX automatically. This is all fine, if you want to use NAT. But if you dont want to use NAT you will have to disable this option and prepare all your vSphere Namespace workoad networks to be of ip subnets you are prepared to route in your network. That is also very fine. And as long as you dont expect to be running 500000+ worker nodes you will have available routed RFC1918 addresses to your disposal.\nThe reason I would like to touch upon the above subject is that it will define how we create our routing rules between a Supervisor cluster a vSphere Namespace workload network with NAT enabled or not.\nNow I get to the part where it really get interesting. If you decide to use the NAT'ed approach and NSX VRF-T0 when you create your first vSphere Namespace (not the default inital system vSphere Namespace, but the first vSphere Namespace you can create after WCP has been installed) NCP (NSX Container Plugin) will automatically create a couple of static routes so the Supervisor Cluster workload network can reach the workload networks in your vSphere Namespace placed under a different Tier-0 than the default workload network is placed, here a VRF Tier-0. These routes will be defined with VRF Route Leaking, meaning they will not go out of its parent Tier-0 to reach certain subnets. And with a NAT enabled workload cluster that is perfect as the we dont have care about the NAT rules created on the workload network, they can talk to each other with their real ip addresses. I will explain this a bit later on. Well that is fine and all, but sometimes VRF Tier-0 is not the best option, we need to use dedicated Tier-0 routers on different NSX edge cluster/edges then there is no automatic creation for these static routes. But where and how do I define these static routes manually? On the Tier-0s themselves? In the physical routers the Tier-0s are peering with (using static or BGP)? Yes, both options are possible. If we decide to create these rules in the Tier-0s themselves we need to create a linknet between the Tier-0s to they can point to each other with their respective subnet routes (can be a regular overlay segment used as L2 between the different Tier-0s).\nBut, there is always a but ðŸ˜„. With a NAT enabled vSphere Namespace, the workload network is not allowed to use its own IP address when going out of the Tier-0, it will be using an IP address from the egress subnet defined, or will it? First, when NAT is enabled on a vSphere Namespace, NCP will create a rule in NSX saying that if you want to talk to your other workload cluster network buddies, you dont have to hide your true identity, you are allowed to use your real name when you want to talk to your buddies. See below for how these rules looks like:\nWait a minute, what does this mean then? Exactly, this means that when it communicates with the other vSphere Namespace network destinations IP/Subnets (see above) it will not be NAT'ed. It will use its real ip address. And that ladies and gentlemen is why creating the static routes directly between the Tier-0s using a linknet between is an easy solution when using NAT in the vSphere Namespace network. Why, how etc, still dont understand? Ok, let me first show you an example below of two such static routes.\nThe above screenshot illustrates a static route defined on the Tier-0 where the Supervisor workload network is placed below behind a Tier-1 gateway. The ip subnet is the workload network cidr/subnet of a vSphere Namespace placed on a different Tier-0 router (its actual ip subnet, not the NAT ip addresses).\nThe above static route is created on the second Tier-0 where the vSphere Namespace number 2 is created and placed. This route is pointing to the actual IP subnet/cidr of the Supervisor workload network, not the NAT address. Just to mention it, common for both these routes is that they are using the linknet interfaces of the Tier-0s respectively. But how? We have this no SNAT rule, but at the same time we have a route-map denying any segment used by a NAT enable vSphere Namespace workload network to be advertised/exposed outside the Tier-0. Yes, exactly, outside the Tier-0. By using the linknet between the Tier-0s for our static routes we are simply telling the Tier-0 routers to go to their respective Tier-0 when it needs to find the respective vSphere Namespace workload networks. The idea here is that the Tier-0s are the \u0026quot;all-seeing-eye\u0026quot; and they truly are. All the T1 gateways configured by NCP/WCP will enable 4 route advertisements, all connected segments, NAT IPs, LB VIPS and Static Routes. This means that all the Tier1 gateways will happily tell the Tier-0 about their networks/subnets/ip addresses they know of to the Tier-0. So when we just create a static route like the two above (e.g 10.101.82.32/27 via linknet interface) the Tier-0 router that is defined as next-hop know exactly where this network is located, which T1 gateway it needs to send it to regardless of NAT rules and route-maps.\nNice, right? This means we dont need to interfere with these static routes in the physical routers, they were supposed to be NAT'ed right? So we dont want them in our physical routers anyway. This traffic will then never leave the \u0026quot;outside\u0026quot; of the Tier-0s via their uplinks connected to their respective BGP/Upstream routers as we are using the \u0026quot;linknet\u0026quot; between them. This will be true as long as the destination subnet is for the subnets defined in the no-nat rules and we have defined the static routes to go over the linknet.\nTo further show evidence of how this looks, let us do a \u0026quot;traffic walk\u0026quot; from the Supervisor Cluster Control Plane vm's workload network to the other workload cluster networks on the second Tier-0 router. First an attempt to draw this:\nThe diagram above tries to illustrate what the actual IP address being used in regards of the destination subnet. If we quickly take a look at the no-snat rules created in NSX again:\nThe first 4 no-snat rules tells the Tier-1 router to NOT do any SNAT on the source IP if the source subnet happens to be 10.101.80/23 (SVC workload network) and the destination happens to be any of the four defined subnets in each NO-SNAT rules (10.101.80/23 itself, 10.101.82.0/24, 10,101.90.0/24 its own ingress cidr, and 10.101.83.0/24).\nSo if I do a tcpdump on one of my workload clusters control plane nodes, and fitering on ip coming from the supervisor control planes workload network. Will I then see the NATed address or will I see their real IP?\n1vmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo tcpdump src net 10.101.80.0/27 2tcpdump: verbose output suppressed, use -v or -vv for full protocol decode 3listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 412:39:06.351632 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1537746084:1537746122, ack 2981756064, win 6516, options [nop,nop,TS val 2335069578 ecr 3559626446], length 38 512:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 91, win 6516, options [nop,nop,TS val 2335069583 ecr 3559634990], length 0 612:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 7589, win 6477, options [nop,nop,TS val 2335069583 ecr 3559634990], length 0 712:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 38:73, ack 7589, win 6516, options [nop,nop,TS val 2335069584 ecr 3559634990], length 35 812:39:06.400033 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 7620, win 6516, options [nop,nop,TS val 2335069626 ecr 3559634991], length 0 912:39:06.794326 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [S], seq 2927440799, win 64240, options [mss 1460,sackOK,TS val 2841421479 ecr 0,nop,wscale 7], length 0 1012:39:06.797334 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 907624276, win 502, options [nop,nop,TS val 2841421483 ecr 3559635431], length 0 1112:39:06.797334 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [F.], seq 0, ack 1, win 502, options [nop,nop,TS val 2841421483 ecr 3559635431], length 0 1212:39:06.800095 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 2, win 502, options [nop,nop,TS val 2841421486 ecr 3559635434], length 0 1312:39:06.809982 IP 10.101.80.3.58013 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1460837974:1460838009, ack 2147601163, win 1312, options [nop,nop,TS val 1221062723 ecr 4234259718], length 35 If I filter on port 6443:\n1vmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo tcpdump port 6443 2tcpdump: verbose output suppressed, use -v or -vv for full protocol decode 3listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 412:40:57.817560 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1537768767:1537768802, ack 2982048853, win 6516, options [nop,nop,TS val 2335181040 ecr 3559745035], length 35 512:40:57.817560 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 35:174, ack 1, win 6516, options [nop,nop,TS val 2335181041 ecr 3559745035], length 139 612:40:57.817643 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [.], ack 35, win 501, options [nop,nop,TS val 3559746454 ecr 2335181040], length 0 712:40:57.817677 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [.], ack 174, win 501, options [nop,nop,TS val 3559746454 ecr 2335181041], length 0 812:40:57.819719 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [P.], seq 1:90, ack 174, win 501, options [nop,nop,TS val 3559746456 ecr 2335181041], length 89 912:40:57.864302 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 90, win 6516, options [nop,nop,TS val 2335181086 ecr 3559746456], length 0 1012:40:58.590194 IP 10.101.92.3.4120 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1810063827:1810063865, ack 3070977968, win 5820, options [nop,nop,TS val 1030353737 ecr 937884951], length 38 What do I see? I see the source address being the real IP addresses from the Supervisor Control Plane VMs (10.101.80.x). I also see 10.101.92.3 which happens to be the workload clusters own ingress.\nAfter these static routes have been added I also just want to do a traceroute from the supervisor vm to the workload cluster control plane node to show how I have altered the next hops it will use to get there:\n1root@422080039f397c9aa239cf40e4535f0d [ ~ ]# traceroute -T -p 22 -i eth1 10.101.82.34 2traceroute to 10.101.82.34 (10.101.82.34), 30 hops max, 60 byte packets 3 1 _gateway (10.101.80.1) 0.265 ms 0.318 ms 0.295 ms 4 2 100.64.0.4 (100.64.0.4) 1.546 ms 1.518 ms 1.517 ms 5 3 10.101.240.13 (10.101.240.13) 9.603 ms 9.551 ms 9.568 ms 6 4 * * * 7 5 10.101.82.34 (10.101.82.34) 10.378 ms 10.296 ms 10.724 ms Now if I do the same traceflow from the same Supervisor vm, but to the workload cluster's vip (10.101.92.0/24) which is exposed via BGP...\n1root@422080039f397c9aa239cf40e4535f0d [ ~ ]# traceroute -T -p 22 -i eth1 10.101.92.3 2traceroute to 10.101.92.3 (10.101.92.3), 30 hops max, 60 byte packets 3 1 _gateway (10.101.80.1) 0.229 ms 0.263 ms 0.237 ms 4 2 100.64.0.4 (100.64.0.4) 1.025 ms 1.006 ms 0.972 ms 5 3 10.101.4.1 (10.101.4.1) 1.405 ms 1.408 ms 1.524 ms 6 4 10.101.7.13 (10.101.7.13) 2.123 ms 2.037 ms 2.019 ms 7 5 10.101.92.3 (10.101.92.3) 2.562 ms 2.197 ms 2.499 ms It will not take the same route to get there. It will actually go all the way out to the physical BGP peers, and then over to the second Tier-0 router.\nBut if I do the same traceroute from my workload cluster nodes, traceroute the Supervisor workload VIP (which is also exposed via BGP)?\n1vmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo traceroute -T -p 22 10.101.90.2 2traceroute to 10.101.90.2 (10.101.90.2), 30 hops max, 60 byte packets 3 1 _gateway (10.101.82.33) 0.494 ms 1.039 ms 1.015 ms 4 2 100.64.0.0 (100.64.0.0) 1.460 ms 1.451 ms 1.441 ms 5 3 10.101.240.10 (10.101.240.10) 3.120 ms 3.108 ms 3.099 ms 6 4 10.101.90.2 (10.101.90.2) 3.115 ms 3.105 ms 3.095 ms It will go over the Tier-0 linknet. Why, because on the second Tier-0 router I have created two static routes pointing to both the Supervisor workload cluster subnet and Supervisor VIP altering the next-hops. See below:\nAnd also, did I mention that there will be created a route-map on the Tier-0 for the vSphere Namespace Networks with corresponding IP-prefix lists prohibiting the workload networks ip subnets to be advertised through BGP/OSPF from the Tier-0 and its upstream bgp neigbours.\nHow will this go then, if we cant by any reason use a linknet between the Tier-0s for these static routes and need to define them in the physical routers? Well that is an interesting question. Let us try to dive into that topic also. Did I mention that we can also decide to disable NAT completely? Well that is also a perfectly fine option. This could also give other benefits for the environments where it is is needed to have these routes in the physical network due to policies, requirements etc. We can create much more granular firewall policies in the perimeter firewalls when we know each node will egress with their actual IP instead of being masked by a NAT ip address. If being masked by a NAT ip address we cant for sure really know which node it is, we can only know that it potentially comes from any node in a vSphere Namespace where this egress subnet is defined (and that can potentially be a couple). Remember how the SNAT rules look like (maybe not as I haven't shown a screenshot of it yet ðŸ˜„)?\nAlso, we dont need to create any static routes, it will be auto advertised by BGP (if using BGP) each time we create a new vSphere Namespace.\nBut, there is a possibilty to still use static routes, or inject them via BGP in the physical network. We need to define the static routes with the correct subnets, pointing to the NAT'ed vSphere Namespace workload network and egress subnet/cidr. So in my physical router I would need to create these rules:\n1ip route 10.101.80.0/23 10.101.4.10 #Workload network Default vSphere Namespace via Tier-0 Uplink(s) 2ip route 10.101.91.0/24 10.101.4.10 #Egress cidr Default vSphere Namespace via Tier-0 Uplink(s) These are the needed static routes for the Supervisor Workload and egress network to be configured in the physical router, if you happen to create a vSphere Namespace which is not NAT'ed these are the only routes needed. If your other vSphere Namespace is also NAT'ed you need to create the routes accordingly for this Namespace also.\nIllustration of static routes in the physical network between two Tier-0s and two NAT enabled vSphere Namespaces:\nIf using routed or a NAT disabled vSphere Namespace, the life will be much easier if using BGP on your Tier-0s.\nBe aware There was a reason you decided to use NAT? When defining these static routes the network which are supposed to not be routed outside the Tier-0s will now suddenly be routed and exposed in your network. Is that something you want? Then you maybe should opt for no NAT anyway?\nTip Some notes on BFD with NSX and BGP/Static routes here\nThis part can be a bit confusing if not fully understanding how network traffic works in Tanzu with vSphere and NSX. Hopefully I managed to explain it so its more understandable.\nAs long as we are only using just one Tier-0 router for all our vSphere Namespaces, regardless of how many different subnets we decide to create pr vSphere Namespace they will be known by the same Tier-0 as the Tier-1 will be default configured to advertise to the Tier-0 its connected networks, yes it also advertise NAT IPs and LoadBalancer IPs but these are also configured on the Tier-0 to be further advertised to the outside world. Its only the Tier-0 that can be configured with BGP, as it is only the Tier-0 that can be configured to talk to the outside world (external network) by a SR T0 using interfaces on the NSX edges (VM or Bare-Metal). This means there is no need for us to create any routes either on the Tier-1 or Tier-0 when creating different vSphere Namespaces with different subnets. But I would not have anything to write about if we just decided to use only one Tier-0 router, would I? ðŸ˜\nNow when all this is clear as day, let us head over to the actual installation and configuration of this.\nNSX and Tanzu configurations with different individual Tier-0s I assume a fully functional Tanzu environment is running with the default Workload network configured with NAT and NSX is the networking component. For this exercise I have prepared my lab to look like this \u0026quot;networking wise\u0026quot;:\nI will deploy two new vSphere Namespaces where I select override Supervisor Network and choose the Tier-0-2 which is another Tier-0 router than my Supervisor workload network is on (this is using the first Tier-0 router)\nIn my lab I use the following IP addresses for the following components:\nNetwork overview Tanzu Management network: 10.101.10.0/24 - connected to a NSX overlay segment - manually created by me Tanzu Workload network (the default Workload network): 10.101.80.0/23 (could be smaller) - will be created automatically as a NSX overlay segment. Ingress: 10.101.90.0/24 Egress: 10.101.91.0/24 I am doing NAT on this network (important to have in mind for later) The first Tier-0 has been configured to use uplinks on vlan 1014 in the following cidr: 10.101.4.0/24 The second Tier-0 (Tier-0-2) will be using uplink on vlan 1017 in the follwing cidr: 10.101.7.0/24 Second vSphere Namespace - will be using Tier-0-2 and NAT disabled Second vSphere Namespace Workload network: 10.101.82.0/24 Second vSphere Namespace ingress network: 10.101.92.0/24 Third vSphere Namespace - will be using Tier-0-2 and NAT enabled Third vSphere Namespace Workload network: 10.101.83.0/24 Third vSphere Namespace ingress network: 10.101.93.0/24 Third vSphere Namespace egress network: 10.101.94.0/24 (where the NAT rules will be created) Using dedicated Tier-0 means we need to deploy additional edges, either in the same NSX edge cluster or a new edge cluster. This can generate some compute and admin overhead. But in some environments its not \u0026quot;allowed\u0026quot; to share two different network classifications over same devices. So we need separate edges for our different Tier-0s. But again, with TKGs we cannot deploy our TKC clusters on other vSphere clusters than our Supervisor cluster has been configured on, so the different TKC cluster will end up on the same shared compute nodes (ESXi). But networking wise they are fully separated.\nDeploy new Edge(s) to support a new Tier-0 As this is my lab, I will not deploy redundant amount of Edges, but will stick with one Edge just to get connectivity up and working. NSX Edge do not support more than 1 SR T0 pr Edge, so we need 1:1 mapping between the SR T0 and Edge. And take into consideration if running this in production we must accommodate potential edge failovers, so we should atleast have two edges responsible for a T0. If running two Tier-0 in an edge cluster we should have 4 edges (if one of them fail).\nThe first thing we need to do is to deploy a new Edge vm from the NSX manager. The new edge will be part of my \u0026quot;common\u0026quot; overlay transportzone as I cant deploy any TKC cluster on other vSphere clusters than where my Supervisor cluster has been enabled. For the VLAN transportzones one can reuse the existing Edge vlan transportzone and the same profile so they get their correct TEP VLAN. For the Uplinks it can be same VLAN trunkport (VDS or NSX VLAN segment) if the vlan trunk range includes the VLAN for the new T0 uplink.\nSo my new edge for this second T0 will be deployed like this:\nAfter the Edge has been deployed its time to create a Edge cluster. Now we need to create a new segment for the coming new Tier-0 router and the Tier-0 linknet:\nThe first segment has been configured to use the edge-vlan-transportzone, and this vlan will be used to peer with the upstream router. The second segment is just a layer 2 overlay segment to be used for link between the two Tier-0s.\nNow we can go ahead and create the new Tier-0:\nGive the Tier-0 a name, select your new Edge cluster. Save and go back to edit it. We need to add the two interfaces uplink to upstream router and link interface between the two tier-0s: Give the interfaces a name, IP address and select the segments we created above for the new Tier-0 uplinks. Select the Edge node and Save Now we have the interfaces, to test if it is up and running you can ping it from your upstream router. The only interface that can be reached is the uplink interface 10.101.7.13. Next configure BGP and the BGP peering with your upstream router:\nThe last thing we need to do in our newly created Tier-0 is to create two static routes that can help us reach the Workload Network on the Supervisor Control Plane nodes on their actual IP addresses (remember our talk above?). On the newly created Tier-0 (Tier-0-2) click on Routing -\u0026gt; Static Routes and add the following route (Supervisor workload network):\nThe two routes created is the Supervisor workload network cidr and the actual ingress vip /32.\nAnd the next-hop is defined with the ip of the other (first) Tier-0 interface on the \u0026quot;linknet\u0026quot; interface between the T0s (not configured on the first Titer-0 yet):\nAdd and Save.\nNow on the first Tier-0 we need a second interface (or two depending on the amount of edges) in the linknet we created earlier.\nName it, ip address Select the edge(s) that will have this/these new interface(s). Save.\nNext up is the route: These route should point to the vSphere workload network cidrs we defined when we created the vSphere Namespaces. The correct cidr is something we get when we create the vSphere Namespace (it is based on the Subnet prefix you configure)\nAnd next-hop (yes you guessed correct) is the linknet interface on the new Tier-0.\nI create the static routes for both the vSphere Namespaces, and I know that it will start using the second /27 subnet in the /24 workload network cidr for the first cluster in each namespace.\nSo we should have something like this now:\nAs mentioned above, these routes is maybe easier to create after we have created the vSphere Network with the correct network definition as we can see them being realized in the NSX manager.\nInfo By adding these static routes on the T0 level as I have done, means this traffic will never leave the Tier-0s, it will go over the linknet between the Tier-0s\nInfo These routes are necessary for the Supervisor and the TKC cluster to be able to reach each others. If they cant, deployment of the TKC clusters will fail, it will just deploy the first Control Plane node and stop there)\nCreate a vSphere Namespace to use our new Tier-0 Head over to vCenter -Workload Management and create a new Namespace:\nAs soon as that is done, go ahead and create the third vSphere Namespace with NAT enabled:\nGive the NS'es a dns compliant name, select the Override Supervisor network settings. From the dropdown select our new Tier-0 router. Uncheck NAT on the vSphere NS number 2 (dont need NAT). Fill in the IP addresses you want to use for the TKC worker nodes, and then the ingress cidr you want. On the vSphere NS number 2 enable NAT and populate an egress cidr also.\nClick Create. Wait a couple of second and head over to NSX and check what has been created there.\nIn the NSX Manager you should now see the following:\nNetwork Topology:\nSegments\nThese network is automatically created by NCP, and the first /27 segment in all vSphere Namespace created will be reserved for Supervisor services, like vSphere pods etc. The second /27 segment is the first available network for the workload clusters. This will in my case start at 10.101.82.32/27 and 10.101.83.32/27 accordingy.\nUnder LoadBalancing we also got a couple of new objects:\nThis is our ingress for the workload cluster Kubernetes API.\nUnder Tier-1 gateways we have new Tier-1 gateways:\nIf you take a closer look at the new Tier-1s, you probably would expect them to be created on the new edge-cluster you created and placed your Tier-0 on? No, its not doing that.\nIt used the edge-cluster the Supervisor has been configured to use. Thats pr design. So dont try to troubleshoot this. Reboot the Edge nodes, NSX managers, ESXi hosts etc. It is like that.\nNow it is time to deploy your new TKC cluster with the new Tier-0. Its the same procedure as every other TKC cluster. Give it a name and place it in the correct Namespace:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: stc-tkc-cluster-dmz 5 namespace: stc-ns-dmz 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 2 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-small #machineclass, get the available classes by running \u0026#39;k get virtualmachineclass\u0026#39; in vSphere ns context 32 - name: storageClass 33 value: vsan-default-storage-policy Then it is just running:\n1kubectl apply -f yaml.file And a couple of minutes later (if all preps have been done correctly) you should have a new TKC cluster using the new T0.\nNow, if the network conditions not were right, the TKC cluster would never be finished. It would stop stop at deploying the first control plane node. But to quickly verify connectivity from the supervisor controlplane vm and the tkc controlplane vm I will SSH into both (described below under troubleshooting) and do a curl against their K8s API VIP respectively:\nFrom one of the supervisor vms \u0026quot;curling\u0026quot; both vSphere NS workload networks k8s api vip from its workload network interface:\n1root@422068ece368739850023f7a81cf5e14 [ ~ ]# curl --interface eth1 https://10.101.93.1:6443 2curl: (60) SSL certificate problem: unable to get local issuer certificate 3More details here: https://curl.se/docs/sslcerts.html 4 5curl failed to verify the legitimacy of the server and therefore could not 6establish a secure connection to it. To learn more about this situation and 7how to fix it, please visit the web page mentioned above. 8root@422068ece368739850023f7a81cf5e14 [ ~ ]# curl --interface eth1 https://10.101.92.1:6443 9curl: (60) SSL certificate problem: unable to get local issuer certificate 10More details here: https://curl.se/docs/sslcerts.html 11 12curl failed to verify the legitimacy of the server and therefore could not 13establish a secure connection to it. To learn more about this situation and 14how to fix it, please visit the web page mentioned above. 15root@422068ece368739850023f7a81cf5e14 [ ~ ]# From the controlplane node of the TKC workload cluster:\n1vmware-system-user@wdc-vrf-cluster-1-2k8tp-gb5g2:~$ curl https://10.101.90.2:6443 2curl: (60) SSL certificate problem: unable to get local issuer certificate 3More details here: https://curl.haxx.se/docs/sslcerts.html 4 5curl failed to verify the legitimacy of the server and therefore could not 6establish a secure connection to it. To learn more about this situation and 7how to fix it, please visit the web page mentioned above. NSX and Tanzu configurations with NSX VRF In NSX-T 3.0 VRF was a new feature, and configuring it was a bit cumbersome, but already from NSX-T 3.1 adding and configuring a VRF Tier-0 is very straightforward. The benefit of using VRF is that it does not dictate the requirement of additional NSX Edges, and we can create many VRF T0s. We can \u0026quot;reuse\u0026quot; the same Edges that has already been configured with a Tier-0. Instead a VRF T0 will be linked to that already existing Tier-0 which will then be the Parent Tier-0. Some settings will be inherited from the parent Tier-0 like BGP AS number for NSX-T versions before 4.1. From NSX-T 4.1 it is now also possible to override the BGP AS number in the VRF-T0, its no longer tied to the parent T0s BGP AS. We can achieve ip-separation by using individual uplinks on the VRF Tier-0s and peer to different upstream routers than our parent Tier-0. The VRF Tier0 will have its own Tier-1 linked to it. So all the way from the physical world to the VM we have a dedicated ip network. To be able to configure VRF Tier-0 we need to make sure the uplinks our Edges have been configured with have the correct vlan trunk range so we can create dedicated VRF Tier0 uplink segments in their respective vlan. The VRF Tier0 will use the same \u0026quot;physical\u0026quot; uplinks as the Edges have been configured with, but using different VLAN for the Tier-0 uplinks. I will go through how I configre VRF T0 in my environment. Pr default there is no route leakage between the parent Tier-0 and the VRF-T0 created, if you want to exhange routes between them we need to create those static routes ourselves. Read more about NSX VRF here.\nTip From NSX-T 4.1 it is now also possible to override the BGP AS number in the VRF-T0, its no longer tied to the parent T0s BGP AS\nIn this part of my lab I use the following IP addresses for the following components:\nNetwork overview Tanzu Management network: 172.21.103.0/24 - connected to a VDS port group - manually created by me Tanzu Workload network (the initial Workload network): 10.103.100.0/23 - will be created automatically as a NSX overlay segment. Ingress: 10.103.200.0/24 Egress: 10.103.201.0/24 I am doing NAT on this network (important to have in mind for later) The first Tier-0 has been configured to use uplinks on vlan 1034 in the following cidr: 10.103.4.0/24 The VRF Tier-0 will be using uplink on vlan 1035 in the follwing cidr: 10.103.5.0/24 Here is a digram showing high-level how VRF-T0 looks like:\nThe Edge VM network config:\nConfigure VRF Tier-0 in NSX Head over the NSX manager -\u0026gt; Networking -\u0026gt; Tier-0 Gateways and click Add Gateway:\nThen give it a name and select the parent Tier0:\nClick save.\nNow head over to Segments and create the VRF-Tier0 Uplink segment:\nGive it a name, select the Edge VLAN Transportzone and enter the VLAN for the VRF T0-uplink (you can also create a vlan Trunk range here instead of creating two distinct segments for both uplinks). In my lab I will only use one uplink.\nClick save\nNow head back to your VRF T0 again and add a interface:\nGive it a name, select external, enter the IP for the uplink you will use to peer with your upstream router, then select the segment created earlier. Select the Edge that will get this interface. Notice also the Access VLAN ID field. There is no need to enter the VLAN here as we only defined one VLAN in our segment, had we created a VLAN range we need to define a VLAN here. It discovers the correct VLAN as we can see. Click save. Remember that for this VLAN to \u0026quot;come through\u0026quot; the Edge needs to be on a trunk-port that allows this VLAN.\nYou can verify the L2 connectivity from your router:\n1root@cpodrouter-v7n31 [ ~ ]# ping 10.103.5.10 2PING 10.103.5.10 (10.103.5.10) 56(84) bytes of data. 364 bytes from 10.103.5.10: icmp_seq=1 ttl=64 time=4.42 ms 464 bytes from 10.103.5.10: icmp_seq=2 ttl=64 time=0.627 ms 564 bytes from 10.103.5.10: icmp_seq=3 ttl=64 time=0.776 ms 6^C 7--- 10.103.5.10 ping statistics --- 83 packets transmitted, 3 received, 0% packet loss, time 10ms 9rtt min/avg/max/mdev = 0.627/1.939/4.416/1.752 ms Now that we have verified that its time for BGP to configured in our upstream router and in our VRF Tier-0. I have already configured my upstream router to accept my VRF T0 as a BGP neighbour, I just need to confgure BGP on my new VRF Tier-0. In the VRF Tier-0 go to BGP and add a bgp neighbour (notice that we need to enable BGP, not enabled by default, and you cant change the BGP as number):\nClick save.\n1Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 210.103.4.10 4 66803 336 345 0 0 0 05:30:33 3 310.103.5.10 4 66803 2 19 0 0 0 00:01:38 0 4172.20.0.1 4 65700 445 437 0 0 0 07:09:43 74 My new neighbour has jouined the party. Now just make sure it will advertise the needed networks. Lets configure that: In the VRF T0, click route re-distribution and SET\nNow my new VRF-Tier 0 is ready to route and accept new linked Tier-1s. How does it look like in the NSX map?\nLooking good.\nLet us get back to this picture when we have deployed a TKC cluster on it.\nCreate a vSphere Namespace to use our new VRF Tier-0 This will be the same approach as above here only difference is we are selecting a VRF Tier0 instead. Here I have selected the VRF Tier-0 and defined the network for it. I have disabled NAT.\nNow what have happened in NSX? Lets have a look. The network topology has been updated:\nA new Tier-1 has been created:\nAnd ofcourse the loadbalancer interface:\nBut the most interesting part is the static routes being created. Let us have a look at these.\nIn the VRF T0 it has created two additonal static routes:\nThose to routes above points to the Supervisor Workload network and the Supervisor Ingress network. Next hop is:\nThese are the Tier0-Tier-1 transit net interface:\nWhat static routes have been configured on the parent Tier-0?\nAnd next-hop is:\nThese routes are pointing to the new vSphere Namespace network, and Ingress network we defined to use the new VRF-Tier0.\nHigh-level overview of the static routes being created automatically by NCP:\nWhen the TKC cluster is deployed the NSX map will look like this:\nA new segment as been added (vnet-domain-c8:5135e3cc-aca4-4c99-8f9f-903e68496937-wdc-ns-1-vrf-wdc-cl-58aaa-0), which is the segment where the TKC workers have been placed. Notice that it is using a /27 subnet as defined in the Namespace Subnet Prefix above. The first segment (/27 chunk) (seg-domain-xxxxx) is always reserved for the Supervisor Services/vSphere Pods. As I decided not to use NAT I can reach the worker nodes IP addresses directly from my management jumpbox (if allowed routing/firewall wise). Note that ping is default disabled/blocked. So to test connectivity try port 22 with SSH/curl/telnet etc.\n1andreasm@linuxvm01:~/tkgs_vsphere7$ ssh 10.103.51.34 2The authenticity of host \u0026#39;10.103.51.34 (10.103.51.34)\u0026#39; can\u0026#39;t be established. 3ECDSA key fingerprint is SHA256:qonxA8ySCbic0YcCAg9i2pLM9Wpb+8+UGpAcU1qAXHs. 4Are you sure you want to continue connecting (yes/no/[fingerprint])? But before you can reach it directly you need to allow this with a firewall rule in NSX as there is a default block rule here:\nIn order to \u0026quot;override\u0026quot; this rule we need to create a rule earlier in the NSX Distributed Firewall. Below is just a test rule I created, its far to open/liberal of course:\nThe group membership in the above rules is just the vnet-domain-c8:5135e3cc-aca4-4c99-8f9f-903e68496937-wdc-ns-1-vrf-wdc-cl-58aaa-0 segment where my TKC workers in this namespace will reside. So if I scale down/up this cluster the content will be dynamically updated. I dont have to update the rule or security group, its done automatic.\nFirewall openings - network diagram I will get back and update this section with a table and update the diagram with more details.\nTroubleshooting To troubleshoot networking scenarios with Tanzu it can sometimes help to SSH into the Supervisor Controlplane VMs and the TKC worker nodes. When I tested out this multi Tier-0 setup I had an issue that only the control plane node of my TKC cluster were being spun up, it never came to deploying the worker nodes. I knew it had to do with connectivity between the Supervisor and TKC. I used NSX Traceflow to verify that connectivity worked as intended which my traceflow in NSX did show me, but still it did not work. So sometimes it is better to see whats going on from the workloads perspective themselves.\nSSH Supervisor VM To log in to the Supervisor VMs we need the root password. This password can be retreived from the vCenter server. SSH into the vCenter server:\n1root@vcsa [ /lib/vmware-wcp ]# ./decryptK8Pwd.py 2Read key from file 3 4Connected to PSQL 5 6Cluster: domain-c35:dd5825a9-8f62-4823-9347-a9723b6800d5 7IP: 172.21.102.81 8PWD: PASSWORD-IS-HERE 9------------------------------------------------------------ 10 11Cluster: domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5 12IP: 10.101.10.21 13PWD: PASSWORD-IS-HERE 14------------------------------------------------------------ Now that we have the root password one can log into the Supervisor VM with SSH and password through the Management Interface (the Workload Interface IP is probably behind NAT so is not reachable OOB):\n1andreasm@andreasm:~/from_ubuntu_vm/tkgs/tkgs-stc-cpod$ ssh root@10.101.10.22 2The authenticity of host \u0026#39;10.101.10.22 (10.101.10.22)\u0026#39; can\u0026#39;t be established. 3ED25519 key fingerprint is SHA256:vmeHlDgquXrZTK3yyevmY2QfISW1WNoTC5TZJblw1J4. 4This key is not known by any other names 5Are you sure you want to continue connecting (yes/no/[fingerprint])? And from in here we can use some basic troubleshooting tools to verify if the different networks can be reached from the Supervisor VM. In the example below I try to verify if it can reach the K8s API VIP for the TKC cluster deployed behind the new Tier-0. I am adding --interface eth1 as I want to specifically use the Workload Network interface on the SVM.\n1curl --interface eth1 https://10.13.52.1:6443 The respons should be immediate, if not you have network reachability issues:\n1curl: (28) Failed to connect to 10.13.52.1 port 6443 after 131108 ms: Couldn\u0026#39;t connect to server What you should see is this:\n1root@423470e48788edd2cd24398f794c5f7b [ ~ ]# curl --interface eth1 https://10.13.52.1:6443 2curl: (60) SSL certificate problem: unable to get local issuer certificate 3More details here: https://curl.se/docs/sslcerts.html 4 5curl failed to verify the legitimacy of the server and therefore could not 6establish a secure connection to it. To learn more about this situation and 7how to fix it, please visit the web page mentioned above. SSH TKC nodes The nodes in a TKC cluster can also be SSH'ed into. If you dont do NAT on your vSphere Namespace network they can be reach directly on their IPs (if from where your SSH jumpbox is allowed routing wise/firewall wise). But if you are NAT'ing then you have to place your SSH jumpbox in the same segment as the TKC nodes you want to SSH into. Or add a second interface on your jumpbox placed in this network. The segment is created in NSX and is called something like this:\nTo get the password for the TKC nodes you can get them with kubectl like this: Put yourselves in the context of the namespace where your workload nodes is deployed:\n1andreasm@andreasm:~$ vsphere-kubectl login --server=10.101.11.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-namespace ns-wdc-1-nat 1andreasm@andreasm:~$ k config current-context 2tkc-cluster-nat Then get the SSH secret:\n1andreasm@andreasm:~$ k get secrets 2NAME TYPE DATA AGE 3default-token-fqvbp kubernetes.io/service-account-token 3 127d 4tkc-cluster-1-antrea-data-values Opaque 1 127d 5tkc-cluster-1-auth-svc-cert kubernetes.io/tls 3 127d 6tkc-cluster-1-ca cluster.x-k8s.io/secret 2 127d 7tkc-cluster-1-capabilities-package clusterbootstrap-secret 1 127d 8tkc-cluster-1-encryption Opaque 1 127d 9tkc-cluster-1-etcd cluster.x-k8s.io/secret 2 127d 10tkc-cluster-1-extensions-ca kubernetes.io/tls 3 127d 11tkc-cluster-1-guest-cluster-auth-service-data-values Opaque 1 127d 12tkc-cluster-1-kapp-controller-data-values Opaque 2 127d 13tkc-cluster-1-kubeconfig cluster.x-k8s.io/secret 1 127d 14tkc-cluster-1-metrics-server-package clusterbootstrap-secret 0 127d 15tkc-cluster-1-node-pool-01-bootstrap-j2r7s-fgmm2 cluster.x-k8s.io/secret 2 42h 16tkc-cluster-1-node-pool-01-bootstrap-j2r7s-r5lcm cluster.x-k8s.io/secret 2 42h 17tkc-cluster-1-node-pool-01-bootstrap-j2r7s-w96ft cluster.x-k8s.io/secret 2 42h 18tkc-cluster-1-pinniped-package clusterbootstrap-secret 1 127d 19tkc-cluster-1-proxy cluster.x-k8s.io/secret 2 127d 20tkc-cluster-1-sa cluster.x-k8s.io/secret 2 127d 21tkc-cluster-1-secretgen-controller-package clusterbootstrap-secret 0 127d 22tkc-cluster-1-ssh kubernetes.io/ssh-auth 1 127d 23tkc-cluster-1-ssh-password Opaque 1 127d 24tkc-cluster-1-ssh-password-hashed Opaque 1 127d I am interested in this one:\n1tkc-cluster-1-ssh-password So I will go ahead and retrieve the content of it:\n1andreasm@andreasm:~$ k get secrets tkc-cluster-1-ssh-password -oyaml 2apiVersion: v1 3data: 4 ssh-passwordkey: aSx--redacted---KJS= #Here is the ssh password in base64 5kind: Secret 6metadata: 7 creationTimestamp: \u0026#34;2022-12-08T10:52:28Z\u0026#34; 8 name: tkc-cluster-1-ssh-password 9 namespace: stc-tkc-ns-1 10 ownerReferences: 11 - apiVersion: cluster.x-k8s.io/v1beta1 12 kind: Cluster 13 name: tkc-cluster-1 14 uid: 4a9c6137-0223-46d8-96d2-ab3564e375fc 15 resourceVersion: \u0026#34;499590\u0026#34; 16 uid: 75b163a3-4e62-4b33-93de-ae46ee314751 17type: Opaque Now I just need to decode the base64 encoded pasword:\n1andreasm@andreasm:~$ echo \u0026#39;aSx--redacted---KJS=\u0026#39; |base64 --decode 2passwordinplaintexthere=andreasm@andreasm:~$ Now we can use this password to log in to the TKC nodes with the user: vmware-system-user\n1ssh vmware-system-user@10.101.51.34 DCLI - VMware Datacenter CLI If you happen to find different tasks easier to perform from CLI instead of GUI I will show here how to create a new vSphere Namespace by using DCLI from the VCSA appliance (vCenter Server). For more information and reference on dcli look here.\nLog in to your vCenter hosting your Supervisor Cluster with SSH and enter shell:\n1andreasm@andreasm:~/$ ssh root@vcsa.cpod-v7n31.az-wdc.cloud-garage.net 2 3VMware vCenter Server 7.0.3.01000 4 5Type: vCenter Server with an embedded Platform Services Controller 6 7(root@vcsa.cpod-v7n31.az-wdc.cloud-garage.net) Password: 8Connected to service 9 10 * List APIs: \u0026#34;help api list\u0026#34; 11 * List Plugins: \u0026#34;help pi list\u0026#34; 12 * Launch BASH: \u0026#34;shell\u0026#34; 13 14Command\u0026gt; shell 15Shell access is granted to root 16root@vcsa [ ~ ]# Type *dcli --help\u0026quot; to see some options:\n1root@vcsa [ ~ ]# dcli --help 2usage: dcli [+server SERVER] [+vmc-server] [+nsx-server [NSX_SERVER]] [+org-id ORG_ID] [+sddc-id SDDC_ID] [+interactive] [+prompt PROMPT] 3 [+skip-server-verification | +cacert-file CACERT_FILE] [+username USERNAME] [+password PASSWORD] [+logout] [+filter FILTER [FILTER ...]] 4 [+formatter {yaml,yamlc,table,xml,xmlc,json,jsonc,jsonp,html,htmlc,csv}] [+verbose] [+log-level {debug,info,warning,error}] [+log-file LOG_FILE] 5 [+generate-json-input] [+generate-required-json-input] [+json-input JSON_INPUT] [+credstore-file CREDSTORE_FILE] 6 [+credstore-add | +credstore-list | +credstore-remove] [+session-manager SESSION_MANAGER] [+configuration-file CONFIGURATION_FILE] [+more] 7 [args [args ...]] 8 9VMware Datacenter Command Line Interface 10 11positional arguments: 12 args CLI command 13 14optional arguments: 15 +server SERVER Specify VAPI Server IP address/DNS name (default: \u0026#39;http://localhost/api\u0026#39;) 16 +vmc-server Switch to indicate connection to VMC server (default VMC URL: \u0026#39;https://vmc.vmware.com\u0026#39;) 17 +nsx-server [NSX_SERVER] 18 Specify NSX on VMC Server or on-prem instance IP address/DNS name (default: \u0026#39;None\u0026#39;) 19 +org-id ORG_ID Specify VMC organization id to connect to NSX instance. Works together with +sddc-id. (default: \u0026#39;None\u0026#39;) 20 +sddc-id SDDC_ID Specify VMC SDDC id to connect to NSX instance. Works together with +org-id. (default: \u0026#39;None\u0026#39;) 21 +interactive Open a CLI shell to invoke commands 22 +prompt PROMPT Prompt for cli shell (default: dcli\u0026gt; ) 23 +skip-server-verification 24 Skip server SSL verification process (default: False) 25 +cacert-file CACERT_FILE 26 Specify the certificate authority certificates for validating SSL connections (format: PEM) (default: \u0026#39;\u0026#39;) 27 +username USERNAME Specify the username for login (default: \u0026#39;\u0026#39;) 28 +password PASSWORD Specify password explicitly (default: False) 29 +logout Requests delete session and remove from credentials store if stored. (default: False) 30 +filter FILTER [FILTER ...] 31 Provide JMESPath expression to filter command output. More info on JMESPath here: http://jmespath.org 32 +formatter {yaml,yamlc,table,xml,xmlc,json,jsonc,jsonp,html,htmlc,csv} 33 Specify the formatter to use to format the command output 34 +verbose Prints verbose output 35 +log-level {debug,info,warning,error} 36 Specify the verbosity for log file. (default: \u0026#39;info\u0026#39;) 37 +log-file LOG_FILE Specify dcli log file (default: \u0026#39;/var/log/vmware/vapi/dcli.log\u0026#39;) 38 +generate-json-input Generate command input template in json 39 +generate-required-json-input 40 Generate command input template in json for required fields only 41 +json-input JSON_INPUT 42 Specifies json value or a json file for command input 43 +credstore-file CREDSTORE_FILE 44 Specify the dcli credential store file (default: \u0026#39;/root/.dcli/.dcli_credstore\u0026#39;) 45 +credstore-add Store the login credentials in credential store without prompting 46 +credstore-list List the login credentials stored in credential store 47 +credstore-remove Remove login credentials from credential store 48 +session-manager SESSION_MANAGER 49 Specify the session manager for credential store remove operation 50 +configuration-file CONFIGURATION_FILE 51 Specify the dcli configuration store file (default: \u0026#39;/root/.dcli/.dcli_configuration\u0026#39;) 52 +more Flag for page-wise output 53root@vcsa [ ~ ]# Enter DCLI interactive mode: All commands in dcli have autocomplete\n1root@vcsa [ ~ ]# dcli +i +server vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net +skip-server-verification 2Welcome to VMware Datacenter CLI (DCLI) 3 4usage: \u0026lt;namespaces\u0026gt; \u0026lt;command\u0026gt; 5 6To auto-complete and browse DCLI namespaces: [TAB] 7If you need more help for a command: vcenter vm get --help 8If you need more help for a namespace: vcenter vm --help 9To execute dcli internal command: env 10For detailed information on DCLI usage visit: http://vmware.com/go/dcli 11 12dcli\u0026gt; Below shows how autocomplete works:\n1root@vcsa [ ~ ]# dcli +i +server vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net +skip-server-verification 2Welcome to VMware Datacenter CLI (DCLI) 3 4usage: \u0026lt;namespaces\u0026gt; \u0026lt;command\u0026gt; 5 6To auto-complete and browse DCLI namespaces: [TAB] 7If you need more help for a command: vcenter vm get --help 8If you need more help for a namespace: vcenter vm --help 9To execute dcli internal command: env 10For detailed information on DCLI usage visit: http://vmware.com/go/dcli 11 12dcli\u0026gt; com vmware vcenter n 13 \u0026gt; namespacemanagement 14 \u0026gt; namespaces 15 \u0026gt; network 1dcli\u0026gt; com vmware vcenter namespaces instances list 2 list 3 getv2 4 update 5 delete 6 listv2 7 createv2 8 set We can tab to autocomplete and/or use the \u0026quot;dropdown\u0026quot; list to scroll through the different options. Nice feature.\nCreate a vSphere Namespace from DCLI, selecting a VRF T0, configure name, network etc (as you would do from the GUI of vCenter in Workload Management):\n1dcli\u0026gt; com vmware vcenter namespaces instances create --cluster domain-c8 --namespace stc-cluster-vrf2 --namespace-network-network-ingress-cidrs \u0026#39;[{\u0026#34;address\u0026#34;: \u0026#34;10.13.54. 20\u0026#34;, \u0026#34;prefix\u0026#34;:24}]\u0026#39; --namespace-network-network-load-balancer-size SMALL --namespace-network-network-namespace-network-cidrs \u0026#39;[{\u0026#34;address\u0026#34;: \u0026#34;10.13.53.0\u0026#34;, \u0026#34;prefix\u0026#34;:24}]\u0026#39; - 3-namespace-network-network-provider NSXT_CONTAINER_PLUGIN --namespace-network-network-nsx-tier0-gateway vrf-1 --namespace-network-network-routed-mode true --namespace-n 4etwork-network-subnet-prefix-length 28 5dcli\u0026gt; To get the --cluster domain id run this:\n1dcli\u0026gt; com vmware vcenter namespaces instances list 2|---------|-----------------------------------|----------------|-----------|----------------------|-------------| 3|cluster |stats |namespace |description|self_service_namespace|config_status| 4|---------|-----------------------------------|----------------|-----------|----------------------|-------------| 5|domain-c8||--------|-----------|------------||stc-cluster-vrf | |False |RUNNING | 6| ||cpu_used|memory_used|storage_used|| | | | | 7| ||--------|-----------|------------|| | | | | 8| ||0 |0 |0 || | | | | 9| ||--------|-----------|------------|| | | | | 10|domain-c8||--------|-----------|------------||stc-cluster-vrf2| |False |RUNNING | 11| ||cpu_used|memory_used|storage_used|| | | | | 12| ||--------|-----------|------------|| | | | | 13| ||0 |0 |0 || | | | | 14| ||--------|-----------|------------|| | | | | 15|---------|-----------------------------------|----------------|-----------|----------------------|-------------| 16dcli\u0026gt; And seconds later the vSphere Namespace is created\nvCenter API - with Postman vCenter has a nice feature included, the API Explorer. This can be found here:\nClick on the hamburger Menu, and find Developer Center:\nAnd from here we have all the API available to us:\nIts a looooong list of available APIs.\nTo be able to authenticate against vCenter with Postman we must create an API Key. So the first we need to do is \u0026quot;login\u0026quot; with post using the following api (this uses a username and password with sufficient acces to vCenter):\n1https://{{vcenter-fqdn}}/api/session In Postman one should create an environment that contains the vCenter IP/FQDN, username and password. So the first action is to POST this API to get the API Key, making sure you set Authorization to Basic Auth from your environment:\nThe response from this POST should be a token. From now you need to use this token to interact with vCenter API. Change the authentication to API Key and use vmware-api-session-id as Key and Token as value.\nNow lets try a GET and see if it works:\nThat worked out fine ðŸ˜„\nWhat about creating a vSphere Namespace from Postman?\nThats very easy, below is an example to create a new vSphere Namespace, and pointing it to my VRF Tier-0 router:\n1{ 2\t\u0026#34;access_list\u0026#34;: [ 3\t{ 4\t\u0026#34;domain\u0026#34;: \u0026#34;cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34;, 5\t\u0026#34;role\u0026#34;: \u0026#34;OWNER\u0026#34;, 6\t\u0026#34;subject\u0026#34;: \u0026#34;andreasm\u0026#34;, 7\t\u0026#34;subject_type\u0026#34;: \u0026#34;USER\u0026#34; 8\t} 9\t], 10\t\u0026#34;cluster\u0026#34;: \u0026#34;domain-c8\u0026#34;, 11\t\u0026#34;namespace\u0026#34;: \u0026#34;stc-cluster-vrf2\u0026#34;, 12\t\u0026#34;namespace_network\u0026#34;: { 13\t\u0026#34;network\u0026#34;: { 14\t\u0026#34;ingress_cidrs\u0026#34;: [ 15\t{ 16\t\u0026#34;address\u0026#34;: \u0026#34;10.13.54.0\u0026#34;, 17\t\u0026#34;prefix\u0026#34;: 24 18\t} 19\t], 20\t\u0026#34;load_balancer_size\u0026#34;: \u0026#34;SMALL\u0026#34;, 21\t\u0026#34;namespace_network_cidrs\u0026#34;: [ 22\t{ 23\t\u0026#34;address\u0026#34;: \u0026#34;10.13.53.0\u0026#34;, 24\t\u0026#34;prefix\u0026#34;: 24 25\t} 26\t], 27\t\u0026#34;nsx_tier0_gateway\u0026#34;: \u0026#34;vrf-1\u0026#34;, 28\t\u0026#34;routed_mode\u0026#34;: true, 29\t\u0026#34;subnet_prefix_length\u0026#34;: 28 30\t}, 31\t\u0026#34;network_provider\u0026#34;: \u0026#34;NSXT_CONTAINER_PLUGIN\u0026#34; 32\t} 33} Paste this into Postman (Body - Raw) and POST it to the following path https://{{vcenter-fqdn}}/api/vcenter/namespaces/instances and the new vSphere Namespace should be created in a jiff.\nAnd in vCenter our new Namespace:\nFor references to the APIs in vCenter and a whole lot of details and explanations have a look here!\n","link":"https://blog.andreasm.io/2023/04/14/tanzu-with-vsphere-and-different-tier-0s/","section":"post","tags":["nsx","tanzu","network"],"title":"Tanzu with vSphere and different Tier-0s"},{"body":"","link":"https://blog.andreasm.io/tags/ako/","section":"tags","tags":null,"title":"ako"},{"body":"","link":"https://blog.andreasm.io/tags/avi/","section":"tags","tags":null,"title":"avi"},{"body":"Tanzu Kubernetes Grid This post will go through how to deploy TKG 2.1, the management cluster, a workload cluster (or two), and the necessary preparations to be done on the underlaying infrastructure to support TKG 2.1. In this post I will use vSphere 8 with vSAN, Avi LoadBalancer, and NSX. So what we want to end up with it something like this:\nPreparations before deployment This post will assume the following:\nvSphere is already installed configured. See more info here and here\nNSX has already been configured (see this post for how to configure NSX). Segments used for both Management cluster and Workload clusters should have DHCP server available. We dont need DHCP for Workload Cluster, but Management needs DHCP. NSX can provide DHCP server functionality for this use *\nNSX Advanced LoadBalancer has been deployed (and configured with a NSX cloud). See this post for how to configure this. **\nImport the VM template for TKG, see here\nA dedicated Linux machine/VM we can use as the bootstrap host, with the Tanzu CLI installed. See more info here\n(*) TKG 2.1 is not tied to NSX the same way as TKGs - So we can choose to use NSX for Security only or the full stack with networking and security. The built in NSX loadbalancer will not be used, I will use the NSX Advanced Loadbalancer (Avi)\n(**) I want to use the NSX cloud in Avi as it gives several benefits such as integration into the NSX manager where Avi automatically creates security groups, tags and services to easily be used in security policy creation and automatic \u0026quot;route plumbing\u0026quot; for the VIPs.\nTKG Management cluster - deployment The first step after all the pre-requirements have been done is to prepare a bootstrap yaml for the management cluster. I will post an example file here and go through what the different fields means and why I have configured them and why I have uncommented some of them. Start by logging into the bootstrap machine, or if you decide to create the bootstrap yaml somewhere else go ahead but we need to copy it over to the bootstrap machine when we are ready to create the the management cluster.\nTo get started with a bootstrap yaml file we can either grab an example from here or in your bootstrap machine there is a folder which contains a default config you can start out with:\n1andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ ll 2total 120 3drwxrwxr-x 18 andreasm andreasm 4096 Mar 24 09:10 ./ 4drwx------ 9 andreasm andreasm 4096 Mar 16 11:32 ../ 5drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 ako/ 6drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 bootstrap-kubeadm/ 7drwxrwxr-x 4 andreasm andreasm 4096 Mar 16 06:52 cert-manager/ 8drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 cluster-api/ 9-rw------- 1 andreasm andreasm 1293 Mar 16 06:52 config.yaml 10-rw------- 1 andreasm andreasm 32007 Mar 16 06:52 config_default.yaml 11drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 control-plane-kubeadm/ 12drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-aws/ 13drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-azure/ 14drwxrwxr-x 6 andreasm andreasm 4096 Mar 16 06:52 infrastructure-docker/ 15drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 infrastructure-ipam-in-cluster/ 16drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-oci/ 17drwxrwxr-x 4 andreasm andreasm 4096 Mar 16 06:52 infrastructure-tkg-service-vsphere/ 18drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-vsphere/ 19drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 kapp-controller-values/ 20-rwxrwxr-x 1 andreasm andreasm 64 Mar 16 06:52 providers.sha256sum* 21-rw------- 1 andreasm andreasm 0 Mar 16 06:52 v0.28.0 22-rw------- 1 andreasm andreasm 747 Mar 16 06:52 vendir.lock.yml 23-rw------- 1 andreasm andreasm 903 Mar 16 06:52 vendir.yml 24drwxrwxr-x 8 andreasm andreasm 4096 Mar 16 06:52 ytt/ 25drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 yttcb/ 26drwxrwxr-x 7 andreasm andreasm 4096 Mar 16 06:52 yttcc/ 27andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ The file you should be looking for is called config_default.yaml . It could be a smart choice to use this as it will include the latest config parameters following the TKG version you have downloaded (Tanzu CLI).\nNow copy this file to a folder of preference and start to edit it. Below is a copy of an example I am using:\n1#! --------------- 2#! Basic config 3#! ------------- 4CLUSTER_NAME: tkg-stc-mgmt-cluster #Name of the TKG mgmt cluster 5CLUSTER_PLAN: dev #Dev or Prod, defines the amount of control plane nodes of the mgmt cluster 6INFRASTRUCTURE_PROVIDER: vsphere #We are deploying on vSphere, could be AWS, Azure 7ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; #Customer Experience Improvement Program - set to true if you will participate 8ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; #Audit logging should be true in production environments 9CLUSTER_CIDR: 100.96.0.0/11 #Kubernetes Cluster CIDR 10SERVICE_CIDR: 100.64.0.0/13 #Kubernetes Services CIDR 11TKG_IP_FAMILY: ipv4 #ipv4 or ipv6 12DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; #Yes to deploy standalone tkg mgmt cluster on vSphere 13 14#! --------------- 15#! vSphere config 16#! ------------- 17VSPHERE_DATACENTER: /cPod-NSXAM-STC #Name of vSphere Datacenter 18VSPHERE_DATASTORE: /cPod-NSXAM-STC/datastore/vsanDatastore #Name and path of vSphere datastore to be used 19VSPHERE_FOLDER: /cPod-NSXAM-STC/vm/TKGm #Name and path to VM folder 20VSPHERE_INSECURE: \u0026#34;false\u0026#34; #True if you dont want to verify vCenter thumprint below 21VSPHERE_NETWORK: /cPod-NSXAM-STC/network/ls-tkg-mgmt #A network portgroup (VDS or NSX Segment) for VM placement 22VSPHERE_CONTROL_PLANE_ENDPOINT: \u0026#34;\u0026#34; #Required if using Kube-Vip, I am using Avi Loadbalancer for this 23VSPHERE_PASSWORD: \u0026#34;password\u0026#34; #vCenter account password for account defined below 24VSPHERE_RESOURCE_POOL: /cPod-NSXAM-STC/host/Cluster/Resources #If you want to use a specific vSphere Resource Pool for the mgmt cluster. Leave it as is if not. 25VSPHERE_SERVER: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net #DNS record to vCenter Server 26VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa sdfgasdgadfgsdg sdfsdf@sdfsdf.net # your bootstrap machineSSH public key 27VSPHERE_TLS_THUMBPRINT: 22:FD # Your vCenter SHA1 Thumbprint 28VSPHERE_USERNAME: user@vspheresso/or/ad/user/domain #A user with the correct permissions 29 30#! --------------- 31#! Node config 32#! ------------- 33OS_ARCH: amd64 34OS_NAME: ubuntu 35OS_VERSION: \u0026#34;20.04\u0026#34; 36VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; 37VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; 38VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; 39VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; 40VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; 41VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; 42CONTROL_PLANE_MACHINE_COUNT: 1 43WORKER_MACHINE_COUNT: 2 44 45#! --------------- 46#! Avi config 47#! ------------- 48AVI_CA_DATA_B64: #Base64 of the Avi Certificate 49AVI_CLOUD_NAME: stc-nsx-cloud #Name of the cloud defined in Avi 50AVI_CONTROL_PLANE_HA_PROVIDER: \u0026#34;true\u0026#34; #True as we want to use Avi as K8s API endpoint 51AVI_CONTROLLER: 172.24.3.50 #IP or Hostname Avi controller or controller cluster 52# Network used to place workload clusters\u0026#39; endpoint VIPs - If you want to use a separate vip for Workload clusters Kubernetes API endpoint 53AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 #Corresponds with network defined in Avi 54AVI_CONTROL_PLANE_NETWORK_CIDR: 10.13.102.0/24 #Corresponds with network defined in Avi 55# Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 56AVI_DATA_NETWORK: vip-tkg-wld-l7 #Corresponds with network defined in Avi 57AVI_DATA_NETWORK_CIDR: 10.13.103.0/24 #Corresponds with network defined in Avi 58# Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 59AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.13.101.0/24 #Corresponds with network defined in Avi 60AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 #Corresponds with network defined in Avi 61# Network used to place management clusters\u0026#39; endpoint VIPs 62AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 #Corresponds with network defined in Avi 63AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.13.100.0/24 #Corresponds with network defined in Avi 64AVI_NSXT_T1LR: /infra/tier-1s/Tier-1 #Path to the NSX T1 you have configured, click on three dots in NSX on the T1 to get the full path. 65AVI_CONTROLLER_VERSION: 22.1.2 #Latest supported version of Avi for TKG 2.1 66AVI_ENABLE: \u0026#34;true\u0026#34; # Enables Avi as Loadbalancer for workloads 67AVI_LABELS: \u0026#34;\u0026#34; #When used Avi is enabled only workload cluster with corresponding label 68AVI_PASSWORD: \u0026#34;password\u0026#34; #Password for the account used in Avi, username defined below 69AVI_SERVICE_ENGINE_GROUP: stc-nsx #Service Engine group for Workload clusters if you want to have separate groups for Workload clusters and Management cluster 70AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: tkgm-se-group #Dedicated Service Engine group for management cluster 71AVI_USERNAME: admin 72AVI_DISABLE_STATIC_ROUTE_SYNC: true #Pod network reachable or not from the Avi Service Engines 73AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true #If you want to use AKO as default ingress controller, false if you plan to use other ingress controllers also. 74AVI_INGRESS_SHARD_VS_SIZE: SMALL #Decides the amount of shared vs pr ip. 75AVI_INGRESS_SERVICE_TYPE: NodePortLocal #NodePortLocal only when using Antrea, otherwise NodePort or ClusterIP 76AVI_CNI_PLUGIN: antrea 77 78#! --------------- 79#! Proxy config 80#! ------------- 81TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; 82 83#! --------------------------------------------------------------------- 84#! Antrea CNI configuration 85#! --------------------------------------------------------------------- 86# ANTREA_NO_SNAT: false 87# ANTREA_TRAFFIC_ENCAP_MODE: \u0026#34;encap\u0026#34; 88# ANTREA_PROXY: false 89# ANTREA_POLICY: true 90# ANTREA_TRACEFLOW: false 91ANTREA_NODEPORTLOCAL: true 92ANTREA_PROXY: true 93ANTREA_ENDPOINTSLICE: true 94ANTREA_POLICY: true 95ANTREA_TRACEFLOW: true 96ANTREA_NETWORKPOLICY_STATS: false 97ANTREA_EGRESS: true 98ANTREA_IPAM: false 99ANTREA_FLOWEXPORTER: false 100ANTREA_SERVICE_EXTERNALIP: false 101ANTREA_MULTICAST: false 102 103#! --------------------------------------------------------------------- 104#! Machine Health Check configuration 105#! --------------------------------------------------------------------- 106ENABLE_MHC: \u0026#34;true\u0026#34; 107ENABLE_MHC_CONTROL_PLANE: true 108ENABLE_MHC_WORKER_NODE: true 109MHC_UNKNOWN_STATUS_TIMEOUT: 5m 110MHC_FALSE_STATUS_TIMEOUT: 12m 111 112#! --------------------------------------------------------------------- 113#! Identity management configuration 114#! --------------------------------------------------------------------- 115 116IDENTITY_MANAGEMENT_TYPE: none #I have disabled this, use kubeconfig instead 117#LDAP_BIND_DN: CN=Andreas M,OU=Users,OU=GUZWARE,DC=guzware,DC=local 118#LDAP_BIND_PASSWORD: \u0026lt;encoded:UHNAc=\u0026gt; 119#LDAP_GROUP_SEARCH_BASE_DN: DC=guzware,DC=local 120#LDAP_GROUP_SEARCH_FILTER: (objectClass=group) 121#LDAP_GROUP_SEARCH_GROUP_ATTRIBUTE: member 122#LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn 123#LDAP_GROUP_SEARCH_USER_ATTRIBUTE: distinguishedName 124#LDAP_HOST: guzad07.guzware.local:636 125#LDAP_ROOT_CA_DATA_B64: LS0tLS1CRUd 126#LDAP_USER_SEARCH_BASE_DN: DC=guzware,DC=local 127#LDAP_USER_SEARCH_FILTER: (objectClass=person) 128#LDAP_USER_SEARCH_NAME_ATTRIBUTE: uid 129#LDAP_USER_SEARCH_USERNAME: uid 130#OIDC_IDENTITY_PROVIDER_CLIENT_ID: \u0026#34;\u0026#34; 131#OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \u0026#34;\u0026#34; 132#OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM: \u0026#34;\u0026#34; 133#OIDC_IDENTITY_PROVIDER_ISSUER_URL: \u0026#34;\u0026#34; 134#OIDC_IDENTITY_PROVIDER_NAME: \u0026#34;\u0026#34; 135#OIDC_IDENTITY_PROVIDER_SCOPES: \u0026#34;\u0026#34; 136#OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \u0026#34;\u0026#34; For additional explanations of the different values see here\nWhen you feel you are ready with the bootstrap yaml file its time to deploy the management cluster. From your bootstrap machine where Tanzu CLI have been installed enter the following command:\n1tanzu mc create --file path/to/cluster-config-file.yaml For more information around this process have a look here\nThe first thing that happens is some validation checks, if those pass it will continue to build a local bootstrap cluster on your bootstrap machine before building the TKG Management cluster in your vSphere cluster.\nNote! If you happen to use a an IP range within 172.16.0.0/12 on your computer you are accessing the bootstrap machine through you should edit the default Docker network. Otherwise you will loose connection to your bootstrap machine. This is done like this:\nAdd or edit, if it exists, the /etc/docker/daemon.json file with the following content:\n1{ 2 \u0026#34;default-address-pools\u0026#34;: 3 [ 4 {\u0026#34;base\u0026#34;:\u0026#34;192.168.0.0/16\u0026#34;,\u0026#34;size\u0026#34;:24} 5 ] 6} Restart docker service or reboot the machine.\nNow back to the tanzu create process, you can monitor the progress from the terminal of your bootstrap machine, and you should after a while see machines being cloned from your template and powered on. In the Avi controller you should also see a new virtual service being created:\nThe ip address depicted above is the sole control plane node as I am deploying a TKG management cluster using plan dev. If the progress in your bootstrap machine indicates that it is done, you can check the status with the following command:\n1tanzu mc get This will give you this output:\n1 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 2 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.9+vmware.1 management dev v1.24.9---vmware.1-tkg.1 3 4 5Details: 6 7NAME READY SEVERITY REASON SINCE MESSAGE 8/tkg-stc-mgmt-cluster True 8d 9â”œâ”€ClusterInfrastructure - VSphereCluster/tkg-stc-mgmt-cluster-xw6xs True 8d 10â”œâ”€ControlPlane - KubeadmControlPlane/tkg-stc-mgmt-cluster-wrxtl True 8d 11â”‚ â””â”€Machine/tkg-stc-mgmt-cluster-wrxtl-gkv5m True 8d 12â””â”€Workers 13 â””â”€MachineDeployment/tkg-stc-mgmt-cluster-md-0-vs9dc True 3d3h 14 â”œâ”€Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-55c649d9fc-gnpz4 True 8d 15 â””â”€Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-55c649d9fc-gwfvt True 8d 16 17 18Providers: 19 20 NAMESPACE NAME TYPE PROVIDERNAME VERSION WATCHNAMESPACE 21 caip-in-cluster-system infrastructure-ipam-in-cluster InfrastructureProvider ipam-in-cluster v0.1.0 22 capi-kubeadm-bootstrap-system bootstrap-kubeadm BootstrapProvider kubeadm v1.2.8 23 capi-kubeadm-control-plane-system control-plane-kubeadm ControlPlaneProvider kubeadm v1.2.8 24 capi-system cluster-api CoreProvider cluster-api v1.2.8 25 capv-system infrastructure-vsphere InfrastructureProvider vsphere v1.5.1 When cluster is ready deployed and before we can access it with our kubectl cli tool we must set the context to it.\n1kubectl config use-context my-mgmnt-cluster-admin@my-mgmnt-cluster But you probably have a dedicated workstation you want to acces the cluster from, then you can export the kubeconfig like this:\n1tanzu mc kubeconfig get --admin --export-file MC-ADMIN-KUBECONFIG Now copy the file to your workstation and accessed the cluster from there.\nTip! Test out this tool to easy manage your Kubernetes configs: https://github.com/sunny0826/kubecm\nThe above is a really great tool:\n1amarqvardsen@amarqvards1MD6T:~$ kubecm switch --ui-size 10 2Use the arrow keys to navigate: â†“ â†‘ â†’ â† and / toggles search 3Select Kube Context 4 ðŸ˜¼ tkc-cluster-1(*) 5 tkgs-cluster-1-admin@tkgs-cluster-1 6 wdc-2-tkc-cluster-1 7 10.13.200.2 8 andreasmk8slab-admin@andreasmk8slab-pinniped 9 ns-wdc-3 10 tkc-cluster-1-routed 11 tkg-mgmt-cluster-admin@tkg-mgmt-cluster 12 stc-tkgm-mgmt-cluster 13â†“ tkg-wld-1-cluster-admin@tkg-wld-1-cluster 14 15--------- Info ---------- 16Name: tkc-cluster-1 17Cluster: 10.13.202.1 18User: wcp:10.13.202.1:andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net Now your TKG management cluster is ready and we can deploy a workload cluster.\nIf you noticed some warnings around conciliation during deployment, you can check whether they failed or not by issuing this command after you have gotten the kubeconfig context in place to the Management cluster with this command:\n1andreasm@tkg-bootstrap:~$ kubectl get pkgi -A 2NAMESPACE NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE 3stc-tkgm-ns-1 stc-tkgm-wld-cluster-1-kapp-controller kapp-controller.tanzu.vmware.com 0.41.5+vmware.1-tkg.1 Reconcile succeeded 7d22h 4stc-tkgm-ns-2 stc-tkgm-wld-cluster-2-kapp-controller kapp-controller.tanzu.vmware.com 0.41.5+vmware.1-tkg.1 Reconcile succeeded 7d16h 5tkg-system ako-operator ako-operator-v2.tanzu.vmware.com 0.28.0+vmware.1-tkg.1-zshippable Reconcile succeeded 8d 6tkg-system tanzu-addons-manager addons-manager.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 7tkg-system tanzu-auth tanzu-auth.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 8tkg-system tanzu-cliplugins cliplugins.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 9tkg-system tanzu-core-management-plugins core-management-plugins.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 10tkg-system tanzu-featuregates featuregates.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 11tkg-system tanzu-framework framework.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 12tkg-system tkg-clusterclass tkg-clusterclass.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 13tkg-system tkg-clusterclass-vsphere tkg-clusterclass-vsphere.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 14tkg-system tkg-pkg tkg.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 15tkg-system tkg-stc-mgmt-cluster-antrea antrea.tanzu.vmware.com 1.7.2+vmware.1-tkg.1-advanced Reconcile succeeded 8d 16tkg-system tkg-stc-mgmt-cluster-capabilities capabilities.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 17tkg-system tkg-stc-mgmt-cluster-load-balancer-and-ingress-service load-balancer-and-ingress-service.tanzu.vmware.com 1.8.2+vmware.1-tkg.1 Reconcile succeeded 8d 18tkg-system tkg-stc-mgmt-cluster-metrics-server metrics-server.tanzu.vmware.com 0.6.2+vmware.1-tkg.1 Reconcile succeeded 8d 19tkg-system tkg-stc-mgmt-cluster-pinniped pinniped.tanzu.vmware.com 0.12.1+vmware.2-tkg.3 Reconcile succeeded 8d 20tkg-system tkg-stc-mgmt-cluster-secretgen-controller secretgen-controller.tanzu.vmware.com 0.11.2+vmware.1-tkg.1 Reconcile succeeded 8d 21tkg-system tkg-stc-mgmt-cluster-tkg-storageclass tkg-storageclass.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 22tkg-system tkg-stc-mgmt-cluster-vsphere-cpi vsphere-cpi.tanzu.vmware.com 1.24.3+vmware.1-tkg.1 Reconcile succeeded 8d 23tkg-system tkg-stc-mgmt-cluster-vsphere-csi vsphere-csi.tanzu.vmware.com 2.6.2+vmware.2-tkg.1 Reconcile succeeded 8d 24tkg-system tkr-service tkr-service.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 25tkg-system tkr-source-controller tkr-source-controller.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 26tkg-system tkr-vsphere-resolver tkr-vsphere-resolver.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d TKG Workload cluster deployment Now that we have done all the initial configs to support our TKG environment on vSphere, NSX and Avi, to deploy a workload cluster is as simple as loading a game on the Commodore 64 ðŸ“¼ From your bootstrap machine make sure you are in the context of your TKG Managment cluster:\n1andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ kubectl config current-context 2tkg-stc-mgmt-cluster-admin@tkg-stc-mgmt-cluster I you prefer to deploy your workload clusters in its own Kubernetes namespace go ahead and create a namespace for your workload cluster like this:\n1kubectl create ns \u0026#34;name-of-namespace\u0026#34; Now to create a workload cluster, this also needs a yaml definition file. The easiest way to achieve such a file is to re-use the bootstramp yaml we created for our TKG Management cluster. For more information deploying a workload cluster in TKG read here.By using the Tanzu CLI we can convert this bootstrap file to a workload cluster yaml definiton file, this is done like this:\n1tanzu cluster create stc-tkgm-wld-cluster-1 --namespace=stc-tkgm-ns-1 --file tkg-mgmt-bootstrap-tkg-2.1.yaml --dry-run \u0026gt; stc-tkg-wld-cluster-1.yaml The command above read the bootstrap yaml file we used to deploy the TKG management cluster, converts it into a yaml file we can use to deploy a workload cluster. It alse removes unnecessary fields not needed for our workload cluster. I am also using the --namespace field to point the config to use the correct namespace and automatically put that into the yaml file. then I am pointing to the TKG Management bootstrap yaml file and finally the --dry-run command to pipe it to a file called stc-tkg-wld-cluster-1.yaml. The result should look something like this:\n1apiVersion: cpi.tanzu.vmware.com/v1alpha1 2kind: VSphereCPIConfig 3metadata: 4 name: stc-tkgm-wld-cluster-1 5 namespace: stc-tkgm-ns-1 6spec: 7 vsphereCPI: 8 ipFamily: ipv4 9 mode: vsphereCPI 10 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 11--- 12apiVersion: csi.tanzu.vmware.com/v1alpha1 13kind: VSphereCSIConfig 14metadata: 15 name: stc-tkgm-wld-cluster-1 16 namespace: stc-tkgm-ns-1 17spec: 18 vsphereCSI: 19 config: 20 datacenter: /cPod-NSXAM-STC 21 httpProxy: \u0026#34;\u0026#34; 22 httpsProxy: \u0026#34;\u0026#34; 23 noProxy: \u0026#34;\u0026#34; 24 region: null 25 tlsThumbprint: 22:FD 26 useTopologyCategories: false 27 zone: null 28 mode: vsphereCSI 29--- 30apiVersion: run.tanzu.vmware.com/v1alpha3 31kind: ClusterBootstrap 32metadata: 33 annotations: 34 tkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.24.9---vmware.1-tkg.1 35 name: stc-tkgm-wld-cluster-1 36 namespace: stc-tkgm-ns-1 37spec: 38 additionalPackages: 39 - refName: metrics-server* 40 - refName: secretgen-controller* 41 - refName: pinniped* 42 cpi: 43 refName: vsphere-cpi* 44 valuesFrom: 45 providerRef: 46 apiGroup: cpi.tanzu.vmware.com 47 kind: VSphereCPIConfig 48 name: stc-tkgm-wld-cluster-1 49 csi: 50 refName: vsphere-csi* 51 valuesFrom: 52 providerRef: 53 apiGroup: csi.tanzu.vmware.com 54 kind: VSphereCSIConfig 55 name: stc-tkgm-wld-cluster-1 56 kapp: 57 refName: kapp-controller* 58--- 59apiVersion: v1 60kind: Secret 61metadata: 62 name: stc-tkgm-wld-cluster-1 63 namespace: stc-tkgm-ns-1 64stringData: 65 password: Password 66 username: andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net 67--- 68apiVersion: cluster.x-k8s.io/v1beta1 69kind: Cluster 70metadata: 71 annotations: 72 osInfo: ubuntu,20.04,amd64 73 tkg/plan: dev 74 labels: 75 tkg.tanzu.vmware.com/cluster-name: stc-tkgm-wld-cluster-1 76 name: stc-tkgm-wld-cluster-1 77 namespace: stc-tkgm-ns-1 78spec: 79 clusterNetwork: 80 pods: 81 cidrBlocks: 82 - 100.96.0.0/11 83 services: 84 cidrBlocks: 85 - 100.64.0.0/13 86 topology: 87 class: tkg-vsphere-default-v1.0.0 88 controlPlane: 89 metadata: 90 annotations: 91 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 92 replicas: 1 93 variables: 94 - name: controlPlaneCertificateRotation 95 value: 96 activate: true 97 daysBefore: 90 98 - name: auditLogging 99 value: 100 enabled: false 101 - name: podSecurityStandard 102 value: 103 audit: baseline 104 deactivated: false 105 warn: baseline 106 - name: apiServerEndpoint 107 value: \u0026#34;\u0026#34; 108 - name: aviAPIServerHAProvider 109 value: true 110 - name: vcenter 111 value: 112 cloneMode: fullClone 113 datacenter: /cPod-NSXAM-STC 114 datastore: /cPod-NSXAM-STC/datastore/vsanDatastore 115 folder: /cPod-NSXAM-STC/vm/TKGm 116 network: /cPod-NSXAM-STC/network/ls-tkg-mgmt #Notice this - if you want to place your workload clusters in a different network change this to your desired portgroup. 117 resourcePool: /cPod-NSXAM-STC/host/Cluster/Resources 118 server: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net 119 storagePolicyID: \u0026#34;\u0026#34; 120 template: /cPod-NSXAM-STC/vm/ubuntu-2004-efi-kube-v1.24.9+vmware.1 121 tlsThumbprint: 22:FD 122 - name: user 123 value: 124 sshAuthorizedKeys: 125 - ssh-rsa 88qv2fowMT65qwpBHUIybHz5Ra2L53zwsv/5yvUej48QLmyAalSNNeH+FIKTkFiuX/WjsHiCIMFisn5dqpc/6x8= 126 - name: controlPlane 127 value: 128 machine: 129 diskGiB: 20 130 memoryMiB: 4096 131 numCPUs: 2 132 - name: worker 133 value: 134 count: 2 135 machine: 136 diskGiB: 20 137 memoryMiB: 4096 138 numCPUs: 2 139 version: v1.24.9+vmware.1 140 workers: 141 machineDeployments: 142 - class: tkg-worker 143 metadata: 144 annotations: 145 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 146 name: md-0 147 replicas: 2 Read through the result, edit if you find something you would like to change. If you want to deploy your workload cluster on a different network than your Management cluster edit this field to reflect the correct portgroup in vCenter:\n1 network: /cPod-NSXAM-STC/network/ls-tkg-mgmt Now that the yaml defintion is ready we can create the first workload cluster like this:\n1tanzu cluster create --file stc-tkg-wld-cluster-1.yaml You can monitor the progress from the terminal of your bootstrap machine. When done check your cluster status with Tanzu CLI (remember to either use -n \u0026quot;nameofnamespace\u0026quot; or just -A):\n1andreasm@tkg-bootstrap:~$ tanzu cluster list -A 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 4 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 Further verifications can be done with this command:\n1andreasm@tkg-bootstrap:~$ tanzu cluster get stc-tkgm-wld-cluster-1 -n stc-tkgm-ns-1 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; v1.24.9---vmware.1-tkg.1 4 5 6Details: 7 8NAME READY SEVERITY REASON SINCE MESSAGE 9/stc-tkgm-wld-cluster-1 True 7d22h 10â”œâ”€ClusterInfrastructure - VSphereCluster/stc-tkgm-wld-cluster-1-lzjxq True 7d22h 11â”œâ”€ControlPlane - KubeadmControlPlane/stc-tkgm-wld-cluster-1-22z8x True 7d22h 12â”‚ â””â”€Machine/stc-tkgm-wld-cluster-1-22z8x-jjb66 True 7d22h 13â””â”€Workers 14 â””â”€MachineDeployment/stc-tkgm-wld-cluster-1-md-0-2qmkw True 3d3h 15 â”œâ”€Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-6c4789d7b5-lj5wl True 7d22h 16 â””â”€Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-6c4789d7b5-wb7k9 True 7d22h If everything is green its time to get the kubeconfig for the cluster so we can start consume it. This is done like this:\n1tanzu cluster kubeconfig get stc-tkgm-wld-cluster-1 --namespace stc-tkgm-ns-1 --admin --export-file stc-tkgm-wld-cluster-1-k8s-config.yaml Now you can copy this to your preferred workstation and start consuming.\nNote! The kubeconfigs I have used here is all admin privileges and is not something you will use in production where you want to have granular user access. I will create a post around user management in both TKGm and TKGs later.\nThe next sections will cover how to upgrade TKG, some configs on the workload clusters themselves around AKO and Antrea.\nAntrea configs If there is a feature you would like to enable in Antrea in one of your workload clusters, we need to create an AntreaConfig by using the AntreaConfig CRD (this is one way of doing it) and apply it on the Namespace where your workload cluster resides. This is the same approach as we do in vSphere 8 with Tanzu - see here\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: stc-tkgm-wld-cluster-1-antrea-package # notice the naming-convention cluster name-antrea-package 5 namespace: stc-tkgm-ns-1 # your vSphere Namespace the TKC cluster is in. 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: true 14 Egress: true 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Avi/AKO configs In TKGm we can override the default AKO settings by using AKODeploymentConfig CRD. We apply this configuration from the TKG Managment cluster on the respective Workload cluster by using labels. An example of such a config yaml:\n1apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 2kind: AKODeploymentConfig 3metadata: 4 name: ako-stc-tkgm-wld-cluster-1 5spec: 6 adminCredentialRef: 7 name: avi-controller-credentials 8 namespace: tkg-system-networking 9 certificateAuthorityRef: 10 name: avi-controller-ca 11 namespace: tkg-system-networking 12 cloudName: stc-nsx-cloud 13 clusterSelector: 14 matchLabels: 15 ako-stc-wld-1: \u0026#34;ako-l7\u0026#34; 16 controller: 172.24.3.50 17 dataNetwork: 18 cidr: 10.13.103.0/24 19 name: vip-tkg-wld-l7 20 controlPlaneNetwork: 21 cidr: 10.13.102.0/24 22 name: vip-tkg-wld-l4 23 extraConfigs: 24 cniPlugin: antrea 25 disableStaticRouteSync: false # required 26 ingress: 27 defaultIngressController: true 28 disableIngressClass: false # required 29 nodeNetworkList: # required 30 - cidrs: 31 - 10.13.21.0/24 32 networkName: ls-tkg-wld-1 33 serviceType: NodePortLocal # required 34 shardVSSize: SMALL # required 35 l4Config: 36 autoFQDN: default 37 networksConfig: 38 nsxtT1LR: /infra/tier-1s/Tier-1 39 serviceEngineGroup: tkgm-se-group Notice the:\n1 clusterSelector: 2 matchLabels: 3 ako-stc-wld-1: \u0026#34;ako-l7\u0026#34; We need to apply this label to our workload cluster. From the TKG management cluster list all your clusters:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get cluster -A 2NAMESPACE NAME PHASE AGE VERSION 3stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Provisioned 7d23h v1.24.9+vmware.1 4stc-tkgm-ns-2 stc-tkgm-wld-cluster-2 Provisioned 7d17h v1.24.9+vmware.1 5tkg-system tkg-stc-mgmt-cluster Provisioned 8d v1.24.9+vmware.1 Apply the above label:\n1kubectl label cluster -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 ako-stc-wld-1=ako-l7 Now run the get cluster command again but with the value --show-labels to see if it has been applied:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get cluster -A --show-labels 2NAMESPACE NAME PHASE AGE VERSION LABELS 3stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Provisioned 7d23h v1.24.9+vmware.1 ako-stc-wld-1=ako-l7,cluster.x-k8s.io/cluster-name=stc-tkgm-wld-cluster-1,networking.tkg.tanzu.vmware.com/avi=ako-stc-tkgm-wld-cluster-1,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.1,tkg.tanzu.vmware.com/cluster-name=stc-tkgm-wld-cluster-1,topology.cluster.x-k8s.io/owned= Looks good. Then we can apply the AKODeploymentConfig above.\n1k apply -f ako-wld-cluster-1.yaml Verify if the AKODeploymentConfig has been applied:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get akodeploymentconfigs.networking.tkg.tanzu.vmware.com 2NAME AGE 3ako-stc-tkgm-wld-cluster-1 7d21h 4ako-stc-tkgm-wld-cluster-2 7d6h 5install-ako-for-all 8d 6install-ako-for-management-cluster 8d Now head back your workload cluster and check the AKO pod whether it has been restarted, if you dont want to wait you can always delete the pod to speed up the changes. To verify the changes have a look at the ako configmap like this:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get configmaps -n avi-system avi-k8s-config -oyaml 2apiVersion: v1 3data: 4 apiServerPort: \u0026#34;8080\u0026#34; 5 autoFQDN: default 6 cloudName: stc-nsx-cloud 7 clusterName: stc-tkgm-ns-1-stc-tkgm-wld-cluster-1 8 cniPlugin: antrea 9 controllerIP: 172.24.3.50 10 controllerVersion: 22.1.2 11 defaultIngController: \u0026#34;true\u0026#34; 12 deleteConfig: \u0026#34;false\u0026#34; 13 disableStaticRouteSync: \u0026#34;false\u0026#34; 14 fullSyncFrequency: \u0026#34;1800\u0026#34; 15 logLevel: INFO 16 nodeNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-wld-1\u0026#34;,\u0026#34;cidrs\u0026#34;:[\u0026#34;10.13.21.0/24\u0026#34;]}]\u0026#39; 17 nsxtT1LR: /infra/tier-1s/Tier-1 18 serviceEngineGroupName: tkgm-se-group 19 serviceType: NodePortLocal 20 shardVSSize: SMALL 21 vipNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;vip-tkg-wld-l7\u0026#34;,\u0026#34;cidr\u0026#34;:\u0026#34;10.13.103.0/24\u0026#34;}]\u0026#39; 22kind: ConfigMap 23metadata: 24 annotations: 25 kapp.k14s.io/identity: v1;avi-system//ConfigMap/avi-k8s-config;v1 26 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;apiServerPort\u0026#34;:\u0026#34;8080\u0026#34;,\u0026#34;autoFQDN\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterName\u0026#34;:\u0026#34;stc-tkgm-ns-1-stc-tkgm-wld-cluster-1\u0026#34;,\u0026#34;cniPlugin\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;controllerIP\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;controllerVersion\u0026#34;:\u0026#34;22.1.2\u0026#34;,\u0026#34;defaultIngController\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;deleteConfig\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;disableStaticRouteSync\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;fullSyncFrequency\u0026#34;:\u0026#34;1800\u0026#34;,\u0026#34;logLevel\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;nodeNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;ls-tkg-wld-1\\\u0026#34;,\\\u0026#34;cidrs\\\u0026#34;:[\\\u0026#34;10.13.21.0/24\\\u0026#34;]}]\u0026#34;,\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;/infra/tier-1s/Tier-1\u0026#34;,\u0026#34;serviceEngineGroupName\u0026#34;:\u0026#34;tkgm-se-group\u0026#34;,\u0026#34;serviceType\u0026#34;:\u0026#34;NodePortLocal\u0026#34;,\u0026#34;shardVSSize\u0026#34;:\u0026#34;SMALL\u0026#34;,\u0026#34;vipNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;vip-tkg-wld-l7\\\u0026#34;,\\\u0026#34;cidr\\\u0026#34;:\\\u0026#34;10.13.103.0/24\\\u0026#34;}]\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1678977773033139694\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.ae838cced3b6caccc5a03bfb3ae65cd7\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;avi-k8s-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;avi-system\u0026#34;}}\u0026#39; 27 kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 28 creationTimestamp: \u0026#34;2023-03-16T14:43:11Z\u0026#34; 29 labels: 30 kapp.k14s.io/app: \u0026#34;1678977773033139694\u0026#34; 31 kapp.k14s.io/association: v1.ae838cced3b6caccc5a03bfb3ae65cd7 32 name: avi-k8s-config 33 namespace: avi-system 34 resourceVersion: \u0026#34;19561\u0026#34; 35 uid: 1baa90b2-e5d7-4177-ae34-6c558b5cfe29 It should reflect the changes we applied...\nAntrea RBAC Antrea comes with a list of Tiers where we can place our Antrea Native Policies. These can also be used to restrict who is allowed to apply policies and not. See this page for more information for now. I will update this section later with my own details - including the integration with NSX.\nUpgrade TKG (from 2.1 to 2.1.1) When a new TKG relase is available we can upgrade to use this new release. The steps I have followed are explained in detail here. I recommend to always follow the updated information there.\nTo upgrade TKG these are the typical steps:\nDownload the latest Tanzu CLI - from my.vmware.com Download the latest Tanzu kubectl - from my.vmware.com Download the latest Photon or Ubuntu OVA VM template - from my.vmware.com Upgrade the TKG Management cluster Upgrade the TKG Workload clusters So lets get into it.\nUpgrade CLI tools and dependencies I have already downloaded the Ubuntu VM image for version 2.1.1 into my vCenter and converted it to a template. I have also downloaded the Tanzu CLI tools and Tanzu kubectl for version 2.1.1. Now I need to install the Tanzu CLI and Tanzu kubectl. So I will getting back into my bootstrap machine used previously where I already have Tanzu CLI 2.1 installed.\nThe first thing I need to is to delete the following file:\n1~/.config/tanzu/tkg/compatibility/tkg-compatibility.yaml Extract the downloaded Tanzu CLI 2.1.1 packages (this will create a cli folder where you are placed. So if you want to use another folder create this first and extract the file in there) :\n1tar -xvf tanzu-cli-bundle-linux-amd64.tar.gz 1andreasm@tkg-bootstrap:~/tanzu$ tar -xvf tanzu-cli-bundle-linux-amd64.2.1.1.tar.gz 2cli/ 3cli/core/ 4cli/core/v0.28.1/ 5cli/core/v0.28.1/tanzu-core-linux_amd64 6cli/tanzu-framework-plugins-standalone-linux-amd64.tar.gz 7cli/tanzu-framework-plugins-context-linux-amd64.tar.gz 8cli/ytt-linux-amd64-v0.43.1+vmware.1.gz 9cli/kapp-linux-amd64-v0.53.2+vmware.1.gz 10cli/imgpkg-linux-amd64-v0.31.1+vmware.1.gz 11cli/kbld-linux-amd64-v0.35.1+vmware.1.gz 12cli/vendir-linux-amd64-v0.30.1+vmware.1.gz Navigate to the cli folder and install the different packages.\nInstall Tanzu CLI:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ sudo install core/v0.28.1/tanzu-core-linux_amd64 /usr/local/bin/tanzu Initialize the Tanzu CLI:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu init 2â„¹ Checking for required plugins... 3â„¹ Installing plugin \u0026#39;secret:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 4â„¹ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; 5â„¹ Installing plugin \u0026#39;login:v0.28.1\u0026#39; 6â„¹ Installing plugin \u0026#39;management-cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 7â„¹ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 8â„¹ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; 9â„¹ Installing plugin \u0026#39;telemetry:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 10â„¹ Successfully installed all required plugins 11âœ” successfully initialized CLI Verify version:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu version 2version: v0.28.1 3buildDate: 2023-03-07 4sha: 0e6704777-dirty Now the Tanzu plugins:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin clean 2âœ” successfully cleaned up all plugins 1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin sync 2â„¹ Checking for required plugins... 3â„¹ Installing plugin \u0026#39;management-cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 4â„¹ Installing plugin \u0026#39;secret:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 5â„¹ Installing plugin \u0026#39;telemetry:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 6â„¹ Installing plugin \u0026#39;cluster:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; 7â„¹ Installing plugin \u0026#39;kubernetes-release:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; 8â„¹ Installing plugin \u0026#39;login:v0.28.1\u0026#39; 9â„¹ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 10â„¹ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; 11â„¹ Installing plugin \u0026#39;feature:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; 12â„¹ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; 13âœ– [unable to fetch the plugin metadata for plugin \u0026#34;login\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;package\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;pinniped-auth\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;isolated-cluster\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64] 14andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin sync 15â„¹ Checking for required plugins... 16â„¹ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; 17â„¹ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; 18â„¹ Installing plugin \u0026#39;login:v0.28.1\u0026#39; 19â„¹ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 20â„¹ Successfully installed all required plugins 21âœ” Done Note! I had to run the comand twice as I ecountered an issue on first try. Now list the plugins:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin list 2Standalone Plugins 3 NAME DESCRIPTION TARGET DISCOVERY VERSION STATUS 4 isolated-cluster isolated-cluster operations default v0.28.1 installed 5 login Login to the platform default v0.28.1 installed 6 pinniped-auth Pinniped authentication operations (usually not directly invoked) default v0.28.1 installed 7 management-cluster Kubernetes management-cluster operations kubernetes default v0.28.1 installed 8 package Tanzu package management kubernetes default v0.28.1 installed 9 secret Tanzu secret management kubernetes default v0.28.1 installed 10 telemetry Configure cluster-wide telemetry settings kubernetes default v0.28.1 installed 11 12Plugins from Context: tkg-stc-mgmt-cluster 13 NAME DESCRIPTION TARGET VERSION STATUS 14 cluster Kubernetes cluster operations kubernetes v0.28.0 installed 15 feature Operate on features and featuregates kubernetes v0.28.0 installed 16 kubernetes-release Kubernetes release operations kubernetes v0.28.0 installed Install the Tanzu kubectl:\n1andreasm@tkg-bootstrap:~/tanzu$ gunzip kubectl-linux-v1.24.10+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu$ chmod ugo+x kubectl-linux-v1.24.10+vmware.1 3andreasm@tkg-bootstrap:~/tanzu$ sudo install kubectl-linux-v1.24.10+vmware.1 /usr/local/bin/kubectl Check version:\n1andreasm@tkg-bootstrap:~/tanzu$ kubectl version 2WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. 3Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.10+vmware.1\u0026#34;, GitCommit:\u0026#34;b980a736cbd2ac0c5f7ca793122fd4231f705889\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-01-24T15:36:34Z\u0026#34;, GoVersion:\u0026#34;go1.19.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 4Kustomize Version: v4.5.4 5Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.9+vmware.1\u0026#34;, GitCommit:\u0026#34;d1d7c19c9b6265a8dcd1b2ab2620ec0fc7cee784\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-12-14T06:23:39Z\u0026#34;, GoVersion:\u0026#34;go1.18.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Install the Carvel tools. From the cli folder first out is ytt. Install ytt:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip ytt-linux-amd64-v0.43.1+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x ytt-linux-amd64-v0.43.1+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./ytt-linux-amd64-v0.43.1+vmware.1 /usr/local/bin/ytt 4andreasm@tkg-bootstrap:~/tanzu/cli$ ytt --version 5ytt version 0.43.1 Instal kapp:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip kapp-linux-amd64-v0.53.2+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x kapp-linux-amd64-v0.53.2+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./kapp-linux-amd64-v0.53.2+vmware.1 /usr/local/bin/kapp 4andreasm@tkg-bootstrap:~/tanzu/cli$ kapp --version 5kapp version 0.53.2 6 7Succeeded Install kbld:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip kbld-linux-amd64-v0.35.1+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x kbld-linux-amd64-v0.35.1+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./kbld-linux-amd64-v0.35.1+vmware.1 /usr/local/bin/kbld 4andreasm@tkg-bootstrap:~/tanzu/cli$ kbld --version 5kbld version 0.35.1 6 7Succeeded Install imgpkg:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip imgpkg-linux-amd64-v0.31.1+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x imgpkg-linux-amd64-v0.31.1+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./imgpkg-linux-amd64-v0.31.1+vmware.1 /usr/local/bin/imgpkg 4andreasm@tkg-bootstrap:~/tanzu/cli$ imgpkg --version 5imgpkg version 0.31.1 6 7Succeeded We have done the verification of the different versions, but we should have Tanzu cli version v0.28.1\nUpgrade the TKG Management cluster Now we can proceed with the upgrade process. One important document to check is this! Known Issues... Check whether you are using environments, if you happen to use them we need to unset them.\n1andreasm@tkg-bootstrap:~/tanzu/cli$ printenv I am clear here and will now start the upgrading of my standalone TKG Management cluster Make sure you are in the context of the TKG management cluster and that you have converted the new Ubuntu VM image as template.\n1andreasm@tkg-bootstrap:~$ kubectl config current-context 2tkg-stc-mgmt-cluster-admin@tkg-stc-mgmt-cluster If not, use the following command:\n1andreasm@tkg-bootstrap:~$ tanzu login 2? Select a server [Use arrows to move, type to filter] 3\u0026gt; tkg-stc-mgmt-cluster() 4 + new server 1andreasm@tkg-bootstrap:~$ tanzu login 2? Select a server tkg-stc-mgmt-cluster() 3âœ” successfully logged in to management cluster using the kubeconfig tkg-stc-mgmt-cluster 4â„¹ Checking for required plugins... 5â„¹ All required plugins are already installed and up-to-date Here goes: (To start the upgrade of the management cluster)\n1andreasm@tkg-bootstrap:~$ tanzu mc upgrade 2Upgrading management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; to TKG version \u0026#39;v2.1.1\u0026#39; with Kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;. Are you sure? [y/N]: Eh.... yes...\nProgress:\n1andreasm@tkg-bootstrap:~$ tanzu mc upgrade 2Upgrading management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; to TKG version \u0026#39;v2.1.1\u0026#39; with Kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;. Are you sure? [y/N]: y 3Validating the compatibility before management cluster upgrade 4Validating for the required environment variables to be set 5Validating for the user configuration secret to be existed in the cluster 6Warning: unable to find component \u0026#39;kube_rbac_proxy\u0026#39; under BoM 7Upgrading management cluster providers... 8 infrastructure-ipam-in-cluster provider\u0026#39;s version is missing in BOM file, so it would not be upgraded 9Checking cert-manager version... 10Cert-manager is already up to date 11Performing upgrade... 12Scaling down Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-system\u0026#34; 13Scaling down Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; 14Scaling down Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; 15Scaling down Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capv-system\u0026#34; 16Deleting Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-system\u0026#34; 17Installing Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-system\u0026#34; 18Deleting Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; 19Installing Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; 20Deleting Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; 21Installing Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; 22Deleting Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capv-system\u0026#34; 23Installing Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;v1.5.3\u0026#34; TargetNamespace=\u0026#34;capv-system\u0026#34; 24Management cluster providers upgraded successfully... 25Preparing addons manager for upgrade 26Upgrading kapp-controller... 27Adding last-applied annotation on kapp-controller... 28Removing old management components... 29Upgrading management components... 30â„¹ Updating package repository \u0026#39;tanzu-management\u0026#39; 31â„¹ Getting package repository \u0026#39;tanzu-management\u0026#39; 32â„¹ Validating provided settings for the package repository 33â„¹ Updating package repository resource 34â„¹ Waiting for \u0026#39;PackageRepository\u0026#39; reconciliation for \u0026#39;tanzu-management\u0026#39; 35â„¹ \u0026#39;PackageRepository\u0026#39; resource install status: Reconciling 36â„¹ \u0026#39;PackageRepository\u0026#39; resource install status: ReconcileSucceeded 37â„¹ Updated package repository \u0026#39;tanzu-management\u0026#39; in namespace \u0026#39;tkg-system\u0026#39; 38â„¹ Installing package \u0026#39;tkg.tanzu.vmware.com\u0026#39; 39â„¹ Updating package \u0026#39;tkg-pkg\u0026#39; 40â„¹ Getting package install for \u0026#39;tkg-pkg\u0026#39; 41â„¹ Getting package metadata for \u0026#39;tkg.tanzu.vmware.com\u0026#39; 42â„¹ Updating secret \u0026#39;tkg-pkg-tkg-system-values\u0026#39; 43â„¹ Updating package install for \u0026#39;tkg-pkg\u0026#39; 44â„¹ Waiting for \u0026#39;PackageInstall\u0026#39; reconciliation for \u0026#39;tkg-pkg\u0026#39; 45â„¹ \u0026#39;PackageInstall\u0026#39; resource install status: ReconcileSucceeded 46â„¹ Updated installed package \u0026#39;tkg-pkg\u0026#39; 47Cleanup core packages repository... 48Core package repository not found, no need to cleanup 49Upgrading management cluster kubernetes version... 50Upgrading kubernetes cluster to `v1.24.10+vmware.1` version, tkr version: `v1.24.10+vmware.1-tkg.2` 51Waiting for kubernetes version to be updated for control plane nodes... 52Waiting for kubernetes version to be updated for worker nodes... In vCenter we should start see some action also:\nTwo control plane nodes:\nNo longer:\n1management cluster is opted out of telemetry - skipping telemetry image upgrade 2Creating tkg-bom versioned ConfigMaps... 3Management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; successfully upgraded to TKG version \u0026#39;v2.1.1\u0026#39; with kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39; 4â„¹ Checking for required plugins... 5â„¹ Installing plugin \u0026#39;kubernetes-release:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 6â„¹ Installing plugin \u0026#39;cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 7â„¹ Installing plugin \u0026#39;feature:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 8â„¹ Successfully installed all required plugins Well, it finished successfully.\nLets verify with Tanzu CLI:\n1andreasm@tkg-bootstrap:~$ tanzu cluster list --include-management-cluster -A 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 4 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 5 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Looks good, notice the different versions. Management cluster is upgraded to latest version, workload clusters are still on its older version. They are up next.\nLets do a last check before we head to Workload cluster upgrade.\n1andreasm@tkg-bootstrap:~$ tanzu mc get 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 4 5 6Details: 7 8NAME READY SEVERITY REASON SINCE MESSAGE 9/tkg-stc-mgmt-cluster True 17m 10â”œâ”€ClusterInfrastructure - VSphereCluster/tkg-stc-mgmt-cluster-xw6xs True 8d 11â”œâ”€ControlPlane - KubeadmControlPlane/tkg-stc-mgmt-cluster-wrxtl True 17m 12â”‚ â””â”€Machine/tkg-stc-mgmt-cluster-wrxtl-csrnt True 24m 13â””â”€Workers 14 â””â”€MachineDeployment/tkg-stc-mgmt-cluster-md-0-vs9dc True 10m 15 â”œâ”€Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-54554f9575-7hdfc True 14m 16 â””â”€Machine/tkg-stc-mgmt-cluster-md-0-vs9dc-54554f9575-ng9lx True 7m4s 17 18 19Providers: 20 21 NAMESPACE NAME TYPE PROVIDERNAME VERSION WATCHNAMESPACE 22 caip-in-cluster-system infrastructure-ipam-in-cluster InfrastructureProvider ipam-in-cluster v0.1.0 23 capi-kubeadm-bootstrap-system bootstrap-kubeadm BootstrapProvider kubeadm v1.2.8 24 capi-kubeadm-control-plane-system control-plane-kubeadm ControlPlaneProvider kubeadm v1.2.8 25 capi-system cluster-api CoreProvider cluster-api v1.2.8 26 capv-system infrastructure-vsphere InfrastructureProvider vsphere v1.5.3 Congrats, head over to next level ðŸ˜„\nUpgrade workload cluster This procedure is much simpler, almost as simple as starting a game in MS-DOS 6.2 requiring a bit over 600kb convential memory. Make sure your are still in the TKG Management cluster context.\nAs done above list out the cluster you have and notice the versions they are on now.:\n1andreasm@tkg-bootstrap:~$ tanzu cluster list --include-management-cluster -A 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 4 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 5 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Check if there are any new releases available from the management cluster:\n1andreasm@tkg-bootstrap:~$ tanzu kubernetes-release get 2 NAME VERSION COMPATIBLE ACTIVE UPDATES AVAILABLE 3 v1.22.17---vmware.1-tkg.2 v1.22.17+vmware.1-tkg.2 True True 4 v1.23.16---vmware.1-tkg.2 v1.23.16+vmware.1-tkg.2 True True 5 v1.24.10---vmware.1-tkg.2 v1.24.10+vmware.1-tkg.2 True True There is one there.. v1.24.10 and its compatible.\nLets check whether there are any updates ready for our workload cluster:\n1andreasm@tkg-bootstrap:~$ tanzu cluster available-upgrades get -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 2 NAME VERSION COMPATIBLE 3 v1.24.10---vmware.1-tkg.2 v1.24.10+vmware.1-tkg.2 True It is...\nLets upgrade it:\n1andreasm@tkg-bootstrap:~$ tanzu cluster upgrade -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 2Upgrading workload cluster \u0026#39;stc-tkgm-wld-cluster-1\u0026#39; to kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;, tkr version \u0026#39;v1.24.10+vmware.1-tkg.2\u0026#39;. Are you sure? [y/N]: y 3Upgrading kubernetes cluster to `v1.24.10+vmware.1` version, tkr version: `v1.24.10+vmware.1-tkg.2` 4Waiting for kubernetes version to be updated for control plane nodes... y for YES\nSit back and wait for the upgrade process is to do its thing. You can monitor the output from the current terminal, and if something is happening in vCenter. Clone operations, power on, power off and delete.\nAnd the result is in:\n1Waiting for kubernetes version to be updated for worker nodes... 2Cluster \u0026#39;stc-tkgm-wld-cluster-1\u0026#39; successfully upgraded to kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39; We have a winner.\nLets quickly check with Tanzu CLI:\n1andreasm@tkg-bootstrap:~$ tanzu cluster get stc-tkgm-wld-cluster-1 -n stc-tkgm-ns-1 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.10+vmware.1 \u0026lt;none\u0026gt; v1.24.10---vmware.1-tkg.2 4 5 6Details: 7 8NAME READY SEVERITY REASON SINCE MESSAGE 9/stc-tkgm-wld-cluster-1 True 11m 10â”œâ”€ClusterInfrastructure - VSphereCluster/stc-tkgm-wld-cluster-1-lzjxq True 8d 11â”œâ”€ControlPlane - KubeadmControlPlane/stc-tkgm-wld-cluster-1-22z8x True 11m 12â”‚ â””â”€Machine/stc-tkgm-wld-cluster-1-22z8x-mtpgs True 15m 13â””â”€Workers 14 â””â”€MachineDeployment/stc-tkgm-wld-cluster-1-md-0-2qmkw True 39m 15 â”œâ”€Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-58c5764865-7xvfn True 8m31s 16 â””â”€Machine/stc-tkgm-wld-cluster-1-md-0-2qmkw-58c5764865-c7rqj True 3m29s Couldn't be better. Thats it then. Its Friday so have a great weekend and thanks for reading.\n","link":"https://blog.andreasm.io/2023/03/22/tanzu-kubernetes-grid-2.1/","section":"post","tags":["TKG 2.1","AVI","AKO","NSX","KUBERNETES","TANZU"],"title":"Tanzu Kubernetes Grid 2.1"},{"body":"","link":"https://blog.andreasm.io/tags/tkg-2.1/","section":"tags","tags":null,"title":"TKG 2.1"},{"body":"","link":"https://blog.andreasm.io/categories/blog/","section":"categories","tags":null,"title":"Blog"},{"body":"","link":"https://blog.andreasm.io/tags/github/","section":"tags","tags":null,"title":"github"},{"body":"","link":"https://blog.andreasm.io/categories/github/","section":"categories","tags":null,"title":"Github"},{"body":"Hugo on Github: I have been running my blog page locally on Kubernetes for a long time now. And it has worked very well. But one always have to try something new, and I have always wanted to explore the option to host it on Github to have one maintenance task less to worry about. To get started with this I got some absolutely great help from my colleague Robert who put me into the right track to get this project rolling. In short this post will cover how I did it (with the help from Robert). The moving parts used in this post is Github, Github Pages, Hugo, git, git submodules, a DNS record for my custom domain name and a Linux terminal with git installed. In Github we will end up with two repositories, one for the Hugo files themselves, and one which will be used our Github Page (the actual webpage of your blog). The goal is to be able to add content and update your blog with just a few commands and it is live. Preparations in Github In my Github account I create two repositories, one for the \u0026quot;Hugo\u0026quot; contents itself (config, themes, contents etc) and one repository which will host the actual Github page itself. Lets dive into the details ðŸ˜„\nIf not already a Github user, head over to Github.com and create yourself a user account. When logged into your Github account, create two repositories:\nOne repository is where you have all your Hugo files, content, posts, pages, config folders, css, archetypes and public folder when generating your pages. This repository is created like this in Github:\nAs this is a free Github account the only option is a Public repository.\nNow the second repositiory is created identically but with one important difference, the repository name. This has to start with your username and the github domain github.io (discard the red warning in example below, I already have my repository created using same name). This repository will be used to host your blog's frontpage/webpage. This is referred to as Github Pages\nNext we need to clone into our two newly created repositories.\nGit To clone a public repo there is no need to authenticate, but you would like to create your content locally and push them to your remote git repo so we need to authenticate to our Github account. Github dont use password and username for git authentication, but instead SSH keys. And when cloning into your repo one need to use the correct way to clone it for the authentication to work. More on that later. First prepare your git environment on your workstation.\nSSH keys On your workstation generate your SSH keys (if not already done):\n1ssh-keygen -t rsa -b 4096 -C \u0026#34;andreasm@ubuntulaptop\u0026#34; Answer default to prompts, enter a desired passphrase if wanted leave empty without any passphrase.\nNow that the SSH keys as generated copy the content from the ~/.ssh/id_rsa.pub by issuing something like this cat ~/.ssh/id_rsa.pub and copy the wole content. Go into your Github account click on your user top right corner and -\u0026gt; settings\nThen SSH and GPT keys:\nAnd then New SSH Key and paste your SSH key. Give it a name.\nNow your Github account can authenticate your workstation by using its public SSH key. To use this approach one have to clone into a project by using git clone git@github.com:andreasm80/blog.local.git where the git@github.com:andreasm80/blog.local.git is found from your repository by clicking at the green Code on your repo:\nGit - continued Now that the SSH keys are configured, our workstation is prepared to authenticate against our Github repositories. Create a folder in your Linux workstation, called something with github or whatever you like. Enter into that folder. Now enter the following command:\n1git clone git@github.com:andreasm80/blog.local.git If you dont specify a folder at the end of the command, git will just create a folder with the same name as the repository your are cloning. If you dont want that add a folder name at the end like this:\n1git clone git@github.com:andreasm80/blog.local.git localfoldername Now enter into the newly created folder: cd blog.local To verify that you are \u0026quot;linked\u0026quot; to the correct repository run the following command:\n1git remote -vvv 2origin\tgit@github.com:andreasm80/blog.local.git (fetch) 3origin\tgit@github.com:andreasm80/blog.local.git (push) I already have a Hugo \u0026quot;project\u0026quot; folder where I have my blog page content stored. Instead of just re-using this folder as is I copy all the content to this new folder. Then I delete the public folder (in the new folder) as this will be recreated later. Before pushing all the newly copied content into the cloned github folder above I need to do some preparation for git.\n1git config --global user.email \u0026#34;email@email.com\u0026#34; #is used to sign your commits 2git config --global user.name \u0026#34;AndreasM\u0026#34; #is used to sign your commits - can be whatever name you want Then I need to tell git which files it should track, commit and push when I am ready. To do this type in:\n1git add . #notice the \u0026#34;.\u0026#34; This means I will add all files in the folder. If you dont tell git this it will not commit and push them. You can check this with the following command:\n1andreasm@ubuntu:~/git status 2On branch main 3Your branch is up to date with \u0026#39;origin/main\u0026#39;. 4 5Changes not staged for commit: 6 (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) 7 (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) 8\tmodified: public (new commits) 9 10Untracked files: 11 (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 12\tcontent/post/2023-03-04-running-hugo-on-github/ 13 14no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Now you can commit by doing this command:\n1git commit -s -m \u0026#34;comment-description\u0026#34; # the -s is for signoff and the -m is the comment/message And the last thing to do now is to push the files locally to your remote github repository.\n1git push If you go into your Github page now you will see all your files there in the repo above, but still there is no working blog-page yet.\nGit submodules As explained above, we need to create some kind of \u0026quot;softlink\u0026quot; for the folder public to point to the repository we will use as our web-page. In git we can use submodules for that. The reason for that is each time you generate your Hugo content, Hugo will create a public folder which contains the actual HTML files. And instead of copying the content of this folder each time you generate your webpage we link this folder to our Github Page repository. This is how to enable git submodules.\nWhile still in your blog.local folder (root of the first repository) enter the following:\n1git submodule add git@github.com:andreasm80/andreasm80.github.io.git public/ #the url is from my second repo being used for Github Pages With the above command I am instructing git to create a submodule point to my remote Github repository I will use as my Github Page and also pointing it to the local folder public.\nThen the next command is to initialize the submodule:\n1git submodule init 2#or a specific module as below 3git submodule init public/ In the current folder we should have file called .gitmodules have a look inside and it should contain something like this:\n1[submodule \u0026#34;public\u0026#34;] 2 path = public 3 url = git@github.com:andreasm80/andreasm80.github.io.git This is how my folder structure looks like now:\n1github/blog.local 2archetypes 3assets 4config 5content 6.git 7.gitmodules 8go.mod 9go.sum 10.hugo_build.lock 11layouts 12LICENSE 13public #this folder was created by the git submodules command 14README.md 15resources 16static Check the status on the submodule you have created:\n1git submodule status 2+6afb3af12e86416ad8ff255d9042d89bd9ddc719 public (heads/master) # status symbols infront, sha, name of submodule and branch Commit the changes we have done so far in our blog.local repo:\n1git add . 2git commit -s -m \u0026#34;added submodule public\u0026#34; 3git push Head over to your Github blog.local repo and you should see that the public folder is quite different from the others:\nAnd if you click on it you will be redirected to the repository of your second repository used for your Github Page.\nNow back to the CLI terminal...\nTo have something to populate the public folder with we can now run the `hugo -v -D' command to generate our webpage. It will write everything needed in our public folder.\n1hugo -v -D Now cd into the public folder. Check the repo it is pointing to:\n1git remote -vvv 2origin\tgit@github.com:andreasm80/andreasm80.github.io.git (fetch) 3origin\tgit@github.com:andreasm80/andreasm80.github.io.git (push) Check if there is something new or untracked here:\n1git status 2Untracked files: 3 (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 4\t2023/03/04/hosting-my-blog-on-github/ 5\ttags/static-site-generator/ 6 7no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Run the following commands:\n1git add . 2Changes to be committed: 3 (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) 4\tnew file: content/post/2023-03-04-running-hugo-on-github/images/GitHub-Logo300px.png 5\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305091118809.png 6\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305091558280.png 7\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305092142364.png 8\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305094906285.png 9\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100514339.png 10\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100657366.png 11\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100847587.png 12\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305101357702.png 13\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305112013507.png 14\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305112521903.png 15\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305113016848.png 16\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305113102660.png 17\tnew file: content/post/2023-03-04-running-hugo-on-github/index.md 18\tmodified: public 19 20 21git commit -s -m \u0026#34;added content\u0026#34; 22git push Now Github should start build your blog-page. This can be seen under Actions here:\nUpdating my blog with this content:\nFor backup reasons it could also be smart to commit and push the blog.local folder also.\nGo back one level (from public to blog.local folder, root)\n1git status 2git add . 3git commit -s -m \u0026#34;added-content\u0026#34; 4git push New machine - set up environment If you need to start from scratch, you have a new machine or whatever you dont have to go through all the steps above, you only need to add the SSH key, creating the git config -global settings. After that It is just as simple as doing this:\n1git clone --recurse-submodules git@github.com:andreasm80/blog.local.git #this will also clone submodules 2# then it is just regular git commands: 3git add . 4git commit -s -m \u0026#34;message\u0026#34; 5git push Custom domain - Github Pages If you would like to present your Github Page (aka blog page) on a different domain you happen to own head over to settings in your Github Pages repository:\nThe on the left side click on Pages\nAnd in Pages type in your custom domain here and enable Enforce HTTPS:\nWhen you click save Github will place a file called CNAME in the root of your Github pages repository (mine andreasm80.github.io) where the content is the dns record you have entered in the Custom Domain field above. So you would need to fetch this locally with git to be in sync again.\n1# you can either enter your submodule(s) directory and run: 2git fetch 3# or you can stay in the \u0026#34;root\u0026#34; folder and enter: 4git submodule foreach \u0026#39;git fetch\u0026#39; 5#which will do a fetch on all submodules Now you need to go to your DNS provider and add a CNAME pointing to your Gitub pages repository name, in my case that is andreasm80.github.io. So I have created this cname record:\nGithub will manage the certificate for your Github Page, so you dont have to worry about that either. After some minutes/hours (depending on how fast DNS is updated) your blog page will be resolved on the custom domain. Mine is https://blog.andreasm.io\n","link":"https://blog.andreasm.io/2023/03/04/hosting-my-blog-on-github/","section":"post","tags":["github","static-site-generator","hugo"],"title":"Hosting my blog on Github"},{"body":"","link":"https://blog.andreasm.io/tags/hugo/","section":"tags","tags":null,"title":"hugo"},{"body":"","link":"https://blog.andreasm.io/tags/static-site-generator/","section":"tags","tags":null,"title":"static-site-generator"},{"body":"Antrea Egress: What is Egress when we talk about Kubernetes? Well if a pod wants to communicate to the outside world, outside the Kubernetes cluster it runs in, out of the worker node the pod resides on, this is egress traffic (definition \u0026quot;the action of going out of or leaving a place\u0026quot; and in network terminology means the direction is outward from itself).\nWhy does egress matter? Well, usually when the pods communicate out, they will use the IP address of the worker node they currently is deployed on. Thats means the actual pod IP is not the address you should be expecting when doing network inspection, tcpdump, firewall rules etc, it is the Kubernetes worker nodes IP addresses. What we call this network feature is NAT, Network Address Translation. All Kubernetes worker nodes will take the actual POD IP and translate it to its own IP before sending the traffic out of itself. And as we know, we don't know where the pod will be deployed, and the pods can be many and will relocate so in certain environments it can be hard, not granular enough to create firewall rules in the perimeter firewall to allow or block traffic from a certain pod when needed when we only can use the IP addresses of the worker nodes.\nThats where Antrea Egress comes in. With Antrea Egress we have the option to dictate which specific IP address the POD can use when communication out by using an IP address that is not its POD IP address but a valid and allowed IP address in the network. You can read more on the Antrea Egress feature here\nAs the diagram below will illustrate, when pods communicate out, the will all get their POD IP addresses translated into the worker node's IP address. And the firewall between worker node and the SQL server are only able to allow or block the IP address of the worker node. That means we potentially allow or block all pods coming from this node, or nodes if we allow the range of all the worker nodes.\nOfcourse we can use Antrea Native Policies which I have written about here or VMware NSX with NCP, and VMware NSX with Antrea Integration to do fine grained security from source. But still there are environments we need to handle rules in perimeter firewalls.\nSo, this post will show how to enable Antrea Egress in vSphere 8 with Tanzu. With the current release of Antrea there is only support of using the same L2 network as worker nodes for the Antrea Egress IP-Pool.\nAs we can see in the diagram above, Antrea Egress has been configured with an IP-Pool the pods can get if we apply Antrea Egress IPs for them to use. It will then take a free IP from the Egress IP Pool and which is within the same L2 subnet as the workers are configured on. This is very easy to do and achieve. No need to create static routes, Antrea takes care of the IP mapping. With this in place the firewall rule is now very strict, I can allow only the IP 10.10.1.40 (which is the IP the POD got from Antrea Egress Pool) and block the worker node ip address.\nBut.... I wanted to go a bit further and make use of L3 anyway for my Antrea Egress IP-Pool by utilizing BGP. Thats where the fun starts and this article is actually about. What I would like to achieve is that the IP address pool I configfure with Antrea Egress is something completely different from what the workers are using, not even the same L2 subnet but a completely different subnet. That means we need to involve some clever routing, and some configuration done on the worker nodes as its actually their IP addresses that becomes the gateway for our Antrea Egress subnets.\nSomething like this:\nThe diagram above shows a pod getting an IP address from the Egress pool which is something completely different from what subnet the worker node itself has. What Antrea does is creating a virtual interface on the worker node and assigns all the relevant ip addresses that are being used by Antrea Egress on that interface. They will use the default route on the worker node itself when going out, but the only component in the network that does know about this Egress subnet is the worker node itself, so it needs to tell this to his buddy routers out there. Either we create a static route on the router (could be the next hop of the worker node, the closest one, or some other hop in the infrastructure) or use BGP. Static route is more or less useless, too many ip addresses to update each time an egress ip is being applied, it could be on any worker node etc. So BGP is the way to go.\nThe Diagram below illustrates what happens if we dont tell our network routers where this network comes from and where it can be reached. It will egress out, but no one knows the way back.\nAs soon as the routers are informed of the address to this IP address they will be more than happy to deliver it for us, thats their job. Imagine being a postman delivering a packet somewhere in a country without any direction, address etc to narrow down his search field. In the scenario above the return traffic will most likely be sent out via a default route to the Internet and never to be seen again ðŸ˜„\nSo after we have been so kind to update with the exact delivery address below, we will get our mail again.\nEnough explanation already, get to the actual config of this.\nConfigure Antrea Egress in TKC (vSphere 8) Deploy your TKC cluster, it must be Ubuntu os for this to work:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-2-tkc-cluster-1 # give your tkc cluster a name 5 namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 3 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-medium 32 - name: storageClass 33 value: vsan-default-storage-policy Apply the correct Antrea configs, enable the Egress feature:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: wdc-2-tkc-cluster-1-antrea-package 5 namespace: wdc-2-ns-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: false 14 Egress: true #This needs to be enabled 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Log in to your newly created TKC cluster:\n1kubectl-vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name tkc-cluster-1 --tanzu-kubernetes-cluster-namespace ns-1 Delete the Antrea Controller and Agent pods. Now that we have done the initial config of our TKC cluster its time to test Antrea Egress within same subnet as worker nodes just to verify that it works.\nFrom now on you should stay in the context of your newly created TKC cluster.\nVerify Antrea Egress works with L2 To be able to use Antrea Egress we need to first start with an IP-Pool definition. So I create my definition like this:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: ExternalIPPool 3metadata: 4 name: antrea-ippool-l2 #just a name of this specific pool 5spec: 6 ipRanges: 7 - start: 10.102.6.40 # make sure not to use already used ips 8 end: 10.102.6.50 # should not overlap with worker nodes 9# - cidr: 10.101.112.0/32 # or you can define a whole range with cidr /32, /27 etc 10 nodeSelector: {} # you can remove the brackets and define which nodes you want below by using labels 11# matchLabels: 12# egress-l2: antrea-egress-l2 Apply your yaml definition above:\n1andreasm@linuxvm01:~/antrea/egress$ k apply -f ippool.wdc2.tkc.cluster-1.yaml 2externalippool.crd.antrea.io/antrea-ippool-l2 created Then we need to define the actual Egress itself. What we do with this config is selecting which pod that should get an Egress ip, from wich Antrea Egress IP pool (we can have several). So here is my example:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: Egress 3metadata: 4 name: antrea-egress-l2 #just a name of this specific Egress config 5spec: 6 appliedTo: 7 podSelector: 8 matchLabels: 9 app: ubuntu-20-04 ###Which pods should get Egress IPs 10 externalIPPool: antrea-ippool-l2 ###The IP pool I defined above. Before I apply it I will just make sure that I have a pod running the these labels, if not I will deploy it and then apply the Egress. So before I apply it I will show pinging from my pod to my jumpbox VM to identify which IP it is using before applying the Egress. And the apply the Egress and see if IP changes from the POD.\nMy ubuntu pod is up and running, I have entered the shell on it and initiates a ping from my pod to my jumpbox VM:\nSo here I can see the POD identifies itself with IP 10.102.6.15. Well which worker is that?:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n prod -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-20-04-c9776f965-t8nmf 1/1 Running 0 20h 20.40.1.2 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 5NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 6wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 7wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 8wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 9wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 20h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 That is this worker: wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds.\nSo far so good. Now let me apply the Egress on this POD.\n1andreasm@linuxvm01:~/antrea/egress$ k apply -f antrea.egress.l2.yaml 2egress.crd.antrea.io/antrea-egress-l2 created Now how does the ping look like?\nThis was as expected was it not? The POD now identifies itself with the IP 10.102.6.40 which happens to be the first IP in the range defined in the pool. Well, this is cool. Now we now that Antrea Egress works. But as mentioned, I want this to be done with a different subnet than the worker nodes. Se lets see have we can do that as \u0026quot;seemless\u0026quot; as possible, as we dont want to SSH into the worker nodes and do a bunch of manual installation, configuration and so on. No, we use Kubernetes for our needs here also.\nConfigure FRR on worker nodes What I want to achieve is to deploy FRR here on my worker nodes unattended to enable BGP pr worker node to my upstream BGP router (remember, to inform about the Egress network no one knows about). The TKC workers are managed appliances, they can be deleted, scaled up and down (more workers, fewer workers.) And Deploying something manual on them are just waste of time. So we need something that deploy FRR automatically on the worker nodes.\nFRR is easy to deploy and configure, and it is included in the Ubuntu default repo (one reason I wanted to use Ubuntu as worker os). FRR is a very good routing protocol suite in Linux and is deployed easy on Ubuntu with \u0026quot;apt install frr\u0026quot;. FRR can be configured to use BGP which is the routing protocol I want to use. FRR needs two config files, daemons and frr.conf. frr.conf is individual pr node (specific IP addresses) so we need to take that into consideration also. So how can I deploy FRR on the worker nodes with their individal configuration files to automatically establish a BGP neighbourship with my Upstream router, and without logging into the actual worker nodes themselves?\nBelow diagram just illustrating a tkc worker node with FRR installed and BGP configured:\nKubernetes and Daemonset. I have created three Daemonset definition files, one for the actual deployment of FRR on all the nodes:\nThen I have created on Daemonset definition to copy the frr.conf and daemons file for the specific worker nodes and the last definition file is used to uninistall everything on the worker nodes themselves (apt purge frr) if needed.\nLets start by just deploy FRR on the workers themselves.\nHere is the defintion for that:\n1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 namespace: kube-system 6 name: node-custom-setup 7 labels: 8 k8s-app: node-custom-setup 9 annotations: 10 command: \u0026amp;cmd apt-get update -qy \u0026amp;\u0026amp; apt-get install -qy frr 11spec: 12 selector: 13 matchLabels: 14 k8s-app: node-custom-setup 15 template: 16 metadata: 17 labels: 18 k8s-app: node-custom-setup 19 spec: 20 hostNetwork: true 21 initContainers: 22 - name: init-node 23 command: 24 - nsenter 25 - --mount=/proc/1/ns/mnt 26 - -- 27 - sh 28 - -c 29 - *cmd 30 image: alpine:3.7 31 securityContext: 32 privileged: true 33 hostPID: true 34 containers: 35 - name: wait 36 image: pause:3.1 37 hostPID: true 38 hostNetwork: true 39 tolerations: 40 - effect: NoSchedule 41 key: node-role.kubernetes.io/master 42 updateStrategy: 43 type: RollingUpdate Before I apply the above definiton I have logged into one of my TKC worker node and just wants to show that there is no FRR installed:\n1sh-5.0# cd /etc/frr 2sh: cd: /etc/frr: No such file or directory 3sh-5.0# systemctl status frr 4Unit frr.service could not be found. 5sh-5.0# hostname 6wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 7sh-5.0# Now apply:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f deploy-frr.yaml 2daemonset.apps/node-custom-setup configured 3 4andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system 5NAME READY STATUS RESTARTS AGE 6antrea-agent-4jrks 2/2 Running 0 20h 7antrea-agent-4khkr 2/2 Running 0 20h 8antrea-agent-4wxb5 2/2 Running 0 20h 9antrea-agent-ccglp 2/2 Running 0 20h 10antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 20h 11coredns-7d8f74b498-j5sjt 1/1 Running 0 21h 12coredns-7d8f74b498-mgqrm 1/1 Running 0 21h 13docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h 14docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h 15docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h 16docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 17etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 18kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 19kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 20kube-proxy-44qxn 1/1 Running 0 21h 21kube-proxy-4x72n 1/1 Running 0 21h 22kube-proxy-shhxb 1/1 Running 0 21h 23kube-proxy-zxhdb 1/1 Running 0 21h 24kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 25metrics-server-6777988975-cxnpv 1/1 Running 0 21h 26node-custom-setup-5rlkr 1/1 Running 0 34s #There they are 27node-custom-setup-7gf2v 1/1 Running 0 62m #There they are 28node-custom-setup-b4j4l 1/1 Running 0 62m #There they are 29node-custom-setup-wjpgz 0/1 Init:0/1 0 1s #There they are Now what has happened on the TKC worker nodes itself:\n1sh-5.0# cd /etc/frr/ 2sh-5.0# pwd 3/etc/frr 4sh-5.0# ls 5daemons frr.conf support_bundle_commands.conf vtysh.conf 6sh-5.0# systemctl status frr 7â— frr.service - FRRouting 8 Loaded: loaded (/lib/systemd/system/frr.service; enabled; vendor preset: enabled) 9 Active: active (running) since Mon 2023-02-20 17:53:33 UTC; 1min 36s ago Wow that looks good. But the frr.conf is more or less empty so it doesnt do anything right now.\nA note on FRR config on the worker nodes Before jumping into this section I would like to elaborate a bit around the frr.conf files being copied. If you are expecting that all worker nodes will be on same BGP AS number and your next-hop BGP neighbors are the same ones and in the same L2 as your worker node (illustrated above) you could probably go with the same config for all worker nodes. Then you can edit the same definition used for the FRR deployment to also copy and install the config in the same operation. The steps I do below describes individual config pr worker node. If you need different BGP AS numbers, multi-hop (next-hop is several hops away), individual update-source interfaces is configured then you need individual frr.config pr node.\nIndividual FRR config on the worker nodes I need to \u0026quot;inject\u0026quot; the correct config for each worker node. So I label each and one with their unique label like this: (I map the names node1-\u0026gt;lowest-ip)\nI have already configured my upstream bgp router to accept my workers as soon as they are configured and ready. This is how this looks.\n1router bgp 65802 2 bgp router-id 172.20.0.102 3 redistribute connected 4 neighbor 10.102.6.15 remote-as 66889 5 neighbor 10.102.6.16 remote-as 66889 6 neighbor 10.102.6.17 remote-as 66889 7 neighbor 172.20.0.1 remote-as 65700 8! 9 address-family ipv6 10 exit-address-family 11 exit 12 13 14cpodrouter-nsxam-wdc-02# show ip bgp summary 15BGP router identifier 172.20.0.102, local AS number 65802 16RIB entries 147, using 16 KiB of memory 17Peers 4, using 36 KiB of memory 18 19Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 2010.102.6.15 4 66889 1191 1197 0 0 0 never Active # Node1 not Established yet 2110.102.6.16 4 66889 0 0 0 0 0 never Active # Node2 not Established yet 2210.102.6.17 4 66889 1201 1197 0 0 0 never Active # Node3 not Established yet 23172.20.0.1 4 65700 19228 19172 0 0 0 01w4d09h 65 To verify again, this is the current output of frr.conf on node1:\n1sh-5.0# cat frr.conf 2# default to using syslog. /etc/rsyslog.d/45-frr.conf places the log 3# in /var/log/frr/frr.log 4log syslog informational 5sh-5.0# This is the definition I use to copy the daemons and frr.conf for the individual worker nodes:\n1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 namespace: kube-system 6 name: node-frr-config 7 labels: 8 k8s-app: node-frr-config 9 annotations: 10 command: \u0026amp;cmd cp /tmp/wdc-2.node1.frr.conf /etc/frr/frr.conf \u0026amp;\u0026amp; cp /tmp/daemons /etc/frr \u0026amp;\u0026amp; systemctl restart frr 11spec: 12 selector: 13 matchLabels: 14 k8s-app: node-frr-config 15 template: 16 metadata: 17 labels: 18 k8s-app: node-frr-config 19 spec: 20 nodeSelector: 21 nodelabel: wdc2-node1 #Here is my specific node selection done 22 hostNetwork: true 23 initContainers: 24 - name: copy-file 25 image: busybox 26 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cp /var/nfs/wdc-2.node1.frr.conf /var/nfs/daemons /data\u0026#39;] 27 volumeMounts: 28 - name: nfs-vol 29 mountPath: /var/nfs # The mountpoint inside the container 30 - name: node-vol 31 mountPath: /data 32 - name: init-node 33 command: 34 - nsenter 35 - --mount=/proc/1/ns/mnt 36 - -- 37 - sh 38 - -c 39 - *cmd 40 image: alpine:3.7 41 securityContext: 42 privileged: true 43 hostPID: true 44 containers: 45 - name: wait 46 image: pause:3.1 47 hostPID: true 48 hostNetwork: true 49 tolerations: 50 - effect: NoSchedule 51 key: node-role.kubernetes.io/master 52 volumes: 53 - name: nfs-vol 54 nfs: 55 server: 10.101.10.99 56 path: /home/andreasm/antrea/egress/FRR/nfs 57 - name: node-vol 58 hostPath: 59 path: /tmp 60 type: Directory 61 updateStrategy: 62 type: RollingUpdate Notice this:\n1 nodeSelector: 2 nodelabel: wdc2-node1 This is used to select the correct node after I have labeled them like this:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 4wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 5wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 6wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 7 8andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k label node wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds nodelabel=wdc2-node1 9node/wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds labeled So its just about time to apply the configs pr node:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f wdc2.frr.node1.config.yaml 2daemonset.apps/node-custom-setup configured 3andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system 4NAME READY STATUS RESTARTS AGE 5antrea-agent-4jrks 2/2 Running 0 21h 6antrea-agent-4khkr 2/2 Running 0 21h 7antrea-agent-4wxb5 2/2 Running 0 21h 8antrea-agent-ccglp 2/2 Running 0 21h 9antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 21h 10coredns-7d8f74b498-j5sjt 1/1 Running 0 21h 11coredns-7d8f74b498-mgqrm 1/1 Running 0 21h 12docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h 13docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h 14docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h 15docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 16etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 17kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 18kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 19kube-proxy-44qxn 1/1 Running 0 21h 20kube-proxy-4x72n 1/1 Running 0 21h 21kube-proxy-shhxb 1/1 Running 0 21h 22kube-proxy-zxhdb 1/1 Running 0 21h 23kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 24metrics-server-6777988975-cxnpv 1/1 Running 0 21h 25node-custom-setup-w4mg5 0/1 Init:0/2 0 4s Now what does my upstream router say:\n1cpodrouter-nsxam-wdc-02# show ip bgp summary 2BGP router identifier 172.20.0.102, local AS number 65802 3RIB entries 149, using 16 KiB of memory 4Peers 4, using 36 KiB of memory 5 6Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 710.102.6.15 4 66889 1202 1209 0 0 0 00:00:55 2 #Hey I am a happy neighbour 810.102.6.16 4 66889 0 0 0 0 0 never Active 910.102.6.17 4 66889 1201 1197 0 0 0 never Active 10172.20.0.1 4 65700 19249 19194 0 0 0 01w4d10h 65 11 12Total number of neighbors 4 13 14Total num. Established sessions 2 15Total num. of routes received 67 Then I just need to deploy on the other two workers.\nMy upstream bgp router is very happy to have new established neighbours:\n1cpodrouter-nsxam-wdc-02# show ip bgp summary 2BGP router identifier 172.20.0.102, local AS number 65802 3RIB entries 153, using 17 KiB of memory 4Peers 4, using 36 KiB of memory 5 6Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 710.102.6.15 4 66889 1221 1232 0 0 0 00:01:01 2 810.102.6.16 4 66889 11 16 0 0 0 00:00:36 2 910.102.6.17 4 66889 1225 1227 0 0 0 00:00:09 2 10172.20.0.1 4 65700 19254 19205 0 0 0 01w4d10h 65 11 12Total number of neighbors 4 Antrea IP Pool outside subnet of worker nodes Lets apply an IP pool which resides outside worker nodes subnet and apply Egress on my test pod again.\nHere is the IP pool config:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: ExternalIPPool 3metadata: 4 name: antrea-ippool-l3 5spec: 6 ipRanges: 7 - start: 10.102.40.41 8 end: 10.102.40.51 9# - cidr: 10.102.40.0/24 10 nodeSelector: {} 11# matchLabels: 12# egress-l3: antrea-egress-l3 And the Egress:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: Egress 3metadata: 4 name: antrea-egress-l3 5spec: 6 appliedTo: 7 podSelector: 8 matchLabels: 9 app: ubuntu-20-04 10 externalIPPool: antrea-ippool-l3 Apply it and check the IP address from the POD....\nWell, how about that?\nI know my workers reside on these ip addresses, but my POD is using a completely different IP address:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 4wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 5wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 6wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 What about the routing table in my upstream bgp router:\n1*\u0026gt; 10.102.40.41/32 10.102.6.17 0 0 66889 ? Well have you seen..\nObjective accomplished----\u0026gt;\n","link":"https://blog.andreasm.io/2023/02/20/antrea-egress/","section":"post","tags":["antrea","kubernetes","tanzu"],"title":"Antrea Egress"},{"body":"","link":"https://blog.andreasm.io/categories/avi/","section":"categories","tags":null,"title":"AVI"},{"body":"","link":"https://blog.andreasm.io/tags/ingress/","section":"tags","tags":null,"title":"ingress"},{"body":"","link":"https://blog.andreasm.io/tags/loadbalancing/","section":"tags","tags":null,"title":"loadbalancing"},{"body":"Deploy Tanzu in vSphere 8 with VDS and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using vSphere VDS networking and Avi as loadbalancer. The goal is to deploy Tanzu by using vSphere Distributed Switch (no NSX this time) and utilize Avi as loadbalancer for Supervisor and workload cluster L4 endpoint (kubernetes API). When that is done I will go through how we also can extend this into L7 (Ingress) by using AKO in our workload clusters.\nThe below diagram is what we should end up with after the basic deployment of Tanzu and Avi:\nAssumptions This post assumes we already have a vSphere environment up and running with vCenter, HA and DRS. Required network to support the basic vSphere stuff like vMotion and shared storage. And the hosts networking has been configured with a Distributed Switch with the corresponding vds portgroups for Management, Frontend network (VIP placement for kubernetes API endpoint) and workload network with corresponding VLANs. In vCenter a content library needs to be created, this is just a local library you give a meaningful name no subscriptions etc.\nAt least one Avi controller is deployed, no cloud added, just deployed and the initial configs done.\nPreparations on the Avi side of things This part of the guide takes place on the newly configured Avi controller(s) which currently only has the initial configuration done\nAvi cloud configurations To prepare Avi for this deployment we need to configure the vCenter cloud. This is done here:\nThere is a Default-Cloud object there we need to convert to a vCenter cloud. This is done by clicking on this button on the far right side: This will bring up the following options: Select VMware vCenter/vSphere NSX Then start populate the relevant vCenter information for your vCenter: When credentials is added slect content library and choose your content library from the list, then click connect and then Save \u0026amp; Relaunch\nWhen the dialog relaunches select the management network and ip address management. I have opted for DHCP (I have DHCP in my mgmt network, if not we can leverage Avi as IPAM provider for the mgmt network also.) This is used for the SE's mgmt interface. Click save for now. Head over to the Template section to add IPAM and DNS.\nWhen using vCenter clouds the different portgroups is automatically added under networks. We need to configure some of them. But for now just create the IPAM and DNS profiles and we configure the networks later accordingly.\nThe DNS Profile (optional, only if you want to use Avi DNS service):\nClick save when done\nThe IPAM profile:\nSelect the Default-Cloud, then select from the list \u0026quot;Usable Networks\u0026quot; the Frontend network vds portgroup corresponding to the frontend network we want to use for our endpoint vips. Click save. You should have your profiles configured now:\nHead back to your cloud again and add your newly created IPAM and DNS profiles.\nAdd the profiles: Before you click finish, make sure you have selected \u0026quot;Prefer Static Routes vs Directly Connected Network\u0026quot; like this: Then click finish...\nAvi network configs Now its time to configure the networks for the SEs (VIP, dataplane). I will go ahead and configure both the Frontend VIP for kubernetes API endpoint, but also the workload network I will use when I add L7 functionality later. Head over to Cloud Resources: Scroll until you find your \u0026quot;Frontend-network\u0026quot; Click edit all the way to the right: In here we need to define the subnet (if not already auto discovered) and add and IP range for the SE's and VIPS. We can decide to create just one range for both, or a range only for SEs, and one for VIPs only. To create a range for both SE and VIP do like this: If you want a specific range for SE and a specific for VIP do like this: Common for both is to deselect the DHCP Enabled option. What we have done now is to tell Avi that Avi is responsible for IP allocation to our SE's when they are deployed and configured to give them each their IP in the Frontend network, and also \u0026quot;carve\u0026quot; out an IP for the VIP when a Virtual Service is created and IP allocation for that service is selected to auto-allocate. The same would go if you decided to not use DHCP for mgmt IP, you would need to defined the network and select only \u0026quot;Use for Service Engine\u0026quot; (no VIPs in the management network)\nAvi service engine group Now its time to prepare the Default Service Engine Group. Head over to Cloud Resources - Service Engine Group\nClick the pencil on the far right side of the Default-Group and make the following changes:\nIn the Advanced tab: Here we select our vSphere cluster for the SE placement, vSphere shared storage, and the Prefix and vCenter folder placement (if you want). Now that is done.\nAvi VRF context Now we need to create a static route for the SE dataplane to know which gateway will take them to the \u0026quot;backend-pool\u0026quot; (the services they are acting as loadbalancer for). This is usually the gateway for the networks in their respective subnet as the dataplane is residing in. Here I prepare the route for the Frontend network, and also the Workload network (so it is already done when moving to the step of enabling L7). Avi controller SSL certificate for Tanzu \u0026quot;integration\u0026quot; The last step is to create a new certificate, or use your own signed certificate, for the Tanzu deployment to use. Head over to Templates - SSL/TLS Certificates. From here we click \u0026quot;Create\u0026quot; in the top right corner: I will go ahead and create a new self-signed certificate: It is important that you use the IP or FQDN of the controller under \u0026quot;Common Name\u0026quot; and under \u0026quot;Subject Alternate Name (SAN)\u0026quot;\nNow head over to Administration - Access Settings: Click edit on the pencil in the top right corner, remove the existing certificates under SSL/TLS Certificate: And replace with the one you created: Now the Avi config is done for this round. Next step is to enable Workload Management in vSphere...\nEnable Workload Management in vSphere This section will cover all the steps to enable Tanzu from vCenter, describing the selections made and the network configs.\nEnable workload management Head over to your vCenter server and click here: (from the the \u0026quot;hamburger menu\u0026quot; top left corner)\nClick the Get Started button:\nStep 1: Select vSphere Distributed Switch Step 2: Select Cluster Deployment, give the supervisor cluster a name and give it a zone name: Step 3: Select your Storage Policy (If VSAN and you dont have created a specific VSAN policy for this use Default Storage Policy): Step 4: Type in the relevant info for your Avi Controller and copy paste the certificate from your Avi controller: The certificate is easily copied from the Avi controller by going to Templates - SSL/TLS Certificates and click the \u0026quot;down arrow\u0026quot;: Then copy the certificate: Paste the content in the Server Certificate field above (step 4) Step 5: Management Network Here we fill in the required information for the Supervisor nodes. Management IP for the nodes themselves (needs connectivity to both vCenter and ESXi hosts, could be in the same mgmt network as vCenter and ESXi). Select the corresponding vds portgroup, select either static or DHCP if you want to use DHCP. Step 6: Workload Network Select the correct vds portgroup for the workload network. The supervisor and the workload nodes will be placed here. Can be static or DHCP. Leave the default \u0026quot;Internal Network for Kubernetes Services\u0026quot;, that is for the internal services (clusterIP etc inside the K8s clusters, they will never be exposed outside). Fill in the necessary config if you go with static. Step 7: Review and Confirm and optionally give the Supervisor endpoint a DNS name which you later can register in your DNS service when we have the L4 IP for the kubernetes API endpoint. Click finish:\nThe whole summary: Now sit back and wait for the creation of the supervisor cluster, it can take a couple of minutes. After a while you can take a look in your Avi controller under Applications and see if something is being created there; You can monitor the process from the Workload management status view by clicking on the \u0026quot;Configuring (View) )\u0026quot;. You can continue work with your vCenter server and go back to this progress bar whenever you want by clicking the hamburger menu Workload management.\nIn your vCenter inventory you should also see the Supervisor VMs and Avi SE's like this: When its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.102.7.11 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters. Lets dive into it.\nvSphere Namespace vSphere with Tanzu workloads, including vSphere Pods, VMs, and Tanzu Kubernetes clusters, are deployed to a vSphere Namespace. You define a vSphere Namespace on a Supervisor and configure it with resource quota and user permissions. Depending on the DevOps needs and workloads they plan to run, you might also assign storage policies, VM classes, and content libraries for fetching the latest Tanzu Kubernetes releases and VM images. source\nCreate a vSphere namespace Now that the Supervisor cluster is ready and running head back to your vCenter and create a vSphere namespace. Click create namespace (above) the select the supervisor to create the namespace on, and give your namespace a name then select the \u0026quot;workload network\u0026quot; you have defined for your workload placement. Now the namespace is being created.\nAdd additional workload networks Sidenote there is also possible to add more \u0026quot;workload networks\u0026quot; after the Supervisor has been configured under Supervisor config if you want to add more \u0026quot;workload networks\u0026quot; for separation etc. To do that head over to Workload Management in vCenter:\nThen select the supervisor tab:\nClick on your supervisor cluster here: Then click the configure tab and go to network and add your additional workload network: After your namespace has been created we need to configure it with access permissions, datastores, content library and vmclasses: Create workload cluster Afte the vSphere Namespace has been configured its time to deploy a workload cluster/TKC cluster (Tanzu Kubernetes Cluster cluster ðŸ˜„ ). From your workstation/jumphost where you downloaded the cli tools login in to the supervisor with access rights to the Supervisor API. (administrator@vsphere.local will have access).\nCustom role in vCenter I created a specific role in my vCenter with these privileges:\nAdded my \u0026quot;supervisor-manager\u0026quot; user in this role as global and in the top tree of my vCenter with inheritance. Also added it as \u0026quot;editor\u0026quot; in my wdc-2-ns-1 vSphere Namespace.\n1kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net When your are logged in it will give you this output and also put the kubernetes config in your ~/.kube/config file.\n1andreasm@linuxvm01:~$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.102.7.11 10 wdc-2-ns-1 11 12If the context you wish to use is not in this list, you may need to try 13logging in again later, or contact your cluster administrator. 14 15To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` When you are logged in prepare your yaml for your first workload cluster and apply it with kubectl apply -f nameof.yaml\nExample:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-2-tkc-cluster-1 # give your tkc cluster a name 5 namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 3 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-medium 32 - name: storageClass 33 value: vsan-default-storage-policy As soon as I apply the above yaml it will deploy the corresponding tkc cluster in your vsphere environment:\nSit back and enjoy while your tkc cluster is being created for you. We can check the status in the vCenter gui:\nor via kubectl:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get cluster -n wdc-2-ns-1 2NAME PHASE AGE VERSION 3wdc-2-tkc-cluster-1 Provisioned 12m v1.23.8+vmware.2 It is ready, now we need to log into it:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name=wdc-2-tkc-cluster-1 --tanzu-kubernetes-cluster-namespace=wdc-2-ns-1 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.102.7.11 10 wdc-2-ns-1 11 wdc-2-tkc-cluster-1 12 13If the context you wish to use is not in this list, you may need to try 14logging in again later, or contact your cluster administrator. 15 16To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` Check if you are able to list ns and pods:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get pods -A 2NAMESPACE NAME READY STATUS RESTARTS AGE 3kube-system antrea-agent-77drs 2/2 Running 0 8m35s 4kube-system antrea-agent-j482r 2/2 Running 0 8m34s 5kube-system antrea-agent-thh5b 2/2 Running 0 8m35s 6kube-system antrea-agent-tz4fb 2/2 Running 0 8m35s 7kube-system antrea-controller-575845467f-pqgll 1/1 Running 0 8m35s 8kube-system coredns-7d8f74b498-ft7rf 1/1 Running 0 10m 9kube-system coredns-7d8f74b498-pqgp7 1/1 Running 0 7m35s 10kube-system docker-registry-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 11kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-6cvz9 1/1 Running 0 8m46s 12kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rgn29 1/1 Running 0 8m34s 13kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rsmfw 1/1 Running 0 9m 14kube-system etcd-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 15kube-system kube-apiserver-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 16kube-system kube-controller-manager-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 17kube-system kube-proxy-67xjk 1/1 Running 0 8m46s 18kube-system kube-proxy-6fttt 1/1 Running 0 8m35s 19kube-system kube-proxy-m4wt8 1/1 Running 0 11m 20kube-system kube-proxy-rbsjw 1/1 Running 0 9m1s 21kube-system kube-scheduler-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 22kube-system metrics-server-6f7c489795-scmm6 1/1 Running 0 8m36s 23secretgen-controller secretgen-controller-6966677567-4hngd 1/1 Running 0 8m26s 24tkg-system kapp-controller-55f9977c86-bqppj 2/2 Running 0 9m22s 25tkg-system tanzu-capabilities-controller-manager-cb4bc7978-qh9s8 1/1 Running 3 (87s ago) 7m54s 26vmware-system-auth guest-cluster-auth-svc-r4rk9 1/1 Running 0 7m48s 27vmware-system-cloud-provider guest-cluster-cloud-provider-859b8dc577-8jlth 1/1 Running 0 8m48s 28vmware-system-csi vsphere-csi-controller-6db86b997-l5glc 6/6 Running 0 8m46s 29vmware-system-csi vsphere-csi-node-7bdpl 3/3 Running 2 (7m34s ago) 8m34s 30vmware-system-csi vsphere-csi-node-q7zqr 3/3 Running 3 (7m32s ago) 8m46s 31vmware-system-csi vsphere-csi-node-r8v2c 3/3 Running 3 (7m34s ago) 8m44s 32vmware-system-csi vsphere-csi-node-zl4m2 3/3 Running 3 (7m40s ago) 8m46s 1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get ns 2NAME STATUS AGE 3default Active 12m 4kube-node-lease Active 12m 5kube-public Active 12m 6kube-system Active 12m 7secretgen-controller Active 9m3s 8tkg-system Active 9m55s 9vmware-system-auth Active 12m 10vmware-system-cloud-provider Active 10m 11vmware-system-csi Active 10m 12vmware-system-tkg Active 12m By default we are not allowed to run anything on our newly created tkc cluster. We need to define some ClusterRoles. I will just apply a global clusterolres on my tkc cluster so I can do what I want with it like this: Apply the psp policy yaml:\n1apiVersion: rbac.authorization.k8s.io/v1 2kind: ClusterRole 3metadata: 4 name: psp:privileged 5rules: 6- apiGroups: [\u0026#39;policy\u0026#39;] 7 resources: [\u0026#39;podsecuritypolicies\u0026#39;] 8 verbs: [\u0026#39;use\u0026#39;] 9 resourceNames: 10 - vmware-system-privileged 11--- 12apiVersion: rbac.authorization.k8s.io/v1 13kind: ClusterRoleBinding 14metadata: 15 name: all:psp:privileged 16roleRef: 17 kind: ClusterRole 18 name: psp:privileged 19 apiGroup: rbac.authorization.k8s.io 20subjects: 21- kind: Group 22 name: system:serviceaccounts 23 apiGroup: rbac.authorization.k8s.io 1kubectl apply -f roles.yaml 2clusterrole.rbac.authorization.k8s.io/psp:privileged created 3clusterrolebinding.rbac.authorization.k8s.io/all:psp:privileged created Now that I am allowed to deploy stuff I am ready to consume the newly cluster. But this blog was how to deploy Tanzu with VDS and Avi Loadbalancer. So far I have only covered the L4 part where Avi is providing me the K8s API endpoints, I will now jump over to the section where I configure both Avi and my tkc cluster to use Ingress (L7) also so I can publish/expose my applications with ingress. That means installing an additional component called AKO in my tkc cluster and configure Avi accordingly.\nConfigure Avi as Ingress controller (L7) For Avi Ingress we need to deploy a component in our TKC cluster called AKO. AKO stands for Avi Kubernetes Operator and introduces the ability to translate our k8s api to the Avi controller so we can make our Avi automatically create vs services for us as soon as we request them from our TKC cluster. To deploy AKO we use Helm. In short we need to add the AKO helm repository, get the ako values, edit them to fit our environment, then install it by using Helm. So let us go through this step-by-step (I have also covered it a while back in an Upstream k8s cluster) but let us do it again here.\nCreate the namespace for the ako pod:\n1k create ns avi-system 2namespace/avi-system created Then add the repo to Helm:\n1helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Check the repo:\n1andreasm@linuxvm01:~$ helm search repo 2NAME CHART VERSION\tAPP VERSION\tDESCRIPTION 3ako/ako 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator 4ako/ako-operator\t1.3.1 1.3.1 A Helm chart for Kubernetes AKO Operator 5ako/amko 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator Get the values.yaml:\n1 helm show values ako/ako --version 1.8.2 \u0026gt; values.yaml Now its time to edit the value file. I will go through the values files, update it accordingly and adjust some configurations in Avi controller.\nThe values.yaml for ako chart:\n1# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. 2replicaCount: 1 3 4image: 5 repository: projects.registry.vmware.com/ako/ako 6 pullPolicy: IfNotPresent 7 8 9AKOSettings: 10 primaryInstance: true 11 enableEvents: \u0026#39;true\u0026#39; 12 logLevel: WARN 13 fullSyncFrequency: \u0026#39;1800\u0026#39; 14 apiServerPort: 8080 15 deleteConfig: \u0026#39;false\u0026#39; 16 disableStaticRouteSync: \u0026#39;false\u0026#39; 17 clusterName: wdc-tkc-cluster-1 # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller 18 cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. 19 enableEVH: false 20 layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. 21 22 namespaceSelector: 23 labelKey: \u0026#39;\u0026#39; 24 labelValue: \u0026#39;\u0026#39; 25 servicesAPI: false 26 vipPerNamespace: \u0026#39;false\u0026#39; 27 28NetworkSettings: 29 nodeNetworkList: 30 # nodeNetworkList: 31 - networkName: \u0026#34;vds-tkc-workload-vlan-1026\u0026#34; # this is the VDS portgroup you have for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter 32 cidrs: 33 - 10.102.6.0/24 # this is the CIDR for your workers 34 enableRHI: false 35 nsxtT1LR: \u0026#39;\u0026#39; 36 bgpPeerLabels: [] 37 # bgpPeerLabels: 38 # - peer1 39 # - peer2 40 vipNetworkList: 41 - networkName: \u0026#34;vds-tkc-frontend-vlan-1027\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. 42 cidr: 10.102.7.0/24 43 44L7Settings: 45 defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. 46 noPGForSNI: false 47 serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal 48 shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. 49 passthroughShardSize: SMALL 50 enableMCI: \u0026#39;false\u0026#39; 51 52L4Settings: 53 defaultDomain: \u0026#39;\u0026#39; 54 autoFQDN: default 55 56 57ControllerSettings: 58 serviceEngineGroupName: Default-Group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). 59 controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.3 60 cloudName: Default-Cloud # The configured cloud name on the Avi controller. 61 controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller 62 tenantName: admin 63 64nodePortSelector: 65 key: \u0026#39;\u0026#39; 66 value: \u0026#39;\u0026#39; 67 68resources: 69 limits: 70 cpu: 350m 71 memory: 400Mi 72 requests: 73 cpu: 200m 74 memory: 300Mi 75 76podSecurityContext: {} 77 78rbac: 79 pspEnable: false 80 81 82avicredentials: 83 username: \u0026#39;admin\u0026#39; # username for the Avi controller 84 password: \u0026#39;password\u0026#39; # password for the Avi controller 85 authtoken: 86 certificateAuthorityData: 87 88 89persistentVolumeClaim: \u0026#39;\u0026#39; 90mountPath: /log 91logFile: avi.log A word around the VIP network used for the L7/Ingress. As we deploy AKO as standalone we are not restricted to use only the components defined to support the install of Tanzu with vSphere, like service engine groups, vip networks etc. We could decide to create a separate VIP network by using a dedicated SE group for these networks. We could also decide to have the SE's using a separate dataplane network than the VIP itself. If going this path there is some config steps that needs to be taken on the network side. Routing to the VIP addresses, either Avi can be configured by using BGP, or we create static routes in the physical routers. But as the VIPs are coming and going (applications are published, deleted, etc) these IPs change. So BGP would be the best option, or use an already defined VLAN as I am doing in this example. In my other post on using NSX and Avi with Tanzu I will show how to use NSX for BGP. Maybe I will update this post also by adding a section where I use BGP from Avi to my upstream router. But for now I will stick with using my VLAN I have called frontend which already have a gateway and a route defined. So all my VIPs will be reachable through this network.\nAntrea NodePortLocal And another word around NodePortLocal. To be able to utilize NodePortLocal your Antrea config in the TKC cluster must be verified whether it is configured with NPL or not. So let us do instead of just assume something.\n1andreasm@linuxvm01:~/ako/ako_vds$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml 2apiVersion: v1 3data: 4 antrea-agent.conf: | 5 featureGates: 6 AntreaProxy: true 7 EndpointSlice: true 8 Traceflow: true 9 NodePortLocal: false 10 AntreaPolicy: true 11 FlowExporter: false 12 NetworkPolicyStats: false 13 Egress: false 14 AntreaIPAM: false 15 Multicast: false 16 ServiceExternalIP: false 17 trafficEncapMode: encap 18 noSNAT: false 19 tunnelType: geneve 20 trafficEncryptionMode: none 21 wireGuard: 22 port: 51820 23 egress: {} 24 serviceCIDR: 20.10.0.0/16 Well that was not good. So we need to enable it. Luckily, with Tanzu with vSphere its quite simple actually. Switch context to your vSphere Namespace, edit an antreaconfig, apply it.\n1andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-ns-1 2Switched to context \u0026#34;wdc-2-ns-1\u0026#34;. 3andreasm@linuxvm01:~/antrea$ k get cluster 4NAME PHASE AGE VERSION 5wdc-2-tkc-cluster-1 Provisioned 3h26m v1.23.8+vmware.2 6andreasm@linuxvm01:~/antrea$ k apply -f antreaconfig-wdc-2-nsx-1.yaml 7Warning: resource antreaconfigs/wdc-2-tkc-cluster-1-antrea-package is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. 8antreaconfig.cni.tanzu.vmware.com/wdc-2-tkc-cluster-1-antrea-package configured The antreaconfig I used:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: wdc-2-tkc-cluster-1-antrea-package # notice the naming-convention tkc cluster name-antrea-package 5 namespace: wdc-2-ns-1 # your vSphere Namespace the TKC cluster is in. 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: false 14 Egress: true 15 NodePortLocal: true # Set this to true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Lets have a look at my Antrea config in my TKC cluster now:\n1andreasm@linuxvm01:~/antrea$ k config use-context 210.102.7.11 wdc-2-ns-1 wdc-2-tkc-cluster-1 3andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-tkc-cluster-1 4Switched to context \u0026#34;wdc-2-tkc-cluster-1\u0026#34;. 5andreasm@linuxvm01:~/antrea$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml 6apiVersion: v1 7data: 8 antrea-agent.conf: | 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 Traceflow: true 13 NodePortLocal: true # Yes, there it is 14 AntreaPolicy: true 15 FlowExporter: false 16 NetworkPolicyStats: true 17 Egress: true 18 AntreaIPAM: false 19 Multicast: false 20 ServiceExternalIP: false 21 trafficEncapMode: encap 22 noSNAT: false 23 tunnelType: geneve 24 trafficEncryptionMode: none 25 wireGuard: 26 port: 51820 27 egress: 28 exceptCIDRs: [] 29 serviceCIDR: 20.10.0.0/16 But... Even though our cluster config has been updated we need to delete the Antrea pods so they can restart and read their new configmap again.\n1andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-controller-575845467f-pqgll 2pod \u0026#34;antrea-controller-575845467f-pqgll\u0026#34; deleted 3andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent- 4antrea-agent-77drs antrea-agent-j482r antrea-agent-thh5b antrea-agent-tz4fb 5andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-77drs 6pod \u0026#34;antrea-agent-77drs\u0026#34; deleted 7andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-j482r 8pod \u0026#34;antrea-agent-j482r\u0026#34; deleted 9andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-thh5b 10pod \u0026#34;antrea-agent-thh5b\u0026#34; deleted 11andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-tz4fb 12pod \u0026#34;antrea-agent-tz4fb\u0026#34; deleted Configure Avi as Ingress controller (L7) - continue Now that we have assured NodePortLocal is configured, its time to deploy AKO. I have also verified that I have the VIP network configured in Avi as I am using the existing network that is already defined. So install AKO then ðŸ˜„\n1helm install ako/ako --generate-name --version 1.8.2 -f ako.vds.wdc-2.values.yaml --namespace=avi-system 2NAME: ako-1675511021 3LAST DEPLOYED: Sat Feb 4 11:43:42 2023 4NAMESPACE: avi-system 5STATUS: deployed 6REVISION: 1 7TEST SUITE: None Verify that the AKO pod is running:\n1andreasm@linuxvm01:~/ako/ako_vds$ k get pods -n avi-system 2NAME READY STATUS RESTARTS AGE 3ako-0 1/1 Running 0 57s Check logs for some immediate messages that needs investigating before trying to deploy a test application.\n1andreasm@linuxvm01:~/ako/ako_vds$ k logs -n avi-system ako-0 22023-02-04T11:43:51.240Z\tINFO\tapi/api.go:52\tSetting route for GET /api/status 32023-02-04T11:43:51.241Z\tINFO\tako-main/main.go:71\tAKO is running with version: v1.8.2 42023-02-04T11:43:51.241Z\tINFO\tapi/api.go:110\tStarting API server at :8080 52023-02-04T11:43:51.242Z\tINFO\tako-main/main.go:81\tWe are running inside kubernetes cluster. Won\u0026#39;t use kubeconfig files. 62023-02-04T11:43:51.265Z\tINFO\tlib/control_config.go:198\tako.vmware.com/v1alpha1/AviInfraSetting enabled on cluster 72023-02-04T11:43:51.270Z\tINFO\tlib/control_config.go:207\tako.vmware.com/v1alpha1/HostRule enabled on cluster 82023-02-04T11:43:51.273Z\tINFO\tlib/control_config.go:216\tako.vmware.com/v1alpha1/HTTPRule enabled on cluster 92023-02-04T11:43:51.290Z\tINFO\tako-main/main.go:150\tKubernetes cluster apiserver version 1.23 102023-02-04T11:43:51.296Z\tINFO\tutils/utils.go:168\tInitializing configmap informer in avi-system 112023-02-04T11:43:51.296Z\tINFO\tlib/dynamic_client.go:118\tSkipped initializing dynamic informers antrea 122023-02-04T11:43:51.445Z\tINFO\tk8s/ako_init.go:455\tSuccessfully connected to AVI controller using existing AKO secret 132023-02-04T11:43:51.446Z\tINFO\tako-main/main.go:261\tValid Avi Secret found, continuing .. 142023-02-04T11:43:51.866Z\tINFO\tcache/avi_ctrl_clients.go:71\tSetting the client version to 22.1.1 152023-02-04T11:43:51.866Z\tINFO\tako-main/main.go:279\tSEgroup name found, continuing .. 162023-02-04T11:43:53.015Z\tINFO\tcache/controller_obj_cache.go:2340\tAvi cluster state is CLUSTER_UP_NO_HA 172023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2901\tSetting cloud vType: CLOUD_VCENTER 182023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2904\tSetting cloud uuid: cloud-ae84c777-ebf8-4b07-878b-880be6b201b5 192023-02-04T11:43:53.176Z\tINFO\tlib/lib.go:291\tSetting AKOUser: ako-wdc-2-tkc-cluster-1 for Avi Objects 202023-02-04T11:43:53.177Z\tINFO\tcache/controller_obj_cache.go:2646\tSkipping the check for SE group labels 212023-02-04T11:43:53.332Z\tINFO\tcache/controller_obj_cache.go:3204\tSetting VRF global found from network vds-tkc-frontend-vlan-1027 222023-02-04T11:43:53.332Z\tINFO\trecord/event.go:282\tEvent(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;avi-system\u0026#34;, Name:\u0026#34;ako-0\u0026#34;, UID:\u0026#34;4b0ee7bf-e5f5-4987-b226-7687c5759b4a\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;41019\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ValidatedUserInput\u0026#39; User input validation completed. 232023-02-04T11:43:53.336Z\tINFO\tlib/lib.go:230\tSetting Disable Sync to: false 242023-02-04T11:43:53.338Z\tINFO\tk8s/ako_init.go:310\tavi k8s configmap created Looks good, let us try do deploy an application and expose it with Ingress\nI have two demo applications, banana and apple. Yaml comes below. I deploy them\n1andreasm@linuxvm01:~/ako$ k create ns fruit 2namespace/fruit created 3andreasm@linuxvm01:~/ako$ k apply -f apple.yaml -f banana.yaml 4pod/apple-app created 5service/apple-service created 6pod/banana-app created 7service/banana-service created yaml for banana and fruit\n1kind: Pod 2apiVersion: v1 3metadata: 4 name: banana-app 5 labels: 6 app: banana 7 namespace: fruit 8spec: 9 containers: 10 - name: banana-app 11 image: hashicorp/http-echo 12 args: 13 - \u0026#34;-text=banana\u0026#34; 14 15--- 16 17kind: Service 18apiVersion: v1 19metadata: 20 name: banana-service 21 namespace: fruit 22spec: 23 selector: 24 app: banana 25 ports: 26 - port: 5678 # Default port for image 1kind: Pod 2apiVersion: v1 3metadata: 4 name: apple-app 5 labels: 6 app: apple 7 namespace: fruit 8spec: 9 containers: 10 - name: apple-app 11 image: hashicorp/http-echo 12 args: 13 - \u0026#34;-text=apple\u0026#34; 14 15--- 16 17kind: Service 18apiVersion: v1 19metadata: 20 name: apple-service 21 namespace: fruit 22spec: 23 selector: 24 app: apple 25 ports: 26 - port: 5678 # Default port for image Then I apply the Ingress:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 namespace: fruit 6# annotations: 7# ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-tkgs.you-have.your-domain.here 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 1k apply -f ingress-example.yaml You should more or less instantly notice a new virtual service in your Avi controller: And let us check the ingress in k8s:\n1andreasm@linuxvm01:~/ako$ k get ingress -n fruit 2NAME CLASS HOSTS ADDRESS PORTS AGE 3ingress-example avi-lb fruit-tkgs.you-have.your-domain.here 10.102.7.15 80 2m49s There it is, with the actual VIP it gets from Avi.\nHeres is the view of the application from the Dashboard view in Avi: Also notice that the SE's now also places itself in the same network as the worker nodes, but still creates the VIP in the frontend-network.\nMeaning our network diagram will now look like this: Now, AKO comes with its own CRDs that one can work with. I will go through these in a separate post.\n","link":"https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-vds-and-avi-loadbalancer/","section":"post","tags":["kubernetes","tanzu","loadbalancing","ingress"],"title":"vSphere 8 with Tanzu using VDS and Avi Loadbalancer"},{"body":"","link":"https://blog.andreasm.io/tags/lodbalancing/","section":"tags","tags":null,"title":"lodbalancing"},{"body":"","link":"https://blog.andreasm.io/categories/nsx/","section":"categories","tags":null,"title":"nsx"},{"body":"Deploy Tanzu in vSphere 8 with NSX and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using NSX networking (including built in L4 loadbalancer) and Avi as L7 loadbalancer. The goal is to deploy Tanzu by using NSX for all networking needs, including the Kubernetes Api endpoint (L4) and utilize Avi as loadbalancer for all L7 (Ingress). The deployment of Tanzu with NSX is an automated process, but it does not include L7 loadbalancing. This post will quickly go through how to configure NSX and Avi to support this setup and also the actual configuration/deployment steps of Tanzu. The following components will be touched upon in this post: NSX, Tanzu, TKC, AKO, NCP, vCenter, AVI and Antrea. All networks needed for this deployment will be handled by NSX, except vCenter, NSX manager and Avi controller but including the management network for the supervisor cluster and Avi SE's. In the end we will also have a quick look at how to use Antrea Egress in one of the TKC clusters.\nWe should end up with the following initial network diagram for this deployment (will update it later in the post reflecting several network for our TKC cluster with and without NAT (without NAT when using Egress): Preparations - NSX config This post assumes a working vSphere environment with storage configured, vMotion network, vSAN (if using vSAN), HA, DRS enabled and configured. So this step will cover the basic NSX config for this use-case. NSX will need some network configured in the physical environment like the Geneve Tunnel VLAN, Uplink VLAN(s) for our T0 in addition to the most likely already defined management network for the placement of NSX managers, NSX edges and Avi controller and/or SE's. So lets jump in.\nInitial configs in the NSX manager The first NSX manager is already deployed. Accept the EULA and skip the NSX tutorial:\nWhen done, head over to System -\u0026gt; Licenses and add your license key. Then, still under system, head over to Appliances and add a cluster IP. Even though you only have 1 NSX manager for test/poc it can make sense to use cluster ip adding, removing nsx managers etc and still point to the same IP.\nClick on Set Virtual IP and type in your wanted cluster ip. Out of the box its the same layer 2 subnet as your controllers are placed in (it possible to use L3 also but that involves an external LoadBalancer, not the built in for this purpose).\nClick save and wait. After some minutes, try to log in via your new cluster IP. All the configs I will do will be used with this IP. It does not matter if you go directly to the NSX manager itself or the cluster IP for this post.\nAfter cluster IP is done, we need to add a Compute Manager which in our case is the vCenter server (not that you have any option besides vCenter). Still under System, go to Fabric expand and find Compute Manager. From there click Add Compute Manager:\nFill in the necessary information for your vCenter and make sure Service Account and Enable Trust is enabled. Next message will ask you to use a Thumprint the vCenter says it has. You could either just say ADD or actually go to vCenter and grab the thumbprint from there and verify or paste it in the SHA-256 field before clicking add.\nHere is how to get the thumbprint from vCenter:\n1root@vcsa [ ~ ]# openssl x509 -in /etc/vmware-vpx/ssl/rui.crt -fingerprint -sha256 -noout 2SHA256 Fingerprint=A1:F2:11:0F:47:D8:7B:02:D1:C9:B6:87:19:C0:65:15:B7:6A:6E:23:67:AD:0C:41:03:13:DA:91:A9:D0:B2:F6 Now when you are absolutely certain, add and wait.\nNSX Profiles: Uplink, Transport Zones and Transport Node Profiles In NSX there is a couple of profiles that needs to be configured, profiles for the Transport nodes, Edge transport nodes, transport zones. Instead of configuring things individually NSX uses profiles so we have a consistent and central place to configure multiple components from. Let start with the Uplink profiles: Under Fabric head over to Profiles.\nHere we need to create two uplink profiles (one for the ESXi transport nodes and one for the NSX edge transport nodes). These profile will dictate the number of uplinks used, mtu size (only for the edges after NSX 3.1) ,vlan for the geneve tunnel and nic teaming. Here we also define multiple teaming policies if we want to dictate certain uplinks to be used for deterministic traffic steering. Which I will do.\nHost uplink:\nIn the \u0026quot;host\u0026quot; uplink profile we define the logical uplinks, (uplink-1 and uplink-2, but we could name them Donald-Duck-1 and 2 if we wanted). We define the default teaming-policy to be Load Balance Source as we want two vmkernels for the host-tep, and default active/active if we create a vlan segment without specifying a teaming policy in the segment. Then we add two more teaming policies with Failover Order and specify one uplink pr policy. The reason for that is because we will go on and create a VLAN segment later where we will place the Edge VM uplinks, and we need to have some control over wich uplink-\u0026gt;nic on the ESXi host the T0 Uplinks go, and we dont want to use teaming on these, and we dont want a standby uplink as BGP is supposed to handle failover for us. This way we steer T0 Uplink 1 out via uplink-1 and T0 Uplink 2 out via uplink-2 and the ESXi hosts has been configured to map uplink-1 to VDS uplink-1 which again is mapping to pNIC0 and uplink-2 to VDS uplink-2 to pNIC1 respectively. In summary we create a vlan segment on the host for the edge VM, then we create a vlan segment for the logical T0 later.\nThen we define the VLAN number for the Geneve tunnel. Notice we dont specify MTU size as this will adjust after what we have in our VDS which we will map to later, so our VDS must have minumum MTU 1700 defined (it works with 1600 also, but in NSX-T 3.2 and later there is a greenfield min MTU of 1700). We will use the same VLAN for both host and edge tep wich was supported from NSX-T 3.1 and forward. But to make that work we cant use VDS portgroups for the T0, it needs to be a NSX VLAN segment. More on that later.\nClick save.\nNext up is the \u0026quot;Edge\u0026quot; Uplink Profile, almost same procedure:\nThe biggest difference is the name of the specific teaming policies, and we specify a MTU size of 1700 as this is what I use in my VDS.\nNow over to Transport Zones\nHere we create three Transport Zones: 1 vlan TZ for host, 1 vlan TZ for edge and 1 overlay which is common for both Edge and Host transport nodes.\nCreate host-vlan-tz:\nIts probably not so obvious in the GUI, but we also define our teaming policies defined in our respective uplinks earlier. Here I enter manually the uplink policy names for my host transport zone so they can be available later when I create a VLAN segment for my T0 uplinks (on the host). Click save.\nWhen done its the edge-vlan-tz:\nCommon for both Transport Zones is VLAN.\nNow the last Transport Zone for now - the Overlay TZ (this one is easy):\nThe next step would be to create the Transport Node Profile, but first we need to create an IP-Pool for the TEP addresses. Head over to Networking and IP Address Pools :\nAdd IP address pool:\nIts only necessary to define the CIDR, IP Range and Gateway IP. Please make sure the range is sufficient to support the max amount of ESXi Transport nodes and Edge nodes you will have. 2 IPs pr device.\nNow back to System -\u0026gt; Fabric again and create the Transport Node Profile under Profile:\nHere we select the vCenter we added earlier, point to the correct VDS we want to use, select the host-profile we created and under IP assignment we use our newly created IP pool and map the uplinks to the corresponding VDS uplinks. Then ADD\nInstall NSX components on the ESXi hosts To enable NSX after our initial configs has been done is fairly straight-forward. Head over to Nodes under System-\u0026gt;Fabric and select the first tab Host Transport Nodes. Select your vCenter under Managed by and select your cluster and click Configure NSX:\nSelect your (only?) transport-node profile your created above:\nClick apply and wait...\nStatus in vCenter - the only status we will see.\nWhen everything is up and green as below, NSX is installed in our ESXi hosts and we are ready to create networks ðŸ˜„\nDeploy NSX Edges Deploying a Edge is quite straight forward, and is done from the NSX manager under System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nBut I need to create two VLAN segments for Edge \u0026quot;data-path/uplink interfaces. As I want to use the same VLAN for both Host Tep and Edge TEP I need to do that. These two segments will only be used for the actual Edge VMs, not the T0 I am going to create later. In my lab I am using VLAN 1013 for TEP and VLAN 1014 and 1015 for T0 Uplink 1 and 2. So that means the first VLAN segment I create will have the VLAN Trunk range 1013-1014 and the second VLAN segment will use VLAN trunk 1013,1015 Head over to the Networking section/Segments in the NSX UI click Add Segment:\nSelect host-uplink-1 under Uplink Teaming Policy and add your VLAN ID under VLAN. Click save\nSame procedure again for the second segment:\nselect host-uplink-2 and the correct vlan trunk accordingly.\nThe result should be two VLAN segments, created in our host-vlan-tz (the host vlan transport zone created earlier)\nNow we can deploy our Edge(s).\nHead over to System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nClick add edge node and start the edge deployment wizard:\nGive the edge a name, then fill in a FQDN name. Not sure if that part have to be actually registered in DNS, but it probably does do any harm if you decide to. Choose a form factor. When using Tanzu it can be potentially many virtual services so you should at least go with Large. You can find more sizing recommendation on the VMware official NSX docs page. Next\nFill inn your username and passwords (if you want easier access to SSH shell for troubleshooting purposes enable SSH now).\nSelect your vCenter, cluster and datastore. The rest default.\nConfigure the basics... This the part of the Edge that communicates with the NSX manager. Next we will configure the networking part that will be used for the T0 uplinks and TEP.\nHere we select the transport zones for the Edge to be part of. Note, it should be part of your overlay transport zone but not part of the host vlan transport zone. Here we have defined a Edge vlan transportzone to be used. This is the transport zone where the segment for the T0 to be created in. One of the reason is that we dont want the segment for the T0 to be visible for the host for potentially other workloads, and the segment is actually created in the Edge, thats where the T0 is realised (The SR part of the T0). Then we select the edge-profile we created earlier, the same IP pool as the hosts. Under uplinks we select the respective vlan segments uplink 1 and 2 created earlier.\nThen finish. It should take a couple of minutes to report ready in the NSX manager ui.\nStatus when ready for duty:\nThere should also be some activity in vCenter deploying the edge. When ready head over to Edge Clusters and create a cluster to put the Edge in. We need an Edge cluster and the edges in an edge cluster before we can do anything with them, even if we only deploy one edge (labs etc).\nThe T0 Now that at least one Edge is up, we should create a T0 so we can make some external connectivity happen (even though NSX have its own networking components and we can create full L3 topology, we cant talk outside NSX from overlay without the Edges). Head over to Network and create a VLAN segment. This time the segment should be placed in the edge-vlan-tz as it is use for T0 uplinks only. Select teaming policy and correct vlan for the T0 uplink 1. I will only use 1 uplink in my lab so I will only create 1 segment for this, I only have 1 upstream router to peer to also.\nNext is heading over to Tier-0 and create a T0:\nThe selection is very limited at first, so give it a name and select HA mode, and edge cluster (the one that we created above).\nClick save and yes to continue edit:\nNow we need to add the interface(s) to the T0. The actual interfaces will be residing on the Edges, but we need to define them in the T0. Click on the 0 under Interfaces (its already two interfaces in the screenshot below).\nGive the interface a name, choose type External, give it the correct IP address to peer with the upstream router, and select the Edge VLAN segment created earlier which maps to the correct uplink (1 or 2). Then select the Edge node that shall have this interface configured.\nClick save. Now as an optional step SSH into the Edge selected above, go the correct vrf and ping the upstream router.\nGet the correct VRF (we are looking for the SR T0 part of the T0)\nEnter the vrf by typing vrf and the number, here it is vrf 1.\nListing the interface with get interfaces one should see the interface we configured above, and we can ping the upstream router to verify L2 connectivity.\nGood, now configure BGP. Expand BGP in your T0 settings view (same place as we configure the interface) adjust your BGP settings accordingly. Click save, enter again and add your BGP peers/neighbors bly clicking on neighbors.\nAdd the IP to the BGP peer you should use and adjust accordingly, like AS number. Click Save\nIt will become green directly, then if you click refresh it will become red, then refresh again it should be green again if everything is correct BGP config wise on both sides. Clicking on the (i) will give you the status also:\nFrom your upstream routes you should see a new neighbor established:\nThe last step on the T0 now is to configure it which networks it should advertise on BGP. That is done under Route re-distribution. In a Tanzu setup we need to advertise NAT, connected and LB VIP from our T1s. That is because Tanzu or NCP creates NAT rules, it creates some LB VIPS and we should also be able to reach our other Overlay segments we create under our T1 (which we have not created yet).\nNow that T0 is configured and peering with the upstream router, I can create segments directly under the T0, or create T1s and then segments connected to the T1 instead. If you do create segments directly attached to the T0 one must configure route advertisement accordingly. As the config above is not advertising any networks from T0, only T1.\nIn NSX there is a neat map over the Network Topology in NSX:\nDeploy Tanzu with NSX Now that networking with NSX is configured and the foundation is ready. Its time to deploy the Supervisor cluster, or enable WCP, Workload Management. Head over to the hamburger menu in top left corner in your vCenter and select * Workload Management*\nClick on Get Started\nThen follow the wizard below:\nSelect NSX and Next\nSelect Cluster Deployment, choose your vCenter cluster, give the supervisor a name and below (not in picture above) enter a zone name.\nNext\nSelect your storage policies, Next\nIn Step 4 - Management Network we configure the network for the Supervisor Control Plane VMs. In my lab I have already created an overlay segment I call ls-mgmt with cidr 10.101.10.0/24 and gateway 10.101.10.1. So I will place my Supervisors also there. Not using DHCP, but just defining a start IP. The Supervisors will consist of three VMs, and a cluster IP. But it will use 5 IP addresses in total in this network. DNS, Search Domains and NTP should be your internal services. In screenshot above I have used an external NTP server. NEXT\nAbove I define the workload network, which the supervisor control plane vms also will be part of, but also for the vSphere Pods. This a default workload network where your TKC cluster can be deployed in (if not overriden when creating a vSphere namespace (later on that topic) ). Select the VDS you have used and configured for NSX. Select your Edge cluster. Add your DNS server(s), select the T0 router you created earlier in NSX. Then leave NAT-Mode enabled, we can create vSphere namespaces later where we override these settings. Then you define the Namespace Network. This is the network your vSphere pods will use, the workload network interface of the Supervisor ControlPlane Nodes, and your TKC cluster nodes. The CIDR size define how many IPs you will have available for your TKC cluster nodes, meaning also the total amount of nodes in this workload network. But dont despair, we can create additional vSphere namespaces and add more networks. So in the above example I give it a /20 cidr (unnecessary big actually, but why not). This Namespace Network will not be exposed to the outside world as NCP creates route-map rules on the T0 not allowing these to be advertised (we have NAT enabled). The Service CIDR is Kubernetes internal network for services. When we deploy a TKC cluster later we define other Kubernetes cluster and pod cidrs. Define the Ingress CIDR, this is the IP address range NCP will use to carve out LoadBalancer VIPs for the Kubernetes API endpoints, for the Supervisor Control Plane, the TKC clusters K8s Api endpoint and even the Service Type LoadBalancer services you decide to created. So all access TO the Supervisor Cluster API endpoint will be accessed through the IP address assigned from this CIDR. When we have NAT enabled it will also ask you to define a Egress CIDR which will be used by NSX to create SNAT rules for the worker nodes to use when communicate OUT. These NAT rules will be created automatically in NSX-T.\nNEXT\nSelect the size of your SVCP and give the SVCP API endpoint a name. This is something that can be registered in DNS when deployment is finished and we know the IP it gets.\nFinish and wait. Its the same (waiting) process as explained here\nIf everything goes well we should have a Supervisor cluster up and running in not that many minutes. 20-30 mins?\nWhen its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.101.11.2 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters.\nNSX components configured by WCP/NCP Before heading over to next section, have a look in NSX and see what happended there:\nUnder Networks -\u0026gt; Segments you will notice networks like this:\nNotice the gateway ip and CIDR. These are created for each vSphere Namespace, the cidr 10.101.96.1/28 is carved out of the CIDR defined in the \u0026quot;default\u0026quot; network when deploying WCP.\nUnder Networks -\u0026gt; T-1 Gateways you will notice a couple of new T1 routers being created:\nUnder Networks -\u0026gt; Load Balancing:\nHere is all the L4 K8s API endpoints created, also the other Service Type Loadbalancer services you choose to expose in your TKC clusters.\nUnder Networks -\u0026gt; NAT and the \u0026quot;domain\u0026quot; T1 there will be auto-created NAT rules, depending on whether NAT is enabled or disabled pr Namespace.\nThen under Security -\u0026gt; Distributed Firewall there will be a new section:\nvSphere Namespace When Supervisor is up and running next step is to create a vSphere Namespace. I will go ahead and create that, but will also use the \u0026quot;override network\u0026quot; to create a separate network for this Namespace and also disable NAT as I want to use this cluster for Antrea Egress explained here.\nA vSphere is a construct in vSphere to adjust indidivual access settings/permissions, resources, network settings or different networks for IP separation. Click on Create Namespace and fill in relevant info. I am choosing to Override Supervisor network settings.\nNotice when I disable NAT Mode there will no longer be a Egress IP to populate. Thats because the TKC nodes under this namespace will not get a NAT rule applied to them in NSX and will be communicating \u0026quot;externally\u0026quot; with their own actual IP address. Ingress will still be relevant as the NSX-T Loadbalancer will create the K8s API endpoint to reach the control plane endpoint your respective TKC clusters.\nThis option to adjust the networks in such a degree is a great flexibility with NSX. Tanzu with VDS does give you the option to select different VDS portgroups on separate vlans, but must be manually created, does not give you NAT, Ingress and Egress and VLAN/Routing must be in place in the physical environment. With NSX all the networking components are automatically created and it includes the NSX Distributed Firewall.\nAfter the vSphere Namespace has been created it is the same procedure to deploy TKC clusters regardless of using VDS or NSX. So instead of me repeating myself and saving the environment for digital ink I will refere to the process I have already described here\nConfigure Avi as Ingress controller (L7) with NSX as L4 LB When using Antrea as the CNI in your TKC cluster (default in vSphere with Tanzu) make sure to enable NodePortLocal. This gives much better control and flexibility, follow how here and NodePortLocal explained here\nConfigure NSX cloud in Avi - network preparations As this guide is using NSX as the underlaying networking platform instead of VDS as this article is using, we also have the benefit of configuring the NSX cloud in Avi instead of the vCenter Cloud. This cloud do come with some additional benefits like automatic Security Group creation in NSX, VIP advertisement through the already configured T0, SE placement dataplane/data network on separate network and VRF context.\nBut before we can consume the NSX cloud we need to configure it. Assuming the Avi controller has already been deployed and initial config is done (username and password, dns, etc) log in to the controller and head over to Administration -\u0026gt; User Credentials:\nAdd the credentials you want to use for both vCenter and NSX-T. The next step is involving NSX. Head over to NSX and create two new networks. One for SE management and one for SE dataplane (the network it will use to reach the backend servers they are loadbalancing). I will now just show a screenshot of three networks already created. One segment ls-avi-se-mgmt for SE mgmt, one segment ls-avi-generic-se-data for SE communication to backend pools, and one segment ls-avi-dns-se-data (I will get back to the last network later when enabling the Avi DNS service).\nThen head over to vCenter and create a local Content Library and call it what you want.\nWhen networks and credentials have been created, back in the Avi gui head over to Infrastructure -\u0026gt; Clouds:\nClick Create and select NSX-T\nStart by giving the NSX cloud a meaningfull name and give a prefix name for the objects being created in NSX by Avi. If you have several Avi controllers using same NSX manager etc. Easier to identify when looking in the NSX manager ui:\nThen proceed (scroll down if needed) to connect to you NSX manager:\nEnter the IP of your NSX manager, if you created a Cluster IP in the NSX manager use that one, and select the credentials for NSX manager create in the Avi earlier.\nConnect.\n!Note Before being able to continue this step we need to have defined some networks in NSX as explained above...\nFill in the Transport zone where you have defined the segment for the management network, select the T1 this segment is attached to and then the segment. This network is created in NSX for the SE management interface. Then go to Data Networks and select the Overlay Transport Zone where the ls-avi-generic-se-data segment is created the ADD the T1 router for this segment from the dropdown list and the corresponding segment. (Ignore the second netowork in the screenshot above.) Then under vCenter Server click add and fill in relevant info to connect the vCenter server your NSX manager is using.\nGive it a meaningful name, it will either already have selected your NSX cloud otherwise select it from the list. Then the credentials and the Content Library from the dropdown.\nThe last section IPAM/DNS we will fill out later (click save):\nNow the cloud should be created.\nSE Groups Head over to Cloud Resources -\u0026gt; Service Engine Groups and create a custom SE-group.\nSelect your newly created cloud from the dropdown list above next to Select Cloud. Click create blue button right side.\nChange the fields marked by a red line. And optionally adjust the Max. Number of Service Engines. This is just to restrict Avi to not deploy too many SE's if it sees fit. Adjust the Virtual Services pr Service Engine to a higher number than 10 (if that is default). This all comes down to performance.\nJump to advanced:\nChange the Service Engine Name Prefix to something useful, then select the vCenter, cluster and datastore. Save\nAvi Networks Now head over to Infrastructure -\u0026gt; VRF Contexts (below Service engine Groups) select correct cloud from dropdown and add a default gateway for the SE dataplane network.\nClick the pencil\nAdd a default route under Static Route and point to the gateway used for the SE dataplane ls-avi-generic-se-data . This is used for the SE's to know where to go to reach the different pools it will loadbalance.\nNext we need to define the different networks for the SE's to use (the dataplane network). Head over to Infrastructure -\u0026gt; Networks (again select correct cloud from dropdown)\nIf there is workload running in these networks they can already be auto-detected, if not you need to create them. In my lab I rely on Avi as the IPAM provider for my different services, a very useful feature in Avi. So the two networks we need to create/and or edit is the ls-avi-se-mgmt and the ls-avi-generic-se-data\nFirst out is the ls-avi-se-mgmt\nDeselect DHCP, we want to use Avi IPAM instead (and I dont have DHCP in these networks). Then fill in the CIDR (IP Subnet), deselect Use Static IP Address for VIPs and Service Engine add a range for the SE to be allow to get an IP from. Then select the Use for Service Engines. This will configure the IPAM pool to only be used for the dataplane for the SE's in the management network. We dont want to have any VIPs here as it will only be used for the SE's to talk to the Avi controller.\nThen it is the ls-avi-generic-se-data network\nSame as above just a different subnet, using the same T1 router defined in NSX.\nThe two above networks will only be used as dataplane network. Meaning they will not be used for any Virtual Service VIP. We also need to define 1 or more VIP networks. Create one more network:\nHere I specify a network which is only used for VS VIP\nAvi IPAM/DNS template Now we need to inform our cloud which VIP networks we can use, that is done under Templates -\u0026gt; IPAM/DNS Profiles\nWhile we are here we create two profiles, one for DNS (for the DNS service) and IPAM profile for the VIP networks. Lets start with the IPAM profile. Click create right corner and select IPAM Profile:\nFill in name, select allocate IP in VRF and select your NSX cloud. Then click add and select your VIP networks defined above. Screenshot below also include a DNS-VIP network which I will use later. Click save.\nNow click create again and select DNS profile:\nGive it a name and type in the domain name you want Avi to use.\nNow go back to Infrastructure -\u0026gt; Clouds and edit your NSX cloud and add the newly added Profiles here:\nSave\nNow Avi is prepared and configured to handle requests from AKO on L7 service Ingress. Next step will be to deploy configure and deploy AKO in our TKC cluster. But first some short words around Avi DNS service.\nAvi DNS service Getting a virtual service with a VIP is easy with Avi, but often we need a DNS record on these VS'es. Avi has a built in DNS service which automatically register a DNS record for your services. The simplest way to make this work out of the box is to create a Forward Zone in your DNS server to the DNS Service IP for a specific subdomain or domain. Then Avi will handle the DNS requests for these specific domains. To make use of Avi DNS service we should dedicate a SE group for this service, and create a dedicated VIP network for it. As we should use a dedicated SE group for the DNS service it would be nice to also have a dedicated SE dataplane network for these SE's. So follow the steps I have done above to create a SE network for the SE service SE's and add this to your cloud. The VIP network also needs to be added to the IPAM Profile created earlier. A note on the additional SE network, this also requires a dedicated T1 router in the NSX environment. So in your NSX environment create an additional T1 router, create segment for the DNS SE datanetwork. This is how to enable the DNS service in Avi after you have prepared the networks, and IPAM profile:\nHead over to Administration -\u0026gt; Settings -\u0026gt; DNS Service:\nThen create virtual service:\nSelect your cloud and configure the DNS service:\nThe VS VIP is configured with a static IP (outside of the DNS VIP IPAM range you have created)\nUnder advanced select the SE group:\nSave. Now the DNS VS is configured, go to templates and add a DNS profile:\nGive it a name, add your domain(s) here. Save\nHead over to the cloud and add your DNS profile.\nNow you just need to configure your backend DNS server to forward the requests for these domains to the Avi DNS VS IP. Using bind this can be done like this:\n1zone \u0026#34;you-have.your-domain.here\u0026#34; { 2 type forward; 3 forward only; 4 forwarders { 10.101.211.9; }; 5}; AKO in TKC I have already deployed a TKC cluster, which is described here\nAlso make sure Antrea is configured with NodePortLocal as described also in the link above.\nSo for Avi to work as Ingress controller we need to deploy AKO (Avi Kubernetes Operator). I have also explained these steps here the only difference is how the value.yaml for AKO is configured. Below is how I have configured it to work in my NSX enabled environment with explanations:\n1# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. 2replicaCount: 1 3 4image: 5 repository: projects.registry.vmware.com/ako/ako 6 pullPolicy: IfNotPresent 7 8 9AKOSettings: 10 primaryInstance: true 11 enableEvents: \u0026#39;true\u0026#39; 12 logLevel: WARN 13 fullSyncFrequency: \u0026#39;1800\u0026#39; 14 apiServerPort: 8080 15 deleteConfig: \u0026#39;false\u0026#39; 16 disableStaticRouteSync: \u0026#39;false\u0026#39; 17 clusterName: wdc-tkc-cluster-1-nsx # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller 18 cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. 19 enableEVH: false 20 layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. 21 22 namespaceSelector: 23 labelKey: \u0026#39;\u0026#39; 24 labelValue: \u0026#39;\u0026#39; 25 servicesAPI: false 26 vipPerNamespace: \u0026#39;false\u0026#39; 27 28NetworkSettings: 29 nodeNetworkList: 30 # nodeNetworkList: 31 - networkName: \u0026#34;vnet-domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5-ns-wdc-1-tkc-cluste-62397-0\u0026#34; # this is the NSX segment created for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter 32 cidrs: 33 - 10.101.112.32/27 # this is the CIDR for your current TKC cluster (make sure you are using right CIDR, seen from NSX) 34 enableRHI: false 35 nsxtT1LR: \u0026#39;Da-Tier-1\u0026#39; #The T1 router in NSX you have defined for the avi-se-dataplane network 36 bgpPeerLabels: [] 37 # bgpPeerLabels: 38 # - peer1 39 # - peer2 40 vipNetworkList: 41 - networkName: \u0026#34;vip-tkc-cluster-1-nsx-wdc-l7\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. 42 cidr: 10.101.210.0/24 43 44L7Settings: 45 defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. 46 noPGForSNI: false 47 serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal 48 shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. 49 passthroughShardSize: SMALL 50 enableMCI: \u0026#39;false\u0026#39; 51 52L4Settings: 53 defaultDomain: \u0026#39;\u0026#39; 54 autoFQDN: default 55 56 57ControllerSettings: 58 serviceEngineGroupName: nsx-se-generic-group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). 59 controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.2 60 cloudName: wdc-1-nsx # The configured cloud name on the Avi controller. 61 controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller 62 tenantName: admin 63 64nodePortSelector: 65 key: \u0026#39;\u0026#39; 66 value: \u0026#39;\u0026#39; 67 68resources: 69 limits: 70 cpu: 350m 71 memory: 400Mi 72 requests: 73 cpu: 200m 74 memory: 300Mi 75 76podSecurityContext: {} 77 78rbac: 79 pspEnable: false 80 81 82avicredentials: 83 username: \u0026#39;admin\u0026#39; # username for the Avi controller 84 password: \u0026#39;password\u0026#39; # password for the Avi controller 85 authtoken: 86 certificateAuthorityData: 87 88 89persistentVolumeClaim: \u0026#39;\u0026#39; 90mountPath: /log 91logFile: avi.log Install AKO with this command:\n1helm install ako/ako --generate-name --version 1.8.2 -f values.yaml --namespace=avi-system Check the logs of the AKO pod if it encountered some issues or not by issuing the command:\n1kubectl logs -n avi-system ako-o If there is no errors there its time to deploy a couple of test applications and the Ingress itself. This is already described here\nThats it. Now L7 is enabled on your TKC cluster with Avi as Ingress controller. There is much that can be configured with AKO CRDs. I will try to update my post here to go through the different possibilites. In the meantime much information is described here\nAviInfraSetting If you need to have separate VIPs/different subnets for certain applications we can use AviInfraSetting to override the \u0026quot;default\u0026quot; settings configured in our values.yaml above. This is a nice feature to override some settings very easy. There is also the option to run several AKO instances pr TKC/k8s cluster like described here which I will go through another time. But now quickly AviInfraSetting\nLets say I want to enable BPG on certain services or adjust my Ingress to be exposed on a different VIP network.\nCreate a yaml definition for AviInfraSetting:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: AviInfraSetting 3metadata: 4 name: enable-bgp-fruit 5spec: 6 seGroup: 7 name: Default-Group 8 network: 9 vipNetworks: 10 - networkName: vds-tkc-frontend-l7-vlan-1028 11 cidr: 10.102.8.0/24 12 nodeNetworks: 13 - networkName: vds-tkc-workload-vlan-1026 14 cidrs: 15 - 10.102.6.0/24 16 enableRhi: true 17 bgpPeerLabels: 18 - cPodRouter In the example above I define the VIP network (here I can override the default confgured from value.yaml), the nodNetwork. Enable RHI, and define a label to be used for BGP (label is from BGP settings here):\nPeer:\nApply the above yaml. To use it, create an additional IngressClass like this:\n1apiVersion: networking.k8s.io/v1 2kind: IngressClass 3metadata: 4 name: avi-lb-bgp #name the IngressClass 5spec: 6 controller: ako.vmware.com/avi-lb #default ingressclass from ako 7 parameters: 8 apiGroup: ako.vmware.com 9 kind: AviInfraSetting #refer to the AviInfraSetting 10 name: enable-bgp-fruit #the name of your AviInfraSetting applied Apply it, then when you apply your Ingress or update it refer to this ingressClass like this:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 namespace: fruit 6 7spec: 8 ingressClassName: avi-lb-bgp #Here you choose your specific IngressClass 9 rules: 10 - host: fruit-tkgs.you-have.your-domain.here 11 http: 12 paths: 13 - path: /apple 14 pathType: Prefix 15 backend: 16 service: 17 name: apple-service 18 port: 19 number: 5678 20 - path: /banana 21 pathType: Prefix 22 backend: 23 service: 24 name: banana-service 25 port: 26 number: 5678 ","link":"https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-nsx-t-avi-loadbalancer/","section":"post","tags":["tanzu","kubernetes","network","lodbalancing","ingress"],"title":"vSphere 8 with Tanzu using NSX-T \u0026 Avi LoadBalancer"},{"body":"What is AKO? AKO is an operator which works as an ingress controller and performs Avi-specific functions in an OpenShift/Kubernetes environment with the Avi Controller. It runs as a pod in the cluster and translates the required OpenShift/Kubernetes objects to Avi objects and automates the implementation of ingresses/routes/services on the Service Engines (SE) via the Avi Controller. ref: link\nHow to install AKO AKO is very easy installed with Helm. Four basic steps needs to be done.\nCreate a namespace for AKO in your kubernetes cluster: kubectl create ns avi-system Add AKO Helm reposistory: helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Get the current values for the versions you want: helm show values ako/ako --version 1.9.1 \u0026gt; values.yaml Deploy (after values have been edited to suit your environment): helm install ako/ako --generate-name --version 1.9.1 -f values.yaml -n avi-system AKO Helm values explained Before deploying AKO there are some parameters that should be configured, or most likely the deployment will fail. Below is an example file where the different fields are explained:\n1# Default values for ako. 2# This is a YAML-formatted file. 3# Declare variables to be passed into your templates. 4 5replicaCount: 1 6 7image: 8 repository: projects.registry.vmware.com/ako/ako #If using your own registry update accordingly 9 pullPolicy: IfNotPresent 10 11### This section outlines the generic AKO settings 12AKOSettings: 13 primaryInstance: true # Defines AKO instance is primary or not. Value `true` indicates that AKO instance is primary. In a multiple AKO deployment in a cluster, only one AKO instance should be primary. Default value: true. 14 enableEvents: \u0026#39;true\u0026#39; # Enables/disables Event broadcasting via AKO 15 logLevel: WARN # enum: INFO|DEBUG|WARN|ERROR 16 fullSyncFrequency: \u0026#39;1800\u0026#39; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. 17 apiServerPort: 8080 # Internal port for AKO\u0026#39;s API server for the liveness probe of the AKO pod default=8080 18 deleteConfig: \u0026#39;false\u0026#39; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI 19 disableStaticRouteSync: \u0026#39;false\u0026#39; # If the POD networks are reachable from the Avi SE, set this knob to true. 20 clusterName: my-cluster # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT 21 cniPlugin: \u0026#39;\u0026#39; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift|antrea|ncp 22 enableEVH: false # This enables the Enhanced Virtual Hosting Model in Avi Controller for the Virtual Services 23 layer7Only: false # If this flag is switched on, then AKO will only do layer 7 loadbalancing.Must be true if used in a TKC cluster / Tanzu with vSphere 24 # NamespaceSelector contains label key and value used for namespacemigration 25 # Same label has to be present on namespace/s which needs migration/sync to AKO 26 namespaceSelector: 27 labelKey: \u0026#39;\u0026#39; 28 labelValue: \u0026#39;\u0026#39; 29 servicesAPI: false # Flag that enables AKO in services API mode: https://kubernetes-sigs.github.io/service-apis/. Currently implemented only for L4. This flag uses the upstream GA APIs which are not backward compatible 30 # with the advancedL4 APIs which uses a fork and a version of v1alpha1pre1 31 vipPerNamespace: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to create Parent VS per Namespace in EVH mode 32 istioEnabled: false # This flag needs to be enabled when AKO is be to brought up in an Istio environment 33 # This is the list of system namespaces from which AKO will not listen any Kubernetes or Openshift object event. 34 blockedNamespaceList: [] 35 # blockedNamespaceList: 36 # - kube-system 37 # - kube-public 38 ipFamily: \u0026#39;\u0026#39; # This flag can take values V4 or V6 (default V4). This is for the backend pools to use ipv6 or ipv4. For frontside VS, use v6cidr 39 40 41### This section outlines the network settings for virtualservices. 42NetworkSettings: 43 ## This list of network and cidrs are used in pool placement network for vcenter cloud. 44 ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. 45 nodeNetworkList: [] 46 # nodeNetworkList: 47 # - networkName: \u0026#34;network-name\u0026#34; 48 # cidrs: 49 # - 10.0.0.1/24 50 # - 11.0.0.1/24 51 enableRHI: false # This is a cluster wide setting for BGP peering. 52 nsxtT1LR: \u0026#39;\u0026#39; # T1 Logical Segment mapping for backend network. Only applies to NSX-T cloud. 53 bgpPeerLabels: [] # Select BGP peers using bgpPeerLabels, for selective VsVip advertisement. 54 # bgpPeerLabels: 55 # - peer1 56 # - peer2 57 vipNetworkList: [] # Network information of the VIP network. Multiple networks allowed only for AWS Cloud. 58 # vipNetworkList: 59 # - networkName: net1 60 # cidr: 100.1.1.0/24 61 # v6cidr: 2002:ðŸ”¢abcd:ffff:c0a8:101/64 # Setting this will enable the VS networks to use ipv6 62### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. 63L7Settings: 64 defaultIngController: \u0026#39;true\u0026#39; 65 noPGForSNI: false # Switching this knob to true, will get rid of poolgroups from SNI VSes. Do not use this flag, if you don\u0026#39;t want http caching. This will be deprecated once the controller support caching on PGs. 66 serviceType: ClusterIP # enum NodePort|ClusterIP|NodePortLocal. NodePortLocal can only be used if Antrea is the CNI 67 shardVSSize: LARGE # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL, DEDICATED 68 passthroughShardSize: SMALL # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL 69 enableMCI: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to start processing multi-cluster ingress objects. 70 71### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. 72L4Settings: 73 defaultDomain: \u0026#39;\u0026#39; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. 74 autoFQDN: default # ENUM: default(\u0026lt;svc\u0026gt;.\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), flat (\u0026lt;svc\u0026gt;-\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), \u0026#34;disabled\u0026#34; If the value is disabled then the FQDN generation is disabled. 75 76### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. 77ControllerSettings: 78 serviceEngineGroupName: Default-Group # Name of the ServiceEngine Group. 79 controllerVersion: \u0026#39;\u0026#39; # The controller API version 80 cloudName: Default-Cloud # The configured cloud name on the Avi controller. 81 controllerHost: \u0026#39;\u0026#39; # IP address or Hostname of Avi Controller 82 tenantName: admin # Name of the tenant where all the AKO objects will be created in AVI. 83 84nodePortSelector: # Only applicable if serviceType is NodePort 85 key: \u0026#39;\u0026#39; 86 value: \u0026#39;\u0026#39; 87 88resources: 89 limits: 90 cpu: 350m 91 memory: 400Mi 92 requests: 93 cpu: 200m 94 memory: 300Mi 95 96securityContext: {} 97 98podSecurityContext: {} 99 100rbac: 101 # Creates the pod security policy if set to true 102 pspEnable: false 103 104 105avicredentials: 106 username: \u0026#39;\u0026#39; 107 password: \u0026#39;\u0026#39; 108 authtoken: 109 certificateAuthorityData: 110 111 112persistentVolumeClaim: \u0026#39;\u0026#39; 113mountPath: /log 114logFile: avi.log More info here\n","link":"https://blog.andreasm.io/2022/10/26/ako-explained/","section":"post","tags":["kubernetes","ingress","loadbalancing","ako"],"title":"AKO Explained"},{"body":"","link":"https://blog.andreasm.io/tags/amko/","section":"tags","tags":null,"title":"amko"},{"body":"","link":"https://blog.andreasm.io/tags/gslb/","section":"tags","tags":null,"title":"gslb"},{"body":"Global Server LoadBalancing in VMware Tanzu with AMKO This post will go through how to configure AVI (NSX ALB) with GSLB in vSphere with Tanzu (TKGs) and an upstream k8s cluster in two different physical locations. I have already covered AKO in my previous posts, this post will assume knowledge of AKO (Avi Kubernetes Operator) and extend upon that with the use of AMKO (Avi Multi-Cluster Kubernetes Operator). The goal is to have the ability to scale my k8s applications between my \u0026quot;sites\u0026quot; and make them geo-redundant. For more information on AVI, AKO and AMKO head over here\nPreparations and diagram over environment used in this post This post will involve a upstream Ubuntu k8s cluster in my home-lab and a remote vSphere with Tanzu cluster. I have deployed one Avi Controller in my home lab and one Avi controller in the remote site. The k8s cluster in my home-lab is defined as the \u0026quot;primary\u0026quot; k8s cluster, the same goes for the Avi controller in my home-lab. There are some networking connectivity between the AVI controllers that needs to be in place such as 443 (API) between the controllers, and the AVI SE's needs to reach the GSLB VS vips on their respective side for GSLB health checks. Site A SE's dataplane needs connectivity to the vip that is created for the GSLB service on site B and vice versa. The primary k8s cluster also needs connectivity to the \u0026quot;secondary\u0026quot; k8s clusters endpoint ip/fqdn, k8s api (port 6443). AMKO needs this connectivity to listen for \u0026quot;GSLB\u0026quot; enabled services in the remote k8s clusters which triggers AMKO to automatically put them in your GSLB service. More on that later in the article. When all preparations are done the final diagram should look something like this:\n(I will not cover what kind of infrastructure that connects the sites together as that is a completely different topic and can be as much). But there will most likely be a firewall involved between the sites, and the above mentioned connectivity needs to be adjusted in the firewall. In this post the following ip subnets will be used:\nSE Dataplane network home-lab: 10.150.1.0/24 (I only have two se's so there will be two addresses from this subnet) (I am running the all services on the same two SE's which is not recommended, one should atleast have dedicated SE's for the AVI DNS service) SE Dataplane network remote-site: 192.168.102.0/24 (Two SE's here also, in remote site I do have dedicated SE's for the AVI DNS Service but they will not be touched upon in this post only the SE's responsible for the GSLB services being created) VIP subnet for services exposed in home-lab k8s cluster: 10.150.12.0/24 (a dedicated vip subnet for all services exposed from this cluster) VIP subnet for services exposed in remote-site tkgs cluster: 192.168.151.0/24 (a dedicated vip subnet for all services exposed from this cluster) For this network setup to work one needs to have routing in place, either with BGP enabled in AVI or static routes. Explanation: The SE's have their own dataplane network, they are also the ones responsible for creating the VIPs you define for your VS. So, if you want your VIPs to be reachable you have to make sure there are routes in your network to the VIPS where the SEs are next hops either with BGP or static routes. The VIP is what it is, a Virtual IP meaning it dont have its own VLAN and gateway in your infrastructure. It is created and realised by the SE's. The SE's are then the gateways for your VIPS. A VIP address could be anything. At the same time the SEs dataplane network needs connectivity to the backend servers it is supposed to loadbalance, so this dataplane network also needs routes to reach those. In this post that means the SE's dataplane network will need reachability to the k8s worker nodes where your apps are running in the home-lab site and in the remote site it needs reachability to the TKGs workers. On a sidenote I am not running routable pods, they are nat-ed trough my workers, and I am using Antrea as CNI with NodePortLocal configured. I also prefer to have a different network for the SE dataplane, different VIP subnets as it is easier to maintain control, isolation, firewall rules etc.\nThe diagram above is very high level, as it does not go into all networking details, firewall rules etc but it gives an overview of the communication needed.\nWhen one have an clear idea of the connectivity requirements we need to form the GSLB \u0026quot;partnership\u0026quot; between the AVI controllers. I was thinking back and forth whether I should cover these steps also but instead I will link to a good friends blog site here that does this brilliantly. Its all about saving the environment of unnecessary digital ink ðŸ˜„. This also goes for AKO deployment. This is also covered here or from the AVI docs page here\nIt should look like this on both controllers when everything is up and ready for GSLB: It should be reflected on the secondary controller as well, except there will be no option to edit.\nTime to deploy AMKO in K8s AMKO can be deployed in two ways. It can be sufficient with only one instance of AMKO deployed in your primary k8s cluster, or you can go the federation approach and deploy AMKO in all your clusters that you want to use GSLB on. Then you will end up with one master instance of AMKO and \u0026quot;followers\u0026quot; or federation member on the others. One of the benefit is that you can promote one of the follower members if the primary is lost. I will go with the simple approach, deploy AMKO once, in my primary k8s cluster in my home-lab.\nAMKO preparations before deploy with Helm AMKO will be deployed by using Helm, so if Helm is not installed do that. To successfully install AMKO there is a couple of things to be done. First, decide which is your primary cluster (where to deploy AMKO). When you have decided that (the easy step) then you need to prepare a secret that contains the context/clusters/users for all the k8s clusters you want to use GSLB on. An example file can be found here. Create this content in a regular file and name the file gslb-members. The naming of the file is important, if you name it differently AMKO will fail as it cant find the secret. I have tried to find a variable that is able override this in the value.yaml for the Helm chart but has not succeeded, so I went with the default naming. When that is populated with the k8s clusters you want, we need to create a secret in our primary k8s cluster like this: kubectl create secret generic gslb-config-secret --from-file gslb-members -n avi-system. The namespace here is the namespace where AKO is already deployed in.\nThis should give you a secret like this:\n1gslb-config-secret Opaque 1 20h A note on kubeconfig for vSphere with Tanzu (TKGs) When logging into a guest cluster in TKGs we usually do this through the supervisor with either vSphere local users or AD users defined in vSphere and we get a timebased token. Its not possible to use this approach. So what I went with was to grab the admin credentials for my TKGs guest cluster and used that context instead. Here is how to do that. This is not a recommended approach, instead one should create and use a service account. Maybe I will get back to this later and update how.\nBack to the AMKO deployment...\nThe secret is ready, now we need to get the value.yaml for the AMKO version we will install. I am using AMKO 1.8.1 (same for AKO). The Helm repo for AMKO is already added if AKO has been installed using Helm, the same repo. If not, add the repo:\n1helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Download the value.yaml:\n1 helm show values ako/amko --version 1.8.1 \u0026gt; values.yaml (there is a typo in the official doc - it points to just amko) Now edit the values.yaml:\n1# This is a YAML-formatted file. 2# Declare variables to be passed into your templates. 3 4replicaCount: 1 5 6image: 7 repository: projects.registry.vmware.com/ako/amko 8 pullPolicy: IfNotPresent 9 10# Configs related to AMKO Federator 11federation: 12 # image repository 13 image: 14 repository: projects.registry.vmware.com/ako/amko-federator 15 pullPolicy: IfNotPresent 16 # cluster context where AMKO is going to be deployed 17 currentCluster: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name - for your leader/primary cluster 18 # Set to true if AMKO on this cluster is the leader 19 currentClusterIsLeader: true 20 # member clusters to federate the GSLBConfig and GDP objects on, if the 21 # current cluster context is part of this list, the federator will ignore it 22 memberClusters: 23 - \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name 24 - \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name 25# Configs related to AMKO Service discovery 26serviceDiscovery: 27 # image repository 28 # image: 29 # repository: projects.registry.vmware.com/ako/amko-service-discovery 30 # pullPolicy: IfNotPresent 31 32# Configs related to Multi-cluster ingress. Note: MultiClusterIngress is a tech preview. 33multiClusterIngress: 34 enable: false 35 36configs: 37 gslbLeaderController: \u0026#39;172.18.5.51\u0026#39; ##### MGMT ip leader/primary avi controller 38 controllerVersion: 22.1.1 39 memberClusters: 40 - clusterContext: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name 41 - clusterContext: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name 42 refreshInterval: 1800 43 logLevel: INFO 44 # Set the below flag to true if a different GSLB Service fqdn is desired than the ingress/route\u0026#39;s 45 # local fqdns. Note that, this field will use AKO\u0026#39;s HostRule objects\u0026#39; to find out the local to global 46 # fqdn mapping. To configure a mapping between the local to global fqdn, configure the hostrule 47 # object as: 48 # [...] 49 # spec: 50 # virtualhost: 51 # fqdn: foo.avi.com 52 # gslb: 53 # fqdn: gs-foo.avi.com 54 useCustomGlobalFqdn: true ####### set this to true if you want to define custom FQDN for GSLB - I use this 55 56gslbLeaderCredentials: 57 username: \u0026#39;admin\u0026#39; ##### username/password AVI Controller 58 password: \u0026#39;password\u0026#39; ##### username/password AVI Controller 59 60globalDeploymentPolicy: 61 # appSelector takes the form of: 62 appSelector: 63 label: 64 app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB 65 # Uncomment below and add the required ingress/route/service label 66 # appSelector: 67 68 # namespaceSelector takes the form of: 69 # namespaceSelector: 70 # label: 71 # ns: gslb \u0026lt;example label key-value for namespace\u0026gt; 72 # Uncomment below and add the reuqired namespace label 73 # namespaceSelector: 74 75 # list of all clusters that the GDP object will be applied to, can take any/all values 76 # from .configs.memberClusters 77 matchClusters: 78 - cluster: \u0026#39;k8slab-admin@k8slab\u0026#39; ####use the context name 79 - cluster: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; ####use the context name 80 81 # list of all clusters and their traffic weights, if unspecified, default weights will be 82 # given (optional). Uncomment below to add the required trafficSplit. 83 # trafficSplit: 84 # - cluster: \u0026#34;cluster1-admin\u0026#34; 85 # weight: 8 86 # - cluster: \u0026#34;cluster2-admin\u0026#34; 87 # weight: 2 88 89 # Uncomment below to specify a ttl value in seconds. By default, the value is inherited from 90 # Avi\u0026#39;s DNS VS. 91 # ttl: 10 92 93 # Uncomment below to specify custom health monitor refs. By default, HTTP/HTTPS path based health 94 # monitors are applied on the GSs. 95 # healthMonitorRefs: 96 # - hmref1 97 # - hmref2 98 99 # Uncomment below to specify a Site Persistence profile ref. By default, Site Persistence is disabled. 100 # Also, note that, Site Persistence is only applicable on secure ingresses/routes and ignored 101 # for all other cases. Follow https://avinetworks.com/docs/20.1/gslb-site-cookie-persistence/ to create 102 # a Site persistence profile. 103 # sitePersistenceRef: gap-1 104 105 # Uncomment below to specify gslb service pool algorithm settings for all gslb services. Applicable 106 # values for lbAlgorithm: 107 # 1. GSLB_ALGORITHM_CONSISTENT_HASH (needs a hashMask field to be set too) 108 # 2. GSLB_ALGORITHM_GEO (needs geoFallback settings to be used for this field) 109 # 3. GSLB_ALGORITHM_ROUND_ROBIN (default) 110 # 4. GSLB_ALGORITHM_TOPOLOGY 111 # 112 # poolAlgorithmSettings: 113 # lbAlgorithm: 114 # hashMask: # required only for lbAlgorithm == GSLB_ALGORITHM_CONSISTENT_HASH 115 # geoFallback: # fallback settings required only for lbAlgorithm == GSLB_ALGORITHM_GEO 116 # lbAlgorithm: # can only have either GSLB_ALGORITHM_ROUND_ROBIN or GSLB_ALGORITHM_CONSISTENT_HASH 117 # hashMask: # required only for fallback lbAlgorithm as GSLB_ALGORITHM_CONSISTENT_HASH 118 119serviceAccount: 120 # Specifies whether a service account should be created 121 create: true 122 # Annotations to add to the service account 123 annotations: {} 124 # The name of the service account to use. 125 # If not set and create is true, a name is generated using the fullname template 126 name: 127 128resources: 129 limits: 130 cpu: 250m 131 memory: 300Mi 132 requests: 133 cpu: 100m 134 memory: 200Mi 135 136service: 137 type: ClusterIP 138 port: 80 139 140rbac: 141 # creates the pod security policy if set to true 142 pspEnable: false 143 144persistentVolumeClaim: \u0026#39;\u0026#39; 145mountPath: /log 146logFile: amko.log 147 148federatorLogFile: amko-federator.log When done, its time to install AMKO like this:\n1helm install ako/amko --generate-name --version 1.8.1 -f /path/to/values.yaml --set configs.gslbLeaderController=\u0026lt;leader_controller_ip\u0026gt; --namespace=avi-system ####There is a typo in the official docs - its pointing to amko only If everything went well you should se a couple of things in your k8s cluster under the namespace avi-system.\n1k get pods -n avi-system 2NAME READY STATUS RESTARTS AGE 3ako-0 1/1 Running 0 25h 4amko-0 2/2 Running 0 20h 5 6k get amkocluster amkocluster-federation -n avi-system 7NAME AGE 8amkocluster-federation 20h 9 10k get gc -n avi-system gc-1 11NAME AGE 12gc-1 20h 13 14k get gdp -n avi-system 15NAME AGE 16global-gdp 20h AMKO is up and running. Time create a GSLB service\nCreate GSLB service You probably already have a bunch of ingress services running, and to make them GSLB \u0026quot;aware\u0026quot; there is not much to be done to achieve that. If you noticed in our value.yaml for the AMKO Helm chart we defined this:\n1globalDeploymentPolicy: 2 # appSelector takes the form of: 3 appSelector: 4 label: 5 app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB So what we need to in our ingress service is to add the below, and then a new section where we define our gslb fqdn.\nHere is my sample ingress applied in my primary k8s cluster:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 labels: #### This is added for GSLB 6 app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml 7 namespace: fruit 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-global.guzware.net #### Specific for this site (Home Lab) 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 29--- #### New section to define a host rule 30apiVersion: ako.vmware.com/v1alpha1 31kind: HostRule 32metadata: 33 namespace: fruit 34 name: gslb-host-rule-fruit 35spec: 36 virtualhost: 37 fqdn: fruit-global.guzware.net #### Specific for this site (Home Lab) 38 enableVirtualHost: true 39 gslb: 40 fqdn: fruit.gslb.guzware.net ####This is common for both sites As soon as it is applied, and there are no errors in AMKO or AKO, it should be visible in your AVI controller GUI: If you click on the name it should take you to next page where it show the GSLB pool members and the status: Screenshot below is when both sites have applied their GSLB services: \u0026quot;\nNext we need to apply gslb settings on the secondary site also:\nThis is what I have deployed on the secondary site (note the difference in domain names specific for that site)\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 labels: #### This is added for GSLB 6 app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml 7 namespace: fruit 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 29--- #### New section to define a host rule 30apiVersion: ako.vmware.com/v1alpha1 31kind: HostRule 32metadata: 33 namespace: fruit 34 name: gslb-host-rule-fruit 35spec: 36 virtualhost: 37 fqdn: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) 38 enableVirtualHost: true 39 gslb: 40 fqdn: fruit.gslb.guzware.net ##### Common for both sites When this is applied Avi will go ahead and put this into the same GSLB service as above, and the screenshot above will be true.\nNow I have the same application deployed in both sites, but equally available whether I am sitting in my home-lab or at the remote-site. There is a bunch of parameters that can be tuned, which I will not go into now (maybe getting back to this and update with further possibilities with GSLB). But one of them can be LoadBalancing algorithms such as Geo Location Source. Say I want the application to be accessed from clients as close to the application as possible. And should one of the sites become unavailable it will still be accessible from one of the sites that are still online. Very cool indeed. For the sake of the demo I am about to show the only thing I change in the default GSLB settings is the TTL, I am setting it to 2 seconds so I can showcase that the application is being load balanced between both sites. Default algorithm is Round-Robin so it should balance between them regardless of the latency difference (accessing the application from my home network in my home lab vs from my home network in the remote-site which has several ms in distance). Heres where I am setting these settings: With a TTL of 2 seconds it should switch faster so I can see the balancing between the two sites. Let me try to access the application from my browser using the gslb fqdn: fruit.gslb.guzware.net/apple\nA refresh of the page and now: To even illustrate more I will run a curl command against the gslb fqdn: Now a ping against the FQDN to show the ip of the corresponding site that answer on the call: Notice the change in ip address but also the latency in ms\nNow I can go ahead and disable one of the site to simulate failover, and the application is still available on the same FQDN. So many possibilities with GSLB.\nThats it then. NSX ALB, AKO with AMKO between two sites, same application available in two physical location, redundancy, scale-out, availability. Stay tuned for more updates in advanced settings - in the future ðŸ˜„\n","link":"https://blog.andreasm.io/2022/10/23/gslb-with-ako-amko-nsx-advanced-loadbalancer/","section":"post","tags":["gslb","ako","amko"],"title":"GSLB With AKO \u0026 AMKO - NSX Advanced LoadBalancer"},{"body":"","link":"https://blog.andreasm.io/tags/custom-resource-definitions/","section":"tags","tags":null,"title":"custom-resource-definitions"},{"body":"AKO settings: What happens if we need to to this\nWhat happens if I need passthrough\nHow does AKO work\n","link":"https://blog.andreasm.io/2022/10/23/we-take-a-look-at-the-ako-crds/","section":"post","tags":["ako","custom-resource-definitions","kubernetes"],"title":"We Take a Look at the AKO Crds"},{"body":"","link":"https://blog.andreasm.io/tags/docker/","section":"tags","tags":null,"title":"docker"},{"body":"","link":"https://blog.andreasm.io/tags/loadbalancer/","section":"tags","tags":null,"title":"loadbalancer"},{"body":"TOPICS: ","link":"https://blog.andreasm.io/2022/10/23/running-the-unifi-controller-in-kubernetes/","section":"post","tags":["unifi","loadbalancer","docker"],"title":"Running the Unifi Controller in Kubernetes"},{"body":"","link":"https://blog.andreasm.io/tags/unifi/","section":"tags","tags":null,"title":"unifi"},{"body":"","link":"https://blog.andreasm.io/tags/grafana/","section":"tags","tags":null,"title":"grafana"},{"body":"","link":"https://blog.andreasm.io/categories/logging/","section":"categories","tags":null,"title":"Logging"},{"body":"","link":"https://blog.andreasm.io/tags/loki/","section":"tags","tags":null,"title":"loki"},{"body":"","link":"https://blog.andreasm.io/categories/monitoring/","section":"categories","tags":null,"title":"Monitoring"},{"body":"Logging and metrics monitoring I wanted to visualize performance metrics in Grafana, and getting the logs from my Kubernetes clusters available centrally. So i chose to go with Grafana as my \u0026quot;dashboard\u0026quot; for visualizing, Prometheus for metrics and Loki for logs. I did fiddle some to get this up and running. But after I while I managed to get it sorted the way I wanted.\nSources used in this article: Bitnami, Grafana and Kube-Prometheus-Stack\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n","link":"https://blog.andreasm.io/2022/10/19/monitoring-with-prometheus-loki-promtail-and-grafana/","section":"post","tags":["loki","grafana","promtail","prometheus"],"title":"Monitoring With Prometheus, Loki, Promtail and Grafana"},{"body":"","link":"https://blog.andreasm.io/tags/prometheus/","section":"tags","tags":null,"title":"prometheus"},{"body":"","link":"https://blog.andreasm.io/tags/promtail/","section":"tags","tags":null,"title":"promtail"},{"body":"","link":"https://blog.andreasm.io/categories/docker/","section":"categories","tags":null,"title":"Docker"},{"body":"","link":"https://blog.andreasm.io/tags/harbor/","section":"tags","tags":null,"title":"harbor"},{"body":"","link":"https://blog.andreasm.io/categories/harbor/","section":"categories","tags":null,"title":"Harbor"},{"body":"","link":"https://blog.andreasm.io/categories/helm/","section":"categories","tags":null,"title":"Helm"},{"body":"","link":"https://blog.andreasm.io/tags/registry/","section":"tags","tags":null,"title":"registry"},{"body":"This post will briefly go through how to deploy (using Helm), configure and use VMware Harbor registry in Kubernetes.\nQuick introduction to Harbor Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. link\nI use myself Harbor in many of my own projects, including the images I make for my Hugo blogsite (this).\nDeploy Harbor with Helm Add helm chart:\n1helm repo add harbor https://helm.goharbor.io 2helm fetch harbor/harbor --untar Before you perform the default helm install of Harbor you want to grab the helm values for the Harbor charts so you can edit some settings to match your environment:\n1helm show values harbor/harbor \u0026gt; harbor.values.yaml The default values you get from the above command includes all available parameter which can be a bit daunting to go through. In the values file I use I have only picked the parameters I needed to set, here:\n1expose: 2 type: ingress 3 tls: 4 enabled: true 5 certSource: secret 6 secret: 7 secretName: \u0026#34;harbor-tls-prod\u0026#34; # certificates you have created with Cert-Manager 8 notarySecretName: \u0026#34;notary-tls-prod\u0026#34; # certificates you have created with Cert-Manager 9 ingress: 10 hosts: 11 core: registry.example.com 12 notary: notary.example.com 13 annotations: 14 kubernetes.io/ingress.class: \u0026#34;avi-lb\u0026#34; 15 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 16externalURL: https://registry.example.com 17harborAdminPassword: \u0026#34;PASSWORD\u0026#34; 18persistence: 19 enabled: true 20 # Setting it to \u0026#34;keep\u0026#34; to avoid removing PVCs during a helm delete 21 # operation. Leaving it empty will delete PVCs after the chart deleted 22 # (this does not apply for PVCs that are created for internal database 23 # and redis components, i.e. they are never deleted automatically) 24 resourcePolicy: \u0026#34;keep\u0026#34; 25 persistentVolumeClaim: 26 registry: 27 # Use the existing PVC which must be created manually before bound, 28 # and specify the \u0026#34;subPath\u0026#34; if the PVC is shared with other components 29 existingClaim: \u0026#34;\u0026#34; 30 # Specify the \u0026#34;storageClass\u0026#34; used to provision the volume. Or the default 31 # StorageClass will be used (the default). 32 # Set it to \u0026#34;-\u0026#34; to disable dynamic provisioning 33 storageClass: \u0026#34;nfs-client\u0026#34; 34 subPath: \u0026#34;\u0026#34; 35 accessMode: ReadWriteOnce 36 size: 50Gi 37 annotations: {} 38 database: 39 existingClaim: \u0026#34;\u0026#34; 40 storageClass: \u0026#34;nfs-client\u0026#34; 41 subPath: \u0026#34;postgres-storage\u0026#34; 42 accessMode: ReadWriteOnce 43 size: 1Gi 44 annotations: {} 45 46portal: 47 tls: 48 existingSecret: harbor-tls-prod When you have edited the values file its time to install:\n1helm install -f harbor.values.yaml harbor-deployment harbor/harbor -n harbor Explanation: \u0026quot;-f\u0026quot; is telling helm to read the values from the specified file after, then the name of your helm installation (here harbor-deployment) then the helm repo and finally the namespace you want it deployed in. A couple of seconds later you should be able to log in to the GUI of Harbor through your webbrowser if everything has been set up right, Ingress, pvc, secrets.\nCertificate You can either use Cert-manager as explained here or bring your own ca signed certificates.\nHarbor GUI To log in to the GUI for the first time open your browser and point it to the externalURL you gave it in your values file and the corresponding harborAdminPassword you defined. From there on you create users and projects and start exploring Harbor.\nUsers: Projects: Docker images To push your images to Harbor execute the following commands:\n1docker login registry.example.com #log in with the user/password you have created in the GUI 2docker tag image-name:tag registry.example.com/project/image-name:tag 3docker push registry.example.com/project/image-name:tag ","link":"https://blog.andreasm.io/2022/10/13/vmware-harbor-registry/","section":"post","tags":["harbor","registry"],"title":"VMware Harbor Registry"},{"body":"","link":"https://blog.andreasm.io/categories/hugo/","section":"categories","tags":null,"title":"hugo"},{"body":"This blog post will cover how I wanted to deploy Hugo to host my blog-page.\nPreparations To achieve what I wanted, deploy an highly available Hugo hosted blog page, I decided to run Hugo in Kubernetes. For that I needed\nKubernetes cluster, obviously, consisting of several workers for the the \u0026quot;hugo\u0026quot; pods to run on (already covered here. Persistent storage (NFS in my case, already covered here) An Ingress controller (already covered here) A docker image with Hugo, nginx and go (will be covered here) Docker installed so you can build the image A place to host the docker image (Docker hub or Harbor registry will be covered here) Create the Docker image Before I can deploy Hugo I need to create an Docker image that contains the necessary bits. I have already created the Dockerfile here:\n1#Install the container\u0026#39;s OS. 2FROM ubuntu:latest as HUGOINSTALL 3 4# Install Hugo. 5RUN apt-get update -y 6RUN apt-get install wget git ca-certificates golang -y 7RUN wget https://github.com/gohugoio/hugo/releases/download/v0.104.3/hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ 8 tar -xvzf hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ 9 chmod +x hugo \u0026amp;\u0026amp; \\ 10 mv hugo /usr/local/bin/hugo \u0026amp;\u0026amp; \\ 11 rm -rf hugo_extended_0.104.3_Linux-64bit.tar.gz 12# Copy the contents of the current working directory to the hugo-site 13# directory. The directory will be created if it doesn\u0026#39;t exist. 14COPY . /hugo-site 15 16# Use Hugo to build the static site files. 17RUN hugo -v --source=/hugo-site --destination=/hugo-site/public 18 19# Install NGINX and deactivate NGINX\u0026#39;s default index.html file. 20# Move the static site files to NGINX\u0026#39;s html directory. 21# This directory is where the static site files will be served from by NGINX. 22FROM nginx:stable-alpine 23RUN mv /usr/share/nginx/html/index.html /usr/share/nginx/html/old-index.html 24COPY --from=HUGOINSTALL /hugo-site/public/ /usr/share/nginx/html/ 25 26# The container will listen on port 80 using the TCP protocol. 27EXPOSE 80 Credits for the Dockerfile as it was initially taken from here. I have updated it, and done some modifications to it.\nBefore building the image with docker, install docker by following this guide.\nBuild the docker image I need to place myself in the same directory as my Dockerfile and execute the following command (Replace \u0026quot;name-you-want-to-give-the-image:\u0026lt;tag\u0026gt;\u0026quot; with something like \u0026quot;hugo-image:v1\u0026quot;):\n1docker build -t name-you-want-to-give-the-image:\u0026lt;tag\u0026gt; . #Note the \u0026#34;.\u0026#34; important Now the image will be built and hosted locally on my \u0026quot;build machine\u0026quot;.\nIf anything goes well it should be listed here:\n1$ docker images 2REPOSITORY TAG IMAGE ID CREATED SIZE 3hugo-image v1 d43ee98c766a 10 secs ago 70MB 4nginx stable-alpine 5685937b6bc1 7 days ago 23.5MB 5ubuntu latest 216c552ea5ba 9 days ago 77.8MB Place the image somewhere easily accessible Now that I have my image I need to make sure it is easily accessible for my Kubernetes workers so they can download the image and deploy it. For that I can use the local docker registry pr control node and worker node. Meaning I need to load the image into all workers and control plane nodes. Not so smooth way to to do it. This is the approach for such a method:\n1docker save -o \u0026lt;path for generated tar file\u0026gt; \u0026lt;image name\u0026gt; #needs to be done on the machine you built the image. Example: docker save -o /home/username/hugo-image.v1.tar hugo-image:v1 This will \u0026quot;download\u0026quot; the image from the local docker repository and create tar file. This tar file needs to be copied to all my workers and additional control plane nodes with scp or other methods I find suitable. When that is done I need to upload the tar to each of their local docker repository with the following command:\n1docker -i load /home/username/hugo-image.v1.tar It is ok to know about this process if you are in non-internet environments etc, but even in non-internet environment we can do this with a private registry. And thats where Harbor can come to the rescue link.\nWith Harbor I can have all my images hosted centrally but dont need access to the internet as it is hosted in my own environment.\nI could also use Docker hub. Create an account there, and use it as my repository. I prefer the Harbor registry, as it provides many features. The continuation of this post will use Harbor, the procedure to upload/download images is the same process as with Docker hub but you log in to your own Harbor registry instead of Docker hub.\nUploading my newly created image is done like this:\n1docker login registry.example.com #FQDN to my selfhosted Harbor registry, and the credentials for an account I have created there. 2docker tag hugo-image:v1 https://registry.example.com/hugo/hugo-image:v1 #\u0026#34;/hugo/\u0026#34; name of project in Harbor 3docker push registry.example.com/hugo/hugo-image:v1 #upload it Thats it. Now I can go ahead and create my deployment.yaml definition file in my Kubernetes cluster, point it to my image hosted at my local Harbor registry (e.g registry.example.com/hugo/hugo-image:v1). But let me go through how I created my Hugo deployment in Kubernetes, as I am so close to see my newly image in action ðŸ˜„ (Will it even work).\nDeploy Hugo in Kubernetes To run my Hugo image in Kubernetes the way I wanted I need to define a Deployment (remember I wanted a highly available Hugo deployment, meaning more than one pod and the ability to scale up/down). The first section of my hugo-deployment.yaml definition file looks like this:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: hugo-site 5 namespace: hugo-site 6spec: 7 replicas: 3 8 selector: 9 matchLabels: 10 app: hugo-site 11 tier: web 12 template: 13 metadata: 14 labels: 15 app: hugo-site 16 tier: web 17 spec: 18 containers: 19 - image: registry.example.com/hugo/hugo-image:v1 20 name: hugo-site 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 80 24 name: hugo-site 25 volumeMounts: 26 - name: persistent-storage 27 mountPath: /usr/share/nginx/html/ 28 volumes: 29 - name: persistent-storage 30 persistentVolumeClaim: 31 claimName: hugo-pv-claim In the above I define name of deployment, specify number of pods with the replica specification, labels, point to my image hosted in Harbor and then what the container mountPath and the peristent volume claim. mountPath is inside the container, and the files/folders mounted is read from the content it sees in the persistent volume claim \u0026quot;hugo-pv-claim\u0026quot;. Thats where Hugo will find the content of the Public folder (after the content has been generated).\nI also needed to define a Service so I can reach/expose the containers contents (webpage) on port 80. This is done with this specification:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: hugo-service 5 namespace: hugo-site 6 labels: 7 svc: hugo-service 8spec: 9 selector: 10 app: hugo-site 11 tier: web 12 ports: 13 - port: 80 Can be saved as a separate \u0026quot;service.yaml\u0026quot; file or pasted into one yaml file. But instead of pointing to my workers IP addresses to read the content each time I wanted to expose it with an Ingress by using AKO and Avi LoadBalancer. This is how I done that:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: hugo-ingress 5 namespace: hugo-site 6 labels: 7 app: hugo-ingress 8 annotations: 9 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 10spec: 11 ingressClassName: avi-lb 12 rules: 13 - host: yikes.guzware.net 14 http: 15 paths: 16 - pathType: Prefix 17 path: / 18 backend: 19 service: 20 name: hugo-service 21 port: 22 number: 80 I define my ingressClassName, the hostname for my Ingress controller to listen for requests on and the Service the Ingress should route all the request to yikes.guzware.net to, which is my hugo-service defined earlier. Could also be saved as a separe yaml file. I have chosen to put all three \u0026quot;kinds\u0026quot; in one yaml file. Which then looks like this:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: hugo-site 5 namespace: hugo-site 6spec: 7 replicas: 3 8 selector: 9 matchLabels: 10 app: hugo-site 11 tier: web 12 template: 13 metadata: 14 labels: 15 app: hugo-site 16 tier: web 17 spec: 18 containers: 19 - image: registry.example.com/hugo/hugo-image:v1 20 name: hugo-site 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 80 24 name: hugo-site 25 volumeMounts: 26 - name: persistent-storage 27 mountPath: /usr/share/nginx/html/ 28 volumes: 29 - name: persistent-storage 30 persistentVolumeClaim: 31 claimName: hugo-pv-claim 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: hugo-service 37 namespace: hugo-site 38 labels: 39 svc: hugo-service 40spec: 41 selector: 42 app: hugo-site 43 tier: web 44 ports: 45 - port: 80 46--- 47apiVersion: networking.k8s.io/v1 48kind: Ingress 49metadata: 50 name: hugo-ingress 51 namespace: hugo-site 52 labels: 53 app: hugo-ingress 54 annotations: 55 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 56spec: 57 ingressClassName: avi-lb 58 rules: 59 - host: yikes.guzware.net 60 http: 61 paths: 62 - pathType: Prefix 63 path: / 64 backend: 65 service: 66 name: hugo-service 67 port: 68 number: 80 Now before my Deployment is ready to be applied I need to create the namespace I have defined in the yaml file above: kubectl create ns hugo-site.\nNow when that is done its time to apply my hugo deployment. kubectl apply -f hugo-deployment.yaml\nI want to check the state of the pods:\n1$ kubectl get pod -n hugo-site 2NAME READY STATUS RESTARTS AGE 3hugo-site-7f95b4644c-5gtld 1/1 Running 0 10s 4hugo-site-7f95b4644c-fnrh5 1/1 Running 0 10s 5hugo-site-7f95b4644c-hc4gw 1/1 Running 0 10s Ok, so far so good. What about my deployment:\n1$ kubectl get deployments.apps -n hugo-site 2NAME READY UP-TO-DATE AVAILABLE AGE 3hugo-site 3/3 3 3 35s Great news. Lets check the Service, Ingress and persistent volume claim.\nService:\n1$ kubectl get service -n hugo-site 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3hugo-service ClusterIP 10.99.25.113 \u0026lt;none\u0026gt; 80/TCP 46s Ingress:\n1$ kubectl get ingress -n hugo-site 2NAME CLASS HOSTS ADDRESS PORTS AGE 3hugo-ingress avi-lb yikes.guzware.net x.x.x.x 80 54s PVC:\n1NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 2hugo-pv-claim Bound pvc-b2395264-4500-4d74-8a5c-8d79f9df8d63 10Gi RWO nfs-client 59s Well that looks promising. Will I be able to access my hugo page on yikes.guzware.net ... well yes, otherwise you wouldnt read this.. ðŸ¤£\nCreating and updating content A blog page without content is not so interesting. So just some quick comments on how I create content, and update them.\nI use Typora creating and editing my *.md files. While working with the post (such as now) I run hugo in \u0026quot;server-mode\u0026quot; whith this command: hugo server. If I run this command on of my linux virtual machines through SSH I want to reach the server from my laptop so I add the parameter --bind=ip-of-linux-vm and I can access the page from my laptop on the ip of the linux VM and port 1313. When I am done with the article/post for the day I generated the web-page with the command hugo -D -v. The updated content of my public folder after I have generated the page is mirrored to the NFS path that is used in my PVC shown above and my containers picks up the updated content instantly. Thats how I do, it works and I find it easy to maintain and operate. And, if one of my workers fails, I have more pods still available on the remaining workers. If a pod fails Kubernetes will just take care of that for me as I have declared a set of pods(replicas) that should run. If I run my Kubernetes environment in Tanzu and one of my workers fails, that will also be automatically taken care of.\n","link":"https://blog.andreasm.io/2022/10/12/hugo-in-kubernetes/","section":"post","tags":["hugo","static-content-generator","docker","kubernetes"],"title":"Hugo in Kubernetes"},{"body":"","link":"https://blog.andreasm.io/tags/static-content-generator/","section":"tags","tags":null,"title":"static-content-generator"},{"body":"","link":"https://blog.andreasm.io/tags/pinniped/","section":"tags","tags":null,"title":"pinniped"},{"body":"","link":"https://blog.andreasm.io/categories/pinniped/","section":"categories","tags":null,"title":"Pinniped"},{"body":"How to use Pinniped as the authentication service in Kubernets with OpenLDAP\nGoal: Deploy an authentication service to handle RBAC in Kubernetes Purpose: User/access management in Kubernetes\nPinniped introduction ","link":"https://blog.andreasm.io/2022/10/11/pinniped-authentication-service/","section":"post","tags":["rbac","authentication","pinniped","kubernetes"],"title":"Pinniped Authentication Service"},{"body":"","link":"https://blog.andreasm.io/tags/rbac/","section":"tags","tags":null,"title":"rbac"},{"body":"","link":"https://blog.andreasm.io/categories/rbac/","section":"categories","tags":null,"title":"RBAC"},{"body":"This article will quickly go through how to create wildcard certificates and automatically renew them with Lets Encrypt and Cert-Manager\nCert-Manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\nIt can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI.\nIt will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry. link\nInstall Cert-Manager I prefer the Helm way so lets add the cert-manager helm chart:\n1helm repo add jetstack https://charts.jetstack.io 2helm repo update Then we need to deploy cert-manager. This can be done out-of-the-box with the commands given from the official docs (this also installed the necessary CRDs):\n1helm install \\ 2 cert-manager jetstack/cert-manager \\ 3 --namespace cert-manager \\ 4 --create-namespace \\ 5 --version v1.9.1 \\ 6 --set installCRDs=true Or if you need to customize some settings, as I needed to do, I used this command:\n1helm install -f /path/to/cert-manager.values.yaml cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1 --set installCRDs=true --set \u0026#39;extraArgs={--dns01-recursive-nameservers-only,--dns01-recursive-nameservers=xx.xx.xx.xx:53\\,xx.xx.xx:53}\u0026#39; The above command takes care of the cert-manager installation including the necessary CRDs, but it will also adjust the DNS servers Cert-Manager will use to verify the ownership of my domain.\nDNS01 - Wildcard certificate In this post I will go with wildcard certificate creation. I find it easier to use instead of having a separate cert for everthing I do, as long as they are within the same subdomain. So if I have my services in *.example.com they can use the same certificate. But if I have services in *.int.example.com I can not use the same certificate as LetsEncrypt certificates dont support that. Then you need to create a separate wildcard cert for each subdomain. But Cert-manager will handle that for you very easy.\nThe offiicial Cert-Manager supported DNS01 providers are:\nACMEDNS Akamai AzureDNS CloudFlare Google Route53 DigitalOcean RFC2136 There is also an option to use Webhooks. I did try that as my previous DNS registrar were not on the DNS01 supported list. I did not succeed with using the webhook approach. It could be an issue with the specific webhooks I used or even with my registrar so I decided to migrate over to CloudFlare which is on the supported list, \u0026quot;out of the box\u0026quot;.\nIssuer - CloudFlare and LetsEncrypt The first we need to do is to create a secret for Cert-Manager to use when \u0026quot;interacting\u0026quot; with CloudFlare. I went with API Token. So head over to your CloudFlare control panel and create a token for Cert Manager like this: Here is the permissions: Now use the tokens to create your secret:\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: cloudflare-api-token-secret 5 namespace: cert-manager 6type: Opaque 7stringData: 8 api-token: Apply it kubect apply -f name.of.yaml\nNow create your issuer. LetsEncrypt have two repos, one called staging and one production. Start out with staging until everything works so you dont hit the LetsEncrypt limit. In regards to this I created two issuers, one for staging and one for production. When everything was working and I have verified the certificates etc I deployed the certs using the prod-issuer.\nIssuer-staging:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-staging 5spec: 6 acme: 7 # ACME Server 8 # prod : https://acme-v02.api.letsencrypt.org/directory 9 # staging : https://acme-staging-v02.api.letsencrypt.org/directory 10 server: https://acme-staging-v02.api.letsencrypt.org/directory 11 # ACME Email address 12 email: xxx.xxx@xxx.xxx 13 privateKeySecretRef: 14 name: letsencrypt-key-staging # staging or production 15 solvers: 16 - dns01: 17 cloudflare: 18 apiTokenSecretRef: 19 name: cloudflare-api-token-secret ## created and applied above 20 key: api-token Issuer-production:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-prod 5 namespace: cert-manager 6spec: 7 acme: 8 # ACME Server 9 # prod : https://acme-v02.api.letsencrypt.org/directory 10 # staging : https://acme-staging-v02.api.letsencrypt.org/directory 11 server: https://acme-v02.api.letsencrypt.org/directory 12 # ACME Email address 13 email: xxx.xxx@xxx.xxx 14 privateKeySecretRef: 15 name: letsencrypt-key-prod # staging or production 16 solvers: 17 - dns01: 18 cloudflare: 19 apiTokenSecretRef: 20 name: cloudflare-api-token-secret # created and applied above 21 key: api-token Request certificate Now that the groundwork for cert-manager has been setup, its time to \u0026quot;print\u0026quot; some certificates. Prepare your yamls for both the staging key and production key.\nWildcard-staging:\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: name-tls-test 5 namespace: namespace-you-want-the-cert-in 6spec: 7 secretName: name-tls-staging 8 issuerRef: 9 name: letsencrypt-staging 10 kind: ClusterIssuer 11 duration: 2160h # 90d 12 renewBefore: 720h # 30d before SSL will expire, renew it 13 dnsNames: 14 - \u0026#34;*.example.com\u0026#34; Wildcard-production:\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: name-tls-production 5 namespace: namespace-you-want-the-cert-in 6spec: 7 secretName: name-tls-prod 8 issuerRef: 9 name: letsencrypt-prod 10 kind: ClusterIssuer 11 duration: 2160h # 90d 12 renewBefore: 720h # 30d before SSL will expire, renew it 13 dnsNames: 14 - \u0026#34;*.example.com\u0026#34; Apply the staging request first. Check your certificate status with this command:\n1$ kubectl get certificate -n namespace-you-wrote 2NAME READY SECRET AGE 3name-tls-staging True name-tls-staging 8d Please note that it can take a couple of minutes before the certificate is ready. This applies for production also.\nIf everything went well, delete your staging certificate and apply your production certificate with the production yaml. Thats it. Now Cert-Manager will take care of updating your certificate for your, sit back and enjoy your applications with your always up to date certificates.\nTroubleshooting tips, commands If something should fail there is a couple of commands you can use to figure out whats going on.\n1$ kubectl get issuer 2$ kubectl get clusterissuer 3$ kubectl describe issuer 4$ kubectl describe clusterissuer 5$ kubectl describe certificaterequest 6$ kubectl describe order 7$ kubectl get challenges 8$ kubectl describe challenges For more detailed explanation go here\n","link":"https://blog.andreasm.io/2022/10/11/cert-manager-and-letsencrypt/","section":"post","tags":["cert-manager","certificate","kubernetes"],"title":"Cert Manager and Letsencrypt"},{"body":"","link":"https://blog.andreasm.io/tags/certificate/","section":"tags","tags":null,"title":"certificate"},{"body":"","link":"https://blog.andreasm.io/categories/certificate/","section":"categories","tags":null,"title":"Certificate"},{"body":"","link":"https://blog.andreasm.io/tags/network-policies/","section":"tags","tags":null,"title":"network-policies"},{"body":"What is the NSX Antrea integration Assumptions: I assume that the reader is aware of what Antrea is, and what a CNI is and also what NSX is. If not head over here to read more on Antrea and here to read more on NSX.\nFor many years VMware NSX has help many customer secure their workload by using the NSX Distributed Firewall. As NSX has evolved over the years the different platform it supports has also broadened, from virtual machines, bare metal server, cloud workload and kubernetes pods. NSX has had support for security policies in Kubernetes for a long time also with the CNI NCP Read about NCP here. Recently (almost a year ago since I wrote this article, so not so recent in the world of IT) it also got support for using the Antrea CNI. What does that mean then. Well, it mean we can now \u0026quot;connect\u0026quot; our Antrea CNI enabled clusters to the NSX manager to manage Antrea Native Policies in combination with NSX Distributed Firewall policies. With this integration Antrea will report inventory, such as nodes, pods, services, ip addresses, k8s labels into the NSX manager. This opens up for a clever way of creating and managing security policies from the NSX manager inside the Antrea enabled clusters. Antrea is supported in almost all kinds of Kubernetes platforms, VMware Tanzu solutions, upstream k8s, ARM, public cloud etc so it is very flexible. And with the rich information NSX gets from Antrea we can create more clever security policies by using the native kubernetes labels to form security group membership based on these labels.\nFrom the official VMware NSX documentation:\nBenefits of Integration\nThe integration of Antrea Kubernetes clusters to NSX enables the following capabilities:\nView Antrea Kubernetes cluster resources in the NSX Manager UI (Policy mode). Centrally manage groups and security policies in NSX that reference Antrea Kubernetes clusters and NSX resources (for example, VMs). Distribute the NSX security policies to the Kubernetes clusters for enforcement in the cluster by the Antrea CNI. Extend the NSX network diagnostic and troubleshooting features to the Antrea Kubernetes clusters, such as collecting support bundles, monitoring logs, and performing Traceflow operations. Monitor the runtime state and health status of Antrea Kubernetes cluster components and Antrea Agents in the NSX Manager UI. Antrea NSX integration architecture To understand a bit more how this works, we need to go through a couple of components that is in involved to get this integration in place.\nThe official documentation has this part covered very well and I will just quote the information here. Or go directly to the source here\nThe integration architecture explains the information exchanged between a Kubernetes cluster that uses Antrea CNI and the NSX Manager Appliance, which is deployed in NSX.\nThis documentation does not explain the functions of Antrea components in a Kubernetes (K8s) cluster. To understand the Antrea architecture and the functions of Antrea components in a Kubernetes cluster, see the Antrea documentation portal at https://antrea.io/docs.\nThis main objective of this documentation is to understand the functions of the Antrea NSX Adapter that integrates a Kubernetes cluster with Antrea CNI to the NSX Manager Appliance.\nAntrea NSX Adapter\nThis component runs as a pod on one of the Kubernetes Control Plane nodes. Antrea NSX Adapter consists of the following two subcomponents:\nManagement Plane Adapter (MP Adapter) Central Control Plane Adapter (CCP Adapter) Management Plane Adapter communicates with the NSX Management Plane (Policy), Kubernetes API Server, and Antrea Controller. Central Control Plane Adapter communicates with the NSX Central Control Plane (CCP) and Kubernetes API Server.\nFunctions of the Management Plane Adapter\nWatches the Kubernetes resource inventory from Kubernetes API and reports the inventory to NSX Manager. Resource inventory of an Antrea Kubernetes cluster includes resources, such as Pods, Ingress, Services, Network Policies, Namespaces, and Nodes. Responds to the policy statistics query from NSX Manager. It receives the statistics from the Antrea Controller API or the statistics that are exported by the Antrea Agent on each K8s worker node, and reports the statistics to NSX Manager. Receives troubleshooting operation requests from NSX Manager, sends the requests to Antrea Controller API server, collects the results, and returns the information to NSX Manager. Examples of troubleshooting operations include Traceflow requests, Support Bundle collection requests, log collection requests. Watches the runtime state and health status of an Antrea Kubernetes cluster from the Antrea Monitoring CustomResourceDefinition (CRD) objects and reports the status to NSX Manager. The status is reported on a per cluster basis. For example, the health status of the following components is reported to the NSX Manager: Management Plane Adapter Central Control Plane Adapter Antrea Controller Antrea Agents Functions of the Central Control Plane Adapter\nReceives the Distributed Firewall (DFW) rules and groups from NSX Central Control Plane, translates them to Antrea policies, and creates Antrea policy CRDs in K8s API. Watches the policy realization status from both K8s network polices and native Antrea policy CRDs and reports the status to NSX Central Control Plane. Stateless Nature of the Central Control Plane Adapter\nThe Central Control Plane Adapter is stateless. Each time the adapter restarts or reconnects to K8s API or NSX Manager, it always synchronizes the state with K8s API and NSX Central Control Plane. Resynchronization of the state ensures the following:\nThe latest Antrea policies are always pushed to K8s API as native Antrea policy CRDs. The stale policy CRDs are removed if the corresponding security policies are deleted in NSX. This post will cover the installation steps of the Antrea/NSX integration.\nManaging Antrea Native Policies from the NSX manager For more information how to manage Antrea Polcies from the NSX manager I have created this post and this post\n","link":"https://blog.andreasm.io/2022/10/11/nsx-antrea-integration/","section":"post","tags":["security","network-policies","antrea","nsx"],"title":"NSX Antrea Integration"},{"body":"This week I was fortunate to get hold of a VMC on AWS environment and wanted to test out the possibility of managing my K8s security policies from my on-prem NSX manager by utilizing the integration of Antrea in NSX. I haven't covered that specific integration part in a blog yet, but in short: by using Antrea as your CNI and you are running NSX-T 3.2 you can manage all your K8s policies from the NSX manager GUI. Thats a big thing. Manage your k8s policies from the same place where you manage all your other critical security policies. Your K8s clusters does not have to be in the same datacenter as your NSX manager. You can utilize VMC on AWS as your scale-out, prod/test/dev platform and still manage your K8s security policies centrally from the same NSX manager.\nIn this post I will go through how this is done and how it works.\nVMC on AWS VMC on AWS comes with NSX, but it is not yet on the version that has the NSX-T integration. So what I wanted to do was to use the VMC NSX manager to cover all the vm-level microsegmentation and let my on-prem NSX manager handle the Antrea security policies. To illustrate want I want to achieve:\nVMC on AWS to on-prem connectivity VMC on AWS supports a variety of connectivity options to your on-prem environment. I have gone with IPSec VPN. Where I configure IPsec on the VMC NSX manager to negotiate with my on-prem firewall to terminate the VPN connection. In VMC I have two networks: Management and Workload. I configured both subnets in my IPsec config as I wanted the flexibility to reach both subnets from my on-prem environment. To get the integration working I had to make sure that the subnet on my on-prem NSX manager resided on also was configured. So the IPsec configurations were done accordingly to support that: Two subnets from VMC and one from on-prem (where my NSX managers resides).\nIPsec config from my VMC NSX manager\nIPsec config from my on-prem firewall\nWhen IPsec tunnel was up I logged on to the VMC NSX manager and configured the \u0026quot;North/South\u0026quot; security policies allowing my Workload segment to any. I created a NSX Security Group (\u0026quot;VMs\u0026quot;) with membership criteria in place to grab my Workload Segment VMs (workload). This was just to make it convenient for myself during the test. We can of course (and should) be more granular in making these policies. But we also have the NSX Distributed Firewall which I will come to later.\nNorth/South policies\nNow I had the necessary connectivity and security policies in place for me to log on to the VMC vCenter from my on-prem management jumpbox and deploy my k8s worker nodes.\nVMC on AWS K8s worker nodes In VMC vCenter I deployed three Ubuntu worker nodes, and configured them to be one master worker and two worker nodes by following my previous blog post covering these steps:\nhttp://yikes.guzware.net/2020/10/08/ako-with-antrea-on-native-k8s-cluster/#Deploy_Kubernetes_on_Ubuntu_2004\nThree freshly deployed VMs in VMC to form my k8s cluster\n1NAME STATUS ROLES AGE VERSION 2vmc-k8s-master-01 Ready control-plane,master 2d22h v1.21.8 3vmc-k8s-worker-01 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 4vmc-k8s-worker-02 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 After the cluster was up I needed to install Antrea as my CNI.\nDownload the Antrea release from here: https://customerconnect.vmware.com/downloads/info/slug/networking_security/vmware_antrea/1_0\nAfter it has been downloaded, unpack it and upload the image to your k8s master and worker nodes by issuing the docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz\nThen apply it by using the manifest antrea-advanced-v1.2.3+vmware.3.yml found under the /antrea-advanced-1.2.3+vmware.3.19009828/manifests folder. Like this: kubectl apply -f antrea-advanced-v1.2.3+vmware.3.yml and Antrea should spin right up and you have a fully working K8s cluster:\n1NAME READY STATUS RESTARTS AGE 2antrea-agent-2pdcr 2/2 Running 0 2d22h 3antrea-agent-6glpz 2/2 Running 0 2d22h 4antrea-agent-8zzc4 2/2 Running 0 2d22h 5antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d20h 6coredns-558bd4d5db-2j7jf 1/1 Running 0 2d22h 7coredns-558bd4d5db-kd2db 1/1 Running 0 2d22h 8etcd-vmc-k8s-master-01 1/1 Running 0 2d22h 9kube-apiserver-vmc-k8s-master-01 1/1 Running 0 2d22h 10kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 2d22h 11kube-proxy-rvzs6 1/1 Running 0 2d22h 12kube-proxy-tnkxv 1/1 Running 0 2d22h 13kube-proxy-xv77f 1/1 Running 0 2d22h 14kube-scheduler-vmc-k8s-master-01 1/1 Running 0 2d22h Antrea NSX integration Next up is to configure the Antrea NSX integration. This is done by following this guide:\nhttps://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-DFD8033B-22E2-4D7A-BD58-F68814ECDEB1.html\nIts very well described and easy to follow. So instead of me rewriting it here I just point to it. But in general it makes up of a couple of steps needed.\n1. Download the necessary Antrea Interworking parts, which is included in the Antrea-Advanced zip above\n2. Create a certificate to use for the Principal ID User in your on-prem NSX manager.\n3. Import image to your master and workers (interworking-debian-0.2.0.tar)\n4. Edit the bootstrap-config.yaml (https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-1AC65601-8B35-442D-8613-D3C49F37D1CC.html)\n5. Apply the bootstrap-config-yaml and you should end up with this result in your k8s cluster and in your on-prem NSX manager:\n1vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 17 2d22h My VMC k8s cluster: vmc-ant-cluster (in addition to my other on-prem k8s-cluster)\nInventory view from my on-prem NSX manager\nOne can immediately see useful information in the on-prem NSX manager about the \u0026quot;remote\u0026quot; VMC K8s cluster such as Nodes, Pods and Services in the Inventory view. If I click on the respective numbers I can dive into more useful information. By clicking on \u0026quot;Pods\u0026quot;\nPod state, ips, nodes they are residing on etc\nEven in this view I can click on the labels links to get the lables NSX gets from kubernetes and Antrea:\nBy clicking on Services I get all the services running in my VMC k8s cluster\nAnd the service labels:\nAll this information is very useful, as they can be used to create the security groups in NSX and use those groups in security policies.\nClicking on the Nodes I also get very useful information:\nVMC on AWS NSX distributed firewall As I mentioned earlier VMC on AWS also comes with NSX and one should utilize this to segment/create security polices on your worker nodes there. I have just created some simple rules allowing the \u0026quot;basic\u0026quot; needs for my workers, and then created some specific rules for what they are allowed to where all unspecified traffic is blocked by a default block rule.\nBare in mind that this is a demo environment and not representing any production environment as such, but some rules are in place to showcase that I am utilizing the NSX distributed firewall in VMC to microsegment my workload there.\nThe \u0026quot;basic\u0026quot; needs rules are the following: Under \u0026quot;Infrastructure\u0026quot; I am allowing my master and worker nodes to \u0026quot;consume\u0026quot; NTP and DNS. In this environment I do not have any local DNS and NTP servers as they are all public. DNS I am using Google's public DNS servers 8.8.8.8 and 8.8.4.4 and NTP the workers are using \u0026quot;time.ubuntu.com\u0026quot;. I have created a security group consisting of the known DNS servers ip, as I know what they are. But the NTP server's IP I do not know so I have created a security group with members only consisting of RFC1918 subnets and created a negated policy indicating that they are only allowed to reach NTP servers if they not reside on any RFC1918 subnet.\nNTP and DNS allow rules under Infrastructure\nUnder \u0026quot;Environment\u0026quot; I have created a Jump to Application policy that matches my K8s master/worker nodes\nJump to Application\nUnder \u0026quot;Application\u0026quot; I have a rule that is allowing internet access (could be done on the North/South Gateway Firewall section also) by indication that HTTP/HTTPS is allowed as long as it is not any RFC1918 subnet.\nAllow \u0026quot;internet access\u0026quot;\nFurther under Application I am specifying a bit more granular rules for what the k8s cluster is allowed in/out. Again, this is just some simple rules restricting the k8s cluster to not allow any-any by utilizing the NSX DFW already in VMC on AWS. One can and should be more granular, but its to give you an idea.\nIn the K8s-Backbone security policy section below I am allowing HTTP in to the k8s cluster as I am planning to run an k8s application there that uses HTTP where I allow a specific IP subnet/range as source and my loadbalancer IP range as the destination.\nThen I allow SSH to the master/workers for management purposes. Then I am creating some specific rules allowing the necessary ports needed for the Antrea control-plane to communicate with my on-prem NSX manager, which are: TCP 443, 1234 and 1235. Then I create an \u0026quot;Intra\u0026quot; rule allowing the master/workers to talk freely between each other. This should and can also be much more tightened down. When those rules are done processed they will hit a default block rule.\nVMC k8s cluster policy\nDefault drop\nAntrea policies from on-prem NSX manager Now when the \u0026quot;backbone\u0026quot; is ready configured and deployed its time to spin up some applications in my \u0026quot;VMC K8s cluster\u0026quot; and apply some Antrea security policies. Its now time for the magic to begin ;-)\nIn my on-prem environment I already have a couple of Antrea enabled K8s clusters running. On them a couple of demo applications are already running and protected by Antrea security policies created from my on-prem NSX manager. I like to use an application called Yelb (which I have used in previous blog posts here). This application consist of 4 pods. All pods doing their separate thing for the application to work. I have a frontend pod which is hosting the web-page for the application, I have an application pod, db pod and a cache pod. The necessary connectivity between looks like this:\nYelb pods connectivity\nTo make security policy creation easy I make use of all the information I get from Antrea and Kubernetes in form of \u0026quot;Labels\u0026quot;. These labels are translated into tags in NSX. Which makes it very easy to use, and for \u0026quot;non\u0026quot; developers to use as \u0026quot;human-readable\u0026quot; elements instead of IP adresses, pods unique names etc. In this example I want to microsegment the pods that makes up the application \u0026quot;Yelb\u0026quot;.\nCreating NSX Security Groups for Antrea Security Policy Before I create the actual Antrea Security Policies I will create a couple of security groups based on the tags I use in K8s for the application Yelb. The Yelb manifest looks like this:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: redis-server 5 labels: 6 app: redis-server 7 tier: cache 8 namespace: yelb 9spec: 10 type: ClusterIP 11 ports: 12 - port: 6379 13 selector: 14 app: redis-server 15 tier: cache 16--- 17apiVersion: v1 18kind: Service 19metadata: 20 name: yelb-db 21 labels: 22 app: yelb-db 23 tier: backenddb 24 namespace: yelb 25spec: 26 type: ClusterIP 27 ports: 28 - port: 5432 29 selector: 30 app: yelb-db 31 tier: backenddb 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: yelb-appserver 37 labels: 38 app: yelb-appserver 39 tier: middletier 40 namespace: yelb 41spec: 42 type: ClusterIP 43 ports: 44 - port: 4567 45 selector: 46 app: yelb-appserver 47 tier: middletier 48--- 49apiVersion: v1 50kind: Service 51metadata: 52 name: yelb-ui 53 labels: 54 app: yelb-ui 55 tier: frontend 56 namespace: yelb 57spec: 58 type: LoadBalancer 59 ports: 60 - port: 80 61 protocol: TCP 62 targetPort: 80 63 selector: 64 app: yelb-ui 65 tier: frontend 66--- 67apiVersion: v1 68kind: ReplicationController 69metadata: 70 name: yelb-ui 71 namespace: yelb 72spec: 73 replicas: 1 74 template: 75 metadata: 76 labels: 77 app: yelb-ui 78 tier: frontend 79 spec: 80 containers: 81 - name: yelb-ui 82 image: mreferre/yelb-ui:0.3 83 ports: 84 - containerPort: 80 85--- 86apiVersion: apps/v1 87kind: Deployment 88metadata: 89 name: redis-server 90 namespace: yelb 91spec: 92 selector: 93 matchLabels: 94 app: redis-server 95 replicas: 1 96 template: 97 metadata: 98 labels: 99 app: redis-server 100 tier: cache 101 spec: 102 containers: 103 - name: redis-server 104 image: redis:4.0.2 105 ports: 106 - containerPort: 6379 107--- 108apiVersion: apps/v1 109kind: Deployment 110metadata: 111 name: yelb-db 112 namespace: yelb 113spec: 114 selector: 115 matchLabels: 116 app: yelb-db 117 replicas: 1 118 template: 119 metadata: 120 labels: 121 app: yelb-db 122 tier: backenddb 123 spec: 124 containers: 125 - name: yelb-db 126 image: mreferre/yelb-db:0.3 127 ports: 128 - containerPort: 5432 129--- 130apiVersion: apps/v1 131kind: Deployment 132metadata: 133 name: yelb-appserver 134 namespace: yelb 135spec: 136 selector: 137 matchLabels: 138 app: yelb-appserver 139 replicas: 1 140 template: 141 metadata: 142 labels: 143 app: yelb-appserver 144 tier: middletier 145 spec: 146 containers: 147 - name: yelb-appserver 148 image: mreferre/yelb-appserver:0.3 149 ports: 150 - containerPort: 4567 As we can see there is a couple of labels that distinguish the different components in the application which I can map to the application topology above. I only want to allow the frontend to talk to the \u0026quot;app-server\u0026quot;, and the app-server to the \u0026quot;db-server\u0026quot; and \u0026quot;cache-server\u0026quot;. And only on the needed ports. All else should be dropped. On my on-prem NSX manager I have created these groups for the already running on-prem Yelb application. I have created four 5 groups for the Yelb application in total. One group for the frontend (\u0026quot;ui-server\u0026quot;), one for the middletier (app server), one for the backend-db (\u0026quot;db-server\u0026quot;), one for the cache-tier (\u0026quot;cache server\u0026quot;) and one last for all the pods in this application:\nSecurity groups filtering out the Yelb pods\nThe membership criteria inside those groups are made up like this, where I am using the labels in my Yelb manifest (these labels are autopopulated so you dont have to guess). Tag equals label frontend and scope equals label dis:k8s:tier:\nGroup definition for the frontend\nThe same goes for the other groups just using their respective labels. The members should then look like this:\nOnly the frontend pod\nThen I have created a security group that selects all pods in my namespace Yelb by using the label Yelb like this:\nWhich then selects all my pods in the namespace Yelb:\nNow I have my security groups and can go on and create my security policies.\nAntrea security policies from NSX manager Head over to Security, Distributed Firewall section in the on-prem NSX manager to start creating security policies based on your security groups. These are the rules I have created for my application Yelb:\nAntrea Security Policies from the NSX manager\nFirst rule allows traffic from my Avi SE's that are being used to create the service loadbalancer for my application Yelb to the Yelb frontend on HTTP only. Notice that the source part here is in the \u0026quot;Applied to field\u0026quot; (goes for all rules in this example). Thee second rule allows traffic from the frontend to the middletier (\u0026quot;app-server\u0026quot;) on port 4567 only. The third rule allows traffic from middletier to backend-db (\u0026quot;db-server\u0026quot; on port 5432 only. The fourth rule allows traffic from middletier to cache (redis cache) on port 6379 only. All rules according to the topology maps above. Then the last rule is where I am using the namespace selection to select all pods in the namespace Yelb to drop all else not specified above.\nTo verify this I can use the Traceflow feature in Antrea from the NSX manager like this:\n(Head over to Plan \u0026amp; Troubleshoot, Traffic Analysis, Traceflow in your NSX manager)\nChoose Antrea Traceflow, choose the Antrea cluster where your application resides, then select TCP under Protocol type, type in Destination Port (4567) and choose where your pods are from the source and destination. In the screenshot above I want to verify that the needed ports are allowed between Frontend and middletier (application pod).\nClick trace:\nWell that worked, now if I change to port to something else like 4568, am I then still allowed to do that?\nNo, I am not. That is because I have my drop rule in place remember:\nI could go on and test all pod to pod connectivity (I have), but you can trust me their are doing their job. Just to save some screenshots. So that is it, I have microsegmented my Yelb application. But what if I want to scale out this application to my VMC environment. I want to achieve the same thing there. Why not, all our groundwork has already been done so lets head out and spin up the same applicaion on our VMC K8s cluster. Whats going to happen in my on-prem NSX manager. This is cool!\nAntrea security policies in my VMC k8s cluster managed by my on-prem NSX manager Before I deploy my Yelb application in my VMC K8s cluster I want to refresh the memory by showing what my NSX manager knows about the VMC K8s cluster inventory. Lets take a look again. Head over to Inventory in my on-prem NSX manager and take a look at my VMC-ANT-CLUSTER:\n3 nodes you say, and 18 pods you say... Are any of them my Yelb pods?\nNo Yelb pods here...\nNo, there are no yelb pods here. Lets make that a reality. Nothing reported in k8s either:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h\nSpin up the Yelb application in my VMC k8s cluster by using the same manifest:\nkubectl apply -f yelb-lb.yaml\nThe result in my k8s cluster:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h yelb redis-server-74556bbcb7-fk85b 1/1 Running 0 4s yelb yelb-appserver-6b6dbbddc9-9nkd4 1/1 Running 0 4s yelb yelb-db-5444d69cd8-dcfcp 1/1 Running 0 4s yelb yelb-ui-f74hn 1/1 Running 0 4s\nWhat does my on-prem NSX manager reports?\nHmm 22 pods\nAnd a lot of yelb pods\nInstantly my NSX manager shows them in my inventory that they are up.\nNow, are they being protected by any Antrea security policies? Lets us do the same test as above by using Antrea Traceflow from the on-prem NSX manager with same ports as above (frontend to app 4567 and 4568).\nTraceflow from my on-prem NSX manager in the VMC k8s cluster\nNotice the selection I have done, everything is the VMC k8s cluster.\nNeed port is allowed\nThat is allowed, what about 4568 (which is not needed):\nAlso allowed\nThat is also allowed. I cant have that. How can I make use of my already created policy for this application as easy as possible instead of creating all the rules all over?\nWhell, lets test that. Head over to Security, Distributed firewall in my on-prem NSX manager.\nNotice the Applied to field here:\nWhat happens if I click on it?\nIt only shows me my local Antrea k8s cluster. That is also visible if I list the members in the groups being used in the rules:\nOne pod from my local k8s cluster\nWhat if I add the VMC Antrea cluster?\nCick on the pencil and select your remote VMC K8s cluster:\nApply and Publish:\nNow lets have a look inside our groups being used by our security policy:\nMore pods\nThe Yelb namespace group:\nInstantly my security groups are being updated with more members!!!\nRemember how I created the membership criteria of the groups? Labels from my manifest, and labels for the namespace? Antrea is cluster aware, and dont have to specify a specific namespace to select labels from one specific namespace, it can select from all namespaces as long as the label matches. This is really cool.\nNow what about my security policies in my VMC k8s cluster? Is it enforcing anything?\nLets check, doing a traceflow again. Now only on a disallowed port 4568:\nResult:\nDropped\nThe Security policy is in place, enforcing what it is told to do. The only thing I did in my local NSX manager was to add the VMC Antrea cluster in my Security Applied to section\n","link":"https://blog.andreasm.io/2022/03/13/managing-your-antrea-k8s-clusters-running-in-vmc-from-your-on-prem-nsx-manager/","section":"post","tags":["antrea","nsx","vmconaws","security"],"title":"Managing your Antrea K8s clusters running in VMC from your on-prem NSX Manager"},{"body":"","link":"https://blog.andreasm.io/tags/vmconaws/","section":"tags","tags":null,"title":"vmconaws"},{"body":"","link":"https://blog.andreasm.io/categories/vmware-cloud/","section":"categories","tags":null,"title":"VMware-Cloud"},{"body":"","link":"https://blog.andreasm.io/tags/nsx-application-platform/","section":"tags","tags":null,"title":"nsx-application-platform"},{"body":"VMware NSX 3.2 is out and packed with new features. One of them is the NSX Application Platform which runs on Kubernetes to provide the NSX ATP (Advanced Threat Protection) functionality such as NSX Intelligence (covered in a previous post), NSX Network Detection and Response (NDR) and NSX Malware. This post will go through how to spin up a K8s cluster for this specific scenario covering the pre-reqs from start to finish. After that the features itself will be covered in separate posts. Through this post NSX Application Platform will be abbreviated into NAPP.\nGetting started To get started with NAPP its important that one has read the prerequisites needed to be in place on the K8s cluster that is hosting NAPP. In short NAPP is currently validated to run on upstream K8s version 1.7 all the way up to version 1.21, VMware Tanzu (TKC) versions 1.17.17 to 1.21.2. In addtion to a K8s cluster itself NAPP also needs a Registry supporting images/helm charts. In this walkthrough Harbor will be used. I will also go with an upstream K8s cluster version 1.21.8 running on Ubuntu nodes.\nSources being used to cover this post is mainly from VMware's official documentation. So I list them here as an easy way to reference as they are providing the necessary and important information on how, requirements and all the steps detailed to get NAPP up and running. The purpose of this post is to just go through the steps as a more step by step guide. If more information is needed, head over to our official documentation. Below are the links for the software/tools I have used in this post:\nDeploying and Managing the VMware NSX Application Platform\nHarbor registry\nVMware vSphere Container Storage Plugin\nMetalLBÂ LetsEncrypt\nPrerequisites/Context First some context. In this post all K8s nodes are running as virtual machines on VMware vSphere. NSX-T is responsible for the underlaying network connectivity for the nodes and the persistent storage volumes in my K8s cluster is VMFS exposed through the vSphere Container Storage Plugin (CSI). The NSX manager cluster consists of 3 NSX managers and a cluster VIP address.\nThe vSphere environment consists of two ESXi hosts with shared storage from a FC SAN and vCenter managing the ESXi hosts.\nA quick summary of what is needed, tools/software and what I have used in my setup:\nA upstream kubernetes cluster running any of the supported versions stated in the official documentation. I am using 1.21.8 A working CNI, I am using Antrea. A registry supporting images/helm charts using a signed trusted certificate (no support for self-signed certificates). I am using Harbor and LetsEncrypt certificates. A load balancer to expose the NAPP endpoint with a static ip. I am using MetalLB Persistent storage in K8s I am using the vSphere Container Storage Plugin (if you are running the nodes on vSphere). NSX 3.2 of course Required resources for the NAPP form factor you want to go with. The required resources for the K8s cluster supporting NAPP is outlined below (from the official docs page): This post will cover the Advanced form factor where I went with the following node configuration:\n1 master worker/control plane node with 4 vCPUs, 8GB RAM and 200GB local disk (ephemeral storage). For the worker nodes: 3 worker nodes with 16vCPUs, 64GB RAM and 200GB (ephemeral storage) for the persistent storage (the 1TB disk requirement) I went with vSphere CSI to expose a VMFS datastore from vSphere for the persistent volume requirement.\nThe next chapters will go trough the installation of Harbor (registry) and the configuration done there, then the K8s configuration specifically the CSI and MetalLB part as I am following the generic K8s installation covered earlier here: Deploy Kubernetes on Ubuntu 20.04 .\nLets get to it.\nHarbor (registry requirement) Harbor can be run as a pod in Kubernetes or a pod in Docker. I went with the Docker approach and spun up a VM for this sole purpose.\nThe VM is Ubuntu 20.4 with 4vCPU, 8GB RAM and 200GB disk.\nAfter Ubuntu is installed I installed the necessary dependencies to deploy Harbor in Docker by following the Harbor docs here: Harbor Getting Started\nI find it better to just link the steps below instead of copy/paste too much as the steps are very well documented and could also change over time.\nDocker Engine (latest Stable): docker.com/ubuntu Docker Compose (latest Stable): docker.com/linux Prepare the signed cert (if its not already done) NB! The certificate needs to contain the full chain otherwise the Docker client will not accept it. Download the Harbor installer: Harbor installer (I went with 2.4.1) Extract the online installer: tar -zxvf harbor-online-installer-v2.4.1.tgz Edit the harbor.yaml: Go to the folder harbor (result of the extract above) cp the harbor.yml.tmpl to harbor.yml and use your favourite editor and change the following (snippet from the harbor.yml file): Run the installer: sudo ./install.sh --with-chartmuseum The --with-chartmuseum flag is important (The installer is in the same folder as above and if it is not executable make it executable with chmod +x install.sh check/validate whether you are able to log in to your Harbor registry with the following command: sudo docker login FQDNofHarbor --username admin --password (password defined in harbor.yml). It should not complain about the certificate if the certificate is valid. Log in to the Harbor UI by opening a browser and enter your FQDN of your harbor with the use of admin/password. Create a project: I made a public project\nDownload the NAPP images from your my.vmware.com page: VMware-NSX-Application-Platform-3.2.0.0.0.19067744.tgz upload it to your Harbor VM or to another endpoint where you have a Docker client. \u0026quot;Untar\u0026quot; the tgz file Find and edit the upload_artifacts_to_private_harbor.sh by changing the following: DOCKER_REPO=harborFQDN/napp (after the / is the project name you created) DOCKER_USERNAME=admin DOCKER_PASSWORD=Password-Defined-In-Harbor.yml\nSave and run it sudo ./upload_artifacts_to_private_harbor.sh (same here make it executable with chmod +x if its not executable. This takes a long time, so sit back and enjoy or go do something useful like taking a 3km run in about 15 minutes. The end result shoul look something like this in the Harbor GUI: Thats it for Harbor, next up the K8s cluster\nThe K8s cluster where NAPP is deployed As stated in the official documentation for NAPP, K8s needs to be a specific version, it can be upstream K8s, VMware Tanzu (TKC) or other K8s managed platforms such as OpenShift. But the currently validated platforms are at the moment the above two mentioned platforms: upstream K8s and TKC.\nAs I wrote initially I will go with upstream K8s for this. To get this going I prepared 4 VMs where I dedicate one master worker/control-plane node with the above given specifications and 3 worker nodes with the above given specifications. I follow my previous guide for preparing the Ubuntu os and installing K8s here: K8s on Ubuntu so I will not cover this here but just continue from this with the specifics I did to get the CSI driver up, Antrea CNI and MetalLB. First out is the Antrea CNI.\nAssuming the K8s cluster is partially up, due to no CNI is installed. I download the downstream version of Antrea (one can also use the upstream version from the Antrea github repository) from my.vmware.com. One of the reason I want to use the downstream version from my.vmware.com version is that I want to integrate it to my NSX management plane (more on that in an separate post covering NSX-T with Antrea integration). Download the VMware Container Networking with Antrea (Advanced) from your my.vmware.page to your master worker. Unzip the zip file. Copy the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz to all your worker nodes (with scp for example). Found under the folder antrea-advanced-1.2.3+vmware.3.19009828/images (result of the extract previously). Load the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz image on all nodes, including the master worker, with the command sudo docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz Apply the antrea-advanced-v1.2.3+vmware.3.yml found under the folder antrea-advanced-1.2.3+vmware.3.19009828/manifests from your master worker. A second or two later you should have a fully working Antrea CNI in your K8s cluster. Notice that your CoreDNS pods decided to go into a running state. Thats it for the Antrea CNI. Next up is MetalLB When the CNI is up, its time for MetalLB. Installation of MetalLB is easy and well explained here: Install MetalLB. Next is the CSI for persistent volume. For step by step config of vSphere Container Storage Plugin head over the the following link (Getting Started with VMware vSphere Container Storage Plug-in section) and follow the instructions there which are very well described. That is, if you are running the VMs on vSphere and want to utilize VMFS as the underlying storage for your persistent volumes. Works great and is fairly easy to deploy. I might come back later and write up a short summary on this one. When you are done with the step above and have your Storage Class defined its over to NSX for deployment of NAPP - Yes! Deploy NAPP - NSX Application Platform Its finally time to head over to the NSX manager and start deployment of NAPP\nTo get this show started, I again must refer to the prerequisites page for NAPP. I will paste below and make some comments:\nFirst requirement: NSX version 3.2 (First release with NAPP). Pr now 3.2 is the one and only NSX-T release that supports NAPP. License ---------- Certificate: The first statement with CA-signed certificates is ok to follow. But the second one could be something that needs to be checked. This is valid if you are using NSX-T Self-Signed certificates. Image that you started out with one NSX manager, enabled the VIP cluster address, then it may well be that this cluster IP gets the certificate of your first NSX manager. So its important to verify that all three NSX managers are using their own unique certificate and the VIP uses its own unique certificate. If not, one must update the certificates accordingly. In my environment I had unique certificates on all NSX manager nodes, but my VIP was using NSX manager 1's certificate. So I had to update the certificate on the VIP. I generated a new certificate from the NSX manager here: And followed the instructions here to replace the VIP certificate: Replace Certificates The currently validated platforms NAPP is supported to run on, been through that earlier. Harbor is covered previously with a dedicated section. Harbor is not a strict requirement though. One can BYO registry if it supports image/helm charts When using upstream K8s, the config does not have a default token expiry. If using TKC one must generate a long-lived token so NAPP wont log out from the K8s cluster. Described in the docs Service Name FQDN is a dns record that is mapped to the IP the the endpoint service gets when deployed. Thats were I use MetalLB for this purpose. Just to initiate a type LoadBalancer. If there is a firewall between your NSX manager and the NAPP K8s cluster, one must do firewall openings accordingly. Time sync is important here also. The K8s cluster must be synced to the NSX Manager. Going through the deployment of NAPP from the NSX manager GUI Log in to the NSX manager GUI. Head over to System and find the new section on the left side called: NSX Application Platform\nFrom there the first thing thats needed to populate is the urls to your Harbor registry (or other BYO registry). The urls goes like this:\nHelm Repository: https://harbor.guzware.net/chartrepo/napp -\u0026gt; FQDN for the Harbor instance, then chartrepo and then the name of the projecy you created in Harbor Docker Registry: harbor.guzware.net/napp/clustering without HTTPS, almost the same url just swap places on napp (project in Harbor) and clustering Click save url and it should validate ok and present you with this and the option to continue in the bottom right corner: Next is the Form factor and kubeconfig:\nThe first thing is to upload your K8s kubeconfig file. Select upload and it will validate. Should you get a warning the the K8s version is newer than the kubectl client onboard the NSX manager upload a newer client from my.vmware.com The Cluster Type is only Standard for now. Storage Class is what you defined in K8s with the CSI (Persistent Volumes) Service Name (FQDN) registered in DNS Form Factor - Here you will have three choices: Standard, Advanced and Evaluation: I have gone with the Advanced form factor. The result should look like this: Now you should be able to click next to do the Precheck Platform for validation:\nFinally Review \u0026amp; Update\nInstallation starts\nAnd after some eager waiting the end result should look like this:\nA brief summary from the K8s cluster:\nA bunch of pods - nice!\nThats it - next time I will continue with the features NAPP brings to the table: NSX Intelligence, Network Detection and Response and NSX Malware Prevention\n","link":"https://blog.andreasm.io/2022/01/18/vmware-nsx-application-platform/","section":"post","tags":["nsx","nsx-application-platform","security"],"title":"VMware NSX Application Platform"},{"body":"","link":"https://blog.andreasm.io/tags/ids/","section":"tags","tags":null,"title":"ids"},{"body":"","link":"https://blog.andreasm.io/tags/ips/","section":"tags","tags":null,"title":"ips"},{"body":"This page will explain my lab environment, which is used in all the examples, tutorials in this blog.\nLab overview/connectivity - physical, logical and hybrid It is nice to have an overview of how the underlying hardware looks like and when reading my different articles. So I decided to create some diagrams to illustrate this. Which hopefully will help understanding my blog posts further. First out is the physical components (which is relevant for the posts in this blog).\nPhysical hardware My lab consist of two ESXi hosts, one ToR switch (enterprise dc switch with many capabilities) and a fibrechannel SAN (storage).\nLogical overview To make possible all the things I want to do in my lab I am running most of the networking and other features virtually on top of my physical hardware. This includes NSX-T, virtual routers (VM based) in additition to the router functionality in NSX-T and VM based \u0026quot;perimeter\u0026quot; firewall which is based on PfSense. Below is an logical overview of the network topology in my lab.\n","link":"https://blog.andreasm.io/2021/10/19/my-lab/","section":"post","tags":null,"title":"My LAB"},{"body":"","link":"https://blog.andreasm.io/categories/netowkring/","section":"categories","tags":null,"title":"Netowkring"},{"body":"This post will go through the IDS/IPS built-in feature of the NSX distributed firewall.\nAbbreviations used in this article:\nIDS = Intrusion Detection System IPS = Intrusion Prevention System Introduction to VMware NSX distributed IDS \u0026amp; IPS Before we dive into how to configure and use the distributed IDS and IPS feature in NSX let me just go through the basics where I compare the traditional approach with IDS/IPS and the NSX distributed IDS/IPS. This article is a continuation on the article Microsegmentation with VMware NSX\u0026quot; where I talk about east/west and north/south traffic pattern and being in context with the workload its supposed to protect. Where being in context is a key thing, especially when it comes to security policies and IDS/IPS. Know what you are protecting, make the inspection as relevant as possible, inspection done optimal (reduce false positives, maintain performance) and at the right place.\nThe traditional way of using IDS/IPS In a more traditional infrastructure we have the perimeter firewall that is responsible for the \u0026quot;environment\u0026quot; policies, enforcing policies between the environments and allowing/blocking different types of the services from each environment to communicate. In such an scenario it is often also the same perimeter firewall that is enabled with IDS/IPS. In a datacenter full of virtualized workload this leads to hairpinning the traffic to a centralized appliance for inspection with the consequence of reducing performance, a lot of unnecessary traffic is sent out to the physical infrastructure to reach the perimeter firewall and sent back again. The appliance is not in context of the workload its analyzing traffic from/to so its hard to be very specific enough when it comes to the right signatures etc. The picture below illustrates this:\nIDS/IPS with a centralized appliance\nNSX Distributed IDS and IPS To overcome the challenges of hairpinning traffic in an virtualized environment, we need to have the firewall, IDS and IPS enforced where the workload actually resides. This saves unnecessary traffic being sent out on the physical infrastructure if its not meant to go out and it also gives the network logics (firewall/IDS/IPS) to be part of the dataplane where the actual workload its supposed to protect resides and can have much more insight (being in context of) in whats going on. Things as knowing its a Ubuntu 20.04 and MySQL server you are protecting, makes it much easier to create the firewall policies but also much more pinpointed/granular IDS/IPS policies. This leads to very specific IDS/IPS rules, no false positives, better performance. This is where NSX Distributed IDS and IPS comes into play. Both the NSX Distributed Firewall and IDS/IPS runs on the same host as the virtual workload you are protecting. Its not necessary to redirect traffic, no need to change anything in the infrastructure, its as simple as just enabling the feature and create policies. Those policies can be created with an application centric perspective, as we have the ability to know the workload we are protecting as the below illustration:\nIDS/IPS polices with only workload relevant signatures\nIDS/IPS available on each hypervisor host\nHow to use IDS \u0026amp; IPS in VMware NSX-T To get started with IDPS in NSX is very easy, its already installed on your transport nodes when you have them enabled with NSX. In the following sections I will go through the different parts in the NSX gui that involves the IDPS part and finish up with an example of how to create policies.\nEnable IDPS, settings and signature updates When one log in to the NSX manager GUI one will see it is divided into different categories such as Networking, Security, and Inventory. IDPS is certainly a security feature of NSX so we will head over there.\nAfter clicking on the Security tab, it will take us to the Security Overview page:\nNSX Security Overview\nAs one can see this gives us a great summarized view over the different security parts in NSX, the IDPS, URL Analysis, the DFW, Anomalies. To see more details in the specific area click on the respective feature on the left side menu. In our case, this is the Distributed IDS/IPS menu.\nWhen inside the Distributed IDS/IPS section, head over to the settings page:\nSettings page of IDPS\nOn this page we can manage the signatures (versions), see the status on the signatures version, whether there is an update on the signature database, update and or adjust whether updates are done automatically. If we want to view the complete list of available signatures click on \u0026quot;View and Manage global signature set\u0026quot;. It should present us a list of all signatures:\nGlobal signature set\nHere we can search for a specific signature, or signatures based on the filter you choose in the top right corner. Say I want to search for signatures relevant to MySQL, I type in \u0026quot;mysql\u0026quot;:\nMysql filter\nBut I can also search for a specific CVE ID (one that we have recently been alerted on maybe):\nCVE-2017-12636\nOr a filter based on CVSS score, in this example 7.5:\nCVSS 7.5\nWe can also adjust the Global default action on specific signatures from Alert, Drop and Reject:\nBy hovering over the blue (!) we will be presented with an explanation of how this works:\nInstead of overriding the global setting for a set of signatures here, we will do this in the next section \u0026quot;IDPS Profiles\u0026quot;.\nFurther down on the same page is where we enable or disable the IDPS feature. It can be enabled on a vSphere cluster (a set of hosts managed by a vCenter) or standalone ESXi hosts. And its just as simple as clicking the enable button on the right side. It should turn green when enabled.\nNow that IDPS is enabled lets head over to Profiles.\nIDPS profiles The profiles section is where we create our application specific signatures we want to use in our IDPS policies (later). We want to adjust and narrow down the total amount of signatures to be used when we create our idps policies for our workload. If I want to create an IDPS policy for a specific application I should create a profile that matches this to reduce false positives, and maintain an optimal inspection with IDS/IPS as an added security feature on top of the Distributed Firewall. In my demo I am interested in only vulnerabilities affecting product \u0026quot;Linux\u0026quot;.\nLets start out by clicking \u0026quot;Add Profile\u0026quot; and create the profile.\nNew profile\nGive the profile a name and start adjusting the signatures we want to use, we start by deciding the Severity Category (Critical, High, Medium and Low). For more information on these categories look here: https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/administration/GUID-4343E565-7AC2-40C2-8B12-5FC14893A607.html\nAfter the severity category has been decided we can go ahead and further adjust the specifics we are looking for, please also make note of the IDS Signatures Included number as we proceed in our selection.\nBefore any selections done:\nDefault\nI will go ahead with Severity Critical \u0026amp; High\nThen I can proceed with a selection based Attack Types, Attack Targets, CVSS and Products Affected. I will just post a screenshot from each four options:\nSearch is possible\nSearch is possible\nI will adjust my profile with only Products Affected (this is done to justify the demo of IDS):\nI am satisfied with my selection from now. Let look at the profile now:\nNow I am down to 217 signatures in this specific profile. I also have an option now to override the default action when/if IDPS detects anything and how to respond. Click on the \u0026quot;Manage signatures for this profile\u0026quot; and we should be presented with the signatures relevant only for this profile (after the selection is done):\nThere is an important note to take here. If we only have Alert on the signature, and we want to create an IDPS policy with Detect \u0026amp; Prevent one must have the signature action to either Drop or Reject also. Its not sufficient to just create a policy with Detect \u0026amp; Prevent. That is brilliant if we have signatures in the same profile we don't want to be dropped, but we only want to be notified. Then we can have one rule with Detect \u0026amp; Prevent where some traffic is being dropped (Prevent) while the other is just notified (Detect). So if there is one CVE that you really should drop here you have the option to select just the few of those.\nLets apply the profile to a policy/rule in IDPS so it can act upon it.\nIDPS Policy Under rules in the Distributed IDPS section in NSX:\nClick + Add Policy, this creates a new section:\nName it and then click on the three dots on the left side to create a rule:\nShould now look like this:\nNow we need to fill in source, destination, our IDS Profile created earlier, where to apply the policy and action (mode). In the example below I have already created a couple of security groups (security groups explained in the NSX Distributed firewall article) so I just need to add them to the respective source/destination:\nIDS rule applied to groups used in source and destination\nClick publish and your rule is in effect immediately.\nAlso notice that I did not specify a Service in my rule, here you can be more specific by add the service also, if you know its HTTP for example.\nNow let us check if it detects something....\nDistributed IDS/IPS Events To view detected events, and drill down into the occured events, we must head over to the Events section in our Distributed IDS/IPS section.\nHere we can get a an overview with a timeline on when and where the events have occured.\nIf we look at the overview we can quickly notice if there has been any event in the last 24 hours, 48 hours, last 7 days or 14 days. Lets go through the Events page. Below is a screenshot showing last 24 hours.\nOverview\nTo change the timeline from default 24 hours, take a look at the top right corner and click on the arrow to get the drop down menu.\nNow look at the timeline view using Last 24 hours I can see that there has been 1 event represented by a an orange dot.\nThat tells me very quickly that there has atleast been detected an event. The color code tells me that the event is of severity High.\nIf there are many events in the timeline view, I can choose to filter out the severity I am interested in by unchecking the others.\nLegend color codes\nBy hovering over the orange dot I can get more information on the event:\nI can see what kind of event, and how many attempts of the same kind. If you look closer on the timeline view there are several dots represented. Those represent the other attempts of the same kind. The colored dots will only represent unique occurences within the given timeline. Its also possible to adjust your timeline further if you want to inspect events happening at a certain time within the 24 hours timeline by adjusting the blue sliders:\nAdjusted timeline view\nThen if I adjust it to say just before 18:10 and just after (where there is a dot) the orange dot will appear again as this event suddenly will be unique for this specific time. The bigger timeline view will be updated accordingly. Now I want to know more of this specific event. Look further down, it will be a list (if several unique events has occured represented, again within the timeline give above). The detailed list below will also update according to the adjusted timeline.\nIf one take a look at the below event it says under occurence \u0026quot;Single Attempt\u0026quot;, but I know there are multiple attempts, as I saw before I adjusted the timeline. I \u0026quot;reset\u0026quot; the timeline view back to the full 24 hours view it will be updated to multiple attempts.\nSingle attempt at the given timeline\nMultiple attempts over a longer timeline\nNow if one click on the arrow on the left side more information will be revealed.\nIn this view I can see the source (the attacker, where it is initiated from) and the destination (the target, victim of the \u0026quot;attack\u0026quot;). Intrusion activity, detected and prevented. The number of VMs affected. If one click on the number below VMs affected we will also see a list of VM(s) affected with names:\nIf I now go back to my profile defined earlier, I want to change the signature ID 2023995 to drop and also update my policy to Detect \u0026amp; Prevent. Lets see how this affects the detailed view.\nUpdated the profile\nUpdated the policy\nPrevented events\nWith the profile on this specific signature ID sat do drop and the policy sat to Detect \u0026amp; Prevent it also drops the specific attempt. Meaning I can have a good night sleep, or can I....?\nI should probably do something with the source also. But that should be easy now that we know what the source is.\n","link":"https://blog.andreasm.io/2021/10/19/vmware-nsx-ids-ips/","section":"post","tags":["nsx","ids","ips","security"],"title":"VMware NSX IDS \u0026 IPS"},{"body":"","link":"https://blog.andreasm.io/tags/automation/","section":"tags","tags":null,"title":"automation"},{"body":"","link":"https://blog.andreasm.io/tags/home-assistant/","section":"tags","tags":null,"title":"home-assistant"},{"body":"","link":"https://blog.andreasm.io/categories/home-automation/","section":"categories","tags":null,"title":"Home-Automation"},{"body":"When I finish up the other posts I have started on there will be content coming here also\n","link":"https://blog.andreasm.io/2021/07/14/the-home-automation-category/","section":"post","tags":["home-assistant","zwave","automation"],"title":"The Home Automation category"},{"body":"","link":"https://blog.andreasm.io/tags/zwave/","section":"tags","tags":null,"title":"zwave"},{"body":"NSX ALB has a very useful feature built-in, to function as DNS server for your domains defined in your NSX-ALB environment. Meaning that all host-records will be automatically resolved by fqdn as soon as the service is created.\nIf you have followed my other post about how to configure the AKO (Avi Kubernetes Operator) http://yikes.guzware.net/2020/10/08/ako-with-antrea-on-native-k8s-cluster/ you are familiar with creating DNS profiles in NSX-ALB. The first step in configuring NSX-ALB as DNS provider is to configure one or more domain names in NSX-ALB.\nLog in to the NSX-ALB controller GUI: -\u0026gt; Templates -\u0026gt; IPAM/DNS Profiles\nCreate a profile (if you dont already have one) give it a name and add one or more domain names:\nAfter you have configured a DNS profile head over to -\u0026gt; Administration -\u0026gt; Settings -\u0026gt; DNS Service in the controller GUI to create the DNS Virtual Service:\nFrom here one can click \u0026quot;Add Virtual Service\u0026quot; and configure the DNS VS. Go to the empty drop-down list (if you don't already have DNS VS configured) and click Create Virtual Service. Choose your cloud and VRF context.\nOne can also create a DNS VS directly from the Application menu, but by going this way some fields are automatically decided for the use of DNS Service.\nGive the service a name, and adjust accordingly. I have done some adjustment to the service in my environment such as Service port where I add 53 twice and choose Override TCP/UDP on the last one to get DNS on UDP port 53 also. I have also added my backend DNS servers as a pool to this VS to have them do lookup against those if the record is not found locally (not obligatory). Application-Domain-Name should have the same domain name as defined in your DNS Profile attached to your cloud.\nLeave Policies and Analytics as is. Under Advanced you choose your SE pool where your DNS VS should live. As a best practice the DNS SE should not be shared with other VS'es. So create a dedicated pool for the DNS-VS and if resources are scarce you can defined the SE group to only contain one SE (no redundancy for DNS VS though).\nIn my environment I have also created a Conditional forwarder on my backend DNS servers to look for DNS records in my domains defined in the N-ALB environment. Using NSX-ALB DNS provider service is a brilliant feature as I don't have to manually register any applications/services created in N-ALB or from K8s through AKO as this is all handled by the DNS service in N-ALB. My K8s applications can be spun up/down, without having to care about their dns records as this is all handled automatically by the NSX-ALB.\nDemo:\nTake an application created in NSX-ALB\nPing the dns name\nThat's it. Now NSX-ALB handles all your DNS records for you. If you want your backend DNS servers to forward the request to NSX-ALB head over to your DNS servers and either add a Conditional forwarder for your domains or add a Delegated zone as a sub-domain and point to your DNS-VS VIP.\n","link":"https://blog.andreasm.io/2021/07/12/configure-nsx-advanced-load-balancer-nsx-alb-as-dns-provider/","section":"post","tags":["avi","ako","loadbalancing","dns-service"],"title":"Configure NSX Advanced Load Balancer (NSX-ALB) as DNS provider"},{"body":"","link":"https://blog.andreasm.io/tags/dns-service/","section":"tags","tags":null,"title":"dns-service"},{"body":"Use NFS for your PVC needs If you are running vShere with Tanzu, TKG on vSphere or are using vSphere as your hypervisor for your worker-nodes you have the option to use the vSphere CSI plugin here. In Tanzu this is automatically configured and enabled. But if you are not so privileged to have vSphere as your foundation for your environment one have to look at other options. Thats where NFS comes in. To use NFS for your persistent volumes is quite easy to enable in your environment, but there are some pre-reqs that needs to be placed on your workers (including control plane nodes). I will go through the installation steps below.\nPre-reqs A NFS server available, already configured with shares exported. This could be running on any Linux machine in your environment that has sufficient storage to cover your storage needs for your PVCs. Or any other platform that can export NFS shares.\nInstall and configure This post is based on Ubuntu 20.04 as operating system for all the workers.\nThe first package that needs to be in place is the nfs-common package in Ubuntu. This is installed with the below command, and on all your workers (control-plane and workers):\n1sudo apt install nfs-common -y Now that nfs-common is installed on all workers we are ready deploy the NFS subdir external provisioner link. The below commands is done from your controlplane nodes, if not stated otherwise. I prefer to use Helm. If you dont have Helm installed head over here for how to install Helm. With Helm in place execute the following command to add the NFS Subdir External Provisioner chart:\n1helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ Then we need to install the NFS provisioner like this:\n1helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ 2 --set nfs.server=x.x.x.x \\ 3 --set nfs.path=/exported/path Its quite self-explanatory but I will quickly to through it. --set nfs.server=x.x.x.x \\ needs to be updated with the IP address to your NFS server. --set nfs.path=/exported/path needs to be updated to reflect the path your NFS server exports.\nThats it actually, you know have a storageclass available in your cluster using NFS. The default values for the storageclass deployed without editing the NFS subdir external provisioner helm values looks like this:\n1$kubectl get storageclasses.storage.k8s.io 2NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE 3nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 51d Values, additional storageclasses If you want to change the default values get the values file, edit it before you deploy the NFS provisioner or get the file, edit it and update your deployment with helm upgrade To grab the values file run this command:\n1helm show values nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \u0026gt; nfs-prov-values.yaml If you want to add additional storageclasses, say with accessmode to RWX, you deploy NFS provisioner with a value file that has the settings you want. Remember to change the class name.\n1 name: XXXXX 2 3 # Allow volume to be expanded dynamically 4 allowVolumeExpansion: true 5 6 # Method used to reclaim an obsoleted volume 7 reclaimPolicy: Delete 8 9 # When set to false your PVs will not be archived by the provisioner upon deletion of the PVC. 10 archiveOnDelete: true 11 12 # If it exists and has \u0026#39;delete\u0026#39; value, delete the directory. If it exists and has \u0026#39;retain\u0026#39; value, save the directory. 13 # Overrides archiveOnDelete. 14 # Ignored if value not set. 15 onDelete: 16 17 # Specifies a template for creating a directory path via PVC metadata\u0026#39;s such as labels, annotations, name or namespace. 18 # Ignored if value not set. 19 pathPattern: 20 21 # Set access mode - ReadWriteOnce, ReadOnlyMany or ReadWriteMany 22 accessModes: XXXXXXX Above is a snippet from the values file.\nDeploying pods with special privileges If you need to deploy pods with special privileges, often mysql containers, you need to prepare your NFS server and filesystem permission for that. Otherwise they will not be able to write correctly to their PV when they are deployed. The example below is a mysql container that needs to create its own permission on the filesystem it writes to:\n1 spec: 2 securityContext: 3 runAsUser: 999 4 fsGroup: 999 So what I did to solve this is the following: I changed my NFS export to look like this on my NFS server: /path/to/share -alldirs -maproot=\u0026quot;root\u0026quot;:\u0026quot;wheel\u0026quot; Then I need to update the permissions on the NFS server filesystem with these commands and permissions:\n1$chown nobody:nogroup /shared/folder 2$chmod 777 /shared/folder ","link":"https://blog.andreasm.io/2021/07/12/kubernetes-persistent-volumes-with-nfs/","section":"post","tags":["persistent-storage","kubernetes","nfs","pvc"],"title":"Kubernetes Persistent Volumes with NFS"},{"body":"","link":"https://blog.andreasm.io/tags/nfs/","section":"tags","tags":null,"title":"nfs"},{"body":"","link":"https://blog.andreasm.io/tags/persistent-storage/","section":"tags","tags":null,"title":"persistent-storage"},{"body":"","link":"https://blog.andreasm.io/tags/pvc/","section":"tags","tags":null,"title":"pvc"},{"body":"","link":"https://blog.andreasm.io/categories/storage/","section":"categories","tags":null,"title":"Storage"},{"body":"Abbreviations used in this article:\nNSX Advanced Load Balancer = NSX-ALB K8s = Kubernetes (8 letters between the K and s in Kubernetes) SSL = Secure Sockets Layer AKO = Avi Kubernetes Operator (AVI now a VMware product called NSX Advanced Load Balancer) In one of my previous posts I wrote about how to install and configure AKO (Avi Kubernetes Operator) to use as Service type LoadBalancer.\nThis post will try to cover the basics of how to use NSX Advanced LoadBalancer by using AKO to handle our Ingress requests (ingress-controller).\nFor more information on Ingress in Kubernetes\nAn API object that manages external access to the services in a cluster, typically HTTP.\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\nIngress Load Balancer Kubernetes Definition Within Kubernetes or K8s, a collection of routing rules that control how Kubernetes cluster services are accessed by external users is called ingress. Managing ingress in Kubernetes can take one of several approaches.\nAn application can be exposed to external users via a Kubernetes ingress resource; a Kubernetes NodePort service which exposes the application on a port across each node; or using an ingress load balancer for Kubernetes that points to a service in your cluster.\nAn external load balancer routes external traffic to a Kubernetes service in your cluster and is associated with a specific IP address. Its precise implementation is controlled by which service types the cloud provider supports. Kubernetes deployments on bare metal may require custom load balancer implementations.\nHowever, properly supported ingress load balancing for Kubernetes is the simplest, more secure way to route traffic. link\nGetting AKO ready While this post assumes AKO is already in place and working in your k8s clusters I will get straight to the parts that involve Ingress. If not head over here to read the official docs how to install Avi Kubernetes Operator (AKO). To verify that you AKO is ready to handle Ingress request, type in this and notice the output:\n1$ kubectl get ingressclasses.networking.k8s.io 2NAME CONTROLLER PARAMETERS AGE 3avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 50d Default secret for TLS Ingress As AKO expects all ingresses with TLS termination to have a key and certificate specified, there is a couple of ways this can be done. We can go with a \u0026quot;pr service\u0026quot;, meaning a dedicated set of keys and certs pr service or a default/common set of keys and certificates that AKO can use if nothing else is specified. To apply the common approach, one common key and certificate for one or more applications we need to add a secret for the AKO. Prepare your router-default.yaml definition file like this (the official docs wants you to put in your cert as is, that does not work so you need to base64 encode both keys and certs and paste in below):\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: router-certs-default 5 namespace: avi-system 6type: kubernetes.io/tls 7data: 8 tls.key: \u0026#34;base64 encoded\u0026#34; 9 tls.crt: \u0026#34;base64 encoded\u0026#34; 10 alt.key: \u0026#34;base64 encoded\u0026#34; 11 alt.crt: \u0026#34;base64 encoded\u0026#34; 12 To base64 encode your keys and certs this can be done like this:\nIf you have the keys and certs in a file, from whatever linux terminal type in:\n1cat cert.pem | base64 -w 0 2cat key.pem | base64 -w 0 Then paste into the above yaml accordingly (tls.key:key, tls.crt:crt) If you have both ECDSA and RSA certs use the alt.key and alt.crt to apply both. As soon as everything is pasted, apply the yaml file kubectl apply -f router-defaults.yaml\nApply your ingress service To create an ingress service you need to define this in yaml. An example below:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: \u0026#34;NameOfIngress-Service\u0026#34; 5 namespace: \u0026#34;Namespaceofwhereyour-service-resides\u0026#34; 6 labels: 7 app: \u0026#34;ifyouwant\u0026#34; 8 annotations: 9 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; #\u0026#34;Indicates to Avi that you want to use TLS\u0026#34; 10spec: 11 ingressClassName: avi-lb #\u0026#34;The default class name for AVI\u0026#34; 12 rules: 13 - host: \u0026#34;FQDN\u0026#34; 14 http: 15 paths: 16 - pathType: Prefix 17 path: / 18 backend: 19 service: 20 name: \u0026#34;which-service-to-point-to\u0026#34; 21 port: 22 number: 80 Hostrules and HTTPrules As I mentioned earlier, you can also define specific rules pr server such as certificates. Here we can use Hostrules and HTTPrules to further adjust granular settings pr service. One Hostrule example below:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: name-of-your-rule 5 namespace: namespace-of-your-service 6spec: 7 virtualhost: 8 fqdn: must-match-hostname-above # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: \u0026#34;name-of-certificate\u0026#34; # This must be already defined in your AVI controller 14 type: ref 15 alternateCertificate: 16 name: \u0026#34;name-of-alternate-cert\u0026#34; # This must be already defined in your AVI controller 17 type: ref 18 sslProfile: System-Standard-PFS 19 termination: edge To get all features available head over to the official docs site here\nHostrule example from the official docs:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: my-host-rule 5 namespace: red 6spec: 7 virtualhost: 8 fqdn: foo.region1.com # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: avi-ssl-key-cert 14 type: ref 15 alternateCertificate: 16 name: avi-ssl-key-cert2 17 type: ref 18 sslProfile: avi-ssl-profile 19 termination: edge 20 gslb: 21 fqdn: foo.com 22 includeAliases: false 23 httpPolicy: 24 policySets: 25 - avi-secure-policy-ref 26 overwrite: false 27 datascripts: 28 - avi-datascript-redirect-app1 29 wafPolicy: avi-waf-policy 30 applicationProfile: avi-app-ref 31 analyticsProfile: avi-analytics-ref 32 errorPageProfile: avi-errorpage-ref 33 analyticsPolicy: # optional 34 fullClientLogs: 35 enabled: true 36 throttle: HIGH 37 logAllHeaders: true 38 tcpSettings: 39 listeners: 40 - port: 8081 41 - port: 6443 42 enableSSL: true 43 loadBalancerIP: 10.10.10.1 44 aliases: # optional 45 - bar.com 46 - baz.com Httprule example from the official docs:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HTTPRule 3metadata: 4 name: my-http-rule 5 namespace: purple-l7 6spec: 7 fqdn: foo.avi.internal 8 paths: 9 - target: /foo 10 healthMonitors: 11 - my-health-monitor-1 12 - my-health-monitor-2 13 loadBalancerPolicy: 14 algorithm: LB_ALGORITHM_CONSISTENT_HASH 15 hash: LB_ALGORITHM_CONSISTENT_HASH_SOURCE_IP_ADDRESS 16 tls: ## This is a re-encrypt to pool 17 type: reencrypt # Mandatory [re-encrypt] 18 sslProfile: avi-ssl-profile 19 destinationCA: |- 20 -----BEGIN CERTIFICATE----- 21 [...] 22 -----END CERTIFICATE----- ","link":"https://blog.andreasm.io/2021/07/11/k8s-ingress-with-nsx-advanced-load-balancer/","section":"post","tags":["avi","ako","ingress","kubernetes"],"title":"K8s Ingress with NSX Advanced Load Balancer"},{"body":" This is an introduction post to Antrea, what it is and which features it has.\nFor more details head over to:\nhttps://antrea.io/ and https://github.com/antrea-io/antrea\nFirst of, Antrea is a CNI. CNI stands for Container Network Interface. As the world moves into Kubernetes more and more, we need a good CNI to support everything from network to security within Kubernetes. Thats where Antrea comes into play.\nAntrea has a rich set of features such as:\nKubernetes-native: Antrea follows best practices to extend the Kubernetes APIs and provide familiar abstractions to users, while also leveraging Kubernetes libraries in its own implementation. Powered by Open vSwitch: Antrea relies on Open vSwitch to implement all networking functions, including Kubernetes Service load-balancing, and to enable hardware offloading in order to support the most demanding workloads. Run everywhere: Run Antrea in private clouds, public clouds and on bare metal, and select the appropriate traffic mode (with or without overlay) based on your infrastructure and use case. Windows Node support: Thanks to the portability of Open vSwitch, Antrea can use the same data plane implementation on both Linux and Windows Kubernetes Nodes. Comprehensive policy model: Antrea provides a comprehensive network policy model, which builds upon Kubernetes Network Policies with new features such as policy tiering, rule priorities and cluster-level policies. Troubleshooting and monitoring tools: Antrea comes with CLI and UI tools which provide visibility and diagnostics capabilities (packet tracing, policy analysis, flow inspection). It exposes Prometheus metrics and supports exporting network flow information which can be visualized in Kibana dashboards. Encryption: Encryption of inter-Node Pod traffic with IPsec tunnels when using an overlay Pod network. Easy deployment: Antrea is deployed by applying a single YAML manifest file. As this blog page evolves, it will cover in more technical posts how to use and configure Antrea with examples. As how this webpage is both handled by Antrea network and security features (yes, this wordpress page is hosted on a native K8s cluster with Antrea as CNI)\n","link":"https://blog.andreasm.io/2021/07/10/antrea-kubernetes-cni/","section":"post","tags":["antrea","kubernetes"],"title":"Antrea - Kubernetes CNI"},{"body":"This post will go through the Antrea-native policy resources and how to use them to secure your Kubernetes environment combined with K8s network policies.\nAbbreviations used in this article:\nContainer Network Interface = CNI Antrea Cluster Network Policies = ACNP Antrea Network Policies = ANP Kubernetes Network Policies = K8s policies or KNP When it comes to securing your K8s infrastructure it can be done in several layers in the infrastructure as a whole. This post will focus on the possibilities within the K8s cluster with features in the Antrea CNI. I will go through the Antrea-native policies (Antrea and K8s policies), with examples of when, how and where to use them. As Antrea-native policy resources can be used together with K8s network policies I will show that also.\nThis post will not cover the additional needs of security in your datacenter before reaching your K8s environment. I will cover this in a later post where I go through the use of NSX Distributed Firewall protecting your k8s clusters together with the security policies in Antrea.\nAntrea-native policy resources - short introduction Antrea comes with a comprehensive policy model. We have the Antrea Cluster Network Policies and Antrea Network Policies. The difference being between those two is that the ACNP applies to all objects on the cluster, where ANP is namespaced meaning its applies to objects within the namespace defined in the policy.\nAntrea policies are tiered, meaning the rules will be following an order of precedence. This makes it very useful to divide the rules into the right categories, having different resources in the organization responsible for the security rules. Sec-ops will have their rules in the beginning setting the \u0026quot;ground\u0026quot; before the application owners can set their rules and finally some block all rules that rules out all that is left. Antrea-native policy resources is working together with K8s network policies where the latter is placed in the Application tier below the ANP and ACNP policies. Antrea comes with a set of default tiers as of installation of Antrea, but there is also possible to add custom tiers. Read more here. Here are the default tiers:\n1Emergency -\u0026gt; Tier name \u0026#34;emergency\u0026#34; with priority \u0026#34;50\u0026#34; 2SecurityOps -\u0026gt; Tier name \u0026#34;securityops\u0026#34; with priority \u0026#34;100\u0026#34; 3NetworkOps -\u0026gt; Tier name \u0026#34;networkops\u0026#34; with priority \u0026#34;150\u0026#34; 4Platform -\u0026gt; Tier name \u0026#34;platform\u0026#34; with priority \u0026#34;200\u0026#34; 5Application -\u0026gt; Tier name \u0026#34;application\u0026#34; with priority \u0026#34;250\u0026#34; 6Baseline -\u0026gt; Tier name \u0026#34;baseline\u0026#34; with priority \u0026#34;253\u0026#34; Take a look at the diagram below and imagine your first rules is placed at the first left tier and more rules in the different tiers all the way to the right:\nMaking use of the Antrea-native policy resources In this section I will describe a demo environment with some namespaces and applications and then go through one way of using Antrea-native policy resources together with K8s network policies. In some of my Antrea-native policies I will use namespace selection based on the actual name of the namespace, but in others I will use selection based on labels. The first example below will make use of labels as selection criteria. I will explain why below. To read more on namespace selection: https://github.com/antrea-io/antrea/blob/main/docs/antrea-network-policy.md#select-namespace-by-name\nMeet the demo environment I will only use one K8s cluster in this example which is based on one master worker and two worker nodes.\nI will create three \u0026quot;environments\u0026quot; by using namespaces and label them according to which \u0026quot;environment\u0026quot; they belong to. The three namespaces will be \u0026quot;test-app\u0026quot;, \u0026quot;dev-app\u0026quot; and \u0026quot;prod-app\u0026quot;. I will then add a label on each namespace with the label env=test, env=dev and env=prod accordingly. Within each namespace I will spin up two pods (an Ubuntu 16.04 and Ubuntu 20.04 pod). What I would like to achieve is that each namespace represents their own environment to simulate scenarios where we do have prod, dev and test environments, where none of the environments are allowed to talk to each other. And by using labels, I can create several namespaces and place them into the correct environment by just \u0026quot;tagging\u0026quot; them with the correct labels (e.g env=dev).\nNow in the next section I will go through how I can isolate, and control those environments with Antrea-native policy resources.\nAntrea Cluster Network Policies (ACNP) The first thing I would like to do is to create some kind of basic separation between those environments/namespaces so they cant communicate with each other, and when that is done I can continue to create more granular application policies within each namespace or environment.\nThe first issue I meet is how to create as few rules as possible to just isolate what I know (the tree namespaces which are labeled with three different labels to create my \u0026quot;environments\u0026quot;) without having to worry about additional namespaces being created and those getting access to the \u0026quot;environments\u0026quot;. In this example I have already created three namespaces named \u0026quot;dev-app\u0026quot;, \u0026quot;prod-app\u0026quot; and \u0026quot;test-app\u0026quot;. I \u0026quot;tag\u0026quot; them in Kubernetes with their corresponding \u0026quot;env\u0026quot; labels: \u0026quot;dev\u0026quot;, \u0026quot;prod\u0026quot; and \u0026quot;test\u0026quot;. The reason I choose that approach is that I then can create several namespaces and choose which environment they belong to instead of doing the selection directly on the name of the namespace. I need to create an Antrea Cluster Network Policy as a \u0026quot;default\u0026quot; rule for each of my known environments so I can at a minimum guarantee that within each namespace or environment \u0026quot;intra-traffic\u0026quot; is allowed (traffic within the namespace or namespaces labeled with the same environment label). Meaning that when I do have a complete Antrea-native policy \u0026quot;framework\u0026quot; in place (with a blocking rule at the end taking care of all that is not specified) I can create new namespaces, but if they are not labeled correctly they will not be allowed to talk to any of my environments. This policy is applied at the SecurityOps tier:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: isolate-dev-env 5spec: 6 priority: 5 7 tier: SecurityOps 8 appliedTo: 9 - namespaceSelector: 10 matchLabels: 11 env: dev 12 ingress: 13 - action: Drop 14 from: 15 - namespaceSelector: 16 matchExpressions: 17 - key: env 18 operator: NotIn 19 values: 20 - dev 21 - action: Allow 22 from: 23 - namespaceSelector: 24 matchLabels: 25 env: dev 26 egress: 27 - action: Allow 28 to: 29 - namespaceSelector: 30 matchLabels: 31 env: dev What I am also doing with this ACNP is saying that if you are member of a namespace with the label \u0026quot;env=dev\u0026quot; you are allowed to ingress the namespace Dev, but not if you are not (\u0026quot;operator: NotIn\u0026quot; in the ingress namespaceSelector).\nAlso note that I am allowing specifically an Action allow to the dev environment within the same policy, the reason being is that when I apply my block-all-else rule later on it will block intra traffic within the same environment if it is not specifically specified that it is allowed in this rule.\nNow I just have to recreate this policy for my other two namespaces.\nAlso note that in the egress part I am only allowing traffic to namespace with the lavel \u0026quot;env=dev\u0026quot;. That does not mean right now that I will only allow traffic to anything else, because I don't have any block rules in my cluster yet. Antrea-native policy resources works a bit different than K8s network policies which only supports creating allow policies. In Antrea one can specify both DROP and ALLOW on both INGRESS and EGRESS. I left this with purpose, because I later in will go ahead create a block all rule. Now lets demonstrate this rule:\nBefore applying ACNP namespace isolation rule:\n1kubectl get pod -n dev-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-16-04-7f876959c6-p5nxp 1/1 Running 0 9d 10.162.1.57 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-6fb66c64cb-9qg2p 1/1 Running 0 9d 10.162.1.56 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 1kubectl get pod -n prod-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-16-04-7f876959c6-sfdvf 1/1 Running 0 9d 10.162.1.64 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-6fb66c64cb-z528m 1/1 Running 0 9d 10.162.1.65 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Above I list out the pods with IP addresses in my two namespaces \u0026quot;dev-app\u0026quot; and \u0026quot;prod-app\u0026quot;\nNow I enter bash of the Ubuntu20.04 pod in \u0026quot;dev-app\u0026quot; namespace and do a ping to the second pod in the same namespace and then ping another pod in the namespace Prod:\n1kubectl exec -it -n dev-app ubuntu-20-04-6fb66c64cb-9qg2p bash 1root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.57 2PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 364 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=0.896 ms 464 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.520 ms 564 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.248 ms 6 7root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.64 8PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. 964 bytes from 10.162.1.64: icmp_seq=1 ttl=64 time=1.03 ms 1064 bytes from 10.162.1.64: icmp_seq=2 ttl=64 time=0.584 ms 1164 bytes from 10.162.1.64: icmp_seq=3 ttl=64 time=0.213 ms I have also written about Octant in one of my posts, in Octant there is an Antrea plugin which gives us some graphical features such as traceflow, which is also a powerful tool to showcase/troubleshoot security policies. Below is a screenshot from Octant before the rule is applied:\nAs you can see, this is allowed. Now I apply my \u0026quot;isolation\u0026quot; ACNP rules \u0026quot;prod\u0026quot;, \u0026quot;dev\u0026quot; \u0026amp; \u0026quot;test\u0026quot;. Also note; to list out the applied ACNP policies the command \u0026quot;kubetcl get acnp\u0026quot; can be used, without looking in a specific namespace as ACNP is clusterwide.\n1kubectl apply -f isolate.environment.prod.negated.yaml 2clusternetworkpolicy.crd.antrea.io/isolate-prod-env created 3kubectl apply -f isolate.environment.dev.negated.yaml 4clusternetworkpolicy.crd.antrea.io/isolate-dev-env created 5kubectl get acnp 6NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 7isolate-dev-env SecurityOps 5 1 1 19s 8isolate-prod-env SecurityOps 6 1 1 25s After they are applied I will try to do same as above:\n1ping 10.162.1.57 2PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 364 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=3.28 ms 464 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.473 ms 564 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.190 ms 664 bytes from 10.162.1.57: icmp_seq=4 ttl=64 time=0.204 ms 7 8ping 10.162.1.64 9PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. Pinging within the same namespace works perfect, but to one of the other namespaces (here the Prod namespace) is not allowed. Works as intended.\nDoing the same traceflow with Octant again:\nSo to recap, this is how it looks like now:\nNow that I have created myself some isolated environments, I also need to allow some basic needs from the environments/namespaces to things such as DNS. So I will go ahead and create such a rule. Also have in mind that I haven't yet applied the last block-all-rule (so they can still reach those services as of now). I will make that rule applied when all the necessary rules are in place beforehand. In a greenfield environment those \u0026quot;baseline\u0026quot; rules would probably be applied as the first thing before the k8s cluster is taken into use.\nGoing down one tier to NetworkOps I will apply this Antrea Policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: allow-core-dns 5spec: 6 priority: 10 7 tier: NetworkOps 8 appliedTo: 9 - namespaceSelector: {} 10 egress: 11 - action: Allow 12 to: 13 - namespaceSelector: 14 matchLabels: 15 kubernetes.io/metadata.name: kube-system 16 ports: 17 - protocol: TCP 18 port: 53 19 - protocol: UDP 20 port: 53 This policy is probably for some rather \u0026quot;wide\u0026quot; as it just does a \u0026quot;wildcard\u0026quot; selection of all namespaces available and gives them access to the backend kube-system (where the coredns pods are located) on protocol TCP and UDP port 53. But again, this post is just to showcase Antrea policies and how they can be used and to give some insights in general.\nDNS allowed showed with Octant:\nOctant Traceflow\nFor now I am finished with the Cluster Policies and will head over to the Network Policies. This is the ACNP policies applied so far:\n1kubectl get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3allow-core-dns NetworkOps 10 3 3 19h 4isolate-dev-env SecurityOps 5 1 1 22h 5isolate-prod-env SecurityOps 6 1 1 22h 6isolate-test-env SecurityOps 7 1 1 19h Antrea Network Policies (ANP) Antrea Network Policies are namespaced. So one of the use cases for ANP could be to create rules specific for the services running in the namespace. It could be allowing ingress on certain selections (e.g label/frontend) which runs the application I want to expose or which makes sense for clients to talk to which is the frontend part of the application. Everything else is backend services which not necessary to expose to clients, but on the other hand it could be that those services needs access to other backend services or services in other namespaces. So with ANP one can create ingress/egress policies by using the different selection options defining what is allowed in and out of the namespace.\nBefore I continue I have now applied my ACNP block-rule in the \u0026quot;Baseline\u0026quot; tier which just blocks all else to make sense of the examples used here in this section. Below is the policy (Note that I have excluded some namespaces in this rule) :\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: block-all-whitelist 5spec: 6 priority: 1000 7 tier: baseline 8 appliedTo: 9 - namespaceSelector: 10 matchExpressions: 11 - key: ns 12 operator: NotIn 13 values: 14 - kube-system 15 - monitoring 16 ingress: 17 - action: Drop 18 from: 19 - namespaceSelector: {} 20 - ipBlock: 21 cidr: 0.0.0.0/0 22 egress: 23 - action: Drop 24 to: 25 - namespaceSelector: {} 26 - ipBlock: 27 cidr: 0.0.0.0/0 Antrea Network Policies Egress rule Now that I have applied my \u0026quot;whitelist\u0026quot; rule I must by now have all my necessary rules in place, otherwise things will stop working (such as access to DNS). I will now apply a policy which is \u0026quot;needed\u0026quot; by the \u0026quot;Prod\u0026quot; environment, which is access to SSH on a remote server. So the policy below is allowing Egress on TCP port 22 to this specific remote SSH server. Lets us apply this policy and test how this works out:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: NetworkPolicy 3metadata: 4 name: allow-prod-env-ssh 5 namespace: prod-app 6spec: 7 priority: 8 8 tier: application 9 appliedTo: 10 - podSelector: {} 11 egress: 12 - action: Allow 13 to: 14 - namespaceSelector: 15 matchLabels: 16 kubernetes.io/metadata.name: prod-app 17 - ipBlock: 18 cidr: 10.100.5.10/32 19 ports: 20 - protocol: TCP 21 port: 22 Just some sanity check before applying the above policy, I am still able to reach all pods within the same namespace due to my \u0026quot;isolation ACNP rules\u0026quot; even though I have my block all rule applied.\nBut I am not allowed to reach anything outside except what is stated in my DNS rule. If I try to reach my remote SSH server from my \u0026quot;Prod\u0026quot; namespace I am not allowed. To illustrate this I have entered \u0026quot;remoted\u0026quot; myself into bash on one of my pods in the Prod namespace and trying to ssh the remote server 10.100.5.10, below is the current result:\n1root@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 2ssh: connect to host 10.100.5.10 port 22: Connection timed out Ok, fine. What does my traceflow say about this also:\nNope cant do it also says Traceflow. Great, everything works out as planned. Now I must apply my policy to allow this.\n1root@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 2andreasm@10.100.5.10\u0026#39;s password: Now I can finally reach my remote SSH server. To confirm again, lets check with Octant:\nThank you very much, that was very kind of you.\nTo summarize so far what we have done. We have applied the ACNP rules/policies to create environment/namespace isolation\n1NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 2allow-core-dns NetworkOps 10 3 3 21h 3block-all-whitelist baseline 20 2 2 25m 4isolate-dev-env SecurityOps 5 1 1 24h 5isolate-prod-env SecurityOps 6 1 1 31m 6isolate-test-env SecurityOps 7 1 1 21h And we have applied a rule to allow some basic \u0026quot;needs\u0026quot; such as DNS with the rule allow-core-dns. And the \u0026quot;block-all-whitelist\u0026quot; policy as a \u0026quot;catch all rule\u0026quot; to block everything not specified in the tiers.\nAnd then we have applied a more application/namespace specific policy with Antrea Network Policy to allow Prod to egress 10.100.5.10 on port 22/TCP. But I have not specified any ingress rules allow access to any services in the namespace Prod coming from outside the namespace. So it is a very lonely/isolated environment for the moment. This is how it looks like now:\nIn the next example I will create an ingress rule to another application that needs to be accessed from the outside.\nAntrea Network Policies Ingress rule To make this section a bit more \u0026quot;understanding\u0026quot; I will use another application as example to easier illustrate the purpose. The example I will be using is a demo application I have been using for several years - Yelb link\nThis application contains of four pods and looks like this:\nYelb diagram\nI already have the application up and running in my environment. But as this application is a bit more complex and contains a frontend which is useless if not exposed or reachable I am exposing this frontend with NSX Advanced Load Balancer. This makes it very easy for me to define the ingress rule as it means I only have to allow the load balancers IPs in my egress rule and not all potential IPs. The load balancers IP's is something I know. Some explanation around the load balancer IP's in my environment is that they are spun up on demand and just pick an IP from a pre-defined IP pool, so instead of pinning the ingress rule to the current IP they have I am bit wide and allow the IP range that is defined. Remember that this is a demo environment and does not represent a production environment. Lets take a look at the policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: NetworkPolicy 3metadata: 4 name: allow-yelb-frontend 5 namespace: yelb 6spec: 7 priority: 5 8 tier: application 9 appliedTo: 10 - podSelector: 11 matchLabels: 12 app: yelb-ui 13 ingress: 14 - action: Allow 15 from: 16 - ipBlock: 17 cidr: 10.161.0.0/24 18 ports: 19 - protocol: TCP 20 port: 80 21 endPort: 80 22 name: AllowInYelbFrontend 23 enableLogging: false 24 egress: 25 - action: Allow 26 to: 27 - ipBlock: 28 cidr: 10.161.0.0/24 29 name: AllowOutYelbFrontend 30 enableLogging: false The CIDR in the rule above is the range my load balancers is \u0026quot;living\u0026quot; in. So instead to narrow it down too much in this demo I just allow the range 10.161.0.0/24 meaning I dont have to worry too much if they are getting new IP's within this range making my application inaccessible. When I apply this rule it will be placed in the tier \u0026quot;Application\u0026quot; (See one of the first diagrams in the beginning of this post) with a priority of 5. The basic policies for this application is already in place such as DNS and intra-communication (allowed to talk within the same namespace/environment which in this example is yelb-app/test).\nNow lets see how it is before applying the rule from the perspective of the NSX Advanced Load Balancer which is being asked to expose the frontend of the application:\nFrom NSX Advanced Load Balancer GUI / application showing pool is down\nAs one can see from the above screenshot, the Service Engines (the actual load balancers) are up and running but the application yelb-ui is down because the pool is unreachable. The pool here is the actual pod containing the frontend part of the Yelb app. So I need to apply the Antrea Network Policy to allow the Service Engines to talk to my pod. If I try to access the frontend via the load balancer VIP its also inaccessible:\nLets just apply the rule:\n1kubectl apply -f yelb.frontend.allow.yaml 2networkpolicy.crd.antrea.io/allow-yelb-frontend created 3kubectl get anp -n yelb 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5allow-yelb-frontend application 5 1 1 11s And now check the NSX Advanced Load Balancer status page and try to access the application through the VIP:\nNSX ALB is showing green\nWell that looks promising, green is a wonderful colour in IT.\nAnd the application is available:\nYelb UI frontend\nThe rule above only gives the NSX Advanced Load Balancers access to the frontend pod on port 80 of the application Yelb. All the other pods are protected by the \u0026quot;environment\u0026quot; block rule and the default block rule. There is one catch though, we dont have any rules protecting traffic between the pods. Lets say the frontend pod (which is exposed to the outside world) is compromised, there is no rule stopping any traffic coming from this pod to the others within the same namespace/and or environment. That is something we should apply.\nMicrosegmenting the application What we shall do now is to make sure that the pods that make up the application Yelb is only allowed to talk to each other on the necessary ports and nothing else. Meaning we create a policy that does a selection of the pods and apply specific rules for each pod/service within the application, if one refer to the diagram above over the Yelb application one can also see that there is no need for the fronted pod to be allowed to talk to the redis or db pod at all so that should be completely blocked.\nI will go ahead and apply a rule that does all the selection for me, and only allow what is needed for the application to work. The policy I will make use of here is K8s Native Network Policy kubernetes.io\nHere is the rule:\n1apiVersion: networking.k8s.io/v1 2kind: NetworkPolicy 3metadata: 4 name: yelb-cache 5 namespace: yelb 6spec: 7 podSelector: 8 matchLabels: 9 tier: cache 10 ingress: 11 - from: 12 - podSelector: 13 matchLabels: 14 tier: middletier 15 - namespaceSelector: 16 matchLabels: 17 tier: middletier 18 ports: 19 - protocol: TCP 20 port: 6379 21 policyTypes: 22 - Ingress 23--- 24apiVersion: networking.k8s.io/v1 25kind: NetworkPolicy 26metadata: 27 name: yelb-backend 28 namespace: yelb 29spec: 30 podSelector: 31 matchLabels: 32 tier: backenddb 33 ingress: 34 - from: 35 - podSelector: 36 matchLabels: 37 tier: middletier 38 - namespaceSelector: 39 matchLabels: 40 tier: middletier 41 ports: 42 - protocol: TCP 43 port: 5432 44 policyTypes: 45 - Ingress 46--- 47apiVersion: networking.k8s.io/v1 48kind: NetworkPolicy 49metadata: 50 name: yelb-middletier 51 namespace: yelb 52spec: 53 podSelector: 54 matchLabels: 55 tier: middletier 56 ingress: 57 - from: 58 - podSelector: {} 59 - namespaceSelector: 60 matchLabels: 61 tier: frontend 62 ports: 63 - protocol: TCP 64 port: 4567 65 policyTypes: 66 - Ingress 67--- 68apiVersion: networking.k8s.io/v1 69kind: NetworkPolicy 70metadata: 71 name: yelb-frontend 72 namespace: yelb 73spec: 74 podSelector: 75 matchLabels: 76 tier: frontend 77 ingress: 78 - from: 79 ports: 80 - protocol: TCP 81 port: 80 82 egress: 83 - to: 84 ports: 85 - protocol: TCP 86 port: 30567 87 policyTypes: 88 - Ingress 89 - Egress As I have already illustrated above I will not go through showing that the pods can talk to each other on all kinds of port, as they can because they do not have any restriction within the same namespace/environment. What I will go through though is how the above policy affects my application.\nThe rule applied:\n1kubectl apply -f k8snp_yelb_policy.yaml 2networkpolicy.networking.k8s.io/yelb-cache created 3networkpolicy.networking.k8s.io/yelb-backend created 4networkpolicy.networking.k8s.io/yelb-middletier created 5networkpolicy.networking.k8s.io/yelb-frontend created 6 7kubectl get networkpolicies.networking.k8s.io -n yelb 8NAME POD-SELECTOR AGE 9yelb-backend tier=backenddb 80s 10yelb-cache tier=cache 80s 11yelb-frontend tier=frontend 80s 12yelb-middletier tier=middletier 80s So to illustrate I will paste a diagram with the rules applied, and go ahead an see if I am allowed and not allowed to reach pods on ports not specified.\nYelb diagram with policies\nThe first thing I will try is to see if the frontend pod can reach the appserver on the specified port 4567:\nOctant Antrea Traceflow\nAnd the result is in:\nNow, what if I just change the port to something else, say DNS 53... Will it succeed?\n","link":"https://blog.andreasm.io/2021/07/10/antrea-network-policies/","section":"post","tags":["network-policies","antrea","security"],"title":"Antrea Network Policies"},{"body":"","link":"https://blog.andreasm.io/tags/informational/","section":"tags","tags":null,"title":"informational"},{"body":"This post will go through one way of securing your workloads with VMware NSX. It will cover the different tools and features built into NSX to achieve a robust and automated way of securing your workload. It will go through the use of Security Groups, how they can be utilized, and how to create security policies in the distributed firewall section of NSX-T with the use of the security groups.\nIntroduction to NSX Distributed Firewall If we take a look inside a modern datacenter we will discover very soon that there is not so much bare metal anymore (physical server with one operating system and often many services to utilize the resources), most workload today is virtualized. From a network perspective the traffic pattern has shifted from being very much north/south to very much east/west. A typical traffic distribution today between north/south and east/west is a 10% (+/-) north/south and 90%(+/-) east/west. When the traffic pattern consisted of a high amount north/south it made sense to have our perimeter firewall regulate and enforce firewall rules in and out of the DC and between server workload. Due to server virtualization a major part of the DC the workload consist of many virtual machine instances with very specific services and \u0026quot;intra\u0026quot; communication (east/west) is a large part. It is operationally a tough task to manage a perimeter firewall to be the \u0026quot;policy enforcer\u0026quot; between workload in the east/west \u0026quot;zone\u0026quot;. It is also very hard for a discrete appliance to be part of the context (it is outside of the dataplane/context of the workload it is trying to protect).. Will delve into this in more detail later in the article. Will also illustrate east/west and north/south traffic pattern.\n","link":"https://blog.andreasm.io/2021/07/10/microsegmentation-with-vmware-nsx/","section":"post","tags":["informational"],"title":"Microsegmentation with VMware NSX"},{"body":"When starting out a microsegmentation journey with VMware NSX it will be very important to have a tool that gives you all the visibility and insight you need. This is crucial if you dont know your applications requirements in detail and to make the right decisions in defining your NSX security policies.\nNSX Intelligence is your tool for that. This post is just to show a couple of screenshots of how it looks, and the next post will go more into detail how it works and how to use it.\n","link":"https://blog.andreasm.io/2021/07/10/nsx-intelligence-quick-overview/","section":"post","tags":["informational"],"title":"NSX Intelligence - quick overview"},{"body":"","link":"https://blog.andreasm.io/categories/nsx-t/","section":"categories","tags":null,"title":"nsx-t"},{"body":"","link":"https://blog.andreasm.io/categories/vmare-nsx-intelligence/","section":"categories","tags":null,"title":"vmare nsx intelligence"},{"body":"","link":"https://blog.andreasm.io/categories/vmware-nsx/","section":"categories","tags":null,"title":"vmware nsx"},{"body":"This post will cover the steps to bring up a Kubernetes cluster in Ubuntu, then how to install and configure Antrea as CNI and how to install and configure NSX Advanced Load Balancer to use as a servicetype Load Balancer in the k8s environment with the use of Avi Kubernetes Operator.\nAbbreviations used in this post:\nNSX Advanced Load Balancer = NSX ALB Avi Kubernetes Operator = AKO Kubernetes = k8s Container Network Interface = CNI Load Balancer = lb Introduction to this post When working with pods in a k8s cluster there is often the use of nodePort, clusterIP and LoadBalancer. In a three tiered application very often only one of the tier is necessary to expose to the \u0026quot;outside\u0026quot; (outside of the k8s cluster) world so clients can reach the application. There are several ways to do this, and one of them is using the service Load Balancer in k8s. The point of this post is to make use of NSX ALB to be the load balancer when you call for the service load balancer in your application. There are some steps needed to be done to get this up and running, as will be covered in this post, and when done you can enjoy the beauty of automatically provisioned lb's by just stating in your yaml that you want a lb for the specific service/pod/application and NSX ALB will take care of all the configuration needed to make your application available through a VIP. One of the steps involved in making this happen is deploying the AKO in your k8s cluster. AKO runs as a pod and is very easy to deploy and configure.\nDiagram over topology Deploy Kubernetes on Ubuntu 20.04 Prepare the Worker and Master nodes. For this part the initial process was taken from here with modifications from my side.\nThis will be a small 3-node cluster with 1 Master and 2 Worker Nodes.\nInstall 3 Ubuntu VMs, or one and clone from that.\n1sudo hostnamectl set-hostname \u0026lt;hostname\u0026gt; By using Ubuntu Server as image make sure openssh is installed.\nDisable Swap\n1sudo swapoff -a Edit fstab and comment out the swap entry.\nVerify with the command:\n1free -h Install packages on the master and worker nodes: On all nodes: 1sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl Add Kubernetes repository:\nAdd key to Apt:\n1sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Add repo:\n1cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 2deb https://apt.kubernetes.io/ kubernetes-xenial main 3EOF Install Kubeadm, Kubelet and Kubectl\n1sudo apt-get update 2 3sudo apt-get install -y kubelet kubeadm kubectl Note, this will install default Kubernetes release in apt repo from your distro, if you want to install a specific version, follow this:\nOverview of differente Kubernets versions and dependices:\n_https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/Packages\n_Lets say we want to install Kubeadmin 1.18.9:\n1sudo apt-get install kubelet=1.18.9-00 kubectl kubeadm=1.18.9-00 Check the below link if you can install a later version of kubectl than kubeadmin\nhttps://kubernetes.io/docs/setup/release/version-skew-policy/\nThe latter apt-get install command install kubelet version 1.18.9 and kubeadm 1.18.9 but kubectl will be the default release in apt (1.19.x for me).\nAfter the the three packages have been installed, set them on hold:\n1sudo apt-mark hold kubelet kubeadm kubectl 2 3kubelet set on hold. 4kubeadm set on hold. 5kubectl set on hold. Install Docker container runtime:\n1sudo apt-get install docker.io -y On the master-node init the Kubernetes master worker: 1sudo kubeadm init --pod-network-cidr=10.162.0.0/16 --apiserver-cert-extra-sans apihost.corp.local Change the CIDR accordingly to match your defined pod-network. It comes down to if you want to do a routable or natâ€™ed toplogy. And by using the â€“apiserver-cert-extra variable it will generate the certs to also reflect the dns-name, making it easier to expose this with a name instead of just the ip of the master worker.\nOn the worker-nodes, join the control plane of the master worker: 1sudo kubeadm join apihost.corp.local:6443 --token \u0026lt;toke\u0026gt; --discovery-token-ca-cert-hash \u0026lt;hash\u0026gt; The token and hash is presented to you after you have initâ€™d the master-node in the previous step above.\nAfter you have joined all you worker nodes to the control-plane you have a working Kubernetes cluster, but without any pod-networking plugin (CNI), this will be explained a bit later and is also the reason for creating this guide.\nTo make it easier to access your cluster copy the kube config to your $HOME/.kube folder:\n1mkdir -p $HOME/.kube 2 3sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 4sudo chown $(id -u):$(id -g) $HOME/.kube/config You can now to a kubectl get pods --all-namespaces and notice that the coredns pods have status pending. That is because there is not pod networking up and running yet.\nInstall Antrea as the CNI **To read more about Antrea, go here:\n**https://github.com/vmware-tanzu/antrea\nTo get started with Antrea is very easy.\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/getting-started.md\nI will post a dedicated post for Antrea, covering Antrea Policies and other features in Antrea.\nOn the master worker:\nDecide first which version/release of Antrea you want (latest as of this writing is v1.1.1), this could depend on certain dependencies with other solutions you are going to be using. In my case I am using VMware Advanced LoadBalancer AKO v1.4.2 (Previously AVI Networks) as Service LB for my pods/apps. See this for compatibility guide: https://avinetworks.com/docs/ako/1.4/ako-compatibility-guide/\nTag the release you want to be installed of Antrea:\n_TAG=v1.1.1\n_Again, this is all done on the master-worker\nThen apply:\nkubectl apply -f [https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml](https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml)\nThis will automatically create the needed interfaces on all the workers. After a short time check your pods:\n1Kubectl get pods --all-namespaces 2kube-system antrea-agent-bzdkx 2/2 Running 0 23h 3kube-system antrea-agent-lvdxk 2/2 Running 0 23h 4kube-system antrea-agent-zqp6d 2/2 Running 0 23h 5kube-system antrea-controller-55946849c-hczkw 1/1 Running 0 23h 6kube-system antrea-octant-f59b76dd9-6gj82 1/1 Running 0 10h 7kube-system coredns-66bff467f8-6qz2q 1/1 Running 0 23h 8kube-system coredns-66bff467f8-cd6dw 1/1 Running 0 23h Coredns is now running and antrea controller/agents have been installed.\nIf you look at your worker nodes, they have also been configured with some extra interfaces:\n4: ovs-system:\n5: genev_sys_6081:\n6: antrea-gw0:\n**You now have a working container network interface, what now?\n**Wouldnâ€™t it be cool to utilize a service load balancer to easily scale and expose your frontends?\nInstall AVI Kubernetes Operator Here comes AKO (Avi Kubernetes Operator), VMware Advanced LoadBalancer\nTo read more about AKO visit: https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes and https://avinetworks.com/docs/ako/1.4/ako-release-notes/\nTo be able to install AKO there are some prereqs that needs to be done on both your k8s cluster and NSX-ALB controller.\nPrepare NSX-ALB for AKO Lets start with the pre-reqs on the NSX-ALB controller side by logging into the controller GUI.\nDisclaimer, I have followed some of the official documentation here: https://avinetworks.com/docs/ako/1.2/ako-installation/ but I couldnâ€™t follow everything there as it did not work in my environment.\nThis guide assumes NSX-ALB has been configured with a default-cloud with vCenter and the networks for your VIP subnet, node-network already has been defined as vDS portgroups in vCenter. In additon to basic knowledge of NSX-ALB.\nIn the NSX-ALB Controller\nCreate a dedicated SE group for the K8s cluster you want to use with AVI, define it the way you would like:\nCreate DNS and IPAM profiles\nIn IPAM define this:\nName, cloud and the VIP network for the SEs frontend facing IP. The network that should be reachable outside the node-network.\nGo to infrastructure and select your newly created IPAM/DNS profiles in your default-cloud:\nWhile you are editing the default-cloud also make sure you have configured these settings:\nDHCP is for the management IP interface on the SEs. The two other options â€œPrefer Staticâ€ \u0026amp; Use Staticâ€ I have to use otherwise it will not work, the AKO pod refuses to start, crashloopbackoff. And I am not doing BGP on the Avi side, only from the T0 of NSX. One can use the default global routing context as it can be shared, no need to define a custom routing context.\nUnder network select the management network and your SE group created for AKO SEs. And ofcourse enable DHCP:\nNow we are done on the NSX-ALB controller side. Lets jump back to our K8s Master worker to install AKO.\nThe initial steps here I have taken from:\nhttps://www.vrealize.it/2020/09/15/installing-antrea-container-networking-and-avi-kubernets-operator-ako-for-ingress/\nInstall AKO: AKO is installed with helm, so we need to install helm on our master worker:\nsudo snap install helm --classic\nOr whatever preferred way to install packages in Ubuntu.\n**Create a namespace for the AKO pod:\n**\n1 kubectl create ns avi-system **Add AKO incubator repository\n**\n1 helm repo add ako https://projects.registry.vmware.com/chartrepo/ako **Search for available charts and find the version number:\n**\n1helm search repo 2 3 NAME CHART VERSION\tAPP VERSION\tDESCRIPTION 4 ako/ako 1.4.2 1.4.2 A helm chart for Avi Kubernetes Operator 5 Download AKO values.yaml\n1 helm show values ako/ako --version 1.4.2 \u0026gt; values.yaml This needs to be edited according to your needs. See the comments in the value.yaml and you will se whats needed to be updated.\nI will publish my value.yaml file here and comment what I edited.\nI will remove the default comments and replace them with my own, just refer to de default comments by looking at the original yaml file here:\nhttps://raw.githubusercontent.com/avinetworks/avi-helm-charts/master/charts/stable/ako/values.yaml\n1# Default values for ako. 2# This is a YAML-formatted file. 3# Declare variables to be passed into your templates. 4 5replicaCount: 1 6 7image: 8 repository: avinetworks/ako 9 pullPolicy: IfNotPresent 10 11 12### This section outlines the generic AKO settings 13AKOSettings: 14 logLevel: \u0026#34;INFO\u0026#34; #enum: INFO|DEBUG|WARN|ERROR 15 fullSyncFrequency: \u0026#34;1800\u0026#34; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. 16 apiServerPort: 8080 # Specify the port for the API server, default is set as 8080 // EmptyAllowed: false 17 deleteConfig: \u0026#34;false\u0026#34; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI 18 disableStaticRouteSync: \u0026#34;false\u0026#34; # If the POD networks are reachable from the Avi SE, set this knob to true. 19 clusterName: \u0026#34;GuzK8s\u0026#34; # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT 20 cniPlugin: \u0026#34;\u0026#34; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift 21 22### This section outlines the network settings for virtualservices. 23NetworkSettings: 24 ## This list of network and cidrs are used in pool placement network for vcenter cloud. 25 ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. 26 # nodeNetworkList: [] 27 nodeNetworkList: 28 - networkName: \u0026#34;Native-K8s-cluster\u0026#34; 29 cidrs: 30 - 192.168.0.0/24 # NODE network 31 subnetIP: \u0026#34;10.150.4.0\u0026#34; # Subnet IP of the vip network 32 subnetPrefix: \u0026#34;255.255.255.0\u0026#34; # Subnet Prefix of the vip network 33 networkName: \u0026#34;NativeK8sVIP\u0026#34; # Network Name of the vip network 34 35### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. 36L7Settings: 37 defaultIngController: \u0026#34;true\u0026#34; 38 l7ShardingScheme: \u0026#34;hostname\u0026#34; 39 serviceType: \u0026#34;ClusterIP\u0026#34; #enum NodePort|ClusterIP 40 shardVSSize: \u0026#34;SMALL\u0026#34; # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL 41 passthroughShardSize: \u0026#34;SMALL\u0026#34; # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL 42 43### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. 44L4Settings: 45 defaultDomain: \u0026#34;\u0026#34; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. 46 47### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. 48ControllerSettings: 49 serviceEngineGroupName: \u0026#34;k8s2se\u0026#34; # Name of the ServiceEngine Group. 50 controllerVersion: \u0026#34;20.1.1\u0026#34; # The controller API version 51 cloudName: \u0026#34;Default-Cloud\u0026#34; # The configured cloud name on the Avi controller. 52 controllerIP: \u0026#34;172.18.5.50\u0026#34; 53 54nodePortSelector: # Only applicable if serviceType is NodePort 55 key: \u0026#34;\u0026#34; 56 value: \u0026#34;\u0026#34; 57 58resources: 59 limits: 60 cpu: 250m 61 memory: 300Mi 62 requests: 63 cpu: 100m 64 memory: 75Mi 65 66podSecurityContext: {} 67 # fsGroup: 2000 68 69 70avicredentials: 71 username: \u0026#34;admin\u0026#34; 72 password: \u0026#34;PASSWORD\u0026#34; 73 74 75service: 76 type: ClusterIP 77 port: 80 78 79 80persistentVolumeClaim: \u0026#34;\u0026#34; 81mountPath: \u0026#34;/log\u0026#34; 82logFile: \u0026#34;avi.log\u0026#34; Remember that YAMLâ€™s are indentation sensitive.. so watch your spaces ðŸ˜‰\n**Deploy the AKO controller:\n**helm install ako/ako --generate-name --version 1.4.2 -f avi-values.yaml --namespace=avi-system\nOne can verify that it has been installed with the following command:\nhelm list --namespace=avi-system\nand\nKubectl get pods --namespace=avi-system\nNAMEÂ READYÂ STATUSÂ RESTARTSÂ AGE\nako-0Â 1/1Â RunningÂ 0Â 15h\nOne can also check the logs of the pod:\nkubectl logs --namespace=avi-system ako-0\nIf you experience a lot of restarts, go through your config again, I struggled a lot to get it running in my lab the first time after I figured out there were some configs I had to to. I suspected some issues with the pod itself, but the problem was on the NSX-ALB controller side and values.yaml parameters.\nNow to test out the automatic creation of SE and ingress for your frontend install an application and change the service type to use loadBalancer. Everything is automagically created for you. Monitor progress in the NSX-ALB Controller and vCenter.\nNSX-ALB deploys the SE OVAs, as soon as they are up and running they will be automatically configured and you can access your application through the NSX-ALB VIP IP. You can of ofcourse scale the amount of SEs and so on from within the Avi controller.\nThe ips on the right side is the pods, and my frontend/public facing IP is:\nWhich can also be received by using:\nkubectl get service --namespace=app\nNAMEÂ TYPEÂ CLUSTER-IPÂ EXTERNAL-IPÂ PORT(S)Â AGE\nfrontend-externalÂ LoadBalancerÂ 10.96.131.185Â 10.150.4.2Â 80:31874/TCPÂ 15h\nNow we have all the power and features from NSX Advanced LoadBalancer, with full visibility and logging. What about monitoring and features of Antra, the CNI plugin?\nInstall Octant - opensource dashboard for K8s https://github.com/vmware-tanzu/octant\nOctant with Antrea Plugin as a POD\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/octant-plugin-installation.md\nUpgrading the components used in this blog Upgrade Antrea Say you are running Antrea version v0.9.1 and want to upgrade to v0.10.1. Do a rolling upgrade like this:\nFirst out is the Antrea-Controller!\nkubectl set image deployments/antrea-controller antrea-controller=antrea/antrea-ubuntu:v0.10.1 --namespace=kube-system\nCheck status on upgrade:\nkubectl rollout status deployments/antrea-controller --namespace=kube-system\nThis upgrades the Antrea Controller, next up is the Antrea Agents.\nUpgrading the Antrea-agents:\nkubectl set image daemonset/antrea-agent antrea-agent=antrea/antrea-ubuntu:v0.10.1 --namespace=kube-system\nAs you are doing the rolling upgrades, one can monitor the process by following the pods or use Octant and get real-time updates when the agents have been upgraded.\nDoing changes in the antrea yaml file, if changes is not updated, do a rollut restart:\nkubectl rollout restart daemonset --namespace=kube-system antrea-agent\n","link":"https://blog.andreasm.io/2020/10/08/nsx-advanced-loadbalancer-with-antrea-on-native-k8s/","section":"post","tags":["kubernetes","avi","ako","antrea"],"title":"NSX Advanced LoadBalancer with Antrea on Native K8s"},{"body":"To be able to deploy an Edge node or nodes in your lab or other environment where you only have 2 physical nic you must be able to deploy it on a N-VDS switch as you have already migrated all your kernels etc to this one N-VDS switch.\nBut trying to do this from the NSX-T 2.4 manager GUI you will only have the option to deploy it to VSS or VDS portgroups, the N-VDS portgroups are not visible at all.\nSo, I followed this blog by Amit Aneja which explains how this works.\nSo after I read this blog post I sat out to try this. I had to write down the api-script he used by hand because I could not find it when I searched for a example I could use. By using PostMan I filled out this:\n1{ \u0026#34;resource\\_type\u0026#34;: \u0026#34;EdgeNode\u0026#34;, 2 3\u0026#34;display\\_name\u0026#34;: \u0026#34;YourEdgevCenterInventoryName\u0026#34;, \u0026#34;tags\u0026#34;: \\[\\], \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34;Â (Your edge MGMT IP adress) \\], \u0026#34;deployment\\_config\u0026#34;: { \u0026#34;vm\\_deployment\\_config\u0026#34;: { \u0026#34;placement\\_type\u0026#34;: \u0026#34;VsphereDeploymentConfig\u0026#34;, \u0026#34;vc\\_id\u0026#34;: \u0026#34;YourvCenterIDFromNSXTManager\u0026#34;, \u0026#34;management\\_network\\_id\u0026#34;: \u0026#34;YourLSPortGroupIDFromNSXTManager\u0026#34;, \u0026#34;default\\_gateway\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34; \\], \u0026#34;compute\\_id\u0026#34;: \u0026#34;YourClusterIDFromNSXTManager\u0026#34;, \u0026#34;allow\\_ssh\\_root\\_login\u0026#34;: true, \u0026#34;enable\\_ssh\u0026#34;: true, \u0026#34;hostname\u0026#34;: \u0026#34;yourEdge\\_FQDNName\u0026#34;, \u0026#34;storage\\_id\u0026#34;: \u0026#34;YourDataStoreIDfromNSXTManager\u0026#34;, \u0026#34;management\\_port\\_subnets\u0026#34;: \\[ { \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;YourEdgeIPMGMT\\_AddressAgain\u0026#34; \\], \u0026#34;prefix\\_length\u0026#34;: 24 } \\], 4 5\u0026#34;data\\_network\\_ids\u0026#34;: \\[ \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_OverLayVLAN(NotTheHostOverlayVLAN)\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34; \\] }, \u0026#34;form\\_factor\u0026#34;: \u0026#34;SMALL\u0026#34;, 6 7\u0026#34;node\\_user\\_settings\u0026#34;: { \u0026#34;cli\\_username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;root\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34;, \u0026#34;cli\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34; 8 9} } 10 11} Then POST it to your NSX-T manager from Postman and after a short blink, the Edge is deployed, and you have to add it as a transport node in the NSX-T manager. Here it is important that you do this right at once, because (as I found out) this is a one-time config GUI where the first time you will be able to choose the right fp-eth nics. If you try to edit the edge deployment a second time it switches back to only showing the VDS/VSS portgroups. Then you have to redeploy.\nExample screenshots:\nRemember that the Uplink VLANs will belong to their own N-VDS (which you have already defined in their respective Transport Zone) which will not be created on the host, but the Edges. The first N-VDS are already in place on the Hosts. Its only the last two NICs which will be on their own Edge N-VDS switches.\nI am not saying this is a best practice or the right way to do this, but it works in my lab environment so I can fully test out the latest NSX-T 2.4 and continue playing with PKS (when we get CNI plugins for NSX-T 2.4... are we there yet? are we there yet, are we there yet..... its hard to wait ;-) )\n","link":"https://blog.andreasm.io/2019/03/09/deploy-nsx-t-2.4-edge-nodes-on-a-n-vds-logical-switch/","section":"post","tags":["nsx"],"title":"Deploy NSX-T 2.4 Edge Nodes on a N-VDS Logical Switch"},{"body":"","link":"https://blog.andreasm.io/categories/troubleshooting/","section":"categories","tags":null,"title":"Troubleshooting"},{"body":"If you have missing objects in the firewall section before upgrading from NSX-T 2.1 to 2.2 you will experience a General Error in the GUI, on the Dashboard, and in the Firewall section of the GUI. You will even get general error when doing API calls to list the DFW sectionsÂ https://NSXMGRIP/api/v1/firewall/sections:Â { \u0026quot;module\\_name\u0026quot; : \u0026quot;common-services\u0026quot;, \u0026quot;error\\_message\u0026quot; : \u0026quot;General error has occurred.\u0026quot;, \u0026quot;details\u0026quot; : \u0026quot;java.lang.NullPointerException\u0026quot;, \u0026quot;error\\_code\u0026quot; : \u0026quot;100\u0026quot; }\nIf you have upgraded the fix is straight forward. Go to the following KB and dowload the attached jar file.\nUpload this jar file to the NSX-T manager by logging in with root and do a scp command from where you downloaded it. ex: \u0026quot;scp your\\_username@remotehost:nsx-firewall-1.0.jar /tmp\u0026quot;\nThen replace the existing file with the one from the kb article placed here: /opt/vmware/proton-tomcat/webapps/nsxapi/WEB-INF/lib#\nReboot\n","link":"https://blog.andreasm.io/2018/09/05/general-error-nsx-t-manager-firewall-section/","section":"post","tags":["distributed-firewall","nsx","troubleshooting"],"title":"\"General Error\" NSX-T Manager Firewall Section"},{"body":"","link":"https://blog.andreasm.io/tags/distributed-firewall/","section":"tags","tags":null,"title":"distributed-firewall"},{"body":"","link":"https://blog.andreasm.io/tags/troubleshooting/","section":"tags","tags":null,"title":"troubleshooting"},{"body":"","link":"https://blog.andreasm.io/categories/virtualization/","section":"categories","tags":null,"title":"Virtualization"},{"body":"","link":"https://blog.andreasm.io/tags/vsphere/","section":"tags","tags":null,"title":"vsphere"},{"body":"After upgrading vCenter (VCSA) to 6.0 U3 and upgrading the vSphere Replication Appliance to 6.1.2 the plugin in vCenter stops working. Follow this KB\nAnd this KB to enable SSH (for those who are unfamiliar with how to enable SSH in a *nix environment):\n","link":"https://blog.andreasm.io/2017/04/10/vsphere-replication-6.1.2-vcenter-plugin-fails-after-upgrade-vcenter-6.0-u3/","section":"post","tags":["troubleshooting","vsphere","vsphere-replication"],"title":"vSphere Replication 6.1.2 vCenter plugin fails after upgrade (vCenter 6.0 U3)"},{"body":"","link":"https://blog.andreasm.io/tags/vsphere-replication/","section":"tags","tags":null,"title":"vsphere-replication"},{"body":"Welcome to my blog.\nThis is the personal blog of Andreas Marqvardsen. I am currently a Tanzu Lead Solution Engineer @VMware and have been working in the IT industry since 2001. Joined VMware in 2019 as a Solution Engineer in the Networking and Security Business unit focusing on NSX and NSX Advanced LoadBalancer, joined Tanzu Business Unit in 2022 focusing on everything Kubernetes related.\nBefore joining VMware I worked as a consultant doing troubleshooting, design and implementation mostly around VMware solutions. In the beginning it was all about VMware Virtual Infrastructure, now vSphere, the typical hardware architecture Compute, Network and Storage to the whole software defined datacenter. The last years before joining VMware I focused mostly on design and implementation of NSX.\nAlways interested in new technology, understanding how the technology works and how it can be used. This blog is mostly around what I do in my line of work, but it can also reflect some other stuff I find interesting from time to time.\nAreas of interest: Kubernetes, Virtulization, Neworking, Security, Linux, OpenSource solutions, DiY projects, Home-Automation, Electronics..\nI created this blog page many years ago where one of the purposes was to use it as my \u0026quot;knowledge\u0026quot; base for my own use but also at the same time share it publicly for others if it can provide some useful information.\nAll views expressed on this site is solely my own personal views, not my employer or partners.\nContact:\n1E-mail: andreas.marqvardsen[at]gmail.com LinkedIn\n","link":"https://blog.andreasm.io/about/","section":"","tags":null,"title":"About"},{"body":"Lead Solution Engineer @VMware. Been working as an IT consultant since 2004. Where my key areas were delivering professional services on VMware solutions specializing in VMware vSphere, VMware NSX, VSAN, DR-solutions and Hybrid-Cloud. I think I have touched all products coming from VMware since 2003. Some of my certifications are VCIX-DCV, VCIX-NV.\nAreas of interest: Virtulization, Neworking, Security, Linux, OpenSource solutions, BSD, DiY projects, home-automation etc..\nI created this blog page for many years ago where one of the purposes was to use it as my \u0026quot;knowledge\u0026quot; base for different stuff I deploy in my lab. It primarily focuses around my line of work (solutions such as VMware NSX and Tanzu), but will also add other topics out of work scope (different DiY projects, home automation).\n* Wikis, howtos, tips/trick and troubleshooting issues mainly within my own field of competence, as a contribution back to the community for all the help they have given me over the years. Which has been, and still is, very appreciated. Sharing is actually caring.\nAll views expressed on this site is solely my own personal views, not my employer or partners.\n","link":"https://blog.andreasm.io/page/2016-07-14-about/","section":"page","tags":null,"title":"About"},{"body":"","link":"https://blog.andreasm.io/page/","section":"page","tags":null,"title":"Pages"},{"body":"","link":"https://blog.andreasm.io/search/","section":"","tags":null,"title":"Search"},{"body":"","link":"https://blog.andreasm.io/series/","section":"series","tags":null,"title":"Series"}]