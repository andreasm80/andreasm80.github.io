[{"body":"","link":"https://blog.andreasm.io/tags/availability/","section":"tags","tags":null,"title":"availability"},{"body":"","link":"https://blog.andreasm.io/categories/","section":"categories","tags":null,"title":"Categories"},{"body":"","link":"https://blog.andreasm.io/","section":"","tags":null,"title":"From 0.985mhz... to several Ghz"},{"body":"","link":"https://blog.andreasm.io/categories/kubernetes/","section":"categories","tags":null,"title":"Kubernetes"},{"body":"","link":"https://blog.andreasm.io/categories/networking/","section":"categories","tags":null,"title":"Networking"},{"body":"","link":"https://blog.andreasm.io/tags/nsx/","section":"tags","tags":null,"title":"nsx"},{"body":"","link":"https://blog.andreasm.io/post/","section":"post","tags":null,"title":"Posts"},{"body":"","link":"https://blog.andreasm.io/tags/","section":"tags","tags":null,"title":"Tags"},{"body":"","link":"https://blog.andreasm.io/tags/tanzu/","section":"tags","tags":null,"title":"tanzu"},{"body":"","link":"https://blog.andreasm.io/categories/tanzu/","section":"categories","tags":null,"title":"Tanzu"},{"body":"vSphere with Tanzu and Multi-Zone This post will be a brief introduction of how we can deploy and configure vSphere with Tanzu on three vSphere clusters (vSphere Zones) to achieve better availability of the TKG clusters and Supervisor controlplane nodes. This feature came with Sphere 8, not available in vSphere 7. There are some pre-reqs that needs to be in place for this to work in addition to the pre-reqs for deploying vSphere with Tanzu on a single vSphere cluster. I will list them below later.\nFrom the official VMware documentation:\nYou can deploy a Supervisor on three vSphere Zones to provide cluster-level high-availability that protects your Kubernetes workloads against cluster-level failure. A vSphere Zone maps to one vSphere cluster that you can setup as an independent failure domain. In a three-zone deployment, all three vSphere clusters become one Supervisor. You can also deploy a Supervisor on one vSphere cluster, which will automatically create a vSphere Zone and map it to the cluster, unless you use a vSphere cluster that is already mapped to a zone. In a single cluster deployment, the Supervisor only has high availability on host level that is provided by vSphere HA.\nOn a three-zone Supervisor you can run Kubernetes workloads on Tanzu Kubernetes Grid clusters and VMs created by using the VM service. A three-zone Supervisor has the following components:\nSupervisor control plane VM. Three Supervisor control plane VMs in total are created on the Supervisor. In a three-zone deployment, one control plane VM resides on each zone. The three Supervisor control plane VMs are load balanced as each one of them has its own IP address. Additionally, a floating IP address is assigned to one of the VMs and a 5th IP address is reserved for patching purposes. vSphere DRS determines the exact placement of the control plane VMs on the ESXi hosts part of the Supervisor and migrates them when needed. Tanzu Kubernetes Grid and Cluster API. Modules running on the Supervisor and enable the provisioning and management of Tanzu Kubernetes Grid clusters. Virtual Machine Service. A module that is responsible for deploying and running stand-alone VMs and VMs that make up Tanzu Kubernetes Grid clusters. Planning and deployment of a three-zone Supervisor Before heading into the actual deployment, some pre-requirements needs to be in place. I will go through the important ones below such as storage, network and user-roles.\nvSphere ESXi clusters A deployment of a three-zone Supervisor may also be referred to as multi-zone. This can maybe lead to the understanding that we may deploy vSphere with Tanzu multi-zone on 2 vSphere ESXi clusters or even 4 or higher number of vSphere clusters. The only number of vSphere clusters supported in a multi-zone Supervisor deployment (three-zone) is 3 vSphere clusters.\nFrom the official documentation:\nCreate three vSphere clusters with at least 2 hosts. For storage with vSAN, the cluster must have 3 or 4 hosts. Configure storage with vSAN or other shared storage solution for each cluster. Enable vSphere HA and vSphere DRS on Fully Automate or Partially Automate mode. Configure each cluster as an independent failure domain. Configure networking with NSX or vSphere Distributed Switch (vDS) networking for the clusters. Network To be able to deploy a three-zone Supervisor an important requirement on the networking side is that all vSphere cluster to be used is connected to the same VDS. One VDS shared across all three vSphere clusters. From the official documentation:\nIn a three-zone Supervisor configured with NSX as the networking stack, all hosts from all three vSphere clusters mapped to the zones must use be connected to the same VDS and participate in the same NSX Overlay Transport Zone. All hosts must be connected to the same L2 physical device.\nNot sure what is meant with connecting to the same L2 physical device though... But anyhow, this constraint can be a limiting factor and something to be aware of. Some explanations:\nIf you have an NSX environment with one common Overlay transportzone across all vSphere clusters, but configured with three individual VDS switches (1 specific VDS pr vSphere cluster) it is currently not supported. Having individual VDS switches pr cluster allows you to have only relevant portgroups in your vSphere clusters that are configured specifically for their respective clusters network, different vlans, placed in different racks and there is no L2 between them, only L3. The different vSphere clusters can be on different racks with different ToR switches, in a Spine-Leaf topology, different subnets/vlans configured for mgmt, vmotion, vSAN, Geneve tunnel VLAN, VM network and so on.\nAn example when having a dedicated VDS pr vSphere cluster:\nthe illustration above indicates that we have portgroups defined that is valid for the VLAN trunks configured to the attached ToR switches with the corresponding VLAN trunks. So there is no risk of using any of the portgroups defined in the respective VDS in any of the above two vSphere clusters to use VLAN portgoups that has not been defined in the ToR switches trunk-ports. That means you will not end up with any unrelevant portgroups not valid for the respective cluster they are being used in. All this depends on the vlan trunks configured in the ToR switches. There is no need to have portgroups spanning across different vSphere clusters with unrelevant vlan tags. They will just end up not getting anywhere, and create a lot of \u0026quot;noise\u0026quot; for the admin managing these portgroups.\nThe example below illustrates one shared VDS across all vSphere clusters, where we have differentiating ToR VLAN trunks, configured pr rack.\nAs we are using the same VDS across all clusters, all portgroups defined in this shared VDS is accessible and visible for all ESXi hosts participating in this VDS. This means they can easily be used, but they may not be the correct portgroups to be used for this cluster as the underlaying ToR switches may not have the correct vlan trunks for the ESXi hosts uplinks (pNICs).\nThat is just something to be aware of. If your ESXi hosts have been configured with more than two pNICs it is also possible to have one dedicated VDS pr cluster for services specificially for the respective ESXi clusters and one shared for services that are configured identically across the clusters.\nStorage From the official documentation:\nWhen you prepare storage resources for the three-zone Supervisor, keep in mind the following considerations:\nStorage in all three zones does not need to be of the same type. However, having uniform storage in all three clusters provides a consistent performance. For the namespace on the three-zone Supervisor, use a storage policy that is compliant with the shared storage in each of the clusters. The storage policy must be topology aware. Do not remove topology constraints from the storage policy after assigning it to the namespace. Do not mount zonal datastores on other zones. A three-zone Supervisor does not support the following items: Cross-zonal volumes vSAN File volumes (ReadWriteMany Volumes) Static volume provisioning using Register Volume API Workloads that use vSAN Data Persistence platform vSphere Pod vSAN Stretched Clusters VMs with vGPU and instance storage Permissions From the official documentation:\nvSphere Namespaces required privileges:\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Allows disk decommission operations Allows for decommissioning operations of data stores. Data stores Namespaces.ManageDisks Backup Workloads component files Allows for backing up the contents of the etcd cluster (used only in VMware Cloud on AWS). Clusters Namespaces.Backup List accessible namespaces Allows listing the accessible vSphere Namespaces. Clusters Namespaces.ListAccess Modify cluster-wide configuration Allows modifying the cluster-wide configuration, and activating and deactivating cluster namespaces. Clusters Namespaces.ManageCapabilities Modify cluster-wide namespace self-service configuration Allows modifying the namespace self-service configuration. Clusters (for activating and deactivating)Templates(for modifying the configuration)vCenter Server(for creating a template) Namespaces.SelfServiceManage Modify namespace configuration Allows modifying namespace configuration options such as resource allocation and user permissions. Clusters Namespaces.Manage Toggle cluster capabilities Allows manipulating the state of cluster capabilities (used internally only for VMware Cloud on AWS). Clusters NA Upgrade clusters to newer versions Allows initiation of the cluster upgrade. Clusters Namespaces.Upgrade vSphere Zones Privileges\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Attach and Detach vSphere objects for vSphere Zones Allows for adding and removing clusters from a vSphere Zone. Clusters Zone.ObjectAttachable Create, Update and Delete vSphere Zones and their associations Allows for creating and deleting a vSphere Zone. Clusters Zone.Manage Supervisor Services Privileges\nPrivilege Name in the vSphere Client Description Required On Privilege Name in the API Manage Supervisor Services Allows for creating, updating, or deleting a Supervisor Service. Also allows for installing a Supervisor Service on a Supervisor, and creating or deleting a Supervisor Service version. Clusters SupervisorServices.Manage Deployment time Now that we have gone through the requirements I will start det actual installation/deployment steps of vSphere with Tanzu in a three-zone setup. This post is assuming an already configured and working vSphere environment and NSX installation. Will only focus on the actual deployment of vSphere with Tanzu in a three-zone setup.\nThis is how my environment looks like on a vSphere level before enabling the three-zone Supervisor:\nMy vSphere cluster consists of three vSphere cluster with 4 esxi hosts each. They are all connected to the same Distributed Virtual Switch, but with different portgroups as their backend vlans is different between the clusters. The only common portgroups they share is all the overlay segments created from NSX (they are all part of the same Overlay TransportZone). Each cluster has its own VSAN datastore, not shared or stretched between the clusters. VSAN local to every vSphere cluster.\nEnable three-zone Supervisor vSphere Zones Before I can go ahead and enable a three-zone Supervisor I need to create the vSphere Zones which is done here:\nClick on the vCenter in the inventory three -\u0026gt; Configure \u0026gt; vSphere Zones. From there I need to create three zones representing my three vSphere clusters. Click Add New vSphere Zone:\nThen select the cluster it should apply to and finish. The end result looks like this:\nStorage policy Now I need to create a storage policy for the Supervisor. This policy needs to be a zonal policy. Head over to Policies and Profiles here:\nClick on VM Storage Policies:\nAnd click Create:\nGive it a name:\nI am using VSAN so in the next step (2.) I select rules for VSAN storage and the important bit needed for three-zone deployment is the Enable consumption domain\nThen on step 3. I will leave it default unless I have some special VSAN policies I want to apply.\nUnder step 4 the Storage topology type is Zonal\nIn step 5 all the vSAN storages should be listed as compatible\nThen it is just the review and finish\nNow that the storage policy is in place next up is the actual deployment of the three-zone Supervisor.\nDeploy the three-zone Supervisor When all the pre-requirements have been done, the actual enablement of the Supervisor is not so different from a regual Supervisor enablement, but let us go through the steps anyway.\nHead over to Workload Management\nStart the deployment wizard, I am using NSX so I am selecting NSX as the network stack\nIn step 2 I select vSphere Zone Deployment, select all my vSphere Zones, the give the Supervisor a name.\nNotice Step 2 is the only step that is done different from a single-zone deployment.\nIn step I select my newly created \u0026quot;zonal\u0026quot; policy, notice that we only have to select the Control Plane Storage Policy\nThen in step it is the Supervisor management network configurations, here I am using my ls-mgmt NSX overlay segment which is common/shared across all esxi hosts/vSphere cluster.\nIn step 5 its the Supervisor Workload network configurations.\nThen in the last step its finish time, or put in a dns name for the supervisor k8s api endpoint.\nIt should start to deploy as soon as clicking the finish button:\nNow let us head back to vCenter inventory view and check whats going on there.\nAs I can see from the screenshot above, the three supervisors will be distributed across my three vSphere clusters, one Supervisor pr vSphere cluster.\nThis conclues the enabling of a three-zone Supervisor cluster. Next step is to deploy your TKC or guest clusters.\nDeploy a TKC/guest cluster in a three-zone Deploying a TKC cluster in a three-zone is not any different than a single zone, but we need to create storage policy for the workload cluster to be used for deployment.\nThe storage policy I have created look like this and is applied on the vSphere Namespace I create. I will quickly go through the step below:\nThats it, the same policy configuration used for the Supervisor deployment. Then it is all about creating a vSphere Namespace and apply a cluster to it.\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-cluster-small-cidr 5 namespace: test-small-subnet 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 2 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-small #machineclass, get the available classes by running \u0026#39;k get virtualmachineclass\u0026#39; in vSphere ns context 32 - name: storageClass 33 value: all-vsans And the result should the same as for the Supervisor cluster, the workload clusters control plane and worker nodes should be distributed across your vSphere cluster.\nHappy deployment\n","link":"https://blog.andreasm.io/2023/05/03/vsphere-with-tanzu-multi-three-zones-and-nsx/","section":"post","tags":["availability","nsx","tanzu"],"title":"vSphere with Tanzu, Multi (three) Zones and NSX"},{"body":"","link":"https://blog.andreasm.io/tags/network/","section":"tags","tags":null,"title":"network"},{"body":"Tanzu with vSphere using NSX with multiple T0s In this post I will go through how to configure Tanzu with different NSX T0 routers for IP separation use cases, network isolation and multi-tenancy. The first part will involve spinning up dedicated NSX Tier-0 routers by utlizing several NSX Edges and NSX Edge Clusters. The second part will involve using NSX VRF-Tier0. Same needs, two different approaches, and some different configs in NSX.\nSome background to why this is a useful feature: In vSphere with Tanzu with NSX we have the option to override network setting pr vSphere Namespace. That means we can place TKC/Workload clusters on different subnets/segments for ip separation and easy NSX Distributed Firewall policy creation (separation by environments DEV, TEST, PROD etc), but we can also override and define separate NSX Tier-0 routers for separation all the way out to the physical infrastructure. In some environments this is needed as there is guidelines/policies for certain network classifications to be separated, and filtered in physical firewall/security perimeters. Although NSX comes with a powerful and advanced distributed firewall, including Gateway firewall (on Tier1 and Tier0) there is nothing in the way for NSX to be combined in such environments, it just allows for more granular firewall policies before the traffic is eventually shipped out to the perimeter firewalls.\nThe end-goal would be something like this (high level):\nBefore jumping into the actual configuration of this setup I will need to explain a couple of things, as there are some steps that needs to be involved in getting this to work. Lets continue this discussion/monologue in the chapter below.\nRouting considerations Tanzu with vSphere consists of a Supervisor Controlplane VM cluster (3 nodes). These three are configured with two network interfaces, interface ETH0 is placed in the management network. This management network can be NSX overlay segments, or regular VLAN backed segments or VDS/vSwitch portgroups. Its main responsibility is to communicate with the vCenter server API for vm/node creation etc. The second interface (ETH1) on these Supervisor Controlpane VMs is the Workload network. This network's responsibilities are among a couple of things the Kubernetes API endpoint where we can interact with the Kubernetes API to create workload clusters, and here is an important note to remember: It is also used to communicate with the workload clusters being created. If there is no communication from this network to the workload cluster being created, workload cluster creation will fail. So needles to say, it is important that this communication works. When we deploy or enable the Workload Control Plane/Supervisor cluster on a fresh vCenter server and with NSX as the networking stack we are presented with a choice how the workload network can be configured, and the option I am interested in here is the NAT or no NAT option.\nThis option is very easy to select (selected by default) and very easy to deselect, but the impact it does is big. Especially when we in this post are discussing TKG with multi-Tier-0/VRF-T0.\nWhat this option does is deciding whether the workload network in your vSphere Namespace will be routed or not. Meaning that the controlplane and worker nodes actual ip addresses will be directly reachable and exposed (routed) in your network or if it will be masked by NAT rules so they will not be exposed with their real ip addresses in the network. The same approach as a POD in Kubernetes, default it will be Source NAT'ed through its worker node it is running on when it is doing egress traffic (going out and wants to make contact with something). So with NAT enabled on the workload network in a vSphere Namespace the workers will not be using their real ip addresses when they are communicating out, they will be masked by a NAT rule created in NSX automatically. This is all fine, if you want to use NAT. But if you dont want to use NAT you will have to disable this option and prepare all your vSphere Namespace workoad networks to be of ip subnets you are prepared to route in your network. That is also very fine. And as long as you dont expect to be running 500000+ worker nodes you will have available routed RFC1918 addresses to your disposal.\nThe reason I would like to touch upon the above subject is that it will define how we create our routing rules between a Supervisor cluster a vSphere Namespace workload network with NAT enabled or not.\nNow I get to the part where it really get interesting. If you decide to use the NAT'ed approach and NSX VRF-T0 when you create your first vSphere Namespace (not the default inital system vSphere Namespace, but the first vSphere Namespace you can create after WCP has been installed) NCP (NSX Container Plugin) will automatically create a couple of static routes so the Supervisor Cluster workload network can reach the workload networks in your vSphere Namespace placed under a different Tier-0 than the default workload network is placed, here a VRF Tier-0. These routes will be defined with VRF Route Leaking, meaning they will not go out of its parent Tier-0 to reach certain subnets. And with a NAT enabled workload cluster that is perfect as the we dont have care about the NAT rules created on the workload network, they can talk to each other with their real ip addresses. I will explain this a bit later on. Well that is fine and all, but sometimes VRF Tier-0 is not the best option, we need to use dedicated Tier-0 routers on different NSX edge cluster/edges then there is no automatic creation for these static routes. But where and how do I define these static routes manually? On the Tier-0s themselves? In the physical routers the Tier-0s are peering with (using static or BGP)? Yes, both options are possible. If we decide to create these rules in the Tier-0s themselves we need to create a linknet between the Tier-0s to they can point to each other with their respective subnet routes (can be a regular overlay segment used as L2 between the different Tier-0s).\nBut, there is always a but üòÑ. With a NAT enabled vSphere Namespace, the workload network is not allowed to use its own IP address when going out of the Tier-0, it will be using an IP address from the egress subnet defined, or will it? First, when NAT is enabled on a vSphere Namespace, NCP will create a rule in NSX saying that if you want to talk to your other workload cluster network buddies, you dont have to hide your true identity, you are allowed to use your real name when you want to talk to your buddies. See below for how these rules looks like:\nWait a minute, what does this mean then? Exactly, this means that when it communicates with the other vSphere Namespace network destinations IP/Subnets (see above) it will not be NAT'ed. It will use its real ip address. And that ladies and gentlemen is why creating the static routes directly between the Tier-0s using a linknet between is an easy solution when using NAT in the vSphere Namespace network. Why, how etc, still dont understand? Ok, let me first show you an example below of two such static routes.\nThe above screenshot illustrates a static route defined on the Tier-0 where the Supervisor workload network is placed below behind a Tier-1 gateway. The ip subnet is the workload network cidr/subnet of a vSphere Namespace placed on a different Tier-0 router (its actual ip subnet, not the NAT ip addresses).\nThe above static route is created on the second Tier-0 where the vSphere Namespace number 2 is created and placed. This route is pointing to the actual IP subnet/cidr of the Supervisor workload network, not the NAT address. Just to mention it, common for both these routes is that they are using the linknet interfaces of the Tier-0s respectively. But how? We have this no SNAT rule, but at the same time we have a route-map denying any segment used by a NAT enable vSphere Namespace workload network to be advertised/exposed outside the Tier-0. Yes, exactly, outside the Tier-0. By using the linknet between the Tier-0s for our static routes we are simply telling the Tier-0 routers to go to their respective Tier-0 when it needs to find the respective vSphere Namespace workload networks. The idea here is that the Tier-0s are the \u0026quot;all-seeing-eye\u0026quot; and they truly are. All the T1 gateways configured by NCP/WCP will enable 4 route advertisements, all connected segments, NAT IPs, LB VIPS and Static Routes. This means that all the Tier1 gateways will happily tell the Tier-0 about their networks/subnets/ip addresses they know of to the Tier-0. So when we just create a static route like the two above (e.g 10.101.82.32/27 via linknet interface) the Tier-0 router that is defined as next-hop know exactly where this network is located, which T1 gateway it needs to send it to regardless of NAT rules and route-maps.\nNice, right? This means we dont need to interfere with these static routes in the physical routers, they were supposed to be NAT'ed right? So we dont want them in our physical routers anyway. This traffic will then never leave the \u0026quot;outside\u0026quot; of the Tier-0s via their uplinks connected to their respective BGP/Upstream routers as we are using the \u0026quot;linknet\u0026quot; between them. This will be true as long as the destination subnet is for the subnets defined in the no-nat rules and we have defined the static routes to go over the linknet.\nTo further show evidence of how this looks, let us do a \u0026quot;traffic walk\u0026quot; from the Supervisor Cluster Control Plane vm's workload network to the other workload cluster networks on the second Tier-0 router. First an attempt to draw this:\nThe diagram above tries to illustrate what the actual IP address being used in regards of the destination subnet. If we quickly take a look at the no-snat rules created in NSX again:\nThe first 4 no-snat rules tells the Tier-1 router to NOT do any SNAT on the source IP if the source subnet happens to be 10.101.80/23 (SVC workload network) and the destination happens to be any of the four defined subnets in each NO-SNAT rules (10.101.80/23 itself, 10.101.82.0/24, 10,101.90.0/24 its own ingress cidr, and 10.101.83.0/24).\nSo if I do a tcpdump on one of my workload clusters control plane nodes, and fitering on ip coming from the supervisor control planes workload network. Will I then see the NATed address or will I see their real IP?\n1vmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo tcpdump src net 10.101.80.0/27 2tcpdump: verbose output suppressed, use -v or -vv for full protocol decode 3listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 412:39:06.351632 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1537746084:1537746122, ack 2981756064, win 6516, options [nop,nop,TS val 2335069578 ecr 3559626446], length 38 512:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 91, win 6516, options [nop,nop,TS val 2335069583 ecr 3559634990], length 0 612:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 7589, win 6477, options [nop,nop,TS val 2335069583 ecr 3559634990], length 0 712:39:06.356621 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 38:73, ack 7589, win 6516, options [nop,nop,TS val 2335069584 ecr 3559634990], length 35 812:39:06.400033 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 7620, win 6516, options [nop,nop,TS val 2335069626 ecr 3559634991], length 0 912:39:06.794326 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [S], seq 2927440799, win 64240, options [mss 1460,sackOK,TS val 2841421479 ecr 0,nop,wscale 7], length 0 1012:39:06.797334 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 907624276, win 502, options [nop,nop,TS val 2841421483 ecr 3559635431], length 0 1112:39:06.797334 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [F.], seq 0, ack 1, win 502, options [nop,nop,TS val 2841421483 ecr 3559635431], length 0 1212:39:06.800095 IP 10.101.80.2.59524 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 2, win 502, options [nop,nop,TS val 2841421486 ecr 3559635434], length 0 1312:39:06.809982 IP 10.101.80.3.58013 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1460837974:1460838009, ack 2147601163, win 1312, options [nop,nop,TS val 1221062723 ecr 4234259718], length 35 If I filter on port 6443:\n1vmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo tcpdump port 6443 2tcpdump: verbose output suppressed, use -v or -vv for full protocol decode 3listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 412:40:57.817560 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1537768767:1537768802, ack 2982048853, win 6516, options [nop,nop,TS val 2335181040 ecr 3559745035], length 35 512:40:57.817560 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 35:174, ack 1, win 6516, options [nop,nop,TS val 2335181041 ecr 3559745035], length 139 612:40:57.817643 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [.], ack 35, win 501, options [nop,nop,TS val 3559746454 ecr 2335181040], length 0 712:40:57.817677 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [.], ack 174, win 501, options [nop,nop,TS val 3559746454 ecr 2335181041], length 0 812:40:57.819719 IP wdc-vrf-cluster-1-ls6ct-5pgdf.6443 \u0026gt; 10.101.80.2.34116: Flags [P.], seq 1:90, ack 174, win 501, options [nop,nop,TS val 3559746456 ecr 2335181041], length 89 912:40:57.864302 IP 10.101.80.2.34116 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [.], ack 90, win 6516, options [nop,nop,TS val 2335181086 ecr 3559746456], length 0 1012:40:58.590194 IP 10.101.92.3.4120 \u0026gt; wdc-vrf-cluster-1-ls6ct-5pgdf.6443: Flags [P.], seq 1810063827:1810063865, ack 3070977968, win 5820, options [nop,nop,TS val 1030353737 ecr 937884951], length 38 What do I see? I see the source address being the real IP addresses from the Supervisor Control Plane VMs (10.101.80.x). I also see 10.101.92.3 which happens to be the workload clusters own ingress.\nAfter these static routes have been added I also just want to do a traceroute from the supervisor vm to the workload cluster control plane node to show how I have altered the next hops it will use to get there:\n1root@422080039f397c9aa239cf40e4535f0d [ ~ ]# traceroute -T -p 22 -i eth1 10.101.82.34 2traceroute to 10.101.82.34 (10.101.82.34), 30 hops max, 60 byte packets 3 1 _gateway (10.101.80.1) 0.265 ms 0.318 ms 0.295 ms 4 2 100.64.0.4 (100.64.0.4) 1.546 ms 1.518 ms 1.517 ms 5 3 10.101.240.13 (10.101.240.13) 9.603 ms 9.551 ms 9.568 ms 6 4 * * * 7 5 10.101.82.34 (10.101.82.34) 10.378 ms 10.296 ms 10.724 ms Now if I do the same traceflow from the same Supervisor vm, but to the workload cluster's vip (10.101.92.0/24) which is exposed via BGP...\n1root@422080039f397c9aa239cf40e4535f0d [ ~ ]# traceroute -T -p 22 -i eth1 10.101.92.3 2traceroute to 10.101.92.3 (10.101.92.3), 30 hops max, 60 byte packets 3 1 _gateway (10.101.80.1) 0.229 ms 0.263 ms 0.237 ms 4 2 100.64.0.4 (100.64.0.4) 1.025 ms 1.006 ms 0.972 ms 5 3 10.101.4.1 (10.101.4.1) 1.405 ms 1.408 ms 1.524 ms 6 4 10.101.7.13 (10.101.7.13) 2.123 ms 2.037 ms 2.019 ms 7 5 10.101.92.3 (10.101.92.3) 2.562 ms 2.197 ms 2.499 ms It will not take the same route to get there. It will actually go all the way out to the physical BGP peers, and then over to the second Tier-0 router.\nBut if I do the same traceroute from my workload cluster nodes, traceroute the Supervisor workload VIP (which is also exposed via BGP)?\n1vmware-system-user@wdc-vrf-cluster-1-ls6ct-5pgdf:~$ sudo traceroute -T -p 22 10.101.90.2 2traceroute to 10.101.90.2 (10.101.90.2), 30 hops max, 60 byte packets 3 1 _gateway (10.101.82.33) 0.494 ms 1.039 ms 1.015 ms 4 2 100.64.0.0 (100.64.0.0) 1.460 ms 1.451 ms 1.441 ms 5 3 10.101.240.10 (10.101.240.10) 3.120 ms 3.108 ms 3.099 ms 6 4 10.101.90.2 (10.101.90.2) 3.115 ms 3.105 ms 3.095 ms It will go over the Tier-0 linknet. Why, because on the second Tier-0 router I have created two static routes pointing to both the Supervisor workload cluster subnet and Supervisor VIP altering the next-hops. See below:\nAnd also, did I mention that there will be created a route-map on the Tier-0 for the vSphere Namespace Networks with corresponding IP-prefix lists prohibiting the workload networks ip subnets to be advertised through BGP/OSPF from the Tier-0 and its upstream bgp neigbours.\nHow will this go then, if we cant by any reason use a linknet between the Tier-0s for these static routes and need to define them in the physical routers? Well that is an interesting question. Let us try to dive into that topic also. Did I mention that we can also decide to disable NAT completely? Well that is also a perfectly fine option. This could also give other benefits for the environments where it is is needed to have these routes in the physical network due to policies, requirements etc. We can create much more granular firewall policies in the perimeter firewalls when we know each node will egress with their actual IP instead of being masked by a NAT ip address. If being masked by a NAT ip address we cant for sure really know which node it is, we can only know that it potentially comes from any node in a vSphere Namespace where this egress subnet is defined (and that can potentially be a couple). Remember how the SNAT rules look like (maybe not as I haven't shown a screenshot of it yet üòÑ)?\nAlso, we dont need to create any static routes, it will be auto advertised by BGP (if using BGP) each time we create a new vSphere Namespace.\nBut, there is a possibilty to still use static routes, or inject them via BGP in the physical network. We need to define the static routes with the correct subnets, pointing to the NAT'ed vSphere Namespace workload network and egress subnet/cidr. So in my physical router I would need to create these rules:\n1ip route 10.101.80.0/23 10.101.4.10 #Workload network Default vSphere Namespace via Tier-0 Uplink(s) 2ip route 10.101.91.0/24 10.101.4.10 #Egress cidr Default vSphere Namespace via Tier-0 Uplink(s) These are the needed static routes for the Supervisor Workload and egress network to be configured in the physical router, if you happen to create a vSphere Namespace which is not NAT'ed these are the only routes needed. If your other vSphere Namespace is also NAT'ed you need to create the routes accordingly for this Namespace also.\nIllustration of static routes in the physical network between two Tier-0s and two NAT enabled vSphere Namespaces:\nIf using routed or a NAT disabled vSphere Namespace, the life will be much easier if using BGP on your Tier-0s.\nBe aware There was a reason you decided to use NAT? When defining these static routes the network which are supposed to not be routed outside the Tier-0s will now suddenly be routed and exposed in your network. Is that something you want? Then you maybe should opt for no NAT anyway?\nTip Some notes on BFD with NSX and BGP/Static routes here\nThis part can be a bit confusing if not fully understanding how network traffic works in Tanzu with vSphere and NSX. Hopefully I managed to explain it so its more understandable.\nAs long as we are only using just one Tier-0 router for all our vSphere Namespaces, regardless of how many different subnets we decide to create pr vSphere Namespace they will be known by the same Tier-0 as the Tier-1 will be default configured to advertise to the Tier-0 its connected networks, yes it also advertise NAT IPs and LoadBalancer IPs but these are also configured on the Tier-0 to be further advertised to the outside world. Its only the Tier-0 that can be configured with BGP, as it is only the Tier-0 that can be configured to talk to the outside world (external network) by a SR T0 using interfaces on the NSX edges (VM or Bare-Metal). This means there is no need for us to create any routes either on the Tier-1 or Tier-0 when creating different vSphere Namespaces with different subnets. But I would not have anything to write about if we just decided to use only one Tier-0 router, would I? üòÅ\nNow when all this is clear as day, let us head over to the actual installation and configuration of this.\nNSX and Tanzu configurations with different individual Tier-0s I assume a fully functional Tanzu environment is running with the default Workload network configured with NAT and NSX is the networking component. For this exercise I have prepared my lab to look like this \u0026quot;networking wise\u0026quot;:\nI will deploy two new vSphere Namespaces where I select override Supervisor Network and choose the Tier-0-2 which is another Tier-0 router than my Supervisor workload network is on (this is using the first Tier-0 router)\nIn my lab I use the following IP addresses for the following components:\nNetwork overview Tanzu Management network: 10.101.10.0/24 - connected to a NSX overlay segment - manually created by me Tanzu Workload network (the default Workload network): 10.101.80.0/23 (could be smaller) - will be created automatically as a NSX overlay segment. Ingress: 10.101.90.0/24 Egress: 10.101.91.0/24 I am doing NAT on this network (important to have in mind for later) The first Tier-0 has been configured to use uplinks on vlan 1014 in the following cidr: 10.101.4.0/24 The second Tier-0 (Tier-0-2) will be using uplink on vlan 1017 in the follwing cidr: 10.101.7.0/24 Second vSphere Namespace - will be using Tier-0-2 and NAT disabled Second vSphere Namespace Workload network: 10.101.82.0/24 Second vSphere Namespace ingress network: 10.101.92.0/24 Third vSphere Namespace - will be using Tier-0-2 and NAT enabled Third vSphere Namespace Workload network: 10.101.83.0/24 Third vSphere Namespace ingress network: 10.101.93.0/24 Third vSphere Namespace egress network: 10.101.94.0/24 (where the NAT rules will be created) Using dedicated Tier-0 means we need to deploy additional edges, either in the same NSX edge cluster or a new edge cluster. This can generate some compute and admin overhead. But in some environments its not \u0026quot;allowed\u0026quot; to share two different network classifications over same devices. So we need separate edges for our different Tier-0s. But again, with TKGs we cannot deploy our TKC clusters on other vSphere clusters than our Supervisor cluster has been configured on, so the different TKC cluster will end up on the same shared compute nodes (ESXi). But networking wise they are fully separated.\nDeploy new Edge(s) to support a new Tier-0 As this is my lab, I will not deploy redundant amount of Edges, but will stick with one Edge just to get connectivity up and working. NSX Edge do not support more than 1 SR T0 pr Edge, so we need 1:1 mapping between the SR T0 and Edge. And take into consideration if running this in production we must accommodate potential edge failovers, so we should atleast have two edges responsible for a T0. If running two Tier-0 in an edge cluster we should have 4 edges (if one of them fail).\nThe first thing we need to do is to deploy a new Edge vm from the NSX manager. The new edge will be part of my \u0026quot;common\u0026quot; overlay transportzone as I cant deploy any TKC cluster on other vSphere clusters than where my Supervisor cluster has been enabled. For the VLAN transportzones one can reuse the existing Edge vlan transportzone and the same profile so they get their correct TEP VLAN. For the Uplinks it can be same VLAN trunkport (VDS or NSX VLAN segment) if the vlan trunk range includes the VLAN for the new T0 uplink.\nSo my new edge for this second T0 will be deployed like this:\nAfter the Edge has been deployed its time to create a Edge cluster. Now we need to create a new segment for the coming new Tier-0 router and the Tier-0 linknet:\nThe first segment has been configured to use the edge-vlan-transportzone, and this vlan will be used to peer with the upstream router. The second segment is just a layer 2 overlay segment to be used for link between the two Tier-0s.\nNow we can go ahead and create the new Tier-0:\nGive the Tier-0 a name, select your new Edge cluster. Save and go back to edit it. We need to add the two interfaces uplink to upstream router and link interface between the two tier-0s: Give the interfaces a name, IP address and select the segments we created above for the new Tier-0 uplinks. Select the Edge node and Save Now we have the interfaces, to test if it is up and running you can ping it from your upstream router. The only interface that can be reached is the uplink interface 10.101.7.13. Next configure BGP and the BGP peering with your upstream router:\nThe last thing we need to do in our newly created Tier-0 is to create two static routes that can help us reach the Workload Network on the Supervisor Control Plane nodes on their actual IP addresses (remember our talk above?). On the newly created Tier-0 (Tier-0-2) click on Routing -\u0026gt; Static Routes and add the following route (Supervisor workload network):\nThe two routes created is the Supervisor workload network cidr and the actual ingress vip /32.\nAnd the next-hop is defined with the ip of the other (first) Tier-0 interface on the \u0026quot;linknet\u0026quot; interface between the T0s (not configured on the first Titer-0 yet):\nAdd and Save.\nNow on the first Tier-0 we need a second interface (or two depending on the amount of edges) in the linknet we created earlier.\nName it, ip address Select the edge(s) that will have this/these new interface(s). Save.\nNext up is the route: These route should point to the vSphere workload network cidrs we defined when we created the vSphere Namespaces. The correct cidr is something we get when we create the vSphere Namespace (it is based on the Subnet prefix you configure)\nAnd next-hop (yes you guessed correct) is the linknet interface on the new Tier-0.\nI create the static routes for both the vSphere Namespaces, and I know that it will start using the second /27 subnet in the /24 workload network cidr for the first cluster in each namespace.\nSo we should have something like this now:\nAs mentioned above, these routes is maybe easier to create after we have created the vSphere Network with the correct network definition as we can see them being realized in the NSX manager.\nInfo By adding these static routes on the T0 level as I have done, means this traffic will never leave the Tier-0s, it will go over the linknet between the Tier-0s\nInfo These routes are necessary for the Supervisor and the TKC cluster to be able to reach each others. If they cant, deployment of the TKC clusters will fail, it will just deploy the first Control Plane node and stop there)\nCreate a vSphere Namespace to use our new Tier-0 Head over to vCenter -Workload Management and create a new Namespace:\nAs soon as that is done, go ahead and create the third vSphere Namespace with NAT enabled:\nGive the NS'es a dns compliant name, select the Override Supervisor network settings. From the dropdown select our new Tier-0 router. Uncheck NAT on the vSphere NS number 2 (dont need NAT). Fill in the IP addresses you want to use for the TKC worker nodes, and then the ingress cidr you want. On the vSphere NS number 2 enable NAT and populate an egress cidr also.\nClick Create. Wait a couple of second and head over to NSX and check what has been created there.\nIn the NSX Manager you should now see the following:\nNetwork Topology:\nSegments\nThese network is automatically created by NCP, and the first /27 segment in all vSphere Namespace created will be reserved for Supervisor services, like vSphere pods etc. The second /27 segment is the first available network for the workload clusters. This will in my case start at 10.101.82.32/27 and 10.101.83.32/27 accordingy.\nUnder LoadBalancing we also got a couple of new objects:\nThis is our ingress for the workload cluster Kubernetes API.\nUnder Tier-1 gateways we have new Tier-1 gateways:\nIf you take a closer look at the new Tier-1s, you probably would expect them to be created on the new edge-cluster you created and placed your Tier-0 on? No, its not doing that.\nIt used the edge-cluster the Supervisor has been configured to use. Thats pr design. So dont try to troubleshoot this. Reboot the Edge nodes, NSX managers, ESXi hosts etc. It is like that.\nNow it is time to deploy your new TKC cluster with the new Tier-0. Its the same procedure as every other TKC cluster. Give it a name and place it in the correct Namespace:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: stc-tkc-cluster-dmz 5 namespace: stc-ns-dmz 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.30.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.40.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 2 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-small #machineclass, get the available classes by running \u0026#39;k get virtualmachineclass\u0026#39; in vSphere ns context 32 - name: storageClass 33 value: vsan-default-storage-policy Then it is just running:\n1kubectl apply -f yaml.file And a couple of minutes later (if all preps have been done correctly) you should have a new TKC cluster using the new T0.\nNow, if the network conditions not were right, the TKC cluster would never be finished. It would stop stop at deploying the first control plane node. But to quickly verify connectivity from the supervisor controlplane vm and the tkc controlplane vm I will SSH into both (described below under troubleshooting) and do a curl against their K8s API VIP respectively:\nFrom one of the supervisor vms \u0026quot;curling\u0026quot; both vSphere NS workload networks k8s api vip from its workload network interface:\n1root@422068ece368739850023f7a81cf5e14 [ ~ ]# curl --interface eth1 https://10.101.93.1:6443 2curl: (60) SSL certificate problem: unable to get local issuer certificate 3More details here: https://curl.se/docs/sslcerts.html 4 5curl failed to verify the legitimacy of the server and therefore could not 6establish a secure connection to it. To learn more about this situation and 7how to fix it, please visit the web page mentioned above. 8root@422068ece368739850023f7a81cf5e14 [ ~ ]# curl --interface eth1 https://10.101.92.1:6443 9curl: (60) SSL certificate problem: unable to get local issuer certificate 10More details here: https://curl.se/docs/sslcerts.html 11 12curl failed to verify the legitimacy of the server and therefore could not 13establish a secure connection to it. To learn more about this situation and 14how to fix it, please visit the web page mentioned above. 15root@422068ece368739850023f7a81cf5e14 [ ~ ]# From the controlplane node of the TKC workload cluster:\n1vmware-system-user@wdc-vrf-cluster-1-2k8tp-gb5g2:~$ curl https://10.101.90.2:6443 2curl: (60) SSL certificate problem: unable to get local issuer certificate 3More details here: https://curl.haxx.se/docs/sslcerts.html 4 5curl failed to verify the legitimacy of the server and therefore could not 6establish a secure connection to it. To learn more about this situation and 7how to fix it, please visit the web page mentioned above. NSX and Tanzu configurations with NSX VRF In NSX-T 3.0 VRF was a new feature, and configuring it was a bit cumbersome, but already from NSX-T 3.1 adding and configuring a VRF Tier-0 is very straightforward. The benefit of using VRF is that it does not dictate the requirement of additional NSX Edges, and we can create many VRF T0s. We can \u0026quot;reuse\u0026quot; the same Edges that has already been configured with a Tier-0. Instead a VRF T0 will be linked to that already existing Tier-0 which will then be the Parent Tier-0. Some settings will be inherited from the parent Tier-0 like BGP AS number for NSX-T versions before 4.1. From NSX-T 4.1 it is now also possible to override the BGP AS number in the VRF-T0, its no longer tied to the parent T0s BGP AS. We can achieve ip-separation by using individual uplinks on the VRF Tier-0s and peer to different upstream routers than our parent Tier-0. The VRF Tier0 will have its own Tier-1 linked to it. So all the way from the physical world to the VM we have a dedicated ip network. To be able to configure VRF Tier-0 we need to make sure the uplinks our Edges have been configured with have the correct vlan trunk range so we can create dedicated VRF Tier0 uplink segments in their respective vlan. The VRF Tier0 will use the same \u0026quot;physical\u0026quot; uplinks as the Edges have been configured with, but using different VLAN for the Tier-0 uplinks. I will go through how I configre VRF T0 in my environment. Pr default there is no route leakage between the parent Tier-0 and the VRF-T0 created, if you want to exhange routes between them we need to create those static routes ourselves. Read more about NSX VRF here.\nTip From NSX-T 4.1 it is now also possible to override the BGP AS number in the VRF-T0, its no longer tied to the parent T0s BGP AS\nIn this part of my lab I use the following IP addresses for the following components:\nNetwork overview Tanzu Management network: 172.21.103.0/24 - connected to a VDS port group - manually created by me Tanzu Workload network (the initial Workload network): 10.103.100.0/23 - will be created automatically as a NSX overlay segment. Ingress: 10.103.200.0/24 Egress: 10.103.201.0/24 I am doing NAT on this network (important to have in mind for later) The first Tier-0 has been configured to use uplinks on vlan 1034 in the following cidr: 10.103.4.0/24 The VRF Tier-0 will be using uplink on vlan 1035 in the follwing cidr: 10.103.5.0/24 Here is a digram showing high-level how VRF-T0 looks like:\nThe Edge VM network config:\nConfigure VRF Tier-0 in NSX Head over the NSX manager -\u0026gt; Networking -\u0026gt; Tier-0 Gateways and click Add Gateway:\nThen give it a name and select the parent Tier0:\nClick save.\nNow head over to Segments and create the VRF-Tier0 Uplink segment:\nGive it a name, select the Edge VLAN Transportzone and enter the VLAN for the VRF T0-uplink (you can also create a vlan Trunk range here instead of creating two distinct segments for both uplinks). In my lab I will only use one uplink.\nClick save\nNow head back to your VRF T0 again and add a interface:\nGive it a name, select external, enter the IP for the uplink you will use to peer with your upstream router, then select the segment created earlier. Select the Edge that will get this interface. Notice also the Access VLAN ID field. There is no need to enter the VLAN here as we only defined one VLAN in our segment, had we created a VLAN range we need to define a VLAN here. It discovers the correct VLAN as we can see. Click save. Remember that for this VLAN to \u0026quot;come through\u0026quot; the Edge needs to be on a trunk-port that allows this VLAN.\nYou can verify the L2 connectivity from your router:\n1root@cpodrouter-v7n31 [ ~ ]# ping 10.103.5.10 2PING 10.103.5.10 (10.103.5.10) 56(84) bytes of data. 364 bytes from 10.103.5.10: icmp_seq=1 ttl=64 time=4.42 ms 464 bytes from 10.103.5.10: icmp_seq=2 ttl=64 time=0.627 ms 564 bytes from 10.103.5.10: icmp_seq=3 ttl=64 time=0.776 ms 6^C 7--- 10.103.5.10 ping statistics --- 83 packets transmitted, 3 received, 0% packet loss, time 10ms 9rtt min/avg/max/mdev = 0.627/1.939/4.416/1.752 ms Now that we have verified that its time for BGP to configured in our upstream router and in our VRF Tier-0. I have already configured my upstream router to accept my VRF T0 as a BGP neighbour, I just need to confgure BGP on my new VRF Tier-0. In the VRF Tier-0 go to BGP and add a bgp neighbour (notice that we need to enable BGP, not enabled by default, and you cant change the BGP as number):\nClick save.\n1Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 210.103.4.10 4 66803 336 345 0 0 0 05:30:33 3 310.103.5.10 4 66803 2 19 0 0 0 00:01:38 0 4172.20.0.1 4 65700 445 437 0 0 0 07:09:43 74 My new neighbour has jouined the party. Now just make sure it will advertise the needed networks. Lets configure that: In the VRF T0, click route re-distribution and SET\nNow my new VRF-Tier 0 is ready to route and accept new linked Tier-1s. How does it look like in the NSX map?\nLooking good.\nLet us get back to this picture when we have deployed a TKC cluster on it.\nCreate a vSphere Namespace to use our new VRF Tier-0 This will be the same approach as above here only difference is we are selecting a VRF Tier0 instead. Here I have selected the VRF Tier-0 and defined the network for it. I have disabled NAT.\nNow what have happened in NSX? Lets have a look. The network topology has been updated:\nA new Tier-1 has been created:\nAnd ofcourse the loadbalancer interface:\nBut the most interesting part is the static routes being created. Let us have a look at these.\nIn the VRF T0 it has created two additonal static routes:\nThose to routes above points to the Supervisor Workload network and the Supervisor Ingress network. Next hop is:\nThese are the Tier0-Tier-1 transit net interface:\nWhat static routes have been configured on the parent Tier-0?\nAnd next-hop is:\nThese routes are pointing to the new vSphere Namespace network, and Ingress network we defined to use the new VRF-Tier0.\nHigh-level overview of the static routes being created automatically by NCP:\nWhen the TKC cluster is deployed the NSX map will look like this:\nA new segment as been added (vnet-domain-c8:5135e3cc-aca4-4c99-8f9f-903e68496937-wdc-ns-1-vrf-wdc-cl-58aaa-0), which is the segment where the TKC workers have been placed. Notice that it is using a /27 subnet as defined in the Namespace Subnet Prefix above. The first segment (/27 chunk) (seg-domain-xxxxx) is always reserved for the Supervisor Services/vSphere Pods. As I decided not to use NAT I can reach the worker nodes IP addresses directly from my management jumpbox (if allowed routing/firewall wise). Note that ping is default disabled/blocked. So to test connectivity try port 22 with SSH/curl/telnet etc.\n1andreasm@linuxvm01:~/tkgs_vsphere7$ ssh 10.103.51.34 2The authenticity of host \u0026#39;10.103.51.34 (10.103.51.34)\u0026#39; can\u0026#39;t be established. 3ECDSA key fingerprint is SHA256:qonxA8ySCbic0YcCAg9i2pLM9Wpb+8+UGpAcU1qAXHs. 4Are you sure you want to continue connecting (yes/no/[fingerprint])? But before you can reach it directly you need to allow this with a firewall rule in NSX as there is a default block rule here:\nIn order to \u0026quot;override\u0026quot; this rule we need to create a rule earlier in the NSX Distributed Firewall. Below is just a test rule I created, its far to open/liberal of course:\nThe group membership in the above rules is just the vnet-domain-c8:5135e3cc-aca4-4c99-8f9f-903e68496937-wdc-ns-1-vrf-wdc-cl-58aaa-0 segment where my TKC workers in this namespace will reside. So if I scale down/up this cluster the content will be dynamically updated. I dont have to update the rule or security group, its done automatic.\nFirewall openings - network diagram I will get back and update this section with a table and update the diagram with more details.\nTroubleshooting To troubleshoot networking scenarios with Tanzu it can sometimes help to SSH into the Supervisor Controlplane VMs and the TKC worker nodes. When I tested out this multi Tier-0 setup I had an issue that only the control plane node of my TKC cluster were being spun up, it never came to deploying the worker nodes. I knew it had to do with connectivity between the Supervisor and TKC. I used NSX Traceflow to verify that connectivity worked as intended which my traceflow in NSX did show me, but still it did not work. So sometimes it is better to see whats going on from the workloads perspective themselves.\nSSH Supervisor VM To log in to the Supervisor VMs we need the root password. This password can be retreived from the vCenter server. SSH into the vCenter server:\n1root@vcsa [ /lib/vmware-wcp ]# ./decryptK8Pwd.py 2Read key from file 3 4Connected to PSQL 5 6Cluster: domain-c35:dd5825a9-8f62-4823-9347-a9723b6800d5 7IP: 172.21.102.81 8PWD: PASSWORD-IS-HERE 9------------------------------------------------------------ 10 11Cluster: domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5 12IP: 10.101.10.21 13PWD: PASSWORD-IS-HERE 14------------------------------------------------------------ Now that we have the root password one can log into the Supervisor VM with SSH and password through the Management Interface (the Workload Interface IP is probably behind NAT so is not reachable OOB):\n1andreasm@andreasm:~/from_ubuntu_vm/tkgs/tkgs-stc-cpod$ ssh root@10.101.10.22 2The authenticity of host \u0026#39;10.101.10.22 (10.101.10.22)\u0026#39; can\u0026#39;t be established. 3ED25519 key fingerprint is SHA256:vmeHlDgquXrZTK3yyevmY2QfISW1WNoTC5TZJblw1J4. 4This key is not known by any other names 5Are you sure you want to continue connecting (yes/no/[fingerprint])? And from in here we can use some basic troubleshooting tools to verify if the different networks can be reached from the Supervisor VM. In the example below I try to verify if it can reach the K8s API VIP for the TKC cluster deployed behind the new Tier-0. I am adding --interface eth1 as I want to specifically use the Workload Network interface on the SVM.\n1curl --interface eth1 https://10.13.52.1:6443 The respons should be immediate, if not you have network reachability issues:\n1curl: (28) Failed to connect to 10.13.52.1 port 6443 after 131108 ms: Couldn\u0026#39;t connect to server What you should see is this:\n1root@423470e48788edd2cd24398f794c5f7b [ ~ ]# curl --interface eth1 https://10.13.52.1:6443 2curl: (60) SSL certificate problem: unable to get local issuer certificate 3More details here: https://curl.se/docs/sslcerts.html 4 5curl failed to verify the legitimacy of the server and therefore could not 6establish a secure connection to it. To learn more about this situation and 7how to fix it, please visit the web page mentioned above. SSH TKC nodes The nodes in a TKC cluster can also be SSH'ed into. If you dont do NAT on your vSphere Namespace network they can be reach directly on their IPs (if from where your SSH jumpbox is allowed routing wise/firewall wise). But if you are NAT'ing then you have to place your SSH jumpbox in the same segment as the TKC nodes you want to SSH into. Or add a second interface on your jumpbox placed in this network. The segment is created in NSX and is called something like this:\nTo get the password for the TKC nodes you can get them with kubectl like this: Put yourselves in the context of the namespace where your workload nodes is deployed:\n1andreasm@andreasm:~$ vsphere-kubectl login --server=10.101.11.2 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-namespace ns-wdc-1-nat 1andreasm@andreasm:~$ k config current-context 2tkc-cluster-nat Then get the SSH secret:\n1andreasm@andreasm:~$ k get secrets 2NAME TYPE DATA AGE 3default-token-fqvbp kubernetes.io/service-account-token 3 127d 4tkc-cluster-1-antrea-data-values Opaque 1 127d 5tkc-cluster-1-auth-svc-cert kubernetes.io/tls 3 127d 6tkc-cluster-1-ca cluster.x-k8s.io/secret 2 127d 7tkc-cluster-1-capabilities-package clusterbootstrap-secret 1 127d 8tkc-cluster-1-encryption Opaque 1 127d 9tkc-cluster-1-etcd cluster.x-k8s.io/secret 2 127d 10tkc-cluster-1-extensions-ca kubernetes.io/tls 3 127d 11tkc-cluster-1-guest-cluster-auth-service-data-values Opaque 1 127d 12tkc-cluster-1-kapp-controller-data-values Opaque 2 127d 13tkc-cluster-1-kubeconfig cluster.x-k8s.io/secret 1 127d 14tkc-cluster-1-metrics-server-package clusterbootstrap-secret 0 127d 15tkc-cluster-1-node-pool-01-bootstrap-j2r7s-fgmm2 cluster.x-k8s.io/secret 2 42h 16tkc-cluster-1-node-pool-01-bootstrap-j2r7s-r5lcm cluster.x-k8s.io/secret 2 42h 17tkc-cluster-1-node-pool-01-bootstrap-j2r7s-w96ft cluster.x-k8s.io/secret 2 42h 18tkc-cluster-1-pinniped-package clusterbootstrap-secret 1 127d 19tkc-cluster-1-proxy cluster.x-k8s.io/secret 2 127d 20tkc-cluster-1-sa cluster.x-k8s.io/secret 2 127d 21tkc-cluster-1-secretgen-controller-package clusterbootstrap-secret 0 127d 22tkc-cluster-1-ssh kubernetes.io/ssh-auth 1 127d 23tkc-cluster-1-ssh-password Opaque 1 127d 24tkc-cluster-1-ssh-password-hashed Opaque 1 127d I am interested in this one:\n1tkc-cluster-1-ssh-password So I will go ahead and retrieve the content of it:\n1andreasm@andreasm:~$ k get secrets tkc-cluster-1-ssh-password -oyaml 2apiVersion: v1 3data: 4 ssh-passwordkey: aSx--redacted---KJS= #Here is the ssh password in base64 5kind: Secret 6metadata: 7 creationTimestamp: \u0026#34;2022-12-08T10:52:28Z\u0026#34; 8 name: tkc-cluster-1-ssh-password 9 namespace: stc-tkc-ns-1 10 ownerReferences: 11 - apiVersion: cluster.x-k8s.io/v1beta1 12 kind: Cluster 13 name: tkc-cluster-1 14 uid: 4a9c6137-0223-46d8-96d2-ab3564e375fc 15 resourceVersion: \u0026#34;499590\u0026#34; 16 uid: 75b163a3-4e62-4b33-93de-ae46ee314751 17type: Opaque Now I just need to decode the base64 encoded pasword:\n1andreasm@andreasm:~$ echo \u0026#39;aSx--redacted---KJS=\u0026#39; |base64 --decode 2passwordinplaintexthere=andreasm@andreasm:~$ Now we can use this password to log in to the TKC nodes with the user: vmware-system-user\n1ssh vmware-system-user@10.101.51.34 DCLI - VMware Datacenter CLI If you happen to find different tasks easier to perform from CLI instead of GUI I will show here how to create a new vSphere Namespace by using DCLI from the VCSA appliance (vCenter Server). For more information and reference on dcli look here.\nLog in to your vCenter hosting your Supervisor Cluster with SSH and enter shell:\n1andreasm@andreasm:~/$ ssh root@vcsa.cpod-v7n31.az-wdc.cloud-garage.net 2 3VMware vCenter Server 7.0.3.01000 4 5Type: vCenter Server with an embedded Platform Services Controller 6 7(root@vcsa.cpod-v7n31.az-wdc.cloud-garage.net) Password: 8Connected to service 9 10 * List APIs: \u0026#34;help api list\u0026#34; 11 * List Plugins: \u0026#34;help pi list\u0026#34; 12 * Launch BASH: \u0026#34;shell\u0026#34; 13 14Command\u0026gt; shell 15Shell access is granted to root 16root@vcsa [ ~ ]# Type *dcli --help\u0026quot; to see some options:\n1root@vcsa [ ~ ]# dcli --help 2usage: dcli [+server SERVER] [+vmc-server] [+nsx-server [NSX_SERVER]] [+org-id ORG_ID] [+sddc-id SDDC_ID] [+interactive] [+prompt PROMPT] 3 [+skip-server-verification | +cacert-file CACERT_FILE] [+username USERNAME] [+password PASSWORD] [+logout] [+filter FILTER [FILTER ...]] 4 [+formatter {yaml,yamlc,table,xml,xmlc,json,jsonc,jsonp,html,htmlc,csv}] [+verbose] [+log-level {debug,info,warning,error}] [+log-file LOG_FILE] 5 [+generate-json-input] [+generate-required-json-input] [+json-input JSON_INPUT] [+credstore-file CREDSTORE_FILE] 6 [+credstore-add | +credstore-list | +credstore-remove] [+session-manager SESSION_MANAGER] [+configuration-file CONFIGURATION_FILE] [+more] 7 [args [args ...]] 8 9VMware Datacenter Command Line Interface 10 11positional arguments: 12 args CLI command 13 14optional arguments: 15 +server SERVER Specify VAPI Server IP address/DNS name (default: \u0026#39;http://localhost/api\u0026#39;) 16 +vmc-server Switch to indicate connection to VMC server (default VMC URL: \u0026#39;https://vmc.vmware.com\u0026#39;) 17 +nsx-server [NSX_SERVER] 18 Specify NSX on VMC Server or on-prem instance IP address/DNS name (default: \u0026#39;None\u0026#39;) 19 +org-id ORG_ID Specify VMC organization id to connect to NSX instance. Works together with +sddc-id. (default: \u0026#39;None\u0026#39;) 20 +sddc-id SDDC_ID Specify VMC SDDC id to connect to NSX instance. Works together with +org-id. (default: \u0026#39;None\u0026#39;) 21 +interactive Open a CLI shell to invoke commands 22 +prompt PROMPT Prompt for cli shell (default: dcli\u0026gt; ) 23 +skip-server-verification 24 Skip server SSL verification process (default: False) 25 +cacert-file CACERT_FILE 26 Specify the certificate authority certificates for validating SSL connections (format: PEM) (default: \u0026#39;\u0026#39;) 27 +username USERNAME Specify the username for login (default: \u0026#39;\u0026#39;) 28 +password PASSWORD Specify password explicitly (default: False) 29 +logout Requests delete session and remove from credentials store if stored. (default: False) 30 +filter FILTER [FILTER ...] 31 Provide JMESPath expression to filter command output. More info on JMESPath here: http://jmespath.org 32 +formatter {yaml,yamlc,table,xml,xmlc,json,jsonc,jsonp,html,htmlc,csv} 33 Specify the formatter to use to format the command output 34 +verbose Prints verbose output 35 +log-level {debug,info,warning,error} 36 Specify the verbosity for log file. (default: \u0026#39;info\u0026#39;) 37 +log-file LOG_FILE Specify dcli log file (default: \u0026#39;/var/log/vmware/vapi/dcli.log\u0026#39;) 38 +generate-json-input Generate command input template in json 39 +generate-required-json-input 40 Generate command input template in json for required fields only 41 +json-input JSON_INPUT 42 Specifies json value or a json file for command input 43 +credstore-file CREDSTORE_FILE 44 Specify the dcli credential store file (default: \u0026#39;/root/.dcli/.dcli_credstore\u0026#39;) 45 +credstore-add Store the login credentials in credential store without prompting 46 +credstore-list List the login credentials stored in credential store 47 +credstore-remove Remove login credentials from credential store 48 +session-manager SESSION_MANAGER 49 Specify the session manager for credential store remove operation 50 +configuration-file CONFIGURATION_FILE 51 Specify the dcli configuration store file (default: \u0026#39;/root/.dcli/.dcli_configuration\u0026#39;) 52 +more Flag for page-wise output 53root@vcsa [ ~ ]# Enter DCLI interactive mode: All commands in dcli have autocomplete\n1root@vcsa [ ~ ]# dcli +i +server vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net +skip-server-verification 2Welcome to VMware Datacenter CLI (DCLI) 3 4usage: \u0026lt;namespaces\u0026gt; \u0026lt;command\u0026gt; 5 6To auto-complete and browse DCLI namespaces: [TAB] 7If you need more help for a command: vcenter vm get --help 8If you need more help for a namespace: vcenter vm --help 9To execute dcli internal command: env 10For detailed information on DCLI usage visit: http://vmware.com/go/dcli 11 12dcli\u0026gt; Below shows how autocomplete works:\n1root@vcsa [ ~ ]# dcli +i +server vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net +skip-server-verification 2Welcome to VMware Datacenter CLI (DCLI) 3 4usage: \u0026lt;namespaces\u0026gt; \u0026lt;command\u0026gt; 5 6To auto-complete and browse DCLI namespaces: [TAB] 7If you need more help for a command: vcenter vm get --help 8If you need more help for a namespace: vcenter vm --help 9To execute dcli internal command: env 10For detailed information on DCLI usage visit: http://vmware.com/go/dcli 11 12dcli\u0026gt; com vmware vcenter n 13 \u0026gt; namespacemanagement 14 \u0026gt; namespaces 15 \u0026gt; network 1dcli\u0026gt; com vmware vcenter namespaces instances list 2 list 3 getv2 4 update 5 delete 6 listv2 7 createv2 8 set We can tab to autocomplete and/or use the \u0026quot;dropdown\u0026quot; list to scroll through the different options. Nice feature.\nCreate a vSphere Namespace from DCLI, selecting a VRF T0, configure name, network etc (as you would do from the GUI of vCenter in Workload Management):\n1dcli\u0026gt; com vmware vcenter namespaces instances create --cluster domain-c8 --namespace stc-cluster-vrf2 --namespace-network-network-ingress-cidrs \u0026#39;[{\u0026#34;address\u0026#34;: \u0026#34;10.13.54. 20\u0026#34;, \u0026#34;prefix\u0026#34;:24}]\u0026#39; --namespace-network-network-load-balancer-size SMALL --namespace-network-network-namespace-network-cidrs \u0026#39;[{\u0026#34;address\u0026#34;: \u0026#34;10.13.53.0\u0026#34;, \u0026#34;prefix\u0026#34;:24}]\u0026#39; - 3-namespace-network-network-provider NSXT_CONTAINER_PLUGIN --namespace-network-network-nsx-tier0-gateway vrf-1 --namespace-network-network-routed-mode true --namespace-n 4etwork-network-subnet-prefix-length 28 5dcli\u0026gt; To get the --cluster domain id run this:\n1dcli\u0026gt; com vmware vcenter namespaces instances list 2|---------|-----------------------------------|----------------|-----------|----------------------|-------------| 3|cluster |stats |namespace |description|self_service_namespace|config_status| 4|---------|-----------------------------------|----------------|-----------|----------------------|-------------| 5|domain-c8||--------|-----------|------------||stc-cluster-vrf | |False |RUNNING | 6| ||cpu_used|memory_used|storage_used|| | | | | 7| ||--------|-----------|------------|| | | | | 8| ||0 |0 |0 || | | | | 9| ||--------|-----------|------------|| | | | | 10|domain-c8||--------|-----------|------------||stc-cluster-vrf2| |False |RUNNING | 11| ||cpu_used|memory_used|storage_used|| | | | | 12| ||--------|-----------|------------|| | | | | 13| ||0 |0 |0 || | | | | 14| ||--------|-----------|------------|| | | | | 15|---------|-----------------------------------|----------------|-----------|----------------------|-------------| 16dcli\u0026gt; And seconds later the vSphere Namespace is created\nvCenter API - with Postman vCenter has a nice feature included, the API Explorer. This can be found here:\nClick on the hamburger Menu, and find Developer Center:\nAnd from here we have all the API available to us:\nIts a looooong list of available APIs.\nTo be able to authenticate against vCenter with Postman we must create an API Key. So the first we need to do is \u0026quot;login\u0026quot; with post using the following api (this uses a username and password with sufficient acces to vCenter):\n1https://{{vcenter-fqdn}}/api/session In Postman one should create an environment that contains the vCenter IP/FQDN, username and password. So the first action is to POST this API to get the API Key, making sure you set Authorization to Basic Auth from your environment:\nThe response from this POST should be a token. From now you need to use this token to interact with vCenter API. Change the authentication to API Key and use vmware-api-session-id as Key and Token as value.\nNow lets try a GET and see if it works:\nThat worked out fine üòÑ\nWhat about creating a vSphere Namespace from Postman?\nThats very easy, below is an example to create a new vSphere Namespace, and pointing it to my VRF Tier-0 router:\n1{ 2\t\u0026#34;access_list\u0026#34;: [ 3\t{ 4\t\u0026#34;domain\u0026#34;: \u0026#34;cpod-nsxam-stc.az-stc.cloud-garage.net\u0026#34;, 5\t\u0026#34;role\u0026#34;: \u0026#34;OWNER\u0026#34;, 6\t\u0026#34;subject\u0026#34;: \u0026#34;andreasm\u0026#34;, 7\t\u0026#34;subject_type\u0026#34;: \u0026#34;USER\u0026#34; 8\t} 9\t], 10\t\u0026#34;cluster\u0026#34;: \u0026#34;domain-c8\u0026#34;, 11\t\u0026#34;namespace\u0026#34;: \u0026#34;stc-cluster-vrf2\u0026#34;, 12\t\u0026#34;namespace_network\u0026#34;: { 13\t\u0026#34;network\u0026#34;: { 14\t\u0026#34;ingress_cidrs\u0026#34;: [ 15\t{ 16\t\u0026#34;address\u0026#34;: \u0026#34;10.13.54.0\u0026#34;, 17\t\u0026#34;prefix\u0026#34;: 24 18\t} 19\t], 20\t\u0026#34;load_balancer_size\u0026#34;: \u0026#34;SMALL\u0026#34;, 21\t\u0026#34;namespace_network_cidrs\u0026#34;: [ 22\t{ 23\t\u0026#34;address\u0026#34;: \u0026#34;10.13.53.0\u0026#34;, 24\t\u0026#34;prefix\u0026#34;: 24 25\t} 26\t], 27\t\u0026#34;nsx_tier0_gateway\u0026#34;: \u0026#34;vrf-1\u0026#34;, 28\t\u0026#34;routed_mode\u0026#34;: true, 29\t\u0026#34;subnet_prefix_length\u0026#34;: 28 30\t}, 31\t\u0026#34;network_provider\u0026#34;: \u0026#34;NSXT_CONTAINER_PLUGIN\u0026#34; 32\t} 33} Paste this into Postman (Body - Raw) and POST it to the following path https://{{vcenter-fqdn}}/api/vcenter/namespaces/instances and the new vSphere Namespace should be created in a jiff.\nAnd in vCenter our new Namespace:\nFor references to the APIs in vCenter and a whole lot of details and explanations have a look here!\n","link":"https://blog.andreasm.io/2023/04/14/tanzu-with-vsphere-and-different-tier-0s/","section":"post","tags":["nsx","tanzu","network"],"title":"Tanzu with vSphere and different Tier-0s"},{"body":"","link":"https://blog.andreasm.io/tags/ako/","section":"tags","tags":null,"title":"ako"},{"body":"","link":"https://blog.andreasm.io/tags/avi/","section":"tags","tags":null,"title":"avi"},{"body":"","link":"https://blog.andreasm.io/tags/kubernetes/","section":"tags","tags":null,"title":"kubernetes"},{"body":"Tanzu Kubernetes Grid This post will go through how to deploy TKG 2.1, the management cluster, a workload cluster (or two), and the necessary preparations to be done on the underlaying infrastructure to support TKG 2.1. In this post I will use vSphere 8 with vSAN, Avi LoadBalancer, and NSX. So what we want to end up with it something like this:\nPreparations before deployment This post will assume the following:\nvSphere is already installed configured. See more info here and here\nNSX has already been configured (see this post for how to configure NSX). Segments used for both Management cluster and Workload clusters should have DHCP server available. We dont need DHCP for Workload Cluster, but Management needs DHCP. NSX can provide DHCP server functionality for this use *\nNSX Advanced LoadBalancer has been deployed (and configured with a NSX cloud). See this post for how to configure this. **\nImport the VM template for TKG, see here\nA dedicated Linux machine/VM we can use as the bootstrap host, with the Tanzu CLI installed. See more info here\n(*) TKG 2.1 is not tied to NSX the same way as TKGs - So we can choose to use NSX for Security only or the full stack with networking and security. The built in NSX loadbalancer will not be used, I will use the NSX Advanced Loadbalancer (Avi)\n(**) I want to use the NSX cloud in Avi as it gives several benefits such as integration into the NSX manager where Avi automatically creates security groups, tags and services to easily be used in security policy creation and automatic \u0026quot;route plumbing\u0026quot; for the VIPs.\nTKG Management cluster - deployment The first step after all the pre-requirements have been done is to prepare a bootstrap yaml for the management cluster. I will post an example file here and go through what the different fields means and why I have configured them and why I have uncommented some of them. Start by logging into the bootstrap machine, or if you decide to create the bootstrap yaml somewhere else go ahead but we need to copy it over to the bootstrap machine when we are ready to create the the management cluster.\nTo get started with a bootstrap yaml file we can either grab an example from here or in your bootstrap machine there is a folder which contains a default config you can start out with:\n1andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ ll 2total 120 3drwxrwxr-x 18 andreasm andreasm 4096 Mar 24 09:10 ./ 4drwx------ 9 andreasm andreasm 4096 Mar 16 11:32 ../ 5drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 ako/ 6drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 bootstrap-kubeadm/ 7drwxrwxr-x 4 andreasm andreasm 4096 Mar 16 06:52 cert-manager/ 8drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 cluster-api/ 9-rw------- 1 andreasm andreasm 1293 Mar 16 06:52 config.yaml 10-rw------- 1 andreasm andreasm 32007 Mar 16 06:52 config_default.yaml 11drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 control-plane-kubeadm/ 12drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-aws/ 13drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-azure/ 14drwxrwxr-x 6 andreasm andreasm 4096 Mar 16 06:52 infrastructure-docker/ 15drwxrwxr-x 3 andreasm andreasm 4096 Mar 16 06:52 infrastructure-ipam-in-cluster/ 16drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-oci/ 17drwxrwxr-x 4 andreasm andreasm 4096 Mar 16 06:52 infrastructure-tkg-service-vsphere/ 18drwxrwxr-x 5 andreasm andreasm 4096 Mar 16 06:52 infrastructure-vsphere/ 19drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 kapp-controller-values/ 20-rwxrwxr-x 1 andreasm andreasm 64 Mar 16 06:52 providers.sha256sum* 21-rw------- 1 andreasm andreasm 0 Mar 16 06:52 v0.28.0 22-rw------- 1 andreasm andreasm 747 Mar 16 06:52 vendir.lock.yml 23-rw------- 1 andreasm andreasm 903 Mar 16 06:52 vendir.yml 24drwxrwxr-x 8 andreasm andreasm 4096 Mar 16 06:52 ytt/ 25drwxrwxr-x 2 andreasm andreasm 4096 Mar 16 06:52 yttcb/ 26drwxrwxr-x 7 andreasm andreasm 4096 Mar 16 06:52 yttcc/ 27andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ The file you should be looking for is called config_default.yaml . It could be a smart choice to use this as it will include the latest config parameters following the TKG version you have downloaded (Tanzu CLI).\nNow copy this file to a folder of preference and start to edit it. Below is a copy of an example I am using:\n1#! --------------- 2#! Basic config 3#! ------------- 4CLUSTER_NAME: tkg-stc-mgmt-cluster #Name of the TKG mgmt cluster 5CLUSTER_PLAN: dev #Dev or Prod, defines the amount of control plane nodes of the mgmt cluster 6INFRASTRUCTURE_PROVIDER: vsphere #We are deploying on vSphere, could be AWS, Azure 7ENABLE_CEIP_PARTICIPATION: \u0026#34;false\u0026#34; #Customer Experience Improvement Program - set to true if you will participate 8ENABLE_AUDIT_LOGGING: \u0026#34;false\u0026#34; #Audit logging should be true in production environments 9CLUSTER_CIDR: 100.96.0.0/11 #Kubernetes Cluster CIDR 10SERVICE_CIDR: 100.64.0.0/13 #Kubernetes Services CIDR 11TKG_IP_FAMILY: ipv4 #ipv4 or ipv6 12DEPLOY_TKG_ON_VSPHERE7: \u0026#34;true\u0026#34; #Yes to deploy standalone tkg mgmt cluster on vSphere 13 14#! --------------- 15#! vSphere config 16#! ------------- 17VSPHERE_DATACENTER: /cPod-NSXAM-STC #Name of vSphere Datacenter 18VSPHERE_DATASTORE: /cPod-NSXAM-STC/datastore/vsanDatastore #Name and path of vSphere datastore to be used 19VSPHERE_FOLDER: /cPod-NSXAM-STC/vm/TKGm #Name and path to VM folder 20VSPHERE_INSECURE: \u0026#34;false\u0026#34; #True if you dont want to verify vCenter thumprint below 21VSPHERE_NETWORK: /cPod-NSXAM-STC/network/ls-tkg-mgmt #A network portgroup (VDS or NSX Segment) for VM placement 22VSPHERE_CONTROL_PLANE_ENDPOINT: \u0026#34;\u0026#34; #Required if using Kube-Vip, I am using Avi Loadbalancer for this 23VSPHERE_PASSWORD: \u0026#34;password\u0026#34; #vCenter account password for account defined below 24VSPHERE_RESOURCE_POOL: /cPod-NSXAM-STC/host/Cluster/Resources #If you want to use a specific vSphere Resource Pool for the mgmt cluster. Leave it as is if not. 25VSPHERE_SERVER: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net #DNS record to vCenter Server 26VSPHERE_SSH_AUTHORIZED_KEY: ssh-rsa sdfgasdgadfgsdg sdfsdf@sdfsdf.net # your bootstrap machineSSH public key 27VSPHERE_TLS_THUMBPRINT: 22:FD # Your vCenter SHA1 Thumbprint 28VSPHERE_USERNAME: user@vspheresso/or/ad/user/domain #A user with the correct permissions 29 30#! --------------- 31#! Node config 32#! ------------- 33OS_ARCH: amd64 34OS_NAME: ubuntu 35OS_VERSION: \u0026#34;20.04\u0026#34; 36VSPHERE_CONTROL_PLANE_DISK_GIB: \u0026#34;20\u0026#34; 37VSPHERE_CONTROL_PLANE_MEM_MIB: \u0026#34;4096\u0026#34; 38VSPHERE_CONTROL_PLANE_NUM_CPUS: \u0026#34;2\u0026#34; 39VSPHERE_WORKER_DISK_GIB: \u0026#34;20\u0026#34; 40VSPHERE_WORKER_MEM_MIB: \u0026#34;4096\u0026#34; 41VSPHERE_WORKER_NUM_CPUS: \u0026#34;2\u0026#34; 42CONTROL_PLANE_MACHINE_COUNT: 1 43WORKER_MACHINE_COUNT: 2 44 45#! --------------- 46#! Avi config 47#! ------------- 48AVI_CA_DATA_B64: #Base64 of the Avi Certificate 49AVI_CLOUD_NAME: stc-nsx-cloud #Name of the cloud defined in Avi 50AVI_CONTROL_PLANE_HA_PROVIDER: \u0026#34;true\u0026#34; #True as we want to use Avi as K8s API endpoint 51AVI_CONTROLLER: 172.24.3.50 #IP or Hostname Avi controller or controller cluster 52# Network used to place workload clusters\u0026#39; endpoint VIPs - If you want to use a separate vip for Workload clusters Kubernetes API endpoint 53AVI_CONTROL_PLANE_NETWORK: vip-tkg-wld-l4 #Corresponds with network defined in Avi 54AVI_CONTROL_PLANE_NETWORK_CIDR: 10.13.102.0/24 #Corresponds with network defined in Avi 55# Network used to place workload clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 56AVI_DATA_NETWORK: vip-tkg-wld-l7 #Corresponds with network defined in Avi 57AVI_DATA_NETWORK_CIDR: 10.13.103.0/24 #Corresponds with network defined in Avi 58# Network used to place management clusters\u0026#39; services external IPs (load balancer \u0026amp; ingress services) 59AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_CIDR: 10.13.101.0/24 #Corresponds with network defined in Avi 60AVI_MANAGEMENT_CLUSTER_VIP_NETWORK_NAME: vip-tkg-mgmt-l7 #Corresponds with network defined in Avi 61# Network used to place management clusters\u0026#39; endpoint VIPs 62AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_NAME: vip-tkg-mgmt-l4 #Corresponds with network defined in Avi 63AVI_MANAGEMENT_CLUSTER_CONTROL_PLANE_VIP_NETWORK_CIDR: 10.13.100.0/24 #Corresponds with network defined in Avi 64AVI_NSXT_T1LR: /infra/tier-1s/Tier-1 #Path to the NSX T1 you have configured, click on three dots in NSX on the T1 to get the full path. 65AVI_CONTROLLER_VERSION: 22.1.2 #Latest supported version of Avi for TKG 2.1 66AVI_ENABLE: \u0026#34;true\u0026#34; # Enables Avi as Loadbalancer for workloads 67AVI_LABELS: \u0026#34;\u0026#34; #When used Avi is enabled only workload cluster with corresponding label 68AVI_PASSWORD: \u0026#34;password\u0026#34; #Password for the account used in Avi, username defined below 69AVI_SERVICE_ENGINE_GROUP: stc-nsx #Service Engine group for Workload clusters if you want to have separate groups for Workload clusters and Management cluster 70AVI_MANAGEMENT_CLUSTER_SERVICE_ENGINE_GROUP: tkgm-se-group #Dedicated Service Engine group for management cluster 71AVI_USERNAME: admin 72AVI_DISABLE_STATIC_ROUTE_SYNC: true #Pod network reachable or not from the Avi Service Engines 73AVI_INGRESS_DEFAULT_INGRESS_CONTROLLER: true #If you want to use AKO as default ingress controller, false if you plan to use other ingress controllers also. 74AVI_INGRESS_SHARD_VS_SIZE: SMALL #Decides the amount of shared vs pr ip. 75AVI_INGRESS_SERVICE_TYPE: NodePortLocal #NodePortLocal only when using Antrea, otherwise NodePort or ClusterIP 76AVI_CNI_PLUGIN: antrea 77 78#! --------------- 79#! Proxy config 80#! ------------- 81TKG_HTTP_PROXY_ENABLED: \u0026#34;false\u0026#34; 82 83#! --------------------------------------------------------------------- 84#! Antrea CNI configuration 85#! --------------------------------------------------------------------- 86# ANTREA_NO_SNAT: false 87# ANTREA_TRAFFIC_ENCAP_MODE: \u0026#34;encap\u0026#34; 88# ANTREA_PROXY: false 89# ANTREA_POLICY: true 90# ANTREA_TRACEFLOW: false 91ANTREA_NODEPORTLOCAL: true 92ANTREA_PROXY: true 93ANTREA_ENDPOINTSLICE: true 94ANTREA_POLICY: true 95ANTREA_TRACEFLOW: true 96ANTREA_NETWORKPOLICY_STATS: false 97ANTREA_EGRESS: true 98ANTREA_IPAM: false 99ANTREA_FLOWEXPORTER: false 100ANTREA_SERVICE_EXTERNALIP: false 101ANTREA_MULTICAST: false 102 103#! --------------------------------------------------------------------- 104#! Machine Health Check configuration 105#! --------------------------------------------------------------------- 106ENABLE_MHC: \u0026#34;true\u0026#34; 107ENABLE_MHC_CONTROL_PLANE: true 108ENABLE_MHC_WORKER_NODE: true 109MHC_UNKNOWN_STATUS_TIMEOUT: 5m 110MHC_FALSE_STATUS_TIMEOUT: 12m 111 112#! --------------------------------------------------------------------- 113#! Identity management configuration 114#! --------------------------------------------------------------------- 115 116IDENTITY_MANAGEMENT_TYPE: none #I have disabled this, use kubeconfig instead 117#LDAP_BIND_DN: CN=Andreas M,OU=Users,OU=GUZWARE,DC=guzware,DC=local 118#LDAP_BIND_PASSWORD: \u0026lt;encoded:UHNAc=\u0026gt; 119#LDAP_GROUP_SEARCH_BASE_DN: DC=guzware,DC=local 120#LDAP_GROUP_SEARCH_FILTER: (objectClass=group) 121#LDAP_GROUP_SEARCH_GROUP_ATTRIBUTE: member 122#LDAP_GROUP_SEARCH_NAME_ATTRIBUTE: cn 123#LDAP_GROUP_SEARCH_USER_ATTRIBUTE: distinguishedName 124#LDAP_HOST: guzad07.guzware.local:636 125#LDAP_ROOT_CA_DATA_B64: LS0tLS1CRUd 126#LDAP_USER_SEARCH_BASE_DN: DC=guzware,DC=local 127#LDAP_USER_SEARCH_FILTER: (objectClass=person) 128#LDAP_USER_SEARCH_NAME_ATTRIBUTE: uid 129#LDAP_USER_SEARCH_USERNAME: uid 130#OIDC_IDENTITY_PROVIDER_CLIENT_ID: \u0026#34;\u0026#34; 131#OIDC_IDENTITY_PROVIDER_CLIENT_SECRET: \u0026#34;\u0026#34; 132#OIDC_IDENTITY_PROVIDER_GROUPS_CLAIM: \u0026#34;\u0026#34; 133#OIDC_IDENTITY_PROVIDER_ISSUER_URL: \u0026#34;\u0026#34; 134#OIDC_IDENTITY_PROVIDER_NAME: \u0026#34;\u0026#34; 135#OIDC_IDENTITY_PROVIDER_SCOPES: \u0026#34;\u0026#34; 136#OIDC_IDENTITY_PROVIDER_USERNAME_CLAIM: \u0026#34;\u0026#34; For additional explanations of the different values see here\nWhen you feel you are ready with the bootstrap yaml file its time to deploy the management cluster. From your bootstrap machine where Tanzu CLI have been installed enter the following command:\n1tanzu mc create --file path/to/cluster-config-file.yaml For more information around this process have a look here\nThe first thing that happens is some validation checks, if those pass it will continue to build a local bootstrap cluster on your bootstrap machine before building the TKG Management cluster in your vSphere cluster.\nNote! If you happen to use a an IP range within 172.16.0.0/12 on your computer you are accessing the bootstrap machine through you should edit the default Docker network. Otherwise you will loose connection to your bootstrap machine. This is done like this:\nAdd or edit, if it exists, the /etc/docker/daemon.json file with the following content:\n1{ 2 \u0026#34;default-address-pools\u0026#34;: 3 [ 4 {\u0026#34;base\u0026#34;:\u0026#34;192.168.0.0/16\u0026#34;,\u0026#34;size\u0026#34;:24} 5 ] 6} Restart docker service or reboot the machine.\nNow back to the tanzu create process, you can monitor the progress from the terminal of your bootstrap machine, and you should after a while see machines being cloned from your template and powered on. In the Avi controller you should also see a new virtual service being created:\nThe ip address depicted above is the sole control plane node as I am deploying a TKG management cluster using plan dev. If the progress in your bootstrap machine indicates that it is done, you can check the status with the following command:\n1tanzu mc get This will give you this output:\n1 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 2 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.9+vmware.1 management dev v1.24.9---vmware.1-tkg.1 3 4 5Details: 6 7NAME READY SEVERITY REASON SINCE MESSAGE 8/tkg-stc-mgmt-cluster True 8d 9‚îú‚îÄClusterInfrastructure - VSphereCluster/tkg-stc-mgmt-cluster-xw6xs True 8d 10‚îú‚îÄControlPlane - KubeadmControlPlane/tkg-stc-mgmt-cluster-wrxtl True 8d 11‚îÇ ‚îî‚îÄMachine/tkg-stc-mgmt-cluster-wrxtl-gkv5m True 8d 12‚îî‚îÄWorkers 13 ‚îî‚îÄMachineDeployment/tkg-stc-mgmt-cluster-md-0-vs9dc True 3d3h 14 ‚îú‚îÄMachine/tkg-stc-mgmt-cluster-md-0-vs9dc-55c649d9fc-gnpz4 True 8d 15 ‚îî‚îÄMachine/tkg-stc-mgmt-cluster-md-0-vs9dc-55c649d9fc-gwfvt True 8d 16 17 18Providers: 19 20 NAMESPACE NAME TYPE PROVIDERNAME VERSION WATCHNAMESPACE 21 caip-in-cluster-system infrastructure-ipam-in-cluster InfrastructureProvider ipam-in-cluster v0.1.0 22 capi-kubeadm-bootstrap-system bootstrap-kubeadm BootstrapProvider kubeadm v1.2.8 23 capi-kubeadm-control-plane-system control-plane-kubeadm ControlPlaneProvider kubeadm v1.2.8 24 capi-system cluster-api CoreProvider cluster-api v1.2.8 25 capv-system infrastructure-vsphere InfrastructureProvider vsphere v1.5.1 When cluster is ready deployed and before we can access it with our kubectl cli tool we must set the context to it.\n1kubectl config use-context my-mgmnt-cluster-admin@my-mgmnt-cluster But you probably have a dedicated workstation you want to acces the cluster from, then you can export the kubeconfig like this:\n1tanzu mc kubeconfig get --admin --export-file MC-ADMIN-KUBECONFIG Now copy the file to your workstation and accessed the cluster from there.\nTip! Test out this tool to easy manage your Kubernetes configs: https://github.com/sunny0826/kubecm\nThe above is a really great tool:\n1amarqvardsen@amarqvards1MD6T:~$ kubecm switch --ui-size 10 2Use the arrow keys to navigate: ‚Üì ‚Üë ‚Üí ‚Üê and / toggles search 3Select Kube Context 4 üòº tkc-cluster-1(*) 5 tkgs-cluster-1-admin@tkgs-cluster-1 6 wdc-2-tkc-cluster-1 7 10.13.200.2 8 andreasmk8slab-admin@andreasmk8slab-pinniped 9 ns-wdc-3 10 tkc-cluster-1-routed 11 tkg-mgmt-cluster-admin@tkg-mgmt-cluster 12 stc-tkgm-mgmt-cluster 13‚Üì tkg-wld-1-cluster-admin@tkg-wld-1-cluster 14 15--------- Info ---------- 16Name: tkc-cluster-1 17Cluster: 10.13.202.1 18User: wcp:10.13.202.1:andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net Now your TKG management cluster is ready and we can deploy a workload cluster.\nIf you noticed some warnings around conciliation during deployment, you can check whether they failed or not by issuing this command after you have gotten the kubeconfig context in place to the Management cluster with this command:\n1andreasm@tkg-bootstrap:~$ kubectl get pkgi -A 2NAMESPACE NAME PACKAGE NAME PACKAGE VERSION DESCRIPTION AGE 3stc-tkgm-ns-1 stc-tkgm-wld-cluster-1-kapp-controller kapp-controller.tanzu.vmware.com 0.41.5+vmware.1-tkg.1 Reconcile succeeded 7d22h 4stc-tkgm-ns-2 stc-tkgm-wld-cluster-2-kapp-controller kapp-controller.tanzu.vmware.com 0.41.5+vmware.1-tkg.1 Reconcile succeeded 7d16h 5tkg-system ako-operator ako-operator-v2.tanzu.vmware.com 0.28.0+vmware.1-tkg.1-zshippable Reconcile succeeded 8d 6tkg-system tanzu-addons-manager addons-manager.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 7tkg-system tanzu-auth tanzu-auth.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 8tkg-system tanzu-cliplugins cliplugins.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 9tkg-system tanzu-core-management-plugins core-management-plugins.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 10tkg-system tanzu-featuregates featuregates.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 11tkg-system tanzu-framework framework.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 12tkg-system tkg-clusterclass tkg-clusterclass.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 13tkg-system tkg-clusterclass-vsphere tkg-clusterclass-vsphere.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 14tkg-system tkg-pkg tkg.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 15tkg-system tkg-stc-mgmt-cluster-antrea antrea.tanzu.vmware.com 1.7.2+vmware.1-tkg.1-advanced Reconcile succeeded 8d 16tkg-system tkg-stc-mgmt-cluster-capabilities capabilities.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 17tkg-system tkg-stc-mgmt-cluster-load-balancer-and-ingress-service load-balancer-and-ingress-service.tanzu.vmware.com 1.8.2+vmware.1-tkg.1 Reconcile succeeded 8d 18tkg-system tkg-stc-mgmt-cluster-metrics-server metrics-server.tanzu.vmware.com 0.6.2+vmware.1-tkg.1 Reconcile succeeded 8d 19tkg-system tkg-stc-mgmt-cluster-pinniped pinniped.tanzu.vmware.com 0.12.1+vmware.2-tkg.3 Reconcile succeeded 8d 20tkg-system tkg-stc-mgmt-cluster-secretgen-controller secretgen-controller.tanzu.vmware.com 0.11.2+vmware.1-tkg.1 Reconcile succeeded 8d 21tkg-system tkg-stc-mgmt-cluster-tkg-storageclass tkg-storageclass.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 22tkg-system tkg-stc-mgmt-cluster-vsphere-cpi vsphere-cpi.tanzu.vmware.com 1.24.3+vmware.1-tkg.1 Reconcile succeeded 8d 23tkg-system tkg-stc-mgmt-cluster-vsphere-csi vsphere-csi.tanzu.vmware.com 2.6.2+vmware.2-tkg.1 Reconcile succeeded 8d 24tkg-system tkr-service tkr-service.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 25tkg-system tkr-source-controller tkr-source-controller.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d 26tkg-system tkr-vsphere-resolver tkr-vsphere-resolver.tanzu.vmware.com 0.28.0+vmware.1 Reconcile succeeded 8d TKG Workload cluster deployment Now that we have done all the initial configs to support our TKG environment on vSphere, NSX and Avi, to deploy a workload cluster is as simple as loading a game on the Commodore 64 üìº From your bootstrap machine make sure you are in the context of your TKG Managment cluster:\n1andreasm@tkg-bootstrap:~/.config/tanzu/tkg/providers$ kubectl config current-context 2tkg-stc-mgmt-cluster-admin@tkg-stc-mgmt-cluster I you prefer to deploy your workload clusters in its own Kubernetes namespace go ahead and create a namespace for your workload cluster like this:\n1kubectl create ns \u0026#34;name-of-namespace\u0026#34; Now to create a workload cluster, this also needs a yaml definition file. The easiest way to achieve such a file is to re-use the bootstramp yaml we created for our TKG Management cluster. For more information deploying a workload cluster in TKG read here.By using the Tanzu CLI we can convert this bootstrap file to a workload cluster yaml definiton file, this is done like this:\n1tanzu cluster create stc-tkgm-wld-cluster-1 --namespace=stc-tkgm-ns-1 --file tkg-mgmt-bootstrap-tkg-2.1.yaml --dry-run \u0026gt; stc-tkg-wld-cluster-1.yaml The command above read the bootstrap yaml file we used to deploy the TKG management cluster, converts it into a yaml file we can use to deploy a workload cluster. It alse removes unnecessary fields not needed for our workload cluster. I am also using the --namespace field to point the config to use the correct namespace and automatically put that into the yaml file. then I am pointing to the TKG Management bootstrap yaml file and finally the --dry-run command to pipe it to a file called stc-tkg-wld-cluster-1.yaml. The result should look something like this:\n1apiVersion: cpi.tanzu.vmware.com/v1alpha1 2kind: VSphereCPIConfig 3metadata: 4 name: stc-tkgm-wld-cluster-1 5 namespace: stc-tkgm-ns-1 6spec: 7 vsphereCPI: 8 ipFamily: ipv4 9 mode: vsphereCPI 10 tlsCipherSuites: TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 11--- 12apiVersion: csi.tanzu.vmware.com/v1alpha1 13kind: VSphereCSIConfig 14metadata: 15 name: stc-tkgm-wld-cluster-1 16 namespace: stc-tkgm-ns-1 17spec: 18 vsphereCSI: 19 config: 20 datacenter: /cPod-NSXAM-STC 21 httpProxy: \u0026#34;\u0026#34; 22 httpsProxy: \u0026#34;\u0026#34; 23 noProxy: \u0026#34;\u0026#34; 24 region: null 25 tlsThumbprint: 22:FD 26 useTopologyCategories: false 27 zone: null 28 mode: vsphereCSI 29--- 30apiVersion: run.tanzu.vmware.com/v1alpha3 31kind: ClusterBootstrap 32metadata: 33 annotations: 34 tkg.tanzu.vmware.com/add-missing-fields-from-tkr: v1.24.9---vmware.1-tkg.1 35 name: stc-tkgm-wld-cluster-1 36 namespace: stc-tkgm-ns-1 37spec: 38 additionalPackages: 39 - refName: metrics-server* 40 - refName: secretgen-controller* 41 - refName: pinniped* 42 cpi: 43 refName: vsphere-cpi* 44 valuesFrom: 45 providerRef: 46 apiGroup: cpi.tanzu.vmware.com 47 kind: VSphereCPIConfig 48 name: stc-tkgm-wld-cluster-1 49 csi: 50 refName: vsphere-csi* 51 valuesFrom: 52 providerRef: 53 apiGroup: csi.tanzu.vmware.com 54 kind: VSphereCSIConfig 55 name: stc-tkgm-wld-cluster-1 56 kapp: 57 refName: kapp-controller* 58--- 59apiVersion: v1 60kind: Secret 61metadata: 62 name: stc-tkgm-wld-cluster-1 63 namespace: stc-tkgm-ns-1 64stringData: 65 password: Password 66 username: andreasm@cpod-nsxam-stc.az-stc.cloud-garage.net 67--- 68apiVersion: cluster.x-k8s.io/v1beta1 69kind: Cluster 70metadata: 71 annotations: 72 osInfo: ubuntu,20.04,amd64 73 tkg/plan: dev 74 labels: 75 tkg.tanzu.vmware.com/cluster-name: stc-tkgm-wld-cluster-1 76 name: stc-tkgm-wld-cluster-1 77 namespace: stc-tkgm-ns-1 78spec: 79 clusterNetwork: 80 pods: 81 cidrBlocks: 82 - 100.96.0.0/11 83 services: 84 cidrBlocks: 85 - 100.64.0.0/13 86 topology: 87 class: tkg-vsphere-default-v1.0.0 88 controlPlane: 89 metadata: 90 annotations: 91 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 92 replicas: 1 93 variables: 94 - name: controlPlaneCertificateRotation 95 value: 96 activate: true 97 daysBefore: 90 98 - name: auditLogging 99 value: 100 enabled: false 101 - name: podSecurityStandard 102 value: 103 audit: baseline 104 deactivated: false 105 warn: baseline 106 - name: apiServerEndpoint 107 value: \u0026#34;\u0026#34; 108 - name: aviAPIServerHAProvider 109 value: true 110 - name: vcenter 111 value: 112 cloneMode: fullClone 113 datacenter: /cPod-NSXAM-STC 114 datastore: /cPod-NSXAM-STC/datastore/vsanDatastore 115 folder: /cPod-NSXAM-STC/vm/TKGm 116 network: /cPod-NSXAM-STC/network/ls-tkg-mgmt #Notice this - if you want to place your workload clusters in a different network change this to your desired portgroup. 117 resourcePool: /cPod-NSXAM-STC/host/Cluster/Resources 118 server: vcsa.cpod-nsxam-stc.az-stc.cloud-garage.net 119 storagePolicyID: \u0026#34;\u0026#34; 120 template: /cPod-NSXAM-STC/vm/ubuntu-2004-efi-kube-v1.24.9+vmware.1 121 tlsThumbprint: 22:FD 122 - name: user 123 value: 124 sshAuthorizedKeys: 125 - ssh-rsa 88qv2fowMT65qwpBHUIybHz5Ra2L53zwsv/5yvUej48QLmyAalSNNeH+FIKTkFiuX/WjsHiCIMFisn5dqpc/6x8= 126 - name: controlPlane 127 value: 128 machine: 129 diskGiB: 20 130 memoryMiB: 4096 131 numCPUs: 2 132 - name: worker 133 value: 134 count: 2 135 machine: 136 diskGiB: 20 137 memoryMiB: 4096 138 numCPUs: 2 139 version: v1.24.9+vmware.1 140 workers: 141 machineDeployments: 142 - class: tkg-worker 143 metadata: 144 annotations: 145 run.tanzu.vmware.com/resolve-os-image: image-type=ova,os-name=ubuntu 146 name: md-0 147 replicas: 2 Read through the result, edit if you find something you would like to change. If you want to deploy your workload cluster on a different network than your Management cluster edit this field to reflect the correct portgroup in vCenter:\n1 network: /cPod-NSXAM-STC/network/ls-tkg-mgmt Now that the yaml defintion is ready we can create the first workload cluster like this:\n1tanzu cluster create --file stc-tkg-wld-cluster-1.yaml You can monitor the progress from the terminal of your bootstrap machine. When done check your cluster status with Tanzu CLI (remember to either use -n \u0026quot;nameofnamespace\u0026quot; or just -A):\n1andreasm@tkg-bootstrap:~$ tanzu cluster list -A 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 4 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 Further verifications can be done with this command:\n1andreasm@tkg-bootstrap:~$ tanzu cluster get stc-tkgm-wld-cluster-1 -n stc-tkgm-ns-1 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; v1.24.9---vmware.1-tkg.1 4 5 6Details: 7 8NAME READY SEVERITY REASON SINCE MESSAGE 9/stc-tkgm-wld-cluster-1 True 7d22h 10‚îú‚îÄClusterInfrastructure - VSphereCluster/stc-tkgm-wld-cluster-1-lzjxq True 7d22h 11‚îú‚îÄControlPlane - KubeadmControlPlane/stc-tkgm-wld-cluster-1-22z8x True 7d22h 12‚îÇ ‚îî‚îÄMachine/stc-tkgm-wld-cluster-1-22z8x-jjb66 True 7d22h 13‚îî‚îÄWorkers 14 ‚îî‚îÄMachineDeployment/stc-tkgm-wld-cluster-1-md-0-2qmkw True 3d3h 15 ‚îú‚îÄMachine/stc-tkgm-wld-cluster-1-md-0-2qmkw-6c4789d7b5-lj5wl True 7d22h 16 ‚îî‚îÄMachine/stc-tkgm-wld-cluster-1-md-0-2qmkw-6c4789d7b5-wb7k9 True 7d22h If everything is green its time to get the kubeconfig for the cluster so we can start consume it. This is done like this:\n1tanzu cluster kubeconfig get stc-tkgm-wld-cluster-1 --namespace stc-tkgm-ns-1 --admin --export-file stc-tkgm-wld-cluster-1-k8s-config.yaml Now you can copy this to your preferred workstation and start consuming.\nNote! The kubeconfigs I have used here is all admin privileges and is not something you will use in production where you want to have granular user access. I will create a post around user management in both TKGm and TKGs later.\nThe next sections will cover how to upgrade TKG, some configs on the workload clusters themselves around AKO and Antrea.\nAntrea configs If there is a feature you would like to enable in Antrea in one of your workload clusters, we need to create an AntreaConfig by using the AntreaConfig CRD (this is one way of doing it) and apply it on the Namespace where your workload cluster resides. This is the same approach as we do in vSphere 8 with Tanzu - see here\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: stc-tkgm-wld-cluster-1-antrea-package # notice the naming-convention cluster name-antrea-package 5 namespace: stc-tkgm-ns-1 # your vSphere Namespace the TKC cluster is in. 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: true 14 Egress: true 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Avi/AKO configs In TKGm we can override the default AKO settings by using AKODeploymentConfig CRD. We apply this configuration from the TKG Managment cluster on the respective Workload cluster by using labels. An example of such a config yaml:\n1apiVersion: networking.tkg.tanzu.vmware.com/v1alpha1 2kind: AKODeploymentConfig 3metadata: 4 name: ako-stc-tkgm-wld-cluster-1 5spec: 6 adminCredentialRef: 7 name: avi-controller-credentials 8 namespace: tkg-system-networking 9 certificateAuthorityRef: 10 name: avi-controller-ca 11 namespace: tkg-system-networking 12 cloudName: stc-nsx-cloud 13 clusterSelector: 14 matchLabels: 15 ako-stc-wld-1: \u0026#34;ako-l7\u0026#34; 16 controller: 172.24.3.50 17 dataNetwork: 18 cidr: 10.13.103.0/24 19 name: vip-tkg-wld-l7 20 controlPlaneNetwork: 21 cidr: 10.13.102.0/24 22 name: vip-tkg-wld-l4 23 extraConfigs: 24 cniPlugin: antrea 25 disableStaticRouteSync: false # required 26 ingress: 27 defaultIngressController: true 28 disableIngressClass: false # required 29 nodeNetworkList: # required 30 - cidrs: 31 - 10.13.21.0/24 32 networkName: ls-tkg-wld-1 33 serviceType: NodePortLocal # required 34 shardVSSize: SMALL # required 35 l4Config: 36 autoFQDN: default 37 networksConfig: 38 nsxtT1LR: /infra/tier-1s/Tier-1 39 serviceEngineGroup: tkgm-se-group Notice the:\n1 clusterSelector: 2 matchLabels: 3 ako-stc-wld-1: \u0026#34;ako-l7\u0026#34; We need to apply this label to our workload cluster. From the TKG management cluster list all your clusters:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get cluster -A 2NAMESPACE NAME PHASE AGE VERSION 3stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Provisioned 7d23h v1.24.9+vmware.1 4stc-tkgm-ns-2 stc-tkgm-wld-cluster-2 Provisioned 7d17h v1.24.9+vmware.1 5tkg-system tkg-stc-mgmt-cluster Provisioned 8d v1.24.9+vmware.1 Apply the above label:\n1kubectl label cluster -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 ako-stc-wld-1=ako-l7 Now run the get cluster command again but with the value --show-labels to see if it has been applied:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get cluster -A --show-labels 2NAMESPACE NAME PHASE AGE VERSION LABELS 3stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 Provisioned 7d23h v1.24.9+vmware.1 ako-stc-wld-1=ako-l7,cluster.x-k8s.io/cluster-name=stc-tkgm-wld-cluster-1,networking.tkg.tanzu.vmware.com/avi=ako-stc-tkgm-wld-cluster-1,run.tanzu.vmware.com/tkr=v1.24.9---vmware.1-tkg.1,tkg.tanzu.vmware.com/cluster-name=stc-tkgm-wld-cluster-1,topology.cluster.x-k8s.io/owned= Looks good. Then we can apply the AKODeploymentConfig above.\n1k apply -f ako-wld-cluster-1.yaml Verify if the AKODeploymentConfig has been applied:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get akodeploymentconfigs.networking.tkg.tanzu.vmware.com 2NAME AGE 3ako-stc-tkgm-wld-cluster-1 7d21h 4ako-stc-tkgm-wld-cluster-2 7d6h 5install-ako-for-all 8d 6install-ako-for-management-cluster 8d Now head back your workload cluster and check the AKO pod whether it has been restarted, if you dont want to wait you can always delete the pod to speed up the changes. To verify the changes have a look at the ako configmap like this:\n1amarqvardsen@amarqvards1MD6T:~/Kubernetes-library/tkgm/stc-tkgm/stc-tkgm-wld-cluster-1$ k get configmaps -n avi-system avi-k8s-config -oyaml 2apiVersion: v1 3data: 4 apiServerPort: \u0026#34;8080\u0026#34; 5 autoFQDN: default 6 cloudName: stc-nsx-cloud 7 clusterName: stc-tkgm-ns-1-stc-tkgm-wld-cluster-1 8 cniPlugin: antrea 9 controllerIP: 172.24.3.50 10 controllerVersion: 22.1.2 11 defaultIngController: \u0026#34;true\u0026#34; 12 deleteConfig: \u0026#34;false\u0026#34; 13 disableStaticRouteSync: \u0026#34;false\u0026#34; 14 fullSyncFrequency: \u0026#34;1800\u0026#34; 15 logLevel: INFO 16 nodeNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;ls-tkg-wld-1\u0026#34;,\u0026#34;cidrs\u0026#34;:[\u0026#34;10.13.21.0/24\u0026#34;]}]\u0026#39; 17 nsxtT1LR: /infra/tier-1s/Tier-1 18 serviceEngineGroupName: tkgm-se-group 19 serviceType: NodePortLocal 20 shardVSSize: SMALL 21 vipNetworkList: \u0026#39;[{\u0026#34;networkName\u0026#34;:\u0026#34;vip-tkg-wld-l7\u0026#34;,\u0026#34;cidr\u0026#34;:\u0026#34;10.13.103.0/24\u0026#34;}]\u0026#39; 22kind: ConfigMap 23metadata: 24 annotations: 25 kapp.k14s.io/identity: v1;avi-system//ConfigMap/avi-k8s-config;v1 26 kapp.k14s.io/original: \u0026#39;{\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;data\u0026#34;:{\u0026#34;apiServerPort\u0026#34;:\u0026#34;8080\u0026#34;,\u0026#34;autoFQDN\u0026#34;:\u0026#34;default\u0026#34;,\u0026#34;cloudName\u0026#34;:\u0026#34;stc-nsx-cloud\u0026#34;,\u0026#34;clusterName\u0026#34;:\u0026#34;stc-tkgm-ns-1-stc-tkgm-wld-cluster-1\u0026#34;,\u0026#34;cniPlugin\u0026#34;:\u0026#34;antrea\u0026#34;,\u0026#34;controllerIP\u0026#34;:\u0026#34;172.24.3.50\u0026#34;,\u0026#34;controllerVersion\u0026#34;:\u0026#34;22.1.2\u0026#34;,\u0026#34;defaultIngController\u0026#34;:\u0026#34;true\u0026#34;,\u0026#34;deleteConfig\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;disableStaticRouteSync\u0026#34;:\u0026#34;false\u0026#34;,\u0026#34;fullSyncFrequency\u0026#34;:\u0026#34;1800\u0026#34;,\u0026#34;logLevel\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;nodeNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;ls-tkg-wld-1\\\u0026#34;,\\\u0026#34;cidrs\\\u0026#34;:[\\\u0026#34;10.13.21.0/24\\\u0026#34;]}]\u0026#34;,\u0026#34;nsxtT1LR\u0026#34;:\u0026#34;/infra/tier-1s/Tier-1\u0026#34;,\u0026#34;serviceEngineGroupName\u0026#34;:\u0026#34;tkgm-se-group\u0026#34;,\u0026#34;serviceType\u0026#34;:\u0026#34;NodePortLocal\u0026#34;,\u0026#34;shardVSSize\u0026#34;:\u0026#34;SMALL\u0026#34;,\u0026#34;vipNetworkList\u0026#34;:\u0026#34;[{\\\u0026#34;networkName\\\u0026#34;:\\\u0026#34;vip-tkg-wld-l7\\\u0026#34;,\\\u0026#34;cidr\\\u0026#34;:\\\u0026#34;10.13.103.0/24\\\u0026#34;}]\u0026#34;},\u0026#34;kind\u0026#34;:\u0026#34;ConfigMap\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;kapp.k14s.io/app\u0026#34;:\u0026#34;1678977773033139694\u0026#34;,\u0026#34;kapp.k14s.io/association\u0026#34;:\u0026#34;v1.ae838cced3b6caccc5a03bfb3ae65cd7\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;avi-k8s-config\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;avi-system\u0026#34;}}\u0026#39; 27 kapp.k14s.io/original-diff-md5: c6e94dc94aed3401b5d0f26ed6c0bff3 28 creationTimestamp: \u0026#34;2023-03-16T14:43:11Z\u0026#34; 29 labels: 30 kapp.k14s.io/app: \u0026#34;1678977773033139694\u0026#34; 31 kapp.k14s.io/association: v1.ae838cced3b6caccc5a03bfb3ae65cd7 32 name: avi-k8s-config 33 namespace: avi-system 34 resourceVersion: \u0026#34;19561\u0026#34; 35 uid: 1baa90b2-e5d7-4177-ae34-6c558b5cfe29 It should reflect the changes we applied...\nAntrea RBAC Antrea comes with a list of Tiers where we can place our Antrea Native Policies. These can also be used to restrict who is allowed to apply policies and not. See this page for more information for now. I will update this section later with my own details - including the integration with NSX.\nUpgrade TKG (from 2.1 to 2.1.1) When a new TKG relase is available we can upgrade to use this new release. The steps I have followed are explained in detail here. I recommend to always follow the updated information there.\nTo upgrade TKG these are the typical steps:\nDownload the latest Tanzu CLI - from my.vmware.com Download the latest Tanzu kubectl - from my.vmware.com Download the latest Photon or Ubuntu OVA VM template - from my.vmware.com Upgrade the TKG Management cluster Upgrade the TKG Workload clusters So lets get into it.\nUpgrade CLI tools and dependencies I have already downloaded the Ubuntu VM image for version 2.1.1 into my vCenter and converted it to a template. I have also downloaded the Tanzu CLI tools and Tanzu kubectl for version 2.1.1. Now I need to install the Tanzu CLI and Tanzu kubectl. So I will getting back into my bootstrap machine used previously where I already have Tanzu CLI 2.1 installed.\nThe first thing I need to is to delete the following file:\n1~/.config/tanzu/tkg/compatibility/tkg-compatibility.yaml Extract the downloaded Tanzu CLI 2.1.1 packages (this will create a cli folder where you are placed. So if you want to use another folder create this first and extract the file in there) :\n1tar -xvf tanzu-cli-bundle-linux-amd64.tar.gz 1andreasm@tkg-bootstrap:~/tanzu$ tar -xvf tanzu-cli-bundle-linux-amd64.2.1.1.tar.gz 2cli/ 3cli/core/ 4cli/core/v0.28.1/ 5cli/core/v0.28.1/tanzu-core-linux_amd64 6cli/tanzu-framework-plugins-standalone-linux-amd64.tar.gz 7cli/tanzu-framework-plugins-context-linux-amd64.tar.gz 8cli/ytt-linux-amd64-v0.43.1+vmware.1.gz 9cli/kapp-linux-amd64-v0.53.2+vmware.1.gz 10cli/imgpkg-linux-amd64-v0.31.1+vmware.1.gz 11cli/kbld-linux-amd64-v0.35.1+vmware.1.gz 12cli/vendir-linux-amd64-v0.30.1+vmware.1.gz Navigate to the cli folder and install the different packages.\nInstall Tanzu CLI:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ sudo install core/v0.28.1/tanzu-core-linux_amd64 /usr/local/bin/tanzu Initialize the Tanzu CLI:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu init 2‚Ñπ Checking for required plugins... 3‚Ñπ Installing plugin \u0026#39;secret:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 4‚Ñπ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; 5‚Ñπ Installing plugin \u0026#39;login:v0.28.1\u0026#39; 6‚Ñπ Installing plugin \u0026#39;management-cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 7‚Ñπ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 8‚Ñπ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; 9‚Ñπ Installing plugin \u0026#39;telemetry:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 10‚Ñπ Successfully installed all required plugins 11‚úî successfully initialized CLI Verify version:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu version 2version: v0.28.1 3buildDate: 2023-03-07 4sha: 0e6704777-dirty Now the Tanzu plugins:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin clean 2‚úî successfully cleaned up all plugins 1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin sync 2‚Ñπ Checking for required plugins... 3‚Ñπ Installing plugin \u0026#39;management-cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 4‚Ñπ Installing plugin \u0026#39;secret:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 5‚Ñπ Installing plugin \u0026#39;telemetry:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 6‚Ñπ Installing plugin \u0026#39;cluster:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; 7‚Ñπ Installing plugin \u0026#39;kubernetes-release:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; 8‚Ñπ Installing plugin \u0026#39;login:v0.28.1\u0026#39; 9‚Ñπ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 10‚Ñπ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; 11‚Ñπ Installing plugin \u0026#39;feature:v0.28.0\u0026#39; with target \u0026#39;kubernetes\u0026#39; 12‚Ñπ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; 13‚úñ [unable to fetch the plugin metadata for plugin \u0026#34;login\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;package\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;pinniped-auth\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64, unable to fetch the plugin metadata for plugin \u0026#34;isolated-cluster\u0026#34;: could not find the artifact for version:v0.28.1, os:linux, arch:amd64] 14andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin sync 15‚Ñπ Checking for required plugins... 16‚Ñπ Installing plugin \u0026#39;pinniped-auth:v0.28.1\u0026#39; 17‚Ñπ Installing plugin \u0026#39;isolated-cluster:v0.28.1\u0026#39; 18‚Ñπ Installing plugin \u0026#39;login:v0.28.1\u0026#39; 19‚Ñπ Installing plugin \u0026#39;package:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 20‚Ñπ Successfully installed all required plugins 21‚úî Done Note! I had to run the comand twice as I ecountered an issue on first try. Now list the plugins:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ tanzu plugin list 2Standalone Plugins 3 NAME DESCRIPTION TARGET DISCOVERY VERSION STATUS 4 isolated-cluster isolated-cluster operations default v0.28.1 installed 5 login Login to the platform default v0.28.1 installed 6 pinniped-auth Pinniped authentication operations (usually not directly invoked) default v0.28.1 installed 7 management-cluster Kubernetes management-cluster operations kubernetes default v0.28.1 installed 8 package Tanzu package management kubernetes default v0.28.1 installed 9 secret Tanzu secret management kubernetes default v0.28.1 installed 10 telemetry Configure cluster-wide telemetry settings kubernetes default v0.28.1 installed 11 12Plugins from Context: tkg-stc-mgmt-cluster 13 NAME DESCRIPTION TARGET VERSION STATUS 14 cluster Kubernetes cluster operations kubernetes v0.28.0 installed 15 feature Operate on features and featuregates kubernetes v0.28.0 installed 16 kubernetes-release Kubernetes release operations kubernetes v0.28.0 installed Install the Tanzu kubectl:\n1andreasm@tkg-bootstrap:~/tanzu$ gunzip kubectl-linux-v1.24.10+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu$ chmod ugo+x kubectl-linux-v1.24.10+vmware.1 3andreasm@tkg-bootstrap:~/tanzu$ sudo install kubectl-linux-v1.24.10+vmware.1 /usr/local/bin/kubectl Check version:\n1andreasm@tkg-bootstrap:~/tanzu$ kubectl version 2WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short. Use --output=yaml|json to get the full version. 3Client Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.10+vmware.1\u0026#34;, GitCommit:\u0026#34;b980a736cbd2ac0c5f7ca793122fd4231f705889\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2023-01-24T15:36:34Z\u0026#34;, GoVersion:\u0026#34;go1.19.5\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} 4Kustomize Version: v4.5.4 5Server Version: version.Info{Major:\u0026#34;1\u0026#34;, Minor:\u0026#34;24\u0026#34;, GitVersion:\u0026#34;v1.24.9+vmware.1\u0026#34;, GitCommit:\u0026#34;d1d7c19c9b6265a8dcd1b2ab2620ec0fc7cee784\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, BuildDate:\u0026#34;2022-12-14T06:23:39Z\u0026#34;, GoVersion:\u0026#34;go1.18.9\u0026#34;, Compiler:\u0026#34;gc\u0026#34;, Platform:\u0026#34;linux/amd64\u0026#34;} Install the Carvel tools. From the cli folder first out is ytt. Install ytt:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip ytt-linux-amd64-v0.43.1+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x ytt-linux-amd64-v0.43.1+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./ytt-linux-amd64-v0.43.1+vmware.1 /usr/local/bin/ytt 4andreasm@tkg-bootstrap:~/tanzu/cli$ ytt --version 5ytt version 0.43.1 Instal kapp:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip kapp-linux-amd64-v0.53.2+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x kapp-linux-amd64-v0.53.2+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./kapp-linux-amd64-v0.53.2+vmware.1 /usr/local/bin/kapp 4andreasm@tkg-bootstrap:~/tanzu/cli$ kapp --version 5kapp version 0.53.2 6 7Succeeded Install kbld:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip kbld-linux-amd64-v0.35.1+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x kbld-linux-amd64-v0.35.1+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./kbld-linux-amd64-v0.35.1+vmware.1 /usr/local/bin/kbld 4andreasm@tkg-bootstrap:~/tanzu/cli$ kbld --version 5kbld version 0.35.1 6 7Succeeded Install imgpkg:\n1andreasm@tkg-bootstrap:~/tanzu/cli$ gunzip imgpkg-linux-amd64-v0.31.1+vmware.1.gz 2andreasm@tkg-bootstrap:~/tanzu/cli$ chmod ugo+x imgpkg-linux-amd64-v0.31.1+vmware.1 3andreasm@tkg-bootstrap:~/tanzu/cli$ sudo mv ./imgpkg-linux-amd64-v0.31.1+vmware.1 /usr/local/bin/imgpkg 4andreasm@tkg-bootstrap:~/tanzu/cli$ imgpkg --version 5imgpkg version 0.31.1 6 7Succeeded We have done the verification of the different versions, but we should have Tanzu cli version v0.28.1\nUpgrade the TKG Management cluster Now we can proceed with the upgrade process. One important document to check is this! Known Issues... Check whether you are using environments, if you happen to use them we need to unset them.\n1andreasm@tkg-bootstrap:~/tanzu/cli$ printenv I am clear here and will now start the upgrading of my standalone TKG Management cluster Make sure you are in the context of the TKG management cluster and that you have converted the new Ubuntu VM image as template.\n1andreasm@tkg-bootstrap:~$ kubectl config current-context 2tkg-stc-mgmt-cluster-admin@tkg-stc-mgmt-cluster If not, use the following command:\n1andreasm@tkg-bootstrap:~$ tanzu login 2? Select a server [Use arrows to move, type to filter] 3\u0026gt; tkg-stc-mgmt-cluster() 4 + new server 1andreasm@tkg-bootstrap:~$ tanzu login 2? Select a server tkg-stc-mgmt-cluster() 3‚úî successfully logged in to management cluster using the kubeconfig tkg-stc-mgmt-cluster 4‚Ñπ Checking for required plugins... 5‚Ñπ All required plugins are already installed and up-to-date Here goes: (To start the upgrade of the management cluster)\n1andreasm@tkg-bootstrap:~$ tanzu mc upgrade 2Upgrading management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; to TKG version \u0026#39;v2.1.1\u0026#39; with Kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;. Are you sure? [y/N]: Eh.... yes...\nProgress:\n1andreasm@tkg-bootstrap:~$ tanzu mc upgrade 2Upgrading management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; to TKG version \u0026#39;v2.1.1\u0026#39; with Kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;. Are you sure? [y/N]: y 3Validating the compatibility before management cluster upgrade 4Validating for the required environment variables to be set 5Validating for the user configuration secret to be existed in the cluster 6Warning: unable to find component \u0026#39;kube_rbac_proxy\u0026#39; under BoM 7Upgrading management cluster providers... 8 infrastructure-ipam-in-cluster provider\u0026#39;s version is missing in BOM file, so it would not be upgraded 9Checking cert-manager version... 10Cert-manager is already up to date 11Performing upgrade... 12Scaling down Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-system\u0026#34; 13Scaling down Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; 14Scaling down Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; 15Scaling down Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capv-system\u0026#34; 16Deleting Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-system\u0026#34; 17Installing Provider=\u0026#34;cluster-api\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-system\u0026#34; 18Deleting Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; 19Installing Provider=\u0026#34;bootstrap-kubeadm\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-kubeadm-bootstrap-system\u0026#34; 20Deleting Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; 21Installing Provider=\u0026#34;control-plane-kubeadm\u0026#34; Version=\u0026#34;v1.2.8\u0026#34; TargetNamespace=\u0026#34;capi-kubeadm-control-plane-system\u0026#34; 22Deleting Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;\u0026#34; Namespace=\u0026#34;capv-system\u0026#34; 23Installing Provider=\u0026#34;infrastructure-vsphere\u0026#34; Version=\u0026#34;v1.5.3\u0026#34; TargetNamespace=\u0026#34;capv-system\u0026#34; 24Management cluster providers upgraded successfully... 25Preparing addons manager for upgrade 26Upgrading kapp-controller... 27Adding last-applied annotation on kapp-controller... 28Removing old management components... 29Upgrading management components... 30‚Ñπ Updating package repository \u0026#39;tanzu-management\u0026#39; 31‚Ñπ Getting package repository \u0026#39;tanzu-management\u0026#39; 32‚Ñπ Validating provided settings for the package repository 33‚Ñπ Updating package repository resource 34‚Ñπ Waiting for \u0026#39;PackageRepository\u0026#39; reconciliation for \u0026#39;tanzu-management\u0026#39; 35‚Ñπ \u0026#39;PackageRepository\u0026#39; resource install status: Reconciling 36‚Ñπ \u0026#39;PackageRepository\u0026#39; resource install status: ReconcileSucceeded 37‚Ñπ Updated package repository \u0026#39;tanzu-management\u0026#39; in namespace \u0026#39;tkg-system\u0026#39; 38‚Ñπ Installing package \u0026#39;tkg.tanzu.vmware.com\u0026#39; 39‚Ñπ Updating package \u0026#39;tkg-pkg\u0026#39; 40‚Ñπ Getting package install for \u0026#39;tkg-pkg\u0026#39; 41‚Ñπ Getting package metadata for \u0026#39;tkg.tanzu.vmware.com\u0026#39; 42‚Ñπ Updating secret \u0026#39;tkg-pkg-tkg-system-values\u0026#39; 43‚Ñπ Updating package install for \u0026#39;tkg-pkg\u0026#39; 44‚Ñπ Waiting for \u0026#39;PackageInstall\u0026#39; reconciliation for \u0026#39;tkg-pkg\u0026#39; 45‚Ñπ \u0026#39;PackageInstall\u0026#39; resource install status: ReconcileSucceeded 46‚Ñπ Updated installed package \u0026#39;tkg-pkg\u0026#39; 47Cleanup core packages repository... 48Core package repository not found, no need to cleanup 49Upgrading management cluster kubernetes version... 50Upgrading kubernetes cluster to `v1.24.10+vmware.1` version, tkr version: `v1.24.10+vmware.1-tkg.2` 51Waiting for kubernetes version to be updated for control plane nodes... 52Waiting for kubernetes version to be updated for worker nodes... In vCenter we should start see some action also:\nTwo control plane nodes:\nNo longer:\n1management cluster is opted out of telemetry - skipping telemetry image upgrade 2Creating tkg-bom versioned ConfigMaps... 3Management cluster \u0026#39;tkg-stc-mgmt-cluster\u0026#39; successfully upgraded to TKG version \u0026#39;v2.1.1\u0026#39; with kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39; 4‚Ñπ Checking for required plugins... 5‚Ñπ Installing plugin \u0026#39;kubernetes-release:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 6‚Ñπ Installing plugin \u0026#39;cluster:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 7‚Ñπ Installing plugin \u0026#39;feature:v0.28.1\u0026#39; with target \u0026#39;kubernetes\u0026#39; 8‚Ñπ Successfully installed all required plugins Well, it finished successfully.\nLets verify with Tanzu CLI:\n1andreasm@tkg-bootstrap:~$ tanzu cluster list --include-management-cluster -A 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 4 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 5 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Looks good, notice the different versions. Management cluster is upgraded to latest version, workload clusters are still on its older version. They are up next.\nLets do a last check before we head to Workload cluster upgrade.\n1andreasm@tkg-bootstrap:~$ tanzu mc get 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 4 5 6Details: 7 8NAME READY SEVERITY REASON SINCE MESSAGE 9/tkg-stc-mgmt-cluster True 17m 10‚îú‚îÄClusterInfrastructure - VSphereCluster/tkg-stc-mgmt-cluster-xw6xs True 8d 11‚îú‚îÄControlPlane - KubeadmControlPlane/tkg-stc-mgmt-cluster-wrxtl True 17m 12‚îÇ ‚îî‚îÄMachine/tkg-stc-mgmt-cluster-wrxtl-csrnt True 24m 13‚îî‚îÄWorkers 14 ‚îî‚îÄMachineDeployment/tkg-stc-mgmt-cluster-md-0-vs9dc True 10m 15 ‚îú‚îÄMachine/tkg-stc-mgmt-cluster-md-0-vs9dc-54554f9575-7hdfc True 14m 16 ‚îî‚îÄMachine/tkg-stc-mgmt-cluster-md-0-vs9dc-54554f9575-ng9lx True 7m4s 17 18 19Providers: 20 21 NAMESPACE NAME TYPE PROVIDERNAME VERSION WATCHNAMESPACE 22 caip-in-cluster-system infrastructure-ipam-in-cluster InfrastructureProvider ipam-in-cluster v0.1.0 23 capi-kubeadm-bootstrap-system bootstrap-kubeadm BootstrapProvider kubeadm v1.2.8 24 capi-kubeadm-control-plane-system control-plane-kubeadm ControlPlaneProvider kubeadm v1.2.8 25 capi-system cluster-api CoreProvider cluster-api v1.2.8 26 capv-system infrastructure-vsphere InfrastructureProvider vsphere v1.5.3 Congrats, head over to next level üòÑ\nUpgrade workload cluster This procedure is much simpler, almost as simple as starting a game in MS-DOS 6.2 requiring a bit over 600kb convential memory. Make sure your are still in the TKG Management cluster context.\nAs done above list out the cluster you have and notice the versions they are on now.:\n1andreasm@tkg-bootstrap:~$ tanzu cluster list --include-management-cluster -A 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES PLAN TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 4 stc-tkgm-wld-cluster-2 stc-tkgm-ns-2 running 1/1 2/2 v1.24.9+vmware.1 \u0026lt;none\u0026gt; dev v1.24.9---vmware.1-tkg.1 5 tkg-stc-mgmt-cluster tkg-system running 1/1 2/2 v1.24.10+vmware.1 management dev v1.24.10---vmware.1-tkg.2 Check if there are any new releases available from the management cluster:\n1andreasm@tkg-bootstrap:~$ tanzu kubernetes-release get 2 NAME VERSION COMPATIBLE ACTIVE UPDATES AVAILABLE 3 v1.22.17---vmware.1-tkg.2 v1.22.17+vmware.1-tkg.2 True True 4 v1.23.16---vmware.1-tkg.2 v1.23.16+vmware.1-tkg.2 True True 5 v1.24.10---vmware.1-tkg.2 v1.24.10+vmware.1-tkg.2 True True There is one there.. v1.24.10 and its compatible.\nLets check whether there are any updates ready for our workload cluster:\n1andreasm@tkg-bootstrap:~$ tanzu cluster available-upgrades get -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 2 NAME VERSION COMPATIBLE 3 v1.24.10---vmware.1-tkg.2 v1.24.10+vmware.1-tkg.2 True It is...\nLets upgrade it:\n1andreasm@tkg-bootstrap:~$ tanzu cluster upgrade -n stc-tkgm-ns-1 stc-tkgm-wld-cluster-1 2Upgrading workload cluster \u0026#39;stc-tkgm-wld-cluster-1\u0026#39; to kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39;, tkr version \u0026#39;v1.24.10+vmware.1-tkg.2\u0026#39;. Are you sure? [y/N]: y 3Upgrading kubernetes cluster to `v1.24.10+vmware.1` version, tkr version: `v1.24.10+vmware.1-tkg.2` 4Waiting for kubernetes version to be updated for control plane nodes... y for YES\nSit back and wait for the upgrade process is to do its thing. You can monitor the output from the current terminal, and if something is happening in vCenter. Clone operations, power on, power off and delete.\nAnd the result is in:\n1Waiting for kubernetes version to be updated for worker nodes... 2Cluster \u0026#39;stc-tkgm-wld-cluster-1\u0026#39; successfully upgraded to kubernetes version \u0026#39;v1.24.10+vmware.1\u0026#39; We have a winner.\nLets quickly check with Tanzu CLI:\n1andreasm@tkg-bootstrap:~$ tanzu cluster get stc-tkgm-wld-cluster-1 -n stc-tkgm-ns-1 2 NAME NAMESPACE STATUS CONTROLPLANE WORKERS KUBERNETES ROLES TKR 3 stc-tkgm-wld-cluster-1 stc-tkgm-ns-1 running 1/1 2/2 v1.24.10+vmware.1 \u0026lt;none\u0026gt; v1.24.10---vmware.1-tkg.2 4 5 6Details: 7 8NAME READY SEVERITY REASON SINCE MESSAGE 9/stc-tkgm-wld-cluster-1 True 11m 10‚îú‚îÄClusterInfrastructure - VSphereCluster/stc-tkgm-wld-cluster-1-lzjxq True 8d 11‚îú‚îÄControlPlane - KubeadmControlPlane/stc-tkgm-wld-cluster-1-22z8x True 11m 12‚îÇ ‚îî‚îÄMachine/stc-tkgm-wld-cluster-1-22z8x-mtpgs True 15m 13‚îî‚îÄWorkers 14 ‚îî‚îÄMachineDeployment/stc-tkgm-wld-cluster-1-md-0-2qmkw True 39m 15 ‚îú‚îÄMachine/stc-tkgm-wld-cluster-1-md-0-2qmkw-58c5764865-7xvfn True 8m31s 16 ‚îî‚îÄMachine/stc-tkgm-wld-cluster-1-md-0-2qmkw-58c5764865-c7rqj True 3m29s Couldn't be better. Thats it then. Its Friday so have a great weekend and thanks for reading.\n","link":"https://blog.andreasm.io/2023/03/22/tanzu-kubernetes-grid-2.1/","section":"post","tags":["TKG 2.1","AVI","AKO","NSX","KUBERNETES","TANZU"],"title":"Tanzu Kubernetes Grid 2.1"},{"body":"","link":"https://blog.andreasm.io/tags/tkg-2.1/","section":"tags","tags":null,"title":"TKG 2.1"},{"body":"","link":"https://blog.andreasm.io/categories/blog/","section":"categories","tags":null,"title":"Blog"},{"body":"","link":"https://blog.andreasm.io/tags/github/","section":"tags","tags":null,"title":"github"},{"body":"","link":"https://blog.andreasm.io/categories/github/","section":"categories","tags":null,"title":"Github"},{"body":"Hugo on Github: I have been running my blog page locally on Kubernetes for a long time now. And it has worked very well. But one always have to try something new, and I have always wanted to explore the option to host it on Github to have one maintenance task less to worry about. To get started with this I got some absolutely great help from my colleague Robert who put me into the right track to get this project rolling. In short this post will cover how I did it (with the help from Robert). The moving parts used in this post is Github, Github Pages, Hugo, git, git submodules, a DNS record for my custom domain name and a Linux terminal with git installed. In Github we will end up with two repositories, one for the Hugo files themselves, and one which will be used our Github Page (the actual webpage of your blog). The goal is to be able to add content and update your blog with just a few commands and it is live. Preparations in Github In my Github account I create two repositories, one for the \u0026quot;Hugo\u0026quot; contents itself (config, themes, contents etc) and one repository which will host the actual Github page itself. Lets dive into the details üòÑ\nIf not already a Github user, head over to Github.com and create yourself a user account. When logged into your Github account, create two repositories:\nOne repository is where you have all your Hugo files, content, posts, pages, config folders, css, archetypes and public folder when generating your pages. This repository is created like this in Github:\nAs this is a free Github account the only option is a Public repository.\nNow the second repositiory is created identically but with one important difference, the repository name. This has to start with your username and the github domain github.io (discard the red warning in example below, I already have my repository created using same name). This repository will be used to host your blog's frontpage/webpage. This is referred to as Github Pages\nNext we need to clone into our two newly created repositories.\nGit To clone a public repo there is no need to authenticate, but you would like to create your content locally and push them to your remote git repo so we need to authenticate to our Github account. Github dont use password and username for git authentication, but instead SSH keys. And when cloning into your repo one need to use the correct way to clone it for the authentication to work. More on that later. First prepare your git environment on your workstation.\nSSH keys On your workstation generate your SSH keys (if not already done):\n1ssh-keygen -t rsa -b 4096 -C \u0026#34;andreasm@ubuntulaptop\u0026#34; Answer default to prompts, enter a desired passphrase if wanted leave empty without any passphrase.\nNow that the SSH keys as generated copy the content from the ~/.ssh/id_rsa.pub by issuing something like this cat ~/.ssh/id_rsa.pub and copy the wole content. Go into your Github account click on your user top right corner and -\u0026gt; settings\nThen SSH and GPT keys:\nAnd then New SSH Key and paste your SSH key. Give it a name.\nNow your Github account can authenticate your workstation by using its public SSH key. To use this approach one have to clone into a project by using git clone git@github.com:andreasm80/blog.local.git where the git@github.com:andreasm80/blog.local.git is found from your repository by clicking at the green Code on your repo:\nGit - continued Now that the SSH keys are configured, our workstation is prepared to authenticate against our Github repositories. Create a folder in your Linux workstation, called something with github or whatever you like. Enter into that folder. Now enter the following command:\n1git clone git@github.com:andreasm80/blog.local.git If you dont specify a folder at the end of the command, git will just create a folder with the same name as the repository your are cloning. If you dont want that add a folder name at the end like this:\n1git clone git@github.com:andreasm80/blog.local.git localfoldername Now enter into the newly created folder: cd blog.local To verify that you are \u0026quot;linked\u0026quot; to the correct repository run the following command:\n1git remote -vvv 2origin\tgit@github.com:andreasm80/blog.local.git (fetch) 3origin\tgit@github.com:andreasm80/blog.local.git (push) I already have a Hugo \u0026quot;project\u0026quot; folder where I have my blog page content stored. Instead of just re-using this folder as is I copy all the content to this new folder. Then I delete the public folder (in the new folder) as this will be recreated later. Before pushing all the newly copied content into the cloned github folder above I need to do some preparation for git.\n1git config --global user.email \u0026#34;email@email.com\u0026#34; #is used to sign your commits 2git config --global user.name \u0026#34;AndreasM\u0026#34; #is used to sign your commits - can be whatever name you want Then I need to tell git which files it should track, commit and push when I am ready. To do this type in:\n1git add . #notice the \u0026#34;.\u0026#34; This means I will add all files in the folder. If you dont tell git this it will not commit and push them. You can check this with the following command:\n1andreasm@ubuntu:~/git status 2On branch main 3Your branch is up to date with \u0026#39;origin/main\u0026#39;. 4 5Changes not staged for commit: 6 (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) 7 (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) 8\tmodified: public (new commits) 9 10Untracked files: 11 (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 12\tcontent/post/2023-03-04-running-hugo-on-github/ 13 14no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Now you can commit by doing this command:\n1git commit -s -m \u0026#34;comment-description\u0026#34; # the -s is for signoff and the -m is the comment/message And the last thing to do now is to push the files locally to your remote github repository.\n1git push If you go into your Github page now you will see all your files there in the repo above, but still there is no working blog-page yet.\nGit submodules As explained above, we need to create some kind of \u0026quot;softlink\u0026quot; for the folder public to point to the repository we will use as our web-page. In git we can use submodules for that. The reason for that is each time you generate your Hugo content, Hugo will create a public folder which contains the actual HTML files. And instead of copying the content of this folder each time you generate your webpage we link this folder to our Github Page repository. This is how to enable git submodules.\nWhile still in your blog.local folder (root of the first repository) enter the following:\n1git submodule add git@github.com:andreasm80/andreasm80.github.io.git public/ #the url is from my second repo being used for Github Pages With the above command I am instructing git to create a submodule point to my remote Github repository I will use as my Github Page and also pointing it to the local folder public.\nThen the next command is to initialize the submodule:\n1git submodule init 2#or a specific module as below 3git submodule init public/ In the current folder we should have file called .gitmodules have a look inside and it should contain something like this:\n1[submodule \u0026#34;public\u0026#34;] 2 path = public 3 url = git@github.com:andreasm80/andreasm80.github.io.git This is how my folder structure looks like now:\n1github/blog.local 2archetypes 3assets 4config 5content 6.git 7.gitmodules 8go.mod 9go.sum 10.hugo_build.lock 11layouts 12LICENSE 13public #this folder was created by the git submodules command 14README.md 15resources 16static Check the status on the submodule you have created:\n1git submodule status 2+6afb3af12e86416ad8ff255d9042d89bd9ddc719 public (heads/master) # status symbols infront, sha, name of submodule and branch Commit the changes we have done so far in our blog.local repo:\n1git add . 2git commit -s -m \u0026#34;added submodule public\u0026#34; 3git push Head over to your Github blog.local repo and you should see that the public folder is quite different from the others:\nAnd if you click on it you will be redirected to the repository of your second repository used for your Github Page.\nNow back to the CLI terminal...\nTo have something to populate the public folder with we can now run the `hugo -v -D' command to generate our webpage. It will write everything needed in our public folder.\n1hugo -v -D Now cd into the public folder. Check the repo it is pointing to:\n1git remote -vvv 2origin\tgit@github.com:andreasm80/andreasm80.github.io.git (fetch) 3origin\tgit@github.com:andreasm80/andreasm80.github.io.git (push) Check if there is something new or untracked here:\n1git status 2Untracked files: 3 (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) 4\t2023/03/04/hosting-my-blog-on-github/ 5\ttags/static-site-generator/ 6 7no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Run the following commands:\n1git add . 2Changes to be committed: 3 (use \u0026#34;git restore --staged \u0026lt;file\u0026gt;...\u0026#34; to unstage) 4\tnew file: content/post/2023-03-04-running-hugo-on-github/images/GitHub-Logo300px.png 5\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305091118809.png 6\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305091558280.png 7\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305092142364.png 8\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305094906285.png 9\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100514339.png 10\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100657366.png 11\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305100847587.png 12\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305101357702.png 13\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305112013507.png 14\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305112521903.png 15\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305113016848.png 16\tnew file: content/post/2023-03-04-running-hugo-on-github/images/image-20230305113102660.png 17\tnew file: content/post/2023-03-04-running-hugo-on-github/index.md 18\tmodified: public 19 20 21git commit -s -m \u0026#34;added content\u0026#34; 22git push Now Github should start build your blog-page. This can be seen under Actions here:\nUpdating my blog with this content:\nFor backup reasons it could also be smart to commit and push the blog.local folder also.\nGo back one level (from public to blog.local folder, root)\n1git status 2git add . 3git commit -s -m \u0026#34;added-content\u0026#34; 4git push New machine - set up environment If you need to start from scratch, you have a new machine or whatever you dont have to go through all the steps above, you only need to add the SSH key, creating the git config -global settings. After that It is just as simple as doing this:\n1git clone --recurse-submodules git@github.com:andreasm80/blog.local.git #this will also clone submodules 2# then it is just regular git commands: 3git add . 4git commit -s -m \u0026#34;message\u0026#34; 5git push Custom domain - Github Pages If you would like to present your Github Page (aka blog page) on a different domain you happen to own head over to settings in your Github Pages repository:\nThe on the left side click on Pages\nAnd in Pages type in your custom domain here and enable Enforce HTTPS:\nWhen you click save Github will place a file called CNAME in the root of your Github pages repository (mine andreasm80.github.io) where the content is the dns record you have entered in the Custom Domain field above. So you would need to fetch this locally with git to be in sync again.\n1# you can either enter your submodule(s) directory and run: 2git fetch 3# or you can stay in the \u0026#34;root\u0026#34; folder and enter: 4git submodule foreach \u0026#39;git fetch\u0026#39; 5#which will do a fetch on all submodules Now you need to go to your DNS provider and add a CNAME pointing to your Gitub pages repository name, in my case that is andreasm80.github.io. So I have created this cname record:\nGithub will manage the certificate for your Github Page, so you dont have to worry about that either. After some minutes/hours (depending on how fast DNS is updated) your blog page will be resolved on the custom domain. Mine is https://blog.andreasm.io\n","link":"https://blog.andreasm.io/2023/03/04/hosting-my-blog-on-github/","section":"post","tags":["github","static-site-generator","hugo"],"title":"Hosting my blog on Github"},{"body":"","link":"https://blog.andreasm.io/tags/hugo/","section":"tags","tags":null,"title":"hugo"},{"body":"","link":"https://blog.andreasm.io/tags/static-site-generator/","section":"tags","tags":null,"title":"static-site-generator"},{"body":"","link":"https://blog.andreasm.io/tags/antrea/","section":"tags","tags":null,"title":"antrea"},{"body":"Antrea Egress: What is Egress when we talk about Kubernetes? Well if a pod wants to communicate to the outside world, outside the Kubernetes cluster it runs in, out of the worker node the pod resides on, this is egress traffic (definition \u0026quot;the action of going out of or leaving a place\u0026quot; and in network terminology means the direction is outward from itself).\nWhy does egress matter? Well, usually when the pods communicate out, they will use the IP address of the worker node they currently is deployed on. Thats means the actual pod IP is not the address you should be expecting when doing network inspection, tcpdump, firewall rules etc, it is the Kubernetes worker nodes IP addresses. What we call this network feature is NAT, Network Address Translation. All Kubernetes worker nodes will take the actual POD IP and translate it to its own IP before sending the traffic out of itself. And as we know, we don't know where the pod will be deployed, and the pods can be many and will relocate so in certain environments it can be hard, not granular enough to create firewall rules in the perimeter firewall to allow or block traffic from a certain pod when needed when we only can use the IP addresses of the worker nodes.\nThats where Antrea Egress comes in. With Antrea Egress we have the option to dictate which specific IP address the POD can use when communication out by using an IP address that is not its POD IP address but a valid and allowed IP address in the network. You can read more on the Antrea Egress feature here\nAs the diagram below will illustrate, when pods communicate out, the will all get their POD IP addresses translated into the worker node's IP address. And the firewall between worker node and the SQL server are only able to allow or block the IP address of the worker node. That means we potentially allow or block all pods coming from this node, or nodes if we allow the range of all the worker nodes.\nOfcourse we can use Antrea Native Policies which I have written about here or VMware NSX with NCP, and VMware NSX with Antrea Integration to do fine grained security from source. But still there are environments we need to handle rules in perimeter firewalls.\nSo, this post will show how to enable Antrea Egress in vSphere 8 with Tanzu. With the current release of Antrea there is only support of using the same L2 network as worker nodes for the Antrea Egress IP-Pool.\nAs we can see in the diagram above, Antrea Egress has been configured with an IP-Pool the pods can get if we apply Antrea Egress IPs for them to use. It will then take a free IP from the Egress IP Pool and which is within the same L2 subnet as the workers are configured on. This is very easy to do and achieve. No need to create static routes, Antrea takes care of the IP mapping. With this in place the firewall rule is now very strict, I can allow only the IP 10.10.1.40 (which is the IP the POD got from Antrea Egress Pool) and block the worker node ip address.\nBut.... I wanted to go a bit further and make use of L3 anyway for my Antrea Egress IP-Pool by utilizing BGP. Thats where the fun starts and this article is actually about. What I would like to achieve is that the IP address pool I configfure with Antrea Egress is something completely different from what the workers are using, not even the same L2 subnet but a completely different subnet. That means we need to involve some clever routing, and some configuration done on the worker nodes as its actually their IP addresses that becomes the gateway for our Antrea Egress subnets.\nSomething like this:\nThe diagram above shows a pod getting an IP address from the Egress pool which is something completely different from what subnet the worker node itself has. What Antrea does is creating a virtual interface on the worker node and assigns all the relevant ip addresses that are being used by Antrea Egress on that interface. They will use the default route on the worker node itself when going out, but the only component in the network that does know about this Egress subnet is the worker node itself, so it needs to tell this to his buddy routers out there. Either we create a static route on the router (could be the next hop of the worker node, the closest one, or some other hop in the infrastructure) or use BGP. Static route is more or less useless, too many ip addresses to update each time an egress ip is being applied, it could be on any worker node etc. So BGP is the way to go.\nThe Diagram below illustrates what happens if we dont tell our network routers where this network comes from and where it can be reached. It will egress out, but no one knows the way back.\nAs soon as the routers are informed of the address to this IP address they will be more than happy to deliver it for us, thats their job. Imagine being a postman delivering a packet somewhere in a country without any direction, address etc to narrow down his search field. In the scenario above the return traffic will most likely be sent out via a default route to the Internet and never to be seen again üòÑ\nSo after we have been so kind to update with the exact delivery address below, we will get our mail again.\nEnough explanation already, get to the actual config of this.\nConfigure Antrea Egress in TKC (vSphere 8) Deploy your TKC cluster, it must be Ubuntu os for this to work:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-2-tkc-cluster-1 # give your tkc cluster a name 5 namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 3 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-medium 32 - name: storageClass 33 value: vsan-default-storage-policy Apply the correct Antrea configs, enable the Egress feature:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: wdc-2-tkc-cluster-1-antrea-package 5 namespace: wdc-2-ns-1 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: false 14 Egress: true #This needs to be enabled 15 NodePortLocal: true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Log in to your newly created TKC cluster:\n1kubectl-vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=andreasm@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name tkc-cluster-1 --tanzu-kubernetes-cluster-namespace ns-1 Delete the Antrea Controller and Agent pods. Now that we have done the initial config of our TKC cluster its time to test Antrea Egress within same subnet as worker nodes just to verify that it works.\nFrom now on you should stay in the context of your newly created TKC cluster.\nVerify Antrea Egress works with L2 To be able to use Antrea Egress we need to first start with an IP-Pool definition. So I create my definition like this:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: ExternalIPPool 3metadata: 4 name: antrea-ippool-l2 #just a name of this specific pool 5spec: 6 ipRanges: 7 - start: 10.102.6.40 # make sure not to use already used ips 8 end: 10.102.6.50 # should not overlap with worker nodes 9# - cidr: 10.101.112.0/32 # or you can define a whole range with cidr /32, /27 etc 10 nodeSelector: {} # you can remove the brackets and define which nodes you want below by using labels 11# matchLabels: 12# egress-l2: antrea-egress-l2 Apply your yaml definition above:\n1andreasm@linuxvm01:~/antrea/egress$ k apply -f ippool.wdc2.tkc.cluster-1.yaml 2externalippool.crd.antrea.io/antrea-ippool-l2 created Then we need to define the actual Egress itself. What we do with this config is selecting which pod that should get an Egress ip, from wich Antrea Egress IP pool (we can have several). So here is my example:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: Egress 3metadata: 4 name: antrea-egress-l2 #just a name of this specific Egress config 5spec: 6 appliedTo: 7 podSelector: 8 matchLabels: 9 app: ubuntu-20-04 ###Which pods should get Egress IPs 10 externalIPPool: antrea-ippool-l2 ###The IP pool I defined above. Before I apply it I will just make sure that I have a pod running the these labels, if not I will deploy it and then apply the Egress. So before I apply it I will show pinging from my pod to my jumpbox VM to identify which IP it is using before applying the Egress. And the apply the Egress and see if IP changes from the POD.\nMy ubuntu pod is up and running, I have entered the shell on it and initiates a ping from my pod to my jumpbox VM:\nSo here I can see the POD identifies itself with IP 10.102.6.15. Well which worker is that?:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n prod -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-20-04-c9776f965-t8nmf 1/1 Running 0 20h 20.40.1.2 wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 5NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 6wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 7wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 8wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 20h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 9wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 20h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 That is this worker: wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds.\nSo far so good. Now let me apply the Egress on this POD.\n1andreasm@linuxvm01:~/antrea/egress$ k apply -f antrea.egress.l2.yaml 2egress.crd.antrea.io/antrea-egress-l2 created Now how does the ping look like?\nThis was as expected was it not? The POD now identifies itself with the IP 10.102.6.40 which happens to be the first IP in the range defined in the pool. Well, this is cool. Now we now that Antrea Egress works. But as mentioned, I want this to be done with a different subnet than the worker nodes. Se lets see have we can do that as \u0026quot;seemless\u0026quot; as possible, as we dont want to SSH into the worker nodes and do a bunch of manual installation, configuration and so on. No, we use Kubernetes for our needs here also.\nConfigure FRR on worker nodes What I want to achieve is to deploy FRR here on my worker nodes unattended to enable BGP pr worker node to my upstream BGP router (remember, to inform about the Egress network no one knows about). The TKC workers are managed appliances, they can be deleted, scaled up and down (more workers, fewer workers.) And Deploying something manual on them are just waste of time. So we need something that deploy FRR automatically on the worker nodes.\nFRR is easy to deploy and configure, and it is included in the Ubuntu default repo (one reason I wanted to use Ubuntu as worker os). FRR is a very good routing protocol suite in Linux and is deployed easy on Ubuntu with \u0026quot;apt install frr\u0026quot;. FRR can be configured to use BGP which is the routing protocol I want to use. FRR needs two config files, daemons and frr.conf. frr.conf is individual pr node (specific IP addresses) so we need to take that into consideration also. So how can I deploy FRR on the worker nodes with their individal configuration files to automatically establish a BGP neighbourship with my Upstream router, and without logging into the actual worker nodes themselves?\nBelow diagram just illustrating a tkc worker node with FRR installed and BGP configured:\nKubernetes and Daemonset. I have created three Daemonset definition files, one for the actual deployment of FRR on all the nodes:\nThen I have created on Daemonset definition to copy the frr.conf and daemons file for the specific worker nodes and the last definition file is used to uninistall everything on the worker nodes themselves (apt purge frr) if needed.\nLets start by just deploy FRR on the workers themselves.\nHere is the defintion for that:\n1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 namespace: kube-system 6 name: node-custom-setup 7 labels: 8 k8s-app: node-custom-setup 9 annotations: 10 command: \u0026amp;cmd apt-get update -qy \u0026amp;\u0026amp; apt-get install -qy frr 11spec: 12 selector: 13 matchLabels: 14 k8s-app: node-custom-setup 15 template: 16 metadata: 17 labels: 18 k8s-app: node-custom-setup 19 spec: 20 hostNetwork: true 21 initContainers: 22 - name: init-node 23 command: 24 - nsenter 25 - --mount=/proc/1/ns/mnt 26 - -- 27 - sh 28 - -c 29 - *cmd 30 image: alpine:3.7 31 securityContext: 32 privileged: true 33 hostPID: true 34 containers: 35 - name: wait 36 image: pause:3.1 37 hostPID: true 38 hostNetwork: true 39 tolerations: 40 - effect: NoSchedule 41 key: node-role.kubernetes.io/master 42 updateStrategy: 43 type: RollingUpdate Before I apply the above definiton I have logged into one of my TKC worker node and just wants to show that there is no FRR installed:\n1sh-5.0# cd /etc/frr 2sh: cd: /etc/frr: No such file or directory 3sh-5.0# systemctl status frr 4Unit frr.service could not be found. 5sh-5.0# hostname 6wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 7sh-5.0# Now apply:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f deploy-frr.yaml 2daemonset.apps/node-custom-setup configured 3 4andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system 5NAME READY STATUS RESTARTS AGE 6antrea-agent-4jrks 2/2 Running 0 20h 7antrea-agent-4khkr 2/2 Running 0 20h 8antrea-agent-4wxb5 2/2 Running 0 20h 9antrea-agent-ccglp 2/2 Running 0 20h 10antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 20h 11coredns-7d8f74b498-j5sjt 1/1 Running 0 21h 12coredns-7d8f74b498-mgqrm 1/1 Running 0 21h 13docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h 14docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h 15docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h 16docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 17etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 18kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 19kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 20kube-proxy-44qxn 1/1 Running 0 21h 21kube-proxy-4x72n 1/1 Running 0 21h 22kube-proxy-shhxb 1/1 Running 0 21h 23kube-proxy-zxhdb 1/1 Running 0 21h 24kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 25metrics-server-6777988975-cxnpv 1/1 Running 0 21h 26node-custom-setup-5rlkr 1/1 Running 0 34s #There they are 27node-custom-setup-7gf2v 1/1 Running 0 62m #There they are 28node-custom-setup-b4j4l 1/1 Running 0 62m #There they are 29node-custom-setup-wjpgz 0/1 Init:0/1 0 1s #There they are Now what has happened on the TKC worker nodes itself:\n1sh-5.0# cd /etc/frr/ 2sh-5.0# pwd 3/etc/frr 4sh-5.0# ls 5daemons frr.conf support_bundle_commands.conf vtysh.conf 6sh-5.0# systemctl status frr 7‚óè frr.service - FRRouting 8 Loaded: loaded (/lib/systemd/system/frr.service; enabled; vendor preset: enabled) 9 Active: active (running) since Mon 2023-02-20 17:53:33 UTC; 1min 36s ago Wow that looks good. But the frr.conf is more or less empty so it doesnt do anything right now.\nA note on FRR config on the worker nodes Before jumping into this section I would like to elaborate a bit around the frr.conf files being copied. If you are expecting that all worker nodes will be on same BGP AS number and your next-hop BGP neighbors are the same ones and in the same L2 as your worker node (illustrated above) you could probably go with the same config for all worker nodes. Then you can edit the same definition used for the FRR deployment to also copy and install the config in the same operation. The steps I do below describes individual config pr worker node. If you need different BGP AS numbers, multi-hop (next-hop is several hops away), individual update-source interfaces is configured then you need individual frr.config pr node.\nIndividual FRR config on the worker nodes I need to \u0026quot;inject\u0026quot; the correct config for each worker node. So I label each and one with their unique label like this: (I map the names node1-\u0026gt;lowest-ip)\nI have already configured my upstream bgp router to accept my workers as soon as they are configured and ready. This is how this looks.\n1router bgp 65802 2 bgp router-id 172.20.0.102 3 redistribute connected 4 neighbor 10.102.6.15 remote-as 66889 5 neighbor 10.102.6.16 remote-as 66889 6 neighbor 10.102.6.17 remote-as 66889 7 neighbor 172.20.0.1 remote-as 65700 8! 9 address-family ipv6 10 exit-address-family 11 exit 12 13 14cpodrouter-nsxam-wdc-02# show ip bgp summary 15BGP router identifier 172.20.0.102, local AS number 65802 16RIB entries 147, using 16 KiB of memory 17Peers 4, using 36 KiB of memory 18 19Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 2010.102.6.15 4 66889 1191 1197 0 0 0 never Active # Node1 not Established yet 2110.102.6.16 4 66889 0 0 0 0 0 never Active # Node2 not Established yet 2210.102.6.17 4 66889 1201 1197 0 0 0 never Active # Node3 not Established yet 23172.20.0.1 4 65700 19228 19172 0 0 0 01w4d09h 65 To verify again, this is the current output of frr.conf on node1:\n1sh-5.0# cat frr.conf 2# default to using syslog. /etc/rsyslog.d/45-frr.conf places the log 3# in /var/log/frr/frr.log 4log syslog informational 5sh-5.0# This is the definition I use to copy the daemons and frr.conf for the individual worker nodes:\n1--- 2apiVersion: apps/v1 3kind: DaemonSet 4metadata: 5 namespace: kube-system 6 name: node-frr-config 7 labels: 8 k8s-app: node-frr-config 9 annotations: 10 command: \u0026amp;cmd cp /tmp/wdc-2.node1.frr.conf /etc/frr/frr.conf \u0026amp;\u0026amp; cp /tmp/daemons /etc/frr \u0026amp;\u0026amp; systemctl restart frr 11spec: 12 selector: 13 matchLabels: 14 k8s-app: node-frr-config 15 template: 16 metadata: 17 labels: 18 k8s-app: node-frr-config 19 spec: 20 nodeSelector: 21 nodelabel: wdc2-node1 #Here is my specific node selection done 22 hostNetwork: true 23 initContainers: 24 - name: copy-file 25 image: busybox 26 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;cp /var/nfs/wdc-2.node1.frr.conf /var/nfs/daemons /data\u0026#39;] 27 volumeMounts: 28 - name: nfs-vol 29 mountPath: /var/nfs # The mountpoint inside the container 30 - name: node-vol 31 mountPath: /data 32 - name: init-node 33 command: 34 - nsenter 35 - --mount=/proc/1/ns/mnt 36 - -- 37 - sh 38 - -c 39 - *cmd 40 image: alpine:3.7 41 securityContext: 42 privileged: true 43 hostPID: true 44 containers: 45 - name: wait 46 image: pause:3.1 47 hostPID: true 48 hostNetwork: true 49 tolerations: 50 - effect: NoSchedule 51 key: node-role.kubernetes.io/master 52 volumes: 53 - name: nfs-vol 54 nfs: 55 server: 10.101.10.99 56 path: /home/andreasm/antrea/egress/FRR/nfs 57 - name: node-vol 58 hostPath: 59 path: /tmp 60 type: Directory 61 updateStrategy: 62 type: RollingUpdate Notice this:\n1 nodeSelector: 2 nodelabel: wdc2-node1 This is used to select the correct node after I have labeled them like this:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 4wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 5wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 6wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 7 8andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k label node wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds nodelabel=wdc2-node1 9node/wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds labeled So its just about time to apply the configs pr node:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k apply -f wdc2.frr.node1.config.yaml 2daemonset.apps/node-custom-setup configured 3andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get pods -n kube-system 4NAME READY STATUS RESTARTS AGE 5antrea-agent-4jrks 2/2 Running 0 21h 6antrea-agent-4khkr 2/2 Running 0 21h 7antrea-agent-4wxb5 2/2 Running 0 21h 8antrea-agent-ccglp 2/2 Running 0 21h 9antrea-controller-56d86d6b9b-hvrtc 1/1 Running 0 21h 10coredns-7d8f74b498-j5sjt 1/1 Running 0 21h 11coredns-7d8f74b498-mgqrm 1/1 Running 0 21h 12docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t 1/1 Running 0 21h 13docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z 1/1 Running 0 21h 14docker-registry-wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds 1/1 Running 0 21h 15docker-registry-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 16etcd-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 17kube-apiserver-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 18kube-controller-manager-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 19kube-proxy-44qxn 1/1 Running 0 21h 20kube-proxy-4x72n 1/1 Running 0 21h 21kube-proxy-shhxb 1/1 Running 0 21h 22kube-proxy-zxhdb 1/1 Running 0 21h 23kube-scheduler-wdc-2-tkc-cluster-1-xrj44-qq24c 1/1 Running 0 21h 24metrics-server-6777988975-cxnpv 1/1 Running 0 21h 25node-custom-setup-w4mg5 0/1 Init:0/2 0 4s Now what does my upstream router say:\n1cpodrouter-nsxam-wdc-02# show ip bgp summary 2BGP router identifier 172.20.0.102, local AS number 65802 3RIB entries 149, using 16 KiB of memory 4Peers 4, using 36 KiB of memory 5 6Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 710.102.6.15 4 66889 1202 1209 0 0 0 00:00:55 2 #Hey I am a happy neighbour 810.102.6.16 4 66889 0 0 0 0 0 never Active 910.102.6.17 4 66889 1201 1197 0 0 0 never Active 10172.20.0.1 4 65700 19249 19194 0 0 0 01w4d10h 65 11 12Total number of neighbors 4 13 14Total num. Established sessions 2 15Total num. of routes received 67 Then I just need to deploy on the other two workers.\nMy upstream bgp router is very happy to have new established neighbours:\n1cpodrouter-nsxam-wdc-02# show ip bgp summary 2BGP router identifier 172.20.0.102, local AS number 65802 3RIB entries 153, using 17 KiB of memory 4Peers 4, using 36 KiB of memory 5 6Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 710.102.6.15 4 66889 1221 1232 0 0 0 00:01:01 2 810.102.6.16 4 66889 11 16 0 0 0 00:00:36 2 910.102.6.17 4 66889 1225 1227 0 0 0 00:00:09 2 10172.20.0.1 4 65700 19254 19205 0 0 0 01w4d10h 65 11 12Total number of neighbors 4 Antrea IP Pool outside subnet of worker nodes Lets apply an IP pool which resides outside worker nodes subnet and apply Egress on my test pod again.\nHere is the IP pool config:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: ExternalIPPool 3metadata: 4 name: antrea-ippool-l3 5spec: 6 ipRanges: 7 - start: 10.102.40.41 8 end: 10.102.40.51 9# - cidr: 10.102.40.0/24 10 nodeSelector: {} 11# matchLabels: 12# egress-l3: antrea-egress-l3 And the Egress:\n1apiVersion: crd.antrea.io/v1alpha2 2kind: Egress 3metadata: 4 name: antrea-egress-l3 5spec: 6 appliedTo: 7 podSelector: 8 matchLabels: 9 app: ubuntu-20-04 10 externalIPPool: antrea-ippool-l3 Apply it and check the IP address from the POD....\nWell, how about that?\nI know my workers reside on these ip addresses, but my POD is using a completely different IP address:\n1andreasm@linuxvm01:~/antrea/egress/deploy-frr$ k get nodes -o wide 2NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 3wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-qn52t Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.16 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 4wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-wmj7z Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.17 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 5wdc-2-tkc-cluster-1-node-pool-01-5hz5z-5597d8895f-z7cds Ready \u0026lt;none\u0026gt; 21h v1.23.8+vmware.2 10.102.6.15 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 6wdc-2-tkc-cluster-1-xrj44-qq24c Ready control-plane,master 21h v1.23.8+vmware.2 10.102.6.14 \u0026lt;none\u0026gt; Ubuntu 20.04.5 LTS 5.4.0-128-generic containerd://1.6.6 What about the routing table in my upstream bgp router:\n1*\u0026gt; 10.102.40.41/32 10.102.6.17 0 0 66889 ? Well have you seen..\nObjective accomplished----\u0026gt;\n","link":"https://blog.andreasm.io/2023/02/20/antrea-egress/","section":"post","tags":["antrea","kubernetes","tanzu"],"title":"Antrea Egress"},{"body":"","link":"https://blog.andreasm.io/categories/cni/","section":"categories","tags":null,"title":"CNI"},{"body":"","link":"https://blog.andreasm.io/categories/avi/","section":"categories","tags":null,"title":"AVI"},{"body":"","link":"https://blog.andreasm.io/tags/ingress/","section":"tags","tags":null,"title":"ingress"},{"body":"","link":"https://blog.andreasm.io/tags/loadbalancing/","section":"tags","tags":null,"title":"loadbalancing"},{"body":"Deploy Tanzu in vSphere 8 with VDS and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using vSphere VDS networking and Avi as loadbalancer. The goal is to deploy Tanzu by using vSphere Distributed Switch (no NSX this time) and utilize Avi as loadbalancer for Supervisor and workload cluster L4 endpoint (kubernetes API). When that is done I will go through how we also can extend this into L7 (Ingress) by using AKO in our workload clusters.\nThe below diagram is what we should end up with after the basic deployment of Tanzu and Avi:\nAssumptions This post assumes we already have a vSphere environment up and running with vCenter, HA and DRS. Required network to support the basic vSphere stuff like vMotion and shared storage. And the hosts networking has been configured with a Distributed Switch with the corresponding vds portgroups for Management, Frontend network (VIP placement for kubernetes API endpoint) and workload network with corresponding VLANs. In vCenter a content library needs to be created, this is just a local library you give a meaningful name no subscriptions etc.\nAt least one Avi controller is deployed, no cloud added, just deployed and the initial configs done.\nPreparations on the Avi side of things This part of the guide takes place on the newly configured Avi controller(s) which currently only has the initial configuration done\nAvi cloud configurations To prepare Avi for this deployment we need to configure the vCenter cloud. This is done here:\nThere is a Default-Cloud object there we need to convert to a vCenter cloud. This is done by clicking on this button on the far right side: This will bring up the following options: Select VMware vCenter/vSphere NSX Then start populate the relevant vCenter information for your vCenter: When credentials is added slect content library and choose your content library from the list, then click connect and then Save \u0026amp; Relaunch\nWhen the dialog relaunches select the management network and ip address management. I have opted for DHCP (I have DHCP in my mgmt network, if not we can leverage Avi as IPAM provider for the mgmt network also.) This is used for the SE's mgmt interface. Click save for now. Head over to the Template section to add IPAM and DNS.\nWhen using vCenter clouds the different portgroups is automatically added under networks. We need to configure some of them. But for now just create the IPAM and DNS profiles and we configure the networks later accordingly.\nThe DNS Profile (optional, only if you want to use Avi DNS service):\nClick save when done\nThe IPAM profile:\nSelect the Default-Cloud, then select from the list \u0026quot;Usable Networks\u0026quot; the Frontend network vds portgroup corresponding to the frontend network we want to use for our endpoint vips. Click save. You should have your profiles configured now:\nHead back to your cloud again and add your newly created IPAM and DNS profiles.\nAdd the profiles: Before you click finish, make sure you have selected \u0026quot;Prefer Static Routes vs Directly Connected Network\u0026quot; like this: Then click finish...\nAvi network configs Now its time to configure the networks for the SEs (VIP, dataplane). I will go ahead and configure both the Frontend VIP for kubernetes API endpoint, but also the workload network I will use when I add L7 functionality later. Head over to Cloud Resources: Scroll until you find your \u0026quot;Frontend-network\u0026quot; Click edit all the way to the right: In here we need to define the subnet (if not already auto discovered) and add and IP range for the SE's and VIPS. We can decide to create just one range for both, or a range only for SEs, and one for VIPs only. To create a range for both SE and VIP do like this: If you want a specific range for SE and a specific for VIP do like this: Common for both is to deselect the DHCP Enabled option. What we have done now is to tell Avi that Avi is responsible for IP allocation to our SE's when they are deployed and configured to give them each their IP in the Frontend network, and also \u0026quot;carve\u0026quot; out an IP for the VIP when a Virtual Service is created and IP allocation for that service is selected to auto-allocate. The same would go if you decided to not use DHCP for mgmt IP, you would need to defined the network and select only \u0026quot;Use for Service Engine\u0026quot; (no VIPs in the management network)\nAvi service engine group Now its time to prepare the Default Service Engine Group. Head over to Cloud Resources - Service Engine Group\nClick the pencil on the far right side of the Default-Group and make the following changes:\nIn the Advanced tab: Here we select our vSphere cluster for the SE placement, vSphere shared storage, and the Prefix and vCenter folder placement (if you want). Now that is done.\nAvi VRF context Now we need to create a static route for the SE dataplane to know which gateway will take them to the \u0026quot;backend-pool\u0026quot; (the services they are acting as loadbalancer for). This is usually the gateway for the networks in their respective subnet as the dataplane is residing in. Here I prepare the route for the Frontend network, and also the Workload network (so it is already done when moving to the step of enabling L7). Avi controller SSL certificate for Tanzu \u0026quot;integration\u0026quot; The last step is to create a new certificate, or use your own signed certificate, for the Tanzu deployment to use. Head over to Templates - SSL/TLS Certificates. From here we click \u0026quot;Create\u0026quot; in the top right corner: I will go ahead and create a new self-signed certificate: It is important that you use the IP or FQDN of the controller under \u0026quot;Common Name\u0026quot; and under \u0026quot;Subject Alternate Name (SAN)\u0026quot;\nNow head over to Administration - Access Settings: Click edit on the pencil in the top right corner, remove the existing certificates under SSL/TLS Certificate: And replace with the one you created: Now the Avi config is done for this round. Next step is to enable Workload Management in vSphere...\nEnable Workload Management in vSphere This section will cover all the steps to enable Tanzu from vCenter, describing the selections made and the network configs.\nEnable workload management Head over to your vCenter server and click here: (from the the \u0026quot;hamburger menu\u0026quot; top left corner)\nClick the Get Started button:\nStep 1: Select vSphere Distributed Switch Step 2: Select Cluster Deployment, give the supervisor cluster a name and give it a zone name: Step 3: Select your Storage Policy (If VSAN and you dont have created a specific VSAN policy for this use Default Storage Policy): Step 4: Type in the relevant info for your Avi Controller and copy paste the certificate from your Avi controller: The certificate is easily copied from the Avi controller by going to Templates - SSL/TLS Certificates and click the \u0026quot;down arrow\u0026quot;: Then copy the certificate: Paste the content in the Server Certificate field above (step 4) Step 5: Management Network Here we fill in the required information for the Supervisor nodes. Management IP for the nodes themselves (needs connectivity to both vCenter and ESXi hosts, could be in the same mgmt network as vCenter and ESXi). Select the corresponding vds portgroup, select either static or DHCP if you want to use DHCP. Step 6: Workload Network Select the correct vds portgroup for the workload network. The supervisor and the workload nodes will be placed here. Can be static or DHCP. Leave the default \u0026quot;Internal Network for Kubernetes Services\u0026quot;, that is for the internal services (clusterIP etc inside the K8s clusters, they will never be exposed outside). Fill in the necessary config if you go with static. Step 7: Review and Confirm and optionally give the Supervisor endpoint a DNS name which you later can register in your DNS service when we have the L4 IP for the kubernetes API endpoint. Click finish:\nThe whole summary: Now sit back and wait for the creation of the supervisor cluster, it can take a couple of minutes. After a while you can take a look in your Avi controller under Applications and see if something is being created there; You can monitor the process from the Workload management status view by clicking on the \u0026quot;Configuring (View) )\u0026quot;. You can continue work with your vCenter server and go back to this progress bar whenever you want by clicking the hamburger menu Workload management.\nIn your vCenter inventory you should also see the Supervisor VMs and Avi SE's like this: When its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.102.7.11 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters. Lets dive into it.\nvSphere Namespace vSphere with Tanzu workloads, including vSphere Pods, VMs, and Tanzu Kubernetes clusters, are deployed to a vSphere Namespace. You define a vSphere Namespace on a Supervisor and configure it with resource quota and user permissions. Depending on the DevOps needs and workloads they plan to run, you might also assign storage policies, VM classes, and content libraries for fetching the latest Tanzu Kubernetes releases and VM images. source\nCreate a vSphere namespace Now that the Supervisor cluster is ready and running head back to your vCenter and create a vSphere namespace. Click create namespace (above) the select the supervisor to create the namespace on, and give your namespace a name then select the \u0026quot;workload network\u0026quot; you have defined for your workload placement. Now the namespace is being created.\nAdd additional workload networks Sidenote there is also possible to add more \u0026quot;workload networks\u0026quot; after the Supervisor has been configured under Supervisor config if you want to add more \u0026quot;workload networks\u0026quot; for separation etc. To do that head over to Workload Management in vCenter:\nThen select the supervisor tab:\nClick on your supervisor cluster here: Then click the configure tab and go to network and add your additional workload network: After your namespace has been created we need to configure it with access permissions, datastores, content library and vmclasses: Create workload cluster Afte the vSphere Namespace has been configured its time to deploy a workload cluster/TKC cluster (Tanzu Kubernetes Cluster cluster üòÑ ). From your workstation/jumphost where you downloaded the cli tools login in to the supervisor with access rights to the Supervisor API. (administrator@vsphere.local will have access).\nCustom role in vCenter I created a specific role in my vCenter with these privileges:\nAdded my \u0026quot;supervisor-manager\u0026quot; user in this role as global and in the top tree of my vCenter with inheritance. Also added it as \u0026quot;editor\u0026quot; in my wdc-2-ns-1 vSphere Namespace.\n1kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net When your are logged in it will give you this output and also put the kubernetes config in your ~/.kube/config file.\n1andreasm@linuxvm01:~$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.102.7.11 10 wdc-2-ns-1 11 12If the context you wish to use is not in this list, you may need to try 13logging in again later, or contact your cluster administrator. 14 15To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` When you are logged in prepare your yaml for your first workload cluster and apply it with kubectl apply -f nameof.yaml\nExample:\n1apiVersion: cluster.x-k8s.io/v1beta1 2kind: Cluster 3metadata: 4 name: wdc-2-tkc-cluster-1 # give your tkc cluster a name 5 namespace: wdc-2-ns-1 # remember to put it in your defined vSphere Namespace 6spec: 7 clusterNetwork: 8 services: 9 cidrBlocks: [\u0026#34;20.10.0.0/16\u0026#34;] 10 pods: 11 cidrBlocks: [\u0026#34;20.20.0.0/16\u0026#34;] 12 serviceDomain: \u0026#34;cluster.local\u0026#34; 13 topology: 14 class: tanzukubernetescluster 15 version: v1.23.8---vmware.2-tkg.2-zshippable 16 controlPlane: 17 replicas: 1 18 metadata: 19 annotations: 20 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 21 workers: 22 machineDeployments: 23 - class: node-pool 24 name: node-pool-01 25 replicas: 3 26 metadata: 27 annotations: 28 run.tanzu.vmware.com/resolve-os-image: os-name=ubuntu 29 variables: 30 - name: vmClass 31 value: best-effort-medium 32 - name: storageClass 33 value: vsan-default-storage-policy As soon as I apply the above yaml it will deploy the corresponding tkc cluster in your vsphere environment:\nSit back and enjoy while your tkc cluster is being created for you. We can check the status in the vCenter gui:\nor via kubectl:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get cluster -n wdc-2-ns-1 2NAME PHASE AGE VERSION 3wdc-2-tkc-cluster-1 Provisioned 12m v1.23.8+vmware.2 It is ready, now we need to log into it:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ kubectl vsphere login --server=10.102.7.11 --insecure-skip-tls-verify --vsphere-username=supervisor-manager@cpod-nsxam-wdc.az-wdc.cloud-garage.net --tanzu-kubernetes-cluster-name=wdc-2-tkc-cluster-1 --tanzu-kubernetes-cluster-namespace=wdc-2-ns-1 2 3 4KUBECTL_VSPHERE_PASSWORD environment variable is not set. Please enter the password below 5Password: 6Logged in successfully. 7 8You have access to the following contexts: 9 10.102.7.11 10 wdc-2-ns-1 11 wdc-2-tkc-cluster-1 12 13If the context you wish to use is not in this list, you may need to try 14logging in again later, or contact your cluster administrator. 15 16To change context, use `kubectl config use-context \u0026lt;workload name\u0026gt;` Check if you are able to list ns and pods:\n1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get pods -A 2NAMESPACE NAME READY STATUS RESTARTS AGE 3kube-system antrea-agent-77drs 2/2 Running 0 8m35s 4kube-system antrea-agent-j482r 2/2 Running 0 8m34s 5kube-system antrea-agent-thh5b 2/2 Running 0 8m35s 6kube-system antrea-agent-tz4fb 2/2 Running 0 8m35s 7kube-system antrea-controller-575845467f-pqgll 1/1 Running 0 8m35s 8kube-system coredns-7d8f74b498-ft7rf 1/1 Running 0 10m 9kube-system coredns-7d8f74b498-pqgp7 1/1 Running 0 7m35s 10kube-system docker-registry-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 11kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-6cvz9 1/1 Running 0 8m46s 12kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rgn29 1/1 Running 0 8m34s 13kube-system docker-registry-wdc-2-tkc-cluster-1-node-pool-01-4dql7-7b8b84fb4b-rsmfw 1/1 Running 0 9m 14kube-system etcd-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 15kube-system kube-apiserver-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 16kube-system kube-controller-manager-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 17kube-system kube-proxy-67xjk 1/1 Running 0 8m46s 18kube-system kube-proxy-6fttt 1/1 Running 0 8m35s 19kube-system kube-proxy-m4wt8 1/1 Running 0 11m 20kube-system kube-proxy-rbsjw 1/1 Running 0 9m1s 21kube-system kube-scheduler-wdc-2-tkc-cluster-1-n89g2-zc27k 1/1 Running 0 11m 22kube-system metrics-server-6f7c489795-scmm6 1/1 Running 0 8m36s 23secretgen-controller secretgen-controller-6966677567-4hngd 1/1 Running 0 8m26s 24tkg-system kapp-controller-55f9977c86-bqppj 2/2 Running 0 9m22s 25tkg-system tanzu-capabilities-controller-manager-cb4bc7978-qh9s8 1/1 Running 3 (87s ago) 7m54s 26vmware-system-auth guest-cluster-auth-svc-r4rk9 1/1 Running 0 7m48s 27vmware-system-cloud-provider guest-cluster-cloud-provider-859b8dc577-8jlth 1/1 Running 0 8m48s 28vmware-system-csi vsphere-csi-controller-6db86b997-l5glc 6/6 Running 0 8m46s 29vmware-system-csi vsphere-csi-node-7bdpl 3/3 Running 2 (7m34s ago) 8m34s 30vmware-system-csi vsphere-csi-node-q7zqr 3/3 Running 3 (7m32s ago) 8m46s 31vmware-system-csi vsphere-csi-node-r8v2c 3/3 Running 3 (7m34s ago) 8m44s 32vmware-system-csi vsphere-csi-node-zl4m2 3/3 Running 3 (7m40s ago) 8m46s 1andreasm@linuxvm01:~/tkc-wdc-01-vds$ k get ns 2NAME STATUS AGE 3default Active 12m 4kube-node-lease Active 12m 5kube-public Active 12m 6kube-system Active 12m 7secretgen-controller Active 9m3s 8tkg-system Active 9m55s 9vmware-system-auth Active 12m 10vmware-system-cloud-provider Active 10m 11vmware-system-csi Active 10m 12vmware-system-tkg Active 12m By default we are not allowed to run anything on our newly created tkc cluster. We need to define some ClusterRoles. I will just apply a global clusterolres on my tkc cluster so I can do what I want with it like this: Apply the psp policy yaml:\n1apiVersion: rbac.authorization.k8s.io/v1 2kind: ClusterRole 3metadata: 4 name: psp:privileged 5rules: 6- apiGroups: [\u0026#39;policy\u0026#39;] 7 resources: [\u0026#39;podsecuritypolicies\u0026#39;] 8 verbs: [\u0026#39;use\u0026#39;] 9 resourceNames: 10 - vmware-system-privileged 11--- 12apiVersion: rbac.authorization.k8s.io/v1 13kind: ClusterRoleBinding 14metadata: 15 name: all:psp:privileged 16roleRef: 17 kind: ClusterRole 18 name: psp:privileged 19 apiGroup: rbac.authorization.k8s.io 20subjects: 21- kind: Group 22 name: system:serviceaccounts 23 apiGroup: rbac.authorization.k8s.io 1kubectl apply -f roles.yaml 2clusterrole.rbac.authorization.k8s.io/psp:privileged created 3clusterrolebinding.rbac.authorization.k8s.io/all:psp:privileged created Now that I am allowed to deploy stuff I am ready to consume the newly cluster. But this blog was how to deploy Tanzu with VDS and Avi Loadbalancer. So far I have only covered the L4 part where Avi is providing me the K8s API endpoints, I will now jump over to the section where I configure both Avi and my tkc cluster to use Ingress (L7) also so I can publish/expose my applications with ingress. That means installing an additional component called AKO in my tkc cluster and configure Avi accordingly.\nConfigure Avi as Ingress controller (L7) For Avi Ingress we need to deploy a component in our TKC cluster called AKO. AKO stands for Avi Kubernetes Operator and introduces the ability to translate our k8s api to the Avi controller so we can make our Avi automatically create vs services for us as soon as we request them from our TKC cluster. To deploy AKO we use Helm. In short we need to add the AKO helm repository, get the ako values, edit them to fit our environment, then install it by using Helm. So let us go through this step-by-step (I have also covered it a while back in an Upstream k8s cluster) but let us do it again here.\nCreate the namespace for the ako pod:\n1k create ns avi-system 2namespace/avi-system created Then add the repo to Helm:\n1helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Check the repo:\n1andreasm@linuxvm01:~$ helm search repo 2NAME CHART VERSION\tAPP VERSION\tDESCRIPTION 3ako/ako 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator 4ako/ako-operator\t1.3.1 1.3.1 A Helm chart for Kubernetes AKO Operator 5ako/amko 1.8.2 1.8.2 A helm chart for Avi Kubernetes Operator Get the values.yaml:\n1 helm show values ako/ako --version 1.8.2 \u0026gt; values.yaml Now its time to edit the value file. I will go through the values files, update it accordingly and adjust some configurations in Avi controller.\nThe values.yaml for ako chart:\n1# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. 2replicaCount: 1 3 4image: 5 repository: projects.registry.vmware.com/ako/ako 6 pullPolicy: IfNotPresent 7 8 9AKOSettings: 10 primaryInstance: true 11 enableEvents: \u0026#39;true\u0026#39; 12 logLevel: WARN 13 fullSyncFrequency: \u0026#39;1800\u0026#39; 14 apiServerPort: 8080 15 deleteConfig: \u0026#39;false\u0026#39; 16 disableStaticRouteSync: \u0026#39;false\u0026#39; 17 clusterName: wdc-tkc-cluster-1 # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller 18 cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. 19 enableEVH: false 20 layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. 21 22 namespaceSelector: 23 labelKey: \u0026#39;\u0026#39; 24 labelValue: \u0026#39;\u0026#39; 25 servicesAPI: false 26 vipPerNamespace: \u0026#39;false\u0026#39; 27 28NetworkSettings: 29 nodeNetworkList: 30 # nodeNetworkList: 31 - networkName: \u0026#34;vds-tkc-workload-vlan-1026\u0026#34; # this is the VDS portgroup you have for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter 32 cidrs: 33 - 10.102.6.0/24 # this is the CIDR for your workers 34 enableRHI: false 35 nsxtT1LR: \u0026#39;\u0026#39; 36 bgpPeerLabels: [] 37 # bgpPeerLabels: 38 # - peer1 39 # - peer2 40 vipNetworkList: 41 - networkName: \u0026#34;vds-tkc-frontend-vlan-1027\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. 42 cidr: 10.102.7.0/24 43 44L7Settings: 45 defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. 46 noPGForSNI: false 47 serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal 48 shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. 49 passthroughShardSize: SMALL 50 enableMCI: \u0026#39;false\u0026#39; 51 52L4Settings: 53 defaultDomain: \u0026#39;\u0026#39; 54 autoFQDN: default 55 56 57ControllerSettings: 58 serviceEngineGroupName: Default-Group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). 59 controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.3 60 cloudName: Default-Cloud # The configured cloud name on the Avi controller. 61 controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller 62 tenantName: admin 63 64nodePortSelector: 65 key: \u0026#39;\u0026#39; 66 value: \u0026#39;\u0026#39; 67 68resources: 69 limits: 70 cpu: 350m 71 memory: 400Mi 72 requests: 73 cpu: 200m 74 memory: 300Mi 75 76podSecurityContext: {} 77 78rbac: 79 pspEnable: false 80 81 82avicredentials: 83 username: \u0026#39;admin\u0026#39; # username for the Avi controller 84 password: \u0026#39;password\u0026#39; # password for the Avi controller 85 authtoken: 86 certificateAuthorityData: 87 88 89persistentVolumeClaim: \u0026#39;\u0026#39; 90mountPath: /log 91logFile: avi.log A word around the VIP network used for the L7/Ingress. As we deploy AKO as standalone we are not restricted to use only the components defined to support the install of Tanzu with vSphere, like service engine groups, vip networks etc. We could decide to create a separate VIP network by using a dedicated SE group for these networks. We could also decide to have the SE's using a separate dataplane network than the VIP itself. If going this path there is some config steps that needs to be taken on the network side. Routing to the VIP addresses, either Avi can be configured by using BGP, or we create static routes in the physical routers. But as the VIPs are coming and going (applications are published, deleted, etc) these IPs change. So BGP would be the best option, or use an already defined VLAN as I am doing in this example. In my other post on using NSX and Avi with Tanzu I will show how to use NSX for BGP. Maybe I will update this post also by adding a section where I use BGP from Avi to my upstream router. But for now I will stick with using my VLAN I have called frontend which already have a gateway and a route defined. So all my VIPs will be reachable through this network.\nAntrea NodePortLocal And another word around NodePortLocal. To be able to utilize NodePortLocal your Antrea config in the TKC cluster must be verified whether it is configured with NPL or not. So let us do instead of just assume something.\n1andreasm@linuxvm01:~/ako/ako_vds$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml 2apiVersion: v1 3data: 4 antrea-agent.conf: | 5 featureGates: 6 AntreaProxy: true 7 EndpointSlice: true 8 Traceflow: true 9 NodePortLocal: false 10 AntreaPolicy: true 11 FlowExporter: false 12 NetworkPolicyStats: false 13 Egress: false 14 AntreaIPAM: false 15 Multicast: false 16 ServiceExternalIP: false 17 trafficEncapMode: encap 18 noSNAT: false 19 tunnelType: geneve 20 trafficEncryptionMode: none 21 wireGuard: 22 port: 51820 23 egress: {} 24 serviceCIDR: 20.10.0.0/16 Well that was not good. So we need to enable it. Luckily, with Tanzu with vSphere its quite simple actually. Switch context to your vSphere Namespace, edit an antreaconfig, apply it.\n1andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-ns-1 2Switched to context \u0026#34;wdc-2-ns-1\u0026#34;. 3andreasm@linuxvm01:~/antrea$ k get cluster 4NAME PHASE AGE VERSION 5wdc-2-tkc-cluster-1 Provisioned 3h26m v1.23.8+vmware.2 6andreasm@linuxvm01:~/antrea$ k apply -f antreaconfig-wdc-2-nsx-1.yaml 7Warning: resource antreaconfigs/wdc-2-tkc-cluster-1-antrea-package is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically. 8antreaconfig.cni.tanzu.vmware.com/wdc-2-tkc-cluster-1-antrea-package configured The antreaconfig I used:\n1apiVersion: cni.tanzu.vmware.com/v1alpha1 2kind: AntreaConfig 3metadata: 4 name: wdc-2-tkc-cluster-1-antrea-package # notice the naming-convention tkc cluster name-antrea-package 5 namespace: wdc-2-ns-1 # your vSphere Namespace the TKC cluster is in. 6spec: 7 antrea: 8 config: 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 AntreaPolicy: true 13 FlowExporter: false 14 Egress: true 15 NodePortLocal: true # Set this to true 16 AntreaTraceflow: true 17 NetworkPolicyStats: true Lets have a look at my Antrea config in my TKC cluster now:\n1andreasm@linuxvm01:~/antrea$ k config use-context 210.102.7.11 wdc-2-ns-1 wdc-2-tkc-cluster-1 3andreasm@linuxvm01:~/antrea$ k config use-context wdc-2-tkc-cluster-1 4Switched to context \u0026#34;wdc-2-tkc-cluster-1\u0026#34;. 5andreasm@linuxvm01:~/antrea$ k get configmaps -n kube-system antrea-config-f5d8g47b88 -oyaml 6apiVersion: v1 7data: 8 antrea-agent.conf: | 9 featureGates: 10 AntreaProxy: true 11 EndpointSlice: false 12 Traceflow: true 13 NodePortLocal: true # Yes, there it is 14 AntreaPolicy: true 15 FlowExporter: false 16 NetworkPolicyStats: true 17 Egress: true 18 AntreaIPAM: false 19 Multicast: false 20 ServiceExternalIP: false 21 trafficEncapMode: encap 22 noSNAT: false 23 tunnelType: geneve 24 trafficEncryptionMode: none 25 wireGuard: 26 port: 51820 27 egress: 28 exceptCIDRs: [] 29 serviceCIDR: 20.10.0.0/16 But... Even though our cluster config has been updated we need to delete the Antrea pods so they can restart and read their new configmap again.\n1andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-controller-575845467f-pqgll 2pod \u0026#34;antrea-controller-575845467f-pqgll\u0026#34; deleted 3andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent- 4antrea-agent-77drs antrea-agent-j482r antrea-agent-thh5b antrea-agent-tz4fb 5andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-77drs 6pod \u0026#34;antrea-agent-77drs\u0026#34; deleted 7andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-j482r 8pod \u0026#34;antrea-agent-j482r\u0026#34; deleted 9andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-thh5b 10pod \u0026#34;antrea-agent-thh5b\u0026#34; deleted 11andreasm@linuxvm01:~/ako$ k delete pod -n kube-system antrea-agent-tz4fb 12pod \u0026#34;antrea-agent-tz4fb\u0026#34; deleted Configure Avi as Ingress controller (L7) - continue Now that we have assured NodePortLocal is configured, its time to deploy AKO. I have also verified that I have the VIP network configured in Avi as I am using the existing network that is already defined. So install AKO then üòÑ\n1helm install ako/ako --generate-name --version 1.8.2 -f ako.vds.wdc-2.values.yaml --namespace=avi-system 2NAME: ako-1675511021 3LAST DEPLOYED: Sat Feb 4 11:43:42 2023 4NAMESPACE: avi-system 5STATUS: deployed 6REVISION: 1 7TEST SUITE: None Verify that the AKO pod is running:\n1andreasm@linuxvm01:~/ako/ako_vds$ k get pods -n avi-system 2NAME READY STATUS RESTARTS AGE 3ako-0 1/1 Running 0 57s Check logs for some immediate messages that needs investigating before trying to deploy a test application.\n1andreasm@linuxvm01:~/ako/ako_vds$ k logs -n avi-system ako-0 22023-02-04T11:43:51.240Z\tINFO\tapi/api.go:52\tSetting route for GET /api/status 32023-02-04T11:43:51.241Z\tINFO\tako-main/main.go:71\tAKO is running with version: v1.8.2 42023-02-04T11:43:51.241Z\tINFO\tapi/api.go:110\tStarting API server at :8080 52023-02-04T11:43:51.242Z\tINFO\tako-main/main.go:81\tWe are running inside kubernetes cluster. Won\u0026#39;t use kubeconfig files. 62023-02-04T11:43:51.265Z\tINFO\tlib/control_config.go:198\tako.vmware.com/v1alpha1/AviInfraSetting enabled on cluster 72023-02-04T11:43:51.270Z\tINFO\tlib/control_config.go:207\tako.vmware.com/v1alpha1/HostRule enabled on cluster 82023-02-04T11:43:51.273Z\tINFO\tlib/control_config.go:216\tako.vmware.com/v1alpha1/HTTPRule enabled on cluster 92023-02-04T11:43:51.290Z\tINFO\tako-main/main.go:150\tKubernetes cluster apiserver version 1.23 102023-02-04T11:43:51.296Z\tINFO\tutils/utils.go:168\tInitializing configmap informer in avi-system 112023-02-04T11:43:51.296Z\tINFO\tlib/dynamic_client.go:118\tSkipped initializing dynamic informers antrea 122023-02-04T11:43:51.445Z\tINFO\tk8s/ako_init.go:455\tSuccessfully connected to AVI controller using existing AKO secret 132023-02-04T11:43:51.446Z\tINFO\tako-main/main.go:261\tValid Avi Secret found, continuing .. 142023-02-04T11:43:51.866Z\tINFO\tcache/avi_ctrl_clients.go:71\tSetting the client version to 22.1.1 152023-02-04T11:43:51.866Z\tINFO\tako-main/main.go:279\tSEgroup name found, continuing .. 162023-02-04T11:43:53.015Z\tINFO\tcache/controller_obj_cache.go:2340\tAvi cluster state is CLUSTER_UP_NO_HA 172023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2901\tSetting cloud vType: CLOUD_VCENTER 182023-02-04T11:43:53.176Z\tINFO\tcache/controller_obj_cache.go:2904\tSetting cloud uuid: cloud-ae84c777-ebf8-4b07-878b-880be6b201b5 192023-02-04T11:43:53.176Z\tINFO\tlib/lib.go:291\tSetting AKOUser: ako-wdc-2-tkc-cluster-1 for Avi Objects 202023-02-04T11:43:53.177Z\tINFO\tcache/controller_obj_cache.go:2646\tSkipping the check for SE group labels 212023-02-04T11:43:53.332Z\tINFO\tcache/controller_obj_cache.go:3204\tSetting VRF global found from network vds-tkc-frontend-vlan-1027 222023-02-04T11:43:53.332Z\tINFO\trecord/event.go:282\tEvent(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;avi-system\u0026#34;, Name:\u0026#34;ako-0\u0026#34;, UID:\u0026#34;4b0ee7bf-e5f5-4987-b226-7687c5759b4a\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;41019\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;ValidatedUserInput\u0026#39; User input validation completed. 232023-02-04T11:43:53.336Z\tINFO\tlib/lib.go:230\tSetting Disable Sync to: false 242023-02-04T11:43:53.338Z\tINFO\tk8s/ako_init.go:310\tavi k8s configmap created Looks good, let us try do deploy an application and expose it with Ingress\nI have two demo applications, banana and apple. Yaml comes below. I deploy them\n1andreasm@linuxvm01:~/ako$ k create ns fruit 2namespace/fruit created 3andreasm@linuxvm01:~/ako$ k apply -f apple.yaml -f banana.yaml 4pod/apple-app created 5service/apple-service created 6pod/banana-app created 7service/banana-service created yaml for banana and fruit\n1kind: Pod 2apiVersion: v1 3metadata: 4 name: banana-app 5 labels: 6 app: banana 7 namespace: fruit 8spec: 9 containers: 10 - name: banana-app 11 image: hashicorp/http-echo 12 args: 13 - \u0026#34;-text=banana\u0026#34; 14 15--- 16 17kind: Service 18apiVersion: v1 19metadata: 20 name: banana-service 21 namespace: fruit 22spec: 23 selector: 24 app: banana 25 ports: 26 - port: 5678 # Default port for image 1kind: Pod 2apiVersion: v1 3metadata: 4 name: apple-app 5 labels: 6 app: apple 7 namespace: fruit 8spec: 9 containers: 10 - name: apple-app 11 image: hashicorp/http-echo 12 args: 13 - \u0026#34;-text=apple\u0026#34; 14 15--- 16 17kind: Service 18apiVersion: v1 19metadata: 20 name: apple-service 21 namespace: fruit 22spec: 23 selector: 24 app: apple 25 ports: 26 - port: 5678 # Default port for image Then I apply the Ingress:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 namespace: fruit 6# annotations: 7# ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-tkgs.you-have.your-domain.here 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 1k apply -f ingress-example.yaml You should more or less instantly notice a new virtual service in your Avi controller: And let us check the ingress in k8s:\n1andreasm@linuxvm01:~/ako$ k get ingress -n fruit 2NAME CLASS HOSTS ADDRESS PORTS AGE 3ingress-example avi-lb fruit-tkgs.you-have.your-domain.here 10.102.7.15 80 2m49s There it is, with the actual VIP it gets from Avi.\nHeres is the view of the application from the Dashboard view in Avi: Also notice that the SE's now also places itself in the same network as the worker nodes, but still creates the VIP in the frontend-network.\nMeaning our network diagram will now look like this: Now, AKO comes with its own CRDs that one can work with. I will go through these in a separate post.\n","link":"https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-vds-and-avi-loadbalancer/","section":"post","tags":["kubernetes","tanzu","loadbalancing","ingress"],"title":"vSphere 8 with Tanzu using VDS and Avi Loadbalancer"},{"body":"","link":"https://blog.andreasm.io/tags/lodbalancing/","section":"tags","tags":null,"title":"lodbalancing"},{"body":"","link":"https://blog.andreasm.io/categories/nsx/","section":"categories","tags":null,"title":"nsx"},{"body":"Deploy Tanzu in vSphere 8 with NSX and Avi Loadbalancer: This post will go through how to install Tanzu in vSphere 8 using NSX networking (including built in L4 loadbalancer) and Avi as L7 loadbalancer. The goal is to deploy Tanzu by using NSX for all networking needs, including the Kubernetes Api endpoint (L4) and utilize Avi as loadbalancer for all L7 (Ingress). The deployment of Tanzu with NSX is an automated process, but it does not include L7 loadbalancing. This post will quickly go through how to configure NSX and Avi to support this setup and also the actual configuration/deployment steps of Tanzu. The following components will be touched upon in this post: NSX, Tanzu, TKC, AKO, NCP, vCenter, AVI and Antrea. All networks needed for this deployment will be handled by NSX, except vCenter, NSX manager and Avi controller but including the management network for the supervisor cluster and Avi SE's. In the end we will also have a quick look at how to use Antrea Egress in one of the TKC clusters.\nWe should end up with the following initial network diagram for this deployment (will update it later in the post reflecting several network for our TKC cluster with and without NAT (without NAT when using Egress): Preparations - NSX config This post assumes a working vSphere environment with storage configured, vMotion network, vSAN (if using vSAN), HA, DRS enabled and configured. So this step will cover the basic NSX config for this use-case. NSX will need some network configured in the physical environment like the Geneve Tunnel VLAN, Uplink VLAN(s) for our T0 in addition to the most likely already defined management network for the placement of NSX managers, NSX edges and Avi controller and/or SE's. So lets jump in.\nInitial configs in the NSX manager The first NSX manager is already deployed. Accept the EULA and skip the NSX tutorial:\nWhen done, head over to System -\u0026gt; Licenses and add your license key. Then, still under system, head over to Appliances and add a cluster IP. Even though you only have 1 NSX manager for test/poc it can make sense to use cluster ip adding, removing nsx managers etc and still point to the same IP.\nClick on Set Virtual IP and type in your wanted cluster ip. Out of the box its the same layer 2 subnet as your controllers are placed in (it possible to use L3 also but that involves an external LoadBalancer, not the built in for this purpose).\nClick save and wait. After some minutes, try to log in via your new cluster IP. All the configs I will do will be used with this IP. It does not matter if you go directly to the NSX manager itself or the cluster IP for this post.\nAfter cluster IP is done, we need to add a Compute Manager which in our case is the vCenter server (not that you have any option besides vCenter). Still under System, go to Fabric expand and find Compute Manager. From there click Add Compute Manager:\nFill in the necessary information for your vCenter and make sure Service Account and Enable Trust is enabled. Next message will ask you to use a Thumprint the vCenter says it has. You could either just say ADD or actually go to vCenter and grab the thumbprint from there and verify or paste it in the SHA-256 field before clicking add.\nHere is how to get the thumbprint from vCenter:\n1root@vcsa [ ~ ]# openssl x509 -in /etc/vmware-vpx/ssl/rui.crt -fingerprint -sha256 -noout 2SHA256 Fingerprint=A1:F2:11:0F:47:D8:7B:02:D1:C9:B6:87:19:C0:65:15:B7:6A:6E:23:67:AD:0C:41:03:13:DA:91:A9:D0:B2:F6 Now when you are absolutely certain, add and wait.\nNSX Profiles: Uplink, Transport Zones and Transport Node Profiles In NSX there is a couple of profiles that needs to be configured, profiles for the Transport nodes, Edge transport nodes, transport zones. Instead of configuring things individually NSX uses profiles so we have a consistent and central place to configure multiple components from. Let start with the Uplink profiles: Under Fabric head over to Profiles.\nHere we need to create two uplink profiles (one for the ESXi transport nodes and one for the NSX edge transport nodes). These profile will dictate the number of uplinks used, mtu size (only for the edges after NSX 3.1) ,vlan for the geneve tunnel and nic teaming. Here we also define multiple teaming policies if we want to dictate certain uplinks to be used for deterministic traffic steering. Which I will do.\nHost uplink:\nIn the \u0026quot;host\u0026quot; uplink profile we define the logical uplinks, (uplink-1 and uplink-2, but we could name them Donald-Duck-1 and 2 if we wanted). We define the default teaming-policy to be Load Balance Source as we want two vmkernels for the host-tep, and default active/active if we create a vlan segment without specifying a teaming policy in the segment. Then we add two more teaming policies with Failover Order and specify one uplink pr policy. The reason for that is because we will go on and create a VLAN segment later where we will place the Edge VM uplinks, and we need to have some control over wich uplink-\u0026gt;nic on the ESXi host the T0 Uplinks go, and we dont want to use teaming on these, and we dont want a standby uplink as BGP is supposed to handle failover for us. This way we steer T0 Uplink 1 out via uplink-1 and T0 Uplink 2 out via uplink-2 and the ESXi hosts has been configured to map uplink-1 to VDS uplink-1 which again is mapping to pNIC0 and uplink-2 to VDS uplink-2 to pNIC1 respectively. In summary we create a vlan segment on the host for the edge VM, then we create a vlan segment for the logical T0 later.\nThen we define the VLAN number for the Geneve tunnel. Notice we dont specify MTU size as this will adjust after what we have in our VDS which we will map to later, so our VDS must have minumum MTU 1700 defined (it works with 1600 also, but in NSX-T 3.2 and later there is a greenfield min MTU of 1700). We will use the same VLAN for both host and edge tep wich was supported from NSX-T 3.1 and forward. But to make that work we cant use VDS portgroups for the T0, it needs to be a NSX VLAN segment. More on that later.\nClick save.\nNext up is the \u0026quot;Edge\u0026quot; Uplink Profile, almost same procedure:\nThe biggest difference is the name of the specific teaming policies, and we specify a MTU size of 1700 as this is what I use in my VDS.\nNow over to Transport Zones\nHere we create three Transport Zones: 1 vlan TZ for host, 1 vlan TZ for edge and 1 overlay which is common for both Edge and Host transport nodes.\nCreate host-vlan-tz:\nIts probably not so obvious in the GUI, but we also define our teaming policies defined in our respective uplinks earlier. Here I enter manually the uplink policy names for my host transport zone so they can be available later when I create a VLAN segment for my T0 uplinks (on the host). Click save.\nWhen done its the edge-vlan-tz:\nCommon for both Transport Zones is VLAN.\nNow the last Transport Zone for now - the Overlay TZ (this one is easy):\nThe next step would be to create the Transport Node Profile, but first we need to create an IP-Pool for the TEP addresses. Head over to Networking and IP Address Pools :\nAdd IP address pool:\nIts only necessary to define the CIDR, IP Range and Gateway IP. Please make sure the range is sufficient to support the max amount of ESXi Transport nodes and Edge nodes you will have. 2 IPs pr device.\nNow back to System -\u0026gt; Fabric again and create the Transport Node Profile under Profile:\nHere we select the vCenter we added earlier, point to the correct VDS we want to use, select the host-profile we created and under IP assignment we use our newly created IP pool and map the uplinks to the corresponding VDS uplinks. Then ADD\nInstall NSX components on the ESXi hosts To enable NSX after our initial configs has been done is fairly straight-forward. Head over to Nodes under System-\u0026gt;Fabric and select the first tab Host Transport Nodes. Select your vCenter under Managed by and select your cluster and click Configure NSX:\nSelect your (only?) transport-node profile your created above:\nClick apply and wait...\nStatus in vCenter - the only status we will see.\nWhen everything is up and green as below, NSX is installed in our ESXi hosts and we are ready to create networks üòÑ\nDeploy NSX Edges Deploying a Edge is quite straight forward, and is done from the NSX manager under System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nBut I need to create two VLAN segments for Edge \u0026quot;data-path/uplink interfaces. As I want to use the same VLAN for both Host Tep and Edge TEP I need to do that. These two segments will only be used for the actual Edge VMs, not the T0 I am going to create later. In my lab I am using VLAN 1013 for TEP and VLAN 1014 and 1015 for T0 Uplink 1 and 2. So that means the first VLAN segment I create will have the VLAN Trunk range 1013-1014 and the second VLAN segment will use VLAN trunk 1013,1015 Head over to the Networking section/Segments in the NSX UI click Add Segment:\nSelect host-uplink-1 under Uplink Teaming Policy and add your VLAN ID under VLAN. Click save\nSame procedure again for the second segment:\nselect host-uplink-2 and the correct vlan trunk accordingly.\nThe result should be two VLAN segments, created in our host-vlan-tz (the host vlan transport zone created earlier)\nNow we can deploy our Edge(s).\nHead over to System -\u0026gt;Fabric -\u0026gt;Nodes -\u0026gt;Edge Transport Nodes\nClick add edge node and start the edge deployment wizard:\nGive the edge a name, then fill in a FQDN name. Not sure if that part have to be actually registered in DNS, but it probably does do any harm if you decide to. Choose a form factor. When using Tanzu it can be potentially many virtual services so you should at least go with Large. You can find more sizing recommendation on the VMware official NSX docs page. Next\nFill inn your username and passwords (if you want easier access to SSH shell for troubleshooting purposes enable SSH now).\nSelect your vCenter, cluster and datastore. The rest default.\nConfigure the basics... This the part of the Edge that communicates with the NSX manager. Next we will configure the networking part that will be used for the T0 uplinks and TEP.\nHere we select the transport zones for the Edge to be part of. Note, it should be part of your overlay transport zone but not part of the host vlan transport zone. Here we have defined a Edge vlan transportzone to be used. This is the transport zone where the segment for the T0 to be created in. One of the reason is that we dont want the segment for the T0 to be visible for the host for potentially other workloads, and the segment is actually created in the Edge, thats where the T0 is realised (The SR part of the T0). Then we select the edge-profile we created earlier, the same IP pool as the hosts. Under uplinks we select the respective vlan segments uplink 1 and 2 created earlier.\nThen finish. It should take a couple of minutes to report ready in the NSX manager ui.\nStatus when ready for duty:\nThere should also be some activity in vCenter deploying the edge. When ready head over to Edge Clusters and create a cluster to put the Edge in. We need an Edge cluster and the edges in an edge cluster before we can do anything with them, even if we only deploy one edge (labs etc).\nThe T0 Now that at least one Edge is up, we should create a T0 so we can make some external connectivity happen (even though NSX have its own networking components and we can create full L3 topology, we cant talk outside NSX from overlay without the Edges). Head over to Network and create a VLAN segment. This time the segment should be placed in the edge-vlan-tz as it is use for T0 uplinks only. Select teaming policy and correct vlan for the T0 uplink 1. I will only use 1 uplink in my lab so I will only create 1 segment for this, I only have 1 upstream router to peer to also.\nNext is heading over to Tier-0 and create a T0:\nThe selection is very limited at first, so give it a name and select HA mode, and edge cluster (the one that we created above).\nClick save and yes to continue edit:\nNow we need to add the interface(s) to the T0. The actual interfaces will be residing on the Edges, but we need to define them in the T0. Click on the 0 under Interfaces (its already two interfaces in the screenshot below).\nGive the interface a name, choose type External, give it the correct IP address to peer with the upstream router, and select the Edge VLAN segment created earlier which maps to the correct uplink (1 or 2). Then select the Edge node that shall have this interface configured.\nClick save. Now as an optional step SSH into the Edge selected above, go the correct vrf and ping the upstream router.\nGet the correct VRF (we are looking for the SR T0 part of the T0)\nEnter the vrf by typing vrf and the number, here it is vrf 1.\nListing the interface with get interfaces one should see the interface we configured above, and we can ping the upstream router to verify L2 connectivity.\nGood, now configure BGP. Expand BGP in your T0 settings view (same place as we configure the interface) adjust your BGP settings accordingly. Click save, enter again and add your BGP peers/neighbors bly clicking on neighbors.\nAdd the IP to the BGP peer you should use and adjust accordingly, like AS number. Click Save\nIt will become green directly, then if you click refresh it will become red, then refresh again it should be green again if everything is correct BGP config wise on both sides. Clicking on the (i) will give you the status also:\nFrom your upstream routes you should see a new neighbor established:\nThe last step on the T0 now is to configure it which networks it should advertise on BGP. That is done under Route re-distribution. In a Tanzu setup we need to advertise NAT, connected and LB VIP from our T1s. That is because Tanzu or NCP creates NAT rules, it creates some LB VIPS and we should also be able to reach our other Overlay segments we create under our T1 (which we have not created yet).\nNow that T0 is configured and peering with the upstream router, I can create segments directly under the T0, or create T1s and then segments connected to the T1 instead. If you do create segments directly attached to the T0 one must configure route advertisement accordingly. As the config above is not advertising any networks from T0, only T1.\nIn NSX there is a neat map over the Network Topology in NSX:\nDeploy Tanzu with NSX Now that networking with NSX is configured and the foundation is ready. Its time to deploy the Supervisor cluster, or enable WCP, Workload Management. Head over to the hamburger menu in top left corner in your vCenter and select * Workload Management*\nClick on Get Started\nThen follow the wizard below:\nSelect NSX and Next\nSelect Cluster Deployment, choose your vCenter cluster, give the supervisor a name and below (not in picture above) enter a zone name.\nNext\nSelect your storage policies, Next\nIn Step 4 - Management Network we configure the network for the Supervisor Control Plane VMs. In my lab I have already created an overlay segment I call ls-mgmt with cidr 10.101.10.0/24 and gateway 10.101.10.1. So I will place my Supervisors also there. Not using DHCP, but just defining a start IP. The Supervisors will consist of three VMs, and a cluster IP. But it will use 5 IP addresses in total in this network. DNS, Search Domains and NTP should be your internal services. In screenshot above I have used an external NTP server. NEXT\nAbove I define the workload network, which the supervisor control plane vms also will be part of, but also for the vSphere Pods. This a default workload network where your TKC cluster can be deployed in (if not overriden when creating a vSphere namespace (later on that topic) ). Select the VDS you have used and configured for NSX. Select your Edge cluster. Add your DNS server(s), select the T0 router you created earlier in NSX. Then leave NAT-Mode enabled, we can create vSphere namespaces later where we override these settings. Then you define the Namespace Network. This is the network your vSphere pods will use, the workload network interface of the Supervisor ControlPlane Nodes, and your TKC cluster nodes. The CIDR size define how many IPs you will have available for your TKC cluster nodes, meaning also the total amount of nodes in this workload network. But dont despair, we can create additional vSphere namespaces and add more networks. So in the above example I give it a /20 cidr (unnecessary big actually, but why not). This Namespace Network will not be exposed to the outside world as NCP creates route-map rules on the T0 not allowing these to be advertised (we have NAT enabled). The Service CIDR is Kubernetes internal network for services. When we deploy a TKC cluster later we define other Kubernetes cluster and pod cidrs. Define the Ingress CIDR, this is the IP address range NCP will use to carve out LoadBalancer VIPs for the Kubernetes API endpoints, for the Supervisor Control Plane, the TKC clusters K8s Api endpoint and even the Service Type LoadBalancer services you decide to created. So all access TO the Supervisor Cluster API endpoint will be accessed through the IP address assigned from this CIDR. When we have NAT enabled it will also ask you to define a Egress CIDR which will be used by NSX to create SNAT rules for the worker nodes to use when communicate OUT. These NAT rules will be created automatically in NSX-T.\nNEXT\nSelect the size of your SVCP and give the SVCP API endpoint a name. This is something that can be registered in DNS when deployment is finished and we know the IP it gets.\nFinish and wait. Its the same (waiting) process as explained here\nIf everything goes well we should have a Supervisor cluster up and running in not that many minutes. 20-30 mins?\nWhen its done deploying you will see the green status here: Notice the Control Plane Node Address, this is our endpoint IP being served by Avi loadbalancer and the one we use to interact with the Supervisor cluster to create workloads etc..\nEnter the IP in your browser with https://10.101.11.2 and you should see this page: Download the cli tools for your operating system, deploy them so they are in your path. Will use both kubectl and kubectl-vsphere later on Next steps include creating namespace, deploy workload clusters.\nNSX components configured by WCP/NCP Before heading over to next section, have a look in NSX and see what happended there:\nUnder Networks -\u0026gt; Segments you will notice networks like this:\nNotice the gateway ip and CIDR. These are created for each vSphere Namespace, the cidr 10.101.96.1/28 is carved out of the CIDR defined in the \u0026quot;default\u0026quot; network when deploying WCP.\nUnder Networks -\u0026gt; T-1 Gateways you will notice a couple of new T1 routers being created:\nUnder Networks -\u0026gt; Load Balancing:\nHere is all the L4 K8s API endpoints created, also the other Service Type Loadbalancer services you choose to expose in your TKC clusters.\nUnder Networks -\u0026gt; NAT and the \u0026quot;domain\u0026quot; T1 there will be auto-created NAT rules, depending on whether NAT is enabled or disabled pr Namespace.\nThen under Security -\u0026gt; Distributed Firewall there will be a new section:\nvSphere Namespace When Supervisor is up and running next step is to create a vSphere Namespace. I will go ahead and create that, but will also use the \u0026quot;override network\u0026quot; to create a separate network for this Namespace and also disable NAT as I want to use this cluster for Antrea Egress explained here.\nA vSphere is a construct in vSphere to adjust indidivual access settings/permissions, resources, network settings or different networks for IP separation. Click on Create Namespace and fill in relevant info. I am choosing to Override Supervisor network settings.\nNotice when I disable NAT Mode there will no longer be a Egress IP to populate. Thats because the TKC nodes under this namespace will not get a NAT rule applied to them in NSX and will be communicating \u0026quot;externally\u0026quot; with their own actual IP address. Ingress will still be relevant as the NSX-T Loadbalancer will create the K8s API endpoint to reach the control plane endpoint your respective TKC clusters.\nThis option to adjust the networks in such a degree is a great flexibility with NSX. Tanzu with VDS does give you the option to select different VDS portgroups on separate vlans, but must be manually created, does not give you NAT, Ingress and Egress and VLAN/Routing must be in place in the physical environment. With NSX all the networking components are automatically created and it includes the NSX Distributed Firewall.\nAfter the vSphere Namespace has been created it is the same procedure to deploy TKC clusters regardless of using VDS or NSX. So instead of me repeating myself and saving the environment for digital ink I will refere to the process I have already described here\nConfigure Avi as Ingress controller (L7) with NSX as L4 LB When using Antrea as the CNI in your TKC cluster (default in vSphere with Tanzu) make sure to enable NodePortLocal. This gives much better control and flexibility, follow how here and NodePortLocal explained here\nConfigure NSX cloud in Avi - network preparations As this guide is using NSX as the underlaying networking platform instead of VDS as this article is using, we also have the benefit of configuring the NSX cloud in Avi instead of the vCenter Cloud. This cloud do come with some additional benefits like automatic Security Group creation in NSX, VIP advertisement through the already configured T0, SE placement dataplane/data network on separate network and VRF context.\nBut before we can consume the NSX cloud we need to configure it. Assuming the Avi controller has already been deployed and initial config is done (username and password, dns, etc) log in to the controller and head over to Administration -\u0026gt; User Credentials:\nAdd the credentials you want to use for both vCenter and NSX-T. The next step is involving NSX. Head over to NSX and create two new networks. One for SE management and one for SE dataplane (the network it will use to reach the backend servers they are loadbalancing). I will now just show a screenshot of three networks already created. One segment ls-avi-se-mgmt for SE mgmt, one segment ls-avi-generic-se-data for SE communication to backend pools, and one segment ls-avi-dns-se-data (I will get back to the last network later when enabling the Avi DNS service).\nThen head over to vCenter and create a local Content Library and call it what you want.\nWhen networks and credentials have been created, back in the Avi gui head over to Infrastructure -\u0026gt; Clouds:\nClick Create and select NSX-T\nStart by giving the NSX cloud a meaningfull name and give a prefix name for the objects being created in NSX by Avi. If you have several Avi controllers using same NSX manager etc. Easier to identify when looking in the NSX manager ui:\nThen proceed (scroll down if needed) to connect to you NSX manager:\nEnter the IP of your NSX manager, if you created a Cluster IP in the NSX manager use that one, and select the credentials for NSX manager create in the Avi earlier.\nConnect.\n!Note Before being able to continue this step we need to have defined some networks in NSX as explained above...\nFill in the Transport zone where you have defined the segment for the management network, select the T1 this segment is attached to and then the segment. This network is created in NSX for the SE management interface. Then go to Data Networks and select the Overlay Transport Zone where the ls-avi-generic-se-data segment is created the ADD the T1 router for this segment from the dropdown list and the corresponding segment. (Ignore the second netowork in the screenshot above.) Then under vCenter Server click add and fill in relevant info to connect the vCenter server your NSX manager is using.\nGive it a meaningful name, it will either already have selected your NSX cloud otherwise select it from the list. Then the credentials and the Content Library from the dropdown.\nThe last section IPAM/DNS we will fill out later (click save):\nNow the cloud should be created.\nSE Groups Head over to Cloud Resources -\u0026gt; Service Engine Groups and create a custom SE-group.\nSelect your newly created cloud from the dropdown list above next to Select Cloud. Click create blue button right side.\nChange the fields marked by a red line. And optionally adjust the Max. Number of Service Engines. This is just to restrict Avi to not deploy too many SE's if it sees fit. Adjust the Virtual Services pr Service Engine to a higher number than 10 (if that is default). This all comes down to performance.\nJump to advanced:\nChange the Service Engine Name Prefix to something useful, then select the vCenter, cluster and datastore. Save\nAvi Networks Now head over to Infrastructure -\u0026gt; VRF Contexts (below Service engine Groups) select correct cloud from dropdown and add a default gateway for the SE dataplane network.\nClick the pencil\nAdd a default route under Static Route and point to the gateway used for the SE dataplane ls-avi-generic-se-data . This is used for the SE's to know where to go to reach the different pools it will loadbalance.\nNext we need to define the different networks for the SE's to use (the dataplane network). Head over to Infrastructure -\u0026gt; Networks (again select correct cloud from dropdown)\nIf there is workload running in these networks they can already be auto-detected, if not you need to create them. In my lab I rely on Avi as the IPAM provider for my different services, a very useful feature in Avi. So the two networks we need to create/and or edit is the ls-avi-se-mgmt and the ls-avi-generic-se-data\nFirst out is the ls-avi-se-mgmt\nDeselect DHCP, we want to use Avi IPAM instead (and I dont have DHCP in these networks). Then fill in the CIDR (IP Subnet), deselect Use Static IP Address for VIPs and Service Engine add a range for the SE to be allow to get an IP from. Then select the Use for Service Engines. This will configure the IPAM pool to only be used for the dataplane for the SE's in the management network. We dont want to have any VIPs here as it will only be used for the SE's to talk to the Avi controller.\nThen it is the ls-avi-generic-se-data network\nSame as above just a different subnet, using the same T1 router defined in NSX.\nThe two above networks will only be used as dataplane network. Meaning they will not be used for any Virtual Service VIP. We also need to define 1 or more VIP networks. Create one more network:\nHere I specify a network which is only used for VS VIP\nAvi IPAM/DNS template Now we need to inform our cloud which VIP networks we can use, that is done under Templates -\u0026gt; IPAM/DNS Profiles\nWhile we are here we create two profiles, one for DNS (for the DNS service) and IPAM profile for the VIP networks. Lets start with the IPAM profile. Click create right corner and select IPAM Profile:\nFill in name, select allocate IP in VRF and select your NSX cloud. Then click add and select your VIP networks defined above. Screenshot below also include a DNS-VIP network which I will use later. Click save.\nNow click create again and select DNS profile:\nGive it a name and type in the domain name you want Avi to use.\nNow go back to Infrastructure -\u0026gt; Clouds and edit your NSX cloud and add the newly added Profiles here:\nSave\nNow Avi is prepared and configured to handle requests from AKO on L7 service Ingress. Next step will be to deploy configure and deploy AKO in our TKC cluster. But first some short words around Avi DNS service.\nAvi DNS service Getting a virtual service with a VIP is easy with Avi, but often we need a DNS record on these VS'es. Avi has a built in DNS service which automatically register a DNS record for your services. The simplest way to make this work out of the box is to create a Forward Zone in your DNS server to the DNS Service IP for a specific subdomain or domain. Then Avi will handle the DNS requests for these specific domains. To make use of Avi DNS service we should dedicate a SE group for this service, and create a dedicated VIP network for it. As we should use a dedicated SE group for the DNS service it would be nice to also have a dedicated SE dataplane network for these SE's. So follow the steps I have done above to create a SE network for the SE service SE's and add this to your cloud. The VIP network also needs to be added to the IPAM Profile created earlier. A note on the additional SE network, this also requires a dedicated T1 router in the NSX environment. So in your NSX environment create an additional T1 router, create segment for the DNS SE datanetwork. This is how to enable the DNS service in Avi after you have prepared the networks, and IPAM profile:\nHead over to Administration -\u0026gt; Settings -\u0026gt; DNS Service:\nThen create virtual service:\nSelect your cloud and configure the DNS service:\nThe VS VIP is configured with a static IP (outside of the DNS VIP IPAM range you have created)\nUnder advanced select the SE group:\nSave. Now the DNS VS is configured, go to templates and add a DNS profile:\nGive it a name, add your domain(s) here. Save\nHead over to the cloud and add your DNS profile.\nNow you just need to configure your backend DNS server to forward the requests for these domains to the Avi DNS VS IP. Using bind this can be done like this:\n1zone \u0026#34;you-have.your-domain.here\u0026#34; { 2 type forward; 3 forward only; 4 forwarders { 10.101.211.9; }; 5}; AKO in TKC I have already deployed a TKC cluster, which is described here\nAlso make sure Antrea is configured with NodePortLocal as described also in the link above.\nSo for Avi to work as Ingress controller we need to deploy AKO (Avi Kubernetes Operator). I have also explained these steps here the only difference is how the value.yaml for AKO is configured. Below is how I have configured it to work in my NSX enabled environment with explanations:\n1# this file has been edited by me to easier reflect the changes I have done. So all default comments have been removed, and contains only my comments. 2replicaCount: 1 3 4image: 5 repository: projects.registry.vmware.com/ako/ako 6 pullPolicy: IfNotPresent 7 8 9AKOSettings: 10 primaryInstance: true 11 enableEvents: \u0026#39;true\u0026#39; 12 logLevel: WARN 13 fullSyncFrequency: \u0026#39;1800\u0026#39; 14 apiServerPort: 8080 15 deleteConfig: \u0026#39;false\u0026#39; 16 disableStaticRouteSync: \u0026#39;false\u0026#39; 17 clusterName: wdc-tkc-cluster-1-nsx # Here we need to define a name for our specific TKC cluster. This must not be the exact names as the cluster itself, but why not, it MUST be unique across all your TKC clusters if you have multiple AKO enabled TKC/K8s clusters on same Avi controller 18 cniPlugin: \u0026#39;antrea\u0026#39; #This needs to be set to Antrea, the reason is that you would like to configure NodePortLocal. 19 enableEVH: false 20 layer7Only: true # This is very important to set to true as we already have an other AKO instance managing L4 for our k8s api endpoints. We will only configure this instance to use L7. 21 22 namespaceSelector: 23 labelKey: \u0026#39;\u0026#39; 24 labelValue: \u0026#39;\u0026#39; 25 servicesAPI: false 26 vipPerNamespace: \u0026#39;false\u0026#39; 27 28NetworkSettings: 29 nodeNetworkList: 30 # nodeNetworkList: 31 - networkName: \u0026#34;vnet-domain-c8:dd5825a9-8f62-4823-9347-a9723b6800d5-ns-wdc-1-tkc-cluste-62397-0\u0026#34; # this is the NSX segment created for your specific TKC cluster workers running in. In my case the defined portgroup name above. You can see this in vCenter 32 cidrs: 33 - 10.101.112.32/27 # this is the CIDR for your current TKC cluster (make sure you are using right CIDR, seen from NSX) 34 enableRHI: false 35 nsxtT1LR: \u0026#39;Da-Tier-1\u0026#39; #The T1 router in NSX you have defined for the avi-se-dataplane network 36 bgpPeerLabels: [] 37 # bgpPeerLabels: 38 # - peer1 39 # - peer2 40 vipNetworkList: 41 - networkName: \u0026#34;vip-tkc-cluster-1-nsx-wdc-l7\u0026#34; # This can be the same VIP network you have configured in previously for api endpint or it can be a completely new one. I am going the easy route using the same. It can be shared across multiple cluster (is using NodePortLocal), or can be specific for each tkc cluster. 42 cidr: 10.101.210.0/24 43 44L7Settings: 45 defaultIngController: \u0026#39;true\u0026#39; # Specify if this is the only Ingress controller you have or default if using several others. 46 noPGForSNI: false 47 serviceType: NodePortLocal # Here we select nodeportlocal - verify that Antrea is configured to use NodePortLocal 48 shardVSSize: SMALL # I am setting this to small so I can run more services using same IP. 49 passthroughShardSize: SMALL 50 enableMCI: \u0026#39;false\u0026#39; 51 52L4Settings: 53 defaultDomain: \u0026#39;\u0026#39; 54 autoFQDN: default 55 56 57ControllerSettings: 58 serviceEngineGroupName: nsx-se-generic-group # If you dont decide to use the same VIP as k8s api endpoint you could decide to create an additional ServiceEngineGroup for your L7 services (IP separation etc). 59 controllerVersion: \u0026#39;22.1.1\u0026#39; # AKO version 1.8.2 supports Avi 22.1.2 60 cloudName: wdc-1-nsx # The configured cloud name on the Avi controller. 61 controllerHost: \u0026#39;172.21.101.50\u0026#39; # IP address or Hostname of Avi Controller 62 tenantName: admin 63 64nodePortSelector: 65 key: \u0026#39;\u0026#39; 66 value: \u0026#39;\u0026#39; 67 68resources: 69 limits: 70 cpu: 350m 71 memory: 400Mi 72 requests: 73 cpu: 200m 74 memory: 300Mi 75 76podSecurityContext: {} 77 78rbac: 79 pspEnable: false 80 81 82avicredentials: 83 username: \u0026#39;admin\u0026#39; # username for the Avi controller 84 password: \u0026#39;password\u0026#39; # password for the Avi controller 85 authtoken: 86 certificateAuthorityData: 87 88 89persistentVolumeClaim: \u0026#39;\u0026#39; 90mountPath: /log 91logFile: avi.log Install AKO with this command:\n1helm install ako/ako --generate-name --version 1.8.2 -f values.yaml --namespace=avi-system Check the logs of the AKO pod if it encountered some issues or not by issuing the command:\n1kubectl logs -n avi-system ako-o If there is no errors there its time to deploy a couple of test applications and the Ingress itself. This is already described here\nThats it. Now L7 is enabled on your TKC cluster with Avi as Ingress controller. There is much that can be configured with AKO CRDs. I will try to update my post here to go through the different possibilites. In the meantime much information is described here\nAviInfraSetting If you need to have separate VIPs/different subnets for certain applications we can use AviInfraSetting to override the \u0026quot;default\u0026quot; settings configured in our values.yaml above. This is a nice feature to override some settings very easy. There is also the option to run several AKO instances pr TKC/k8s cluster like described here which I will go through another time. But now quickly AviInfraSetting\nLets say I want to enable BPG on certain services or adjust my Ingress to be exposed on a different VIP network.\nCreate a yaml definition for AviInfraSetting:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: AviInfraSetting 3metadata: 4 name: enable-bgp-fruit 5spec: 6 seGroup: 7 name: Default-Group 8 network: 9 vipNetworks: 10 - networkName: vds-tkc-frontend-l7-vlan-1028 11 cidr: 10.102.8.0/24 12 nodeNetworks: 13 - networkName: vds-tkc-workload-vlan-1026 14 cidrs: 15 - 10.102.6.0/24 16 enableRhi: true 17 bgpPeerLabels: 18 - cPodRouter In the example above I define the VIP network (here I can override the default confgured from value.yaml), the nodNetwork. Enable RHI, and define a label to be used for BGP (label is from BGP settings here):\nPeer:\nApply the above yaml. To use it, create an additional IngressClass like this:\n1apiVersion: networking.k8s.io/v1 2kind: IngressClass 3metadata: 4 name: avi-lb-bgp #name the IngressClass 5spec: 6 controller: ako.vmware.com/avi-lb #default ingressclass from ako 7 parameters: 8 apiGroup: ako.vmware.com 9 kind: AviInfraSetting #refer to the AviInfraSetting 10 name: enable-bgp-fruit #the name of your AviInfraSetting applied Apply it, then when you apply your Ingress or update it refer to this ingressClass like this:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 namespace: fruit 6 7spec: 8 ingressClassName: avi-lb-bgp #Here you choose your specific IngressClass 9 rules: 10 - host: fruit-tkgs.you-have.your-domain.here 11 http: 12 paths: 13 - path: /apple 14 pathType: Prefix 15 backend: 16 service: 17 name: apple-service 18 port: 19 number: 5678 20 - path: /banana 21 pathType: Prefix 22 backend: 23 service: 24 name: banana-service 25 port: 26 number: 5678 ","link":"https://blog.andreasm.io/2022/10/26/vsphere-8-with-tanzu-using-nsx-t-avi-loadbalancer/","section":"post","tags":["tanzu","kubernetes","network","lodbalancing","ingress"],"title":"vSphere 8 with Tanzu using NSX-T \u0026 Avi LoadBalancer"},{"body":"What is AKO? AKO is an operator which works as an ingress controller and performs Avi-specific functions in an OpenShift/Kubernetes environment with the Avi Controller. It runs as a pod in the cluster and translates the required OpenShift/Kubernetes objects to Avi objects and automates the implementation of ingresses/routes/services on the Service Engines (SE) via the Avi Controller. ref: link\nHow to install AKO AKO is very easy installed with Helm. Four basic steps needs to be done.\nCreate a namespace for AKO in your kubernetes cluster: kubectl create ns avi-system Add AKO Helm reposistory: helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Get the current values for the versions you want: helm show values ako/ako --version 1.9.1 \u0026gt; values.yaml Deploy (after values have been edited to suit your environment): helm install ako/ako --generate-name --version 1.9.1 -f values.yaml -n avi-system AKO Helm values explained Before deploying AKO there are some parameters that should be configured, or most likely the deployment will fail. Below is an example file where the different fields are explained:\n1# Default values for ako. 2# This is a YAML-formatted file. 3# Declare variables to be passed into your templates. 4 5replicaCount: 1 6 7image: 8 repository: projects.registry.vmware.com/ako/ako #If using your own registry update accordingly 9 pullPolicy: IfNotPresent 10 11### This section outlines the generic AKO settings 12AKOSettings: 13 primaryInstance: true # Defines AKO instance is primary or not. Value `true` indicates that AKO instance is primary. In a multiple AKO deployment in a cluster, only one AKO instance should be primary. Default value: true. 14 enableEvents: \u0026#39;true\u0026#39; # Enables/disables Event broadcasting via AKO 15 logLevel: WARN # enum: INFO|DEBUG|WARN|ERROR 16 fullSyncFrequency: \u0026#39;1800\u0026#39; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. 17 apiServerPort: 8080 # Internal port for AKO\u0026#39;s API server for the liveness probe of the AKO pod default=8080 18 deleteConfig: \u0026#39;false\u0026#39; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI 19 disableStaticRouteSync: \u0026#39;false\u0026#39; # If the POD networks are reachable from the Avi SE, set this knob to true. 20 clusterName: my-cluster # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT 21 cniPlugin: \u0026#39;\u0026#39; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift|antrea|ncp 22 enableEVH: false # This enables the Enhanced Virtual Hosting Model in Avi Controller for the Virtual Services 23 layer7Only: false # If this flag is switched on, then AKO will only do layer 7 loadbalancing.Must be true if used in a TKC cluster / Tanzu with vSphere 24 # NamespaceSelector contains label key and value used for namespacemigration 25 # Same label has to be present on namespace/s which needs migration/sync to AKO 26 namespaceSelector: 27 labelKey: \u0026#39;\u0026#39; 28 labelValue: \u0026#39;\u0026#39; 29 servicesAPI: false # Flag that enables AKO in services API mode: https://kubernetes-sigs.github.io/service-apis/. Currently implemented only for L4. This flag uses the upstream GA APIs which are not backward compatible 30 # with the advancedL4 APIs which uses a fork and a version of v1alpha1pre1 31 vipPerNamespace: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to create Parent VS per Namespace in EVH mode 32 istioEnabled: false # This flag needs to be enabled when AKO is be to brought up in an Istio environment 33 # This is the list of system namespaces from which AKO will not listen any Kubernetes or Openshift object event. 34 blockedNamespaceList: [] 35 # blockedNamespaceList: 36 # - kube-system 37 # - kube-public 38 ipFamily: \u0026#39;\u0026#39; # This flag can take values V4 or V6 (default V4). This is for the backend pools to use ipv6 or ipv4. For frontside VS, use v6cidr 39 40 41### This section outlines the network settings for virtualservices. 42NetworkSettings: 43 ## This list of network and cidrs are used in pool placement network for vcenter cloud. 44 ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. 45 nodeNetworkList: [] 46 # nodeNetworkList: 47 # - networkName: \u0026#34;network-name\u0026#34; 48 # cidrs: 49 # - 10.0.0.1/24 50 # - 11.0.0.1/24 51 enableRHI: false # This is a cluster wide setting for BGP peering. 52 nsxtT1LR: \u0026#39;\u0026#39; # T1 Logical Segment mapping for backend network. Only applies to NSX-T cloud. 53 bgpPeerLabels: [] # Select BGP peers using bgpPeerLabels, for selective VsVip advertisement. 54 # bgpPeerLabels: 55 # - peer1 56 # - peer2 57 vipNetworkList: [] # Network information of the VIP network. Multiple networks allowed only for AWS Cloud. 58 # vipNetworkList: 59 # - networkName: net1 60 # cidr: 100.1.1.0/24 61 # v6cidr: 2002:üî¢abcd:ffff:c0a8:101/64 # Setting this will enable the VS networks to use ipv6 62### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. 63L7Settings: 64 defaultIngController: \u0026#39;true\u0026#39; 65 noPGForSNI: false # Switching this knob to true, will get rid of poolgroups from SNI VSes. Do not use this flag, if you don\u0026#39;t want http caching. This will be deprecated once the controller support caching on PGs. 66 serviceType: ClusterIP # enum NodePort|ClusterIP|NodePortLocal. NodePortLocal can only be used if Antrea is the CNI 67 shardVSSize: LARGE # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL, DEDICATED 68 passthroughShardSize: SMALL # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL 69 enableMCI: \u0026#39;false\u0026#39; # Enabling this flag would tell AKO to start processing multi-cluster ingress objects. 70 71### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. 72L4Settings: 73 defaultDomain: \u0026#39;\u0026#39; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. 74 autoFQDN: default # ENUM: default(\u0026lt;svc\u0026gt;.\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), flat (\u0026lt;svc\u0026gt;-\u0026lt;ns\u0026gt;.\u0026lt;subdomain\u0026gt;), \u0026#34;disabled\u0026#34; If the value is disabled then the FQDN generation is disabled. 75 76### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. 77ControllerSettings: 78 serviceEngineGroupName: Default-Group # Name of the ServiceEngine Group. 79 controllerVersion: \u0026#39;\u0026#39; # The controller API version 80 cloudName: Default-Cloud # The configured cloud name on the Avi controller. 81 controllerHost: \u0026#39;\u0026#39; # IP address or Hostname of Avi Controller 82 tenantName: admin # Name of the tenant where all the AKO objects will be created in AVI. 83 84nodePortSelector: # Only applicable if serviceType is NodePort 85 key: \u0026#39;\u0026#39; 86 value: \u0026#39;\u0026#39; 87 88resources: 89 limits: 90 cpu: 350m 91 memory: 400Mi 92 requests: 93 cpu: 200m 94 memory: 300Mi 95 96securityContext: {} 97 98podSecurityContext: {} 99 100rbac: 101 # Creates the pod security policy if set to true 102 pspEnable: false 103 104 105avicredentials: 106 username: \u0026#39;\u0026#39; 107 password: \u0026#39;\u0026#39; 108 authtoken: 109 certificateAuthorityData: 110 111 112persistentVolumeClaim: \u0026#39;\u0026#39; 113mountPath: /log 114logFile: avi.log More info here\n","link":"https://blog.andreasm.io/2022/10/26/ako-explained/","section":"post","tags":["kubernetes","ingress","loadbalancing","ako"],"title":"AKO Explained"},{"body":"","link":"https://blog.andreasm.io/categories/loadbalancing/","section":"categories","tags":null,"title":"Loadbalancing"},{"body":"","link":"https://blog.andreasm.io/tags/amko/","section":"tags","tags":null,"title":"amko"},{"body":"","link":"https://blog.andreasm.io/tags/gslb/","section":"tags","tags":null,"title":"gslb"},{"body":"Global Server LoadBalancing in VMware Tanzu with AMKO This post will go through how to configure AVI (NSX ALB) with GSLB in vSphere with Tanzu (TKGs) and an upstream k8s cluster in two different physical locations. I have already covered AKO in my previous posts, this post will assume knowledge of AKO (Avi Kubernetes Operator) and extend upon that with the use of AMKO (Avi Multi-Cluster Kubernetes Operator). The goal is to have the ability to scale my k8s applications between my \u0026quot;sites\u0026quot; and make them geo-redundant. For more information on AVI, AKO and AMKO head over here\nPreparations and diagram over environment used in this post This post will involve a upstream Ubuntu k8s cluster in my home-lab and a remote vSphere with Tanzu cluster. I have deployed one Avi Controller in my home lab and one Avi controller in the remote site. The k8s cluster in my home-lab is defined as the \u0026quot;primary\u0026quot; k8s cluster, the same goes for the Avi controller in my home-lab. There are some networking connectivity between the AVI controllers that needs to be in place such as 443 (API) between the controllers, and the AVI SE's needs to reach the GSLB VS vips on their respective side for GSLB health checks. Site A SE's dataplane needs connectivity to the vip that is created for the GSLB service on site B and vice versa. The primary k8s cluster also needs connectivity to the \u0026quot;secondary\u0026quot; k8s clusters endpoint ip/fqdn, k8s api (port 6443). AMKO needs this connectivity to listen for \u0026quot;GSLB\u0026quot; enabled services in the remote k8s clusters which triggers AMKO to automatically put them in your GSLB service. More on that later in the article. When all preparations are done the final diagram should look something like this:\n(I will not cover what kind of infrastructure that connects the sites together as that is a completely different topic and can be as much). But there will most likely be a firewall involved between the sites, and the above mentioned connectivity needs to be adjusted in the firewall. In this post the following ip subnets will be used:\nSE Dataplane network home-lab: 10.150.1.0/24 (I only have two se's so there will be two addresses from this subnet) (I am running the all services on the same two SE's which is not recommended, one should atleast have dedicated SE's for the AVI DNS service) SE Dataplane network remote-site: 192.168.102.0/24 (Two SE's here also, in remote site I do have dedicated SE's for the AVI DNS Service but they will not be touched upon in this post only the SE's responsible for the GSLB services being created) VIP subnet for services exposed in home-lab k8s cluster: 10.150.12.0/24 (a dedicated vip subnet for all services exposed from this cluster) VIP subnet for services exposed in remote-site tkgs cluster: 192.168.151.0/24 (a dedicated vip subnet for all services exposed from this cluster) For this network setup to work one needs to have routing in place, either with BGP enabled in AVI or static routes. Explanation: The SE's have their own dataplane network, they are also the ones responsible for creating the VIPs you define for your VS. So, if you want your VIPs to be reachable you have to make sure there are routes in your network to the VIPS where the SEs are next hops either with BGP or static routes. The VIP is what it is, a Virtual IP meaning it dont have its own VLAN and gateway in your infrastructure. It is created and realised by the SE's. The SE's are then the gateways for your VIPS. A VIP address could be anything. At the same time the SEs dataplane network needs connectivity to the backend servers it is supposed to loadbalance, so this dataplane network also needs routes to reach those. In this post that means the SE's dataplane network will need reachability to the k8s worker nodes where your apps are running in the home-lab site and in the remote site it needs reachability to the TKGs workers. On a sidenote I am not running routable pods, they are nat-ed trough my workers, and I am using Antrea as CNI with NodePortLocal configured. I also prefer to have a different network for the SE dataplane, different VIP subnets as it is easier to maintain control, isolation, firewall rules etc.\nThe diagram above is very high level, as it does not go into all networking details, firewall rules etc but it gives an overview of the communication needed.\nWhen one have an clear idea of the connectivity requirements we need to form the GSLB \u0026quot;partnership\u0026quot; between the AVI controllers. I was thinking back and forth whether I should cover these steps also but instead I will link to a good friends blog site here that does this brilliantly. Its all about saving the environment of unnecessary digital ink üòÑ. This also goes for AKO deployment. This is also covered here or from the AVI docs page here\nIt should look like this on both controllers when everything is up and ready for GSLB: It should be reflected on the secondary controller as well, except there will be no option to edit.\nTime to deploy AMKO in K8s AMKO can be deployed in two ways. It can be sufficient with only one instance of AMKO deployed in your primary k8s cluster, or you can go the federation approach and deploy AMKO in all your clusters that you want to use GSLB on. Then you will end up with one master instance of AMKO and \u0026quot;followers\u0026quot; or federation member on the others. One of the benefit is that you can promote one of the follower members if the primary is lost. I will go with the simple approach, deploy AMKO once, in my primary k8s cluster in my home-lab.\nAMKO preparations before deploy with Helm AMKO will be deployed by using Helm, so if Helm is not installed do that. To successfully install AMKO there is a couple of things to be done. First, decide which is your primary cluster (where to deploy AMKO). When you have decided that (the easy step) then you need to prepare a secret that contains the context/clusters/users for all the k8s clusters you want to use GSLB on. An example file can be found here. Create this content in a regular file and name the file gslb-members. The naming of the file is important, if you name it differently AMKO will fail as it cant find the secret. I have tried to find a variable that is able override this in the value.yaml for the Helm chart but has not succeeded, so I went with the default naming. When that is populated with the k8s clusters you want, we need to create a secret in our primary k8s cluster like this: kubectl create secret generic gslb-config-secret --from-file gslb-members -n avi-system. The namespace here is the namespace where AKO is already deployed in.\nThis should give you a secret like this:\n1gslb-config-secret Opaque 1 20h A note on kubeconfig for vSphere with Tanzu (TKGs) When logging into a guest cluster in TKGs we usually do this through the supervisor with either vSphere local users or AD users defined in vSphere and we get a timebased token. Its not possible to use this approach. So what I went with was to grab the admin credentials for my TKGs guest cluster and used that context instead. Here is how to do that. This is not a recommended approach, instead one should create and use a service account. Maybe I will get back to this later and update how.\nBack to the AMKO deployment...\nThe secret is ready, now we need to get the value.yaml for the AMKO version we will install. I am using AMKO 1.8.1 (same for AKO). The Helm repo for AMKO is already added if AKO has been installed using Helm, the same repo. If not, add the repo:\n1helm repo add ako https://projects.registry.vmware.com/chartrepo/ako Download the value.yaml:\n1 helm show values ako/amko --version 1.8.1 \u0026gt; values.yaml (there is a typo in the official doc - it points to just amko) Now edit the values.yaml:\n1# This is a YAML-formatted file. 2# Declare variables to be passed into your templates. 3 4replicaCount: 1 5 6image: 7 repository: projects.registry.vmware.com/ako/amko 8 pullPolicy: IfNotPresent 9 10# Configs related to AMKO Federator 11federation: 12 # image repository 13 image: 14 repository: projects.registry.vmware.com/ako/amko-federator 15 pullPolicy: IfNotPresent 16 # cluster context where AMKO is going to be deployed 17 currentCluster: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name - for your leader/primary cluster 18 # Set to true if AMKO on this cluster is the leader 19 currentClusterIsLeader: true 20 # member clusters to federate the GSLBConfig and GDP objects on, if the 21 # current cluster context is part of this list, the federator will ignore it 22 memberClusters: 23 - \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name 24 - \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name 25# Configs related to AMKO Service discovery 26serviceDiscovery: 27 # image repository 28 # image: 29 # repository: projects.registry.vmware.com/ako/amko-service-discovery 30 # pullPolicy: IfNotPresent 31 32# Configs related to Multi-cluster ingress. Note: MultiClusterIngress is a tech preview. 33multiClusterIngress: 34 enable: false 35 36configs: 37 gslbLeaderController: \u0026#39;172.18.5.51\u0026#39; ##### MGMT ip leader/primary avi controller 38 controllerVersion: 22.1.1 39 memberClusters: 40 - clusterContext: \u0026#39;k8slab-admin@k8slab\u0026#39; #####use the context name 41 - clusterContext: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; #####use the context name 42 refreshInterval: 1800 43 logLevel: INFO 44 # Set the below flag to true if a different GSLB Service fqdn is desired than the ingress/route\u0026#39;s 45 # local fqdns. Note that, this field will use AKO\u0026#39;s HostRule objects\u0026#39; to find out the local to global 46 # fqdn mapping. To configure a mapping between the local to global fqdn, configure the hostrule 47 # object as: 48 # [...] 49 # spec: 50 # virtualhost: 51 # fqdn: foo.avi.com 52 # gslb: 53 # fqdn: gs-foo.avi.com 54 useCustomGlobalFqdn: true ####### set this to true if you want to define custom FQDN for GSLB - I use this 55 56gslbLeaderCredentials: 57 username: \u0026#39;admin\u0026#39; ##### username/password AVI Controller 58 password: \u0026#39;password\u0026#39; ##### username/password AVI Controller 59 60globalDeploymentPolicy: 61 # appSelector takes the form of: 62 appSelector: 63 label: 64 app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB 65 # Uncomment below and add the required ingress/route/service label 66 # appSelector: 67 68 # namespaceSelector takes the form of: 69 # namespaceSelector: 70 # label: 71 # ns: gslb \u0026lt;example label key-value for namespace\u0026gt; 72 # Uncomment below and add the reuqired namespace label 73 # namespaceSelector: 74 75 # list of all clusters that the GDP object will be applied to, can take any/all values 76 # from .configs.memberClusters 77 matchClusters: 78 - cluster: \u0026#39;k8slab-admin@k8slab\u0026#39; ####use the context name 79 - cluster: \u0026#39;tkgs-cluster-1-admin@tkgs-cluster-1\u0026#39; ####use the context name 80 81 # list of all clusters and their traffic weights, if unspecified, default weights will be 82 # given (optional). Uncomment below to add the required trafficSplit. 83 # trafficSplit: 84 # - cluster: \u0026#34;cluster1-admin\u0026#34; 85 # weight: 8 86 # - cluster: \u0026#34;cluster2-admin\u0026#34; 87 # weight: 2 88 89 # Uncomment below to specify a ttl value in seconds. By default, the value is inherited from 90 # Avi\u0026#39;s DNS VS. 91 # ttl: 10 92 93 # Uncomment below to specify custom health monitor refs. By default, HTTP/HTTPS path based health 94 # monitors are applied on the GSs. 95 # healthMonitorRefs: 96 # - hmref1 97 # - hmref2 98 99 # Uncomment below to specify a Site Persistence profile ref. By default, Site Persistence is disabled. 100 # Also, note that, Site Persistence is only applicable on secure ingresses/routes and ignored 101 # for all other cases. Follow https://avinetworks.com/docs/20.1/gslb-site-cookie-persistence/ to create 102 # a Site persistence profile. 103 # sitePersistenceRef: gap-1 104 105 # Uncomment below to specify gslb service pool algorithm settings for all gslb services. Applicable 106 # values for lbAlgorithm: 107 # 1. GSLB_ALGORITHM_CONSISTENT_HASH (needs a hashMask field to be set too) 108 # 2. GSLB_ALGORITHM_GEO (needs geoFallback settings to be used for this field) 109 # 3. GSLB_ALGORITHM_ROUND_ROBIN (default) 110 # 4. GSLB_ALGORITHM_TOPOLOGY 111 # 112 # poolAlgorithmSettings: 113 # lbAlgorithm: 114 # hashMask: # required only for lbAlgorithm == GSLB_ALGORITHM_CONSISTENT_HASH 115 # geoFallback: # fallback settings required only for lbAlgorithm == GSLB_ALGORITHM_GEO 116 # lbAlgorithm: # can only have either GSLB_ALGORITHM_ROUND_ROBIN or GSLB_ALGORITHM_CONSISTENT_HASH 117 # hashMask: # required only for fallback lbAlgorithm as GSLB_ALGORITHM_CONSISTENT_HASH 118 119serviceAccount: 120 # Specifies whether a service account should be created 121 create: true 122 # Annotations to add to the service account 123 annotations: {} 124 # The name of the service account to use. 125 # If not set and create is true, a name is generated using the fullname template 126 name: 127 128resources: 129 limits: 130 cpu: 250m 131 memory: 300Mi 132 requests: 133 cpu: 100m 134 memory: 200Mi 135 136service: 137 type: ClusterIP 138 port: 80 139 140rbac: 141 # creates the pod security policy if set to true 142 pspEnable: false 143 144persistentVolumeClaim: \u0026#39;\u0026#39; 145mountPath: /log 146logFile: amko.log 147 148federatorLogFile: amko-federator.log When done, its time to install AMKO like this:\n1helm install ako/amko --generate-name --version 1.8.1 -f /path/to/values.yaml --set configs.gslbLeaderController=\u0026lt;leader_controller_ip\u0026gt; --namespace=avi-system ####There is a typo in the official docs - its pointing to amko only If everything went well you should se a couple of things in your k8s cluster under the namespace avi-system.\n1k get pods -n avi-system 2NAME READY STATUS RESTARTS AGE 3ako-0 1/1 Running 0 25h 4amko-0 2/2 Running 0 20h 5 6k get amkocluster amkocluster-federation -n avi-system 7NAME AGE 8amkocluster-federation 20h 9 10k get gc -n avi-system gc-1 11NAME AGE 12gc-1 20h 13 14k get gdp -n avi-system 15NAME AGE 16global-gdp 20h AMKO is up and running. Time create a GSLB service\nCreate GSLB service You probably already have a bunch of ingress services running, and to make them GSLB \u0026quot;aware\u0026quot; there is not much to be done to achieve that. If you noticed in our value.yaml for the AMKO Helm chart we defined this:\n1globalDeploymentPolicy: 2 # appSelector takes the form of: 3 appSelector: 4 label: 5 app: \u0026#39;gslb\u0026#39; #### I am using this selector for services to be used in GSLB So what we need to in our ingress service is to add the below, and then a new section where we define our gslb fqdn.\nHere is my sample ingress applied in my primary k8s cluster:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 labels: #### This is added for GSLB 6 app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml 7 namespace: fruit 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-global.guzware.net #### Specific for this site (Home Lab) 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 29--- #### New section to define a host rule 30apiVersion: ako.vmware.com/v1alpha1 31kind: HostRule 32metadata: 33 namespace: fruit 34 name: gslb-host-rule-fruit 35spec: 36 virtualhost: 37 fqdn: fruit-global.guzware.net #### Specific for this site (Home Lab) 38 enableVirtualHost: true 39 gslb: 40 fqdn: fruit.gslb.guzware.net ####This is common for both sites As soon as it is applied, and there are no errors in AMKO or AKO, it should be visible in your AVI controller GUI: If you click on the name it should take you to next page where it show the GSLB pool members and the status: Screenshot below is when both sites have applied their GSLB services: \u0026quot;\nNext we need to apply gslb settings on the secondary site also:\nThis is what I have deployed on the secondary site (note the difference in domain names specific for that site)\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: ingress-example 5 labels: #### This is added for GSLB 6 app: gslb #### This is added for GSLB - Using the selector I chose in the value.yaml 7 namespace: fruit 8 9spec: 10 ingressClassName: avi-lb 11 rules: 12 - host: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) 13 http: 14 paths: 15 - path: /apple 16 pathType: Prefix 17 backend: 18 service: 19 name: apple-service 20 port: 21 number: 5678 22 - path: /banana 23 pathType: Prefix 24 backend: 25 service: 26 name: banana-service 27 port: 28 number: 5678 29--- #### New section to define a host rule 30apiVersion: ako.vmware.com/v1alpha1 31kind: HostRule 32metadata: 33 namespace: fruit 34 name: gslb-host-rule-fruit 35spec: 36 virtualhost: 37 fqdn: fruit-site-2.lab.guzware.net #### Specific for this site (Remote Site) 38 enableVirtualHost: true 39 gslb: 40 fqdn: fruit.gslb.guzware.net ##### Common for both sites When this is applied Avi will go ahead and put this into the same GSLB service as above, and the screenshot above will be true.\nNow I have the same application deployed in both sites, but equally available whether I am sitting in my home-lab or at the remote-site. There is a bunch of parameters that can be tuned, which I will not go into now (maybe getting back to this and update with further possibilities with GSLB). But one of them can be LoadBalancing algorithms such as Geo Location Source. Say I want the application to be accessed from clients as close to the application as possible. And should one of the sites become unavailable it will still be accessible from one of the sites that are still online. Very cool indeed. For the sake of the demo I am about to show the only thing I change in the default GSLB settings is the TTL, I am setting it to 2 seconds so I can showcase that the application is being load balanced between both sites. Default algorithm is Round-Robin so it should balance between them regardless of the latency difference (accessing the application from my home network in my home lab vs from my home network in the remote-site which has several ms in distance). Heres where I am setting these settings: With a TTL of 2 seconds it should switch faster so I can see the balancing between the two sites. Let me try to access the application from my browser using the gslb fqdn: fruit.gslb.guzware.net/apple\nA refresh of the page and now: To even illustrate more I will run a curl command against the gslb fqdn: Now a ping against the FQDN to show the ip of the corresponding site that answer on the call: Notice the change in ip address but also the latency in ms\nNow I can go ahead and disable one of the site to simulate failover, and the application is still available on the same FQDN. So many possibilities with GSLB.\nThats it then. NSX ALB, AKO with AMKO between two sites, same application available in two physical location, redundancy, scale-out, availability. Stay tuned for more updates in advanced settings - in the future üòÑ\n","link":"https://blog.andreasm.io/2022/10/23/gslb-with-ako-amko-nsx-advanced-loadbalancer/","section":"post","tags":["gslb","ako","amko"],"title":"GSLB With AKO \u0026 AMKO - NSX Advanced LoadBalancer"},{"body":"","link":"https://blog.andreasm.io/tags/custom-resource-definitions/","section":"tags","tags":null,"title":"custom-resource-definitions"},{"body":"AKO settings: What happens if we need to to this\nWhat happens if I need passthrough\nHow does AKO work\n","link":"https://blog.andreasm.io/2022/10/23/we-take-a-look-at-the-ako-crds/","section":"post","tags":["ako","custom-resource-definitions","kubernetes"],"title":"We Take a Look at the AKO Crds"},{"body":"","link":"https://blog.andreasm.io/tags/docker/","section":"tags","tags":null,"title":"docker"},{"body":"","link":"https://blog.andreasm.io/tags/loadbalancer/","section":"tags","tags":null,"title":"loadbalancer"},{"body":"TOPICS: ","link":"https://blog.andreasm.io/2022/10/23/running-the-unifi-controller-in-kubernetes/","section":"post","tags":["unifi","loadbalancer","docker"],"title":"Running the Unifi Controller in Kubernetes"},{"body":"","link":"https://blog.andreasm.io/tags/unifi/","section":"tags","tags":null,"title":"unifi"},{"body":"","link":"https://blog.andreasm.io/tags/grafana/","section":"tags","tags":null,"title":"grafana"},{"body":"","link":"https://blog.andreasm.io/categories/logging/","section":"categories","tags":null,"title":"Logging"},{"body":"","link":"https://blog.andreasm.io/tags/loki/","section":"tags","tags":null,"title":"loki"},{"body":"","link":"https://blog.andreasm.io/categories/monitoring/","section":"categories","tags":null,"title":"Monitoring"},{"body":"Logging and metrics monitoring I wanted to visualize performance metrics in Grafana, and getting the logs from my Kubernetes clusters available centrally. So i chose to go with Grafana as my \u0026quot;dashboard\u0026quot; for visualizing, Prometheus for metrics and Loki for logs. I did fiddle some to get this up and running. But after I while I managed to get it sorted the way I wanted.\nSources used in this article: Bitnami, Grafana and Kube-Prometheus-Stack\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo add grafana https://grafana.github.io/helm-charts\nhelm repo add bitnami https://charts.bitnami.com/bitnami\n","link":"https://blog.andreasm.io/2022/10/19/monitoring-with-prometheus-loki-promtail-and-grafana/","section":"post","tags":["loki","grafana","promtail","prometheus"],"title":"Monitoring With Prometheus, Loki, Promtail and Grafana"},{"body":"","link":"https://blog.andreasm.io/tags/prometheus/","section":"tags","tags":null,"title":"prometheus"},{"body":"","link":"https://blog.andreasm.io/tags/promtail/","section":"tags","tags":null,"title":"promtail"},{"body":"","link":"https://blog.andreasm.io/categories/docker/","section":"categories","tags":null,"title":"Docker"},{"body":"","link":"https://blog.andreasm.io/tags/harbor/","section":"tags","tags":null,"title":"harbor"},{"body":"","link":"https://blog.andreasm.io/categories/harbor/","section":"categories","tags":null,"title":"Harbor"},{"body":"","link":"https://blog.andreasm.io/categories/helm/","section":"categories","tags":null,"title":"Helm"},{"body":"","link":"https://blog.andreasm.io/tags/registry/","section":"tags","tags":null,"title":"registry"},{"body":"This post will briefly go through how to deploy (using Helm), configure and use VMware Harbor registry in Kubernetes.\nQuick introduction to Harbor Harbor is an open source registry that secures artifacts with policies and role-based access control, ensures images are scanned and free from vulnerabilities, and signs images as trusted. Harbor, a CNCF Graduated project, delivers compliance, performance, and interoperability to help you consistently and securely manage artifacts across cloud native compute platforms like Kubernetes and Docker. link\nI use myself Harbor in many of my own projects, including the images I make for my Hugo blogsite (this).\nDeploy Harbor with Helm Add helm chart:\n1helm repo add harbor https://helm.goharbor.io 2helm fetch harbor/harbor --untar Before you perform the default helm install of Harbor you want to grab the helm values for the Harbor charts so you can edit some settings to match your environment:\n1helm show values harbor/harbor \u0026gt; harbor.values.yaml The default values you get from the above command includes all available parameter which can be a bit daunting to go through. In the values file I use I have only picked the parameters I needed to set, here:\n1expose: 2 type: ingress 3 tls: 4 enabled: true 5 certSource: secret 6 secret: 7 secretName: \u0026#34;harbor-tls-prod\u0026#34; # certificates you have created with Cert-Manager 8 notarySecretName: \u0026#34;notary-tls-prod\u0026#34; # certificates you have created with Cert-Manager 9 ingress: 10 hosts: 11 core: registry.example.com 12 notary: notary.example.com 13 annotations: 14 kubernetes.io/ingress.class: \u0026#34;avi-lb\u0026#34; 15 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 16externalURL: https://registry.example.com 17harborAdminPassword: \u0026#34;PASSWORD\u0026#34; 18persistence: 19 enabled: true 20 # Setting it to \u0026#34;keep\u0026#34; to avoid removing PVCs during a helm delete 21 # operation. Leaving it empty will delete PVCs after the chart deleted 22 # (this does not apply for PVCs that are created for internal database 23 # and redis components, i.e. they are never deleted automatically) 24 resourcePolicy: \u0026#34;keep\u0026#34; 25 persistentVolumeClaim: 26 registry: 27 # Use the existing PVC which must be created manually before bound, 28 # and specify the \u0026#34;subPath\u0026#34; if the PVC is shared with other components 29 existingClaim: \u0026#34;\u0026#34; 30 # Specify the \u0026#34;storageClass\u0026#34; used to provision the volume. Or the default 31 # StorageClass will be used (the default). 32 # Set it to \u0026#34;-\u0026#34; to disable dynamic provisioning 33 storageClass: \u0026#34;nfs-client\u0026#34; 34 subPath: \u0026#34;\u0026#34; 35 accessMode: ReadWriteOnce 36 size: 50Gi 37 annotations: {} 38 database: 39 existingClaim: \u0026#34;\u0026#34; 40 storageClass: \u0026#34;nfs-client\u0026#34; 41 subPath: \u0026#34;postgres-storage\u0026#34; 42 accessMode: ReadWriteOnce 43 size: 1Gi 44 annotations: {} 45 46portal: 47 tls: 48 existingSecret: harbor-tls-prod When you have edited the values file its time to install:\n1helm install -f harbor.values.yaml harbor-deployment harbor/harbor -n harbor Explanation: \u0026quot;-f\u0026quot; is telling helm to read the values from the specified file after, then the name of your helm installation (here harbor-deployment) then the helm repo and finally the namespace you want it deployed in. A couple of seconds later you should be able to log in to the GUI of Harbor through your webbrowser if everything has been set up right, Ingress, pvc, secrets.\nCertificate You can either use Cert-manager as explained here or bring your own ca signed certificates.\nHarbor GUI To log in to the GUI for the first time open your browser and point it to the externalURL you gave it in your values file and the corresponding harborAdminPassword you defined. From there on you create users and projects and start exploring Harbor.\nUsers: Projects: Docker images To push your images to Harbor execute the following commands:\n1docker login registry.example.com #log in with the user/password you have created in the GUI 2docker tag image-name:tag registry.example.com/project/image-name:tag 3docker push registry.example.com/project/image-name:tag ","link":"https://blog.andreasm.io/2022/10/13/vmware-harbor-registry/","section":"post","tags":["harbor","registry"],"title":"VMware Harbor Registry"},{"body":"","link":"https://blog.andreasm.io/categories/hugo/","section":"categories","tags":null,"title":"hugo"},{"body":"This blog post will cover how I wanted to deploy Hugo to host my blog-page.\nPreparations To achieve what I wanted, deploy an highly available Hugo hosted blog page, I decided to run Hugo in Kubernetes. For that I needed\nKubernetes cluster, obviously, consisting of several workers for the the \u0026quot;hugo\u0026quot; pods to run on (already covered here. Persistent storage (NFS in my case, already covered here) An Ingress controller (already covered here) A docker image with Hugo, nginx and go (will be covered here) Docker installed so you can build the image A place to host the docker image (Docker hub or Harbor registry will be covered here) Create the Docker image Before I can deploy Hugo I need to create an Docker image that contains the necessary bits. I have already created the Dockerfile here:\n1#Install the container\u0026#39;s OS. 2FROM ubuntu:latest as HUGOINSTALL 3 4# Install Hugo. 5RUN apt-get update -y 6RUN apt-get install wget git ca-certificates golang -y 7RUN wget https://github.com/gohugoio/hugo/releases/download/v0.104.3/hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ 8 tar -xvzf hugo_extended_0.104.3_Linux-64bit.tar.gz \u0026amp;\u0026amp; \\ 9 chmod +x hugo \u0026amp;\u0026amp; \\ 10 mv hugo /usr/local/bin/hugo \u0026amp;\u0026amp; \\ 11 rm -rf hugo_extended_0.104.3_Linux-64bit.tar.gz 12# Copy the contents of the current working directory to the hugo-site 13# directory. The directory will be created if it doesn\u0026#39;t exist. 14COPY . /hugo-site 15 16# Use Hugo to build the static site files. 17RUN hugo -v --source=/hugo-site --destination=/hugo-site/public 18 19# Install NGINX and deactivate NGINX\u0026#39;s default index.html file. 20# Move the static site files to NGINX\u0026#39;s html directory. 21# This directory is where the static site files will be served from by NGINX. 22FROM nginx:stable-alpine 23RUN mv /usr/share/nginx/html/index.html /usr/share/nginx/html/old-index.html 24COPY --from=HUGOINSTALL /hugo-site/public/ /usr/share/nginx/html/ 25 26# The container will listen on port 80 using the TCP protocol. 27EXPOSE 80 Credits for the Dockerfile as it was initially taken from here. I have updated it, and done some modifications to it.\nBefore building the image with docker, install docker by following this guide.\nBuild the docker image I need to place myself in the same directory as my Dockerfile and execute the following command (Replace \u0026quot;name-you-want-to-give-the-image:\u0026lt;tag\u0026gt;\u0026quot; with something like \u0026quot;hugo-image:v1\u0026quot;):\n1docker build -t name-you-want-to-give-the-image:\u0026lt;tag\u0026gt; . #Note the \u0026#34;.\u0026#34; important Now the image will be built and hosted locally on my \u0026quot;build machine\u0026quot;.\nIf anything goes well it should be listed here:\n1$ docker images 2REPOSITORY TAG IMAGE ID CREATED SIZE 3hugo-image v1 d43ee98c766a 10 secs ago 70MB 4nginx stable-alpine 5685937b6bc1 7 days ago 23.5MB 5ubuntu latest 216c552ea5ba 9 days ago 77.8MB Place the image somewhere easily accessible Now that I have my image I need to make sure it is easily accessible for my Kubernetes workers so they can download the image and deploy it. For that I can use the local docker registry pr control node and worker node. Meaning I need to load the image into all workers and control plane nodes. Not so smooth way to to do it. This is the approach for such a method:\n1docker save -o \u0026lt;path for generated tar file\u0026gt; \u0026lt;image name\u0026gt; #needs to be done on the machine you built the image. Example: docker save -o /home/username/hugo-image.v1.tar hugo-image:v1 This will \u0026quot;download\u0026quot; the image from the local docker repository and create tar file. This tar file needs to be copied to all my workers and additional control plane nodes with scp or other methods I find suitable. When that is done I need to upload the tar to each of their local docker repository with the following command:\n1docker -i load /home/username/hugo-image.v1.tar It is ok to know about this process if you are in non-internet environments etc, but even in non-internet environment we can do this with a private registry. And thats where Harbor can come to the rescue link.\nWith Harbor I can have all my images hosted centrally but dont need access to the internet as it is hosted in my own environment.\nI could also use Docker hub. Create an account there, and use it as my repository. I prefer the Harbor registry, as it provides many features. The continuation of this post will use Harbor, the procedure to upload/download images is the same process as with Docker hub but you log in to your own Harbor registry instead of Docker hub.\nUploading my newly created image is done like this:\n1docker login registry.example.com #FQDN to my selfhosted Harbor registry, and the credentials for an account I have created there. 2docker tag hugo-image:v1 https://registry.example.com/hugo/hugo-image:v1 #\u0026#34;/hugo/\u0026#34; name of project in Harbor 3docker push registry.example.com/hugo/hugo-image:v1 #upload it Thats it. Now I can go ahead and create my deployment.yaml definition file in my Kubernetes cluster, point it to my image hosted at my local Harbor registry (e.g registry.example.com/hugo/hugo-image:v1). But let me go through how I created my Hugo deployment in Kubernetes, as I am so close to see my newly image in action üòÑ (Will it even work).\nDeploy Hugo in Kubernetes To run my Hugo image in Kubernetes the way I wanted I need to define a Deployment (remember I wanted a highly available Hugo deployment, meaning more than one pod and the ability to scale up/down). The first section of my hugo-deployment.yaml definition file looks like this:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: hugo-site 5 namespace: hugo-site 6spec: 7 replicas: 3 8 selector: 9 matchLabels: 10 app: hugo-site 11 tier: web 12 template: 13 metadata: 14 labels: 15 app: hugo-site 16 tier: web 17 spec: 18 containers: 19 - image: registry.example.com/hugo/hugo-image:v1 20 name: hugo-site 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 80 24 name: hugo-site 25 volumeMounts: 26 - name: persistent-storage 27 mountPath: /usr/share/nginx/html/ 28 volumes: 29 - name: persistent-storage 30 persistentVolumeClaim: 31 claimName: hugo-pv-claim In the above I define name of deployment, specify number of pods with the replica specification, labels, point to my image hosted in Harbor and then what the container mountPath and the peristent volume claim. mountPath is inside the container, and the files/folders mounted is read from the content it sees in the persistent volume claim \u0026quot;hugo-pv-claim\u0026quot;. Thats where Hugo will find the content of the Public folder (after the content has been generated).\nI also needed to define a Service so I can reach/expose the containers contents (webpage) on port 80. This is done with this specification:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: hugo-service 5 namespace: hugo-site 6 labels: 7 svc: hugo-service 8spec: 9 selector: 10 app: hugo-site 11 tier: web 12 ports: 13 - port: 80 Can be saved as a separate \u0026quot;service.yaml\u0026quot; file or pasted into one yaml file. But instead of pointing to my workers IP addresses to read the content each time I wanted to expose it with an Ingress by using AKO and Avi LoadBalancer. This is how I done that:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: hugo-ingress 5 namespace: hugo-site 6 labels: 7 app: hugo-ingress 8 annotations: 9 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 10spec: 11 ingressClassName: avi-lb 12 rules: 13 - host: yikes.guzware.net 14 http: 15 paths: 16 - pathType: Prefix 17 path: / 18 backend: 19 service: 20 name: hugo-service 21 port: 22 number: 80 I define my ingressClassName, the hostname for my Ingress controller to listen for requests on and the Service the Ingress should route all the request to yikes.guzware.net to, which is my hugo-service defined earlier. Could also be saved as a separe yaml file. I have chosen to put all three \u0026quot;kinds\u0026quot; in one yaml file. Which then looks like this:\n1apiVersion: apps/v1 2kind: Deployment 3metadata: 4 name: hugo-site 5 namespace: hugo-site 6spec: 7 replicas: 3 8 selector: 9 matchLabels: 10 app: hugo-site 11 tier: web 12 template: 13 metadata: 14 labels: 15 app: hugo-site 16 tier: web 17 spec: 18 containers: 19 - image: registry.example.com/hugo/hugo-image:v1 20 name: hugo-site 21 imagePullPolicy: Always 22 ports: 23 - containerPort: 80 24 name: hugo-site 25 volumeMounts: 26 - name: persistent-storage 27 mountPath: /usr/share/nginx/html/ 28 volumes: 29 - name: persistent-storage 30 persistentVolumeClaim: 31 claimName: hugo-pv-claim 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: hugo-service 37 namespace: hugo-site 38 labels: 39 svc: hugo-service 40spec: 41 selector: 42 app: hugo-site 43 tier: web 44 ports: 45 - port: 80 46--- 47apiVersion: networking.k8s.io/v1 48kind: Ingress 49metadata: 50 name: hugo-ingress 51 namespace: hugo-site 52 labels: 53 app: hugo-ingress 54 annotations: 55 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; 56spec: 57 ingressClassName: avi-lb 58 rules: 59 - host: yikes.guzware.net 60 http: 61 paths: 62 - pathType: Prefix 63 path: / 64 backend: 65 service: 66 name: hugo-service 67 port: 68 number: 80 Now before my Deployment is ready to be applied I need to create the namespace I have defined in the yaml file above: kubectl create ns hugo-site.\nNow when that is done its time to apply my hugo deployment. kubectl apply -f hugo-deployment.yaml\nI want to check the state of the pods:\n1$ kubectl get pod -n hugo-site 2NAME READY STATUS RESTARTS AGE 3hugo-site-7f95b4644c-5gtld 1/1 Running 0 10s 4hugo-site-7f95b4644c-fnrh5 1/1 Running 0 10s 5hugo-site-7f95b4644c-hc4gw 1/1 Running 0 10s Ok, so far so good. What about my deployment:\n1$ kubectl get deployments.apps -n hugo-site 2NAME READY UP-TO-DATE AVAILABLE AGE 3hugo-site 3/3 3 3 35s Great news. Lets check the Service, Ingress and persistent volume claim.\nService:\n1$ kubectl get service -n hugo-site 2NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE 3hugo-service ClusterIP 10.99.25.113 \u0026lt;none\u0026gt; 80/TCP 46s Ingress:\n1$ kubectl get ingress -n hugo-site 2NAME CLASS HOSTS ADDRESS PORTS AGE 3hugo-ingress avi-lb yikes.guzware.net x.x.x.x 80 54s PVC:\n1NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE 2hugo-pv-claim Bound pvc-b2395264-4500-4d74-8a5c-8d79f9df8d63 10Gi RWO nfs-client 59s Well that looks promising. Will I be able to access my hugo page on yikes.guzware.net ... well yes, otherwise you wouldnt read this.. ü§£\nCreating and updating content A blog page without content is not so interesting. So just some quick comments on how I create content, and update them.\nI use Typora creating and editing my *.md files. While working with the post (such as now) I run hugo in \u0026quot;server-mode\u0026quot; whith this command: hugo server. If I run this command on of my linux virtual machines through SSH I want to reach the server from my laptop so I add the parameter --bind=ip-of-linux-vm and I can access the page from my laptop on the ip of the linux VM and port 1313. When I am done with the article/post for the day I generated the web-page with the command hugo -D -v. The updated content of my public folder after I have generated the page is mirrored to the NFS path that is used in my PVC shown above and my containers picks up the updated content instantly. Thats how I do, it works and I find it easy to maintain and operate. And, if one of my workers fails, I have more pods still available on the remaining workers. If a pod fails Kubernetes will just take care of that for me as I have declared a set of pods(replicas) that should run. If I run my Kubernetes environment in Tanzu and one of my workers fails, that will also be automatically taken care of.\n","link":"https://blog.andreasm.io/2022/10/12/hugo-in-kubernetes/","section":"post","tags":["hugo","static-content-generator","docker","kubernetes"],"title":"Hugo in Kubernetes"},{"body":"","link":"https://blog.andreasm.io/tags/static-content-generator/","section":"tags","tags":null,"title":"static-content-generator"},{"body":"","link":"https://blog.andreasm.io/tags/authentication/","section":"tags","tags":null,"title":"authentication"},{"body":"","link":"https://blog.andreasm.io/tags/pinniped/","section":"tags","tags":null,"title":"pinniped"},{"body":"","link":"https://blog.andreasm.io/categories/pinniped/","section":"categories","tags":null,"title":"Pinniped"},{"body":"How to use Pinniped as the authentication service in Kubernets with OpenLDAP\nGoal: Deploy an authentication service to handle RBAC in Kubernetes Purpose: User/access management in Kubernetes\nPinniped introduction ","link":"https://blog.andreasm.io/2022/10/11/pinniped-authentication-service/","section":"post","tags":["rbac","authentication","pinniped","kubernetes"],"title":"Pinniped Authentication Service"},{"body":"","link":"https://blog.andreasm.io/tags/rbac/","section":"tags","tags":null,"title":"rbac"},{"body":"","link":"https://blog.andreasm.io/categories/rbac/","section":"categories","tags":null,"title":"RBAC"},{"body":"This article will quickly go through how to create wildcard certificates and automatically renew them with Lets Encrypt and Cert-Manager\nCert-Manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates.\nIt can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI.\nIt will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry. link\nInstall Cert-Manager I prefer the Helm way so lets add the cert-manager helm chart:\n1helm repo add jetstack https://charts.jetstack.io 2helm repo update Then we need to deploy cert-manager. This can be done out-of-the-box with the commands given from the official docs (this also installed the necessary CRDs):\n1helm install \\ 2 cert-manager jetstack/cert-manager \\ 3 --namespace cert-manager \\ 4 --create-namespace \\ 5 --version v1.9.1 \\ 6 --set installCRDs=true Or if you need to customize some settings, as I needed to do, I used this command:\n1helm install -f /path/to/cert-manager.values.yaml cert-manager jetstack/cert-manager --namespace cert-manager --version v1.9.1 --set installCRDs=true --set \u0026#39;extraArgs={--dns01-recursive-nameservers-only,--dns01-recursive-nameservers=xx.xx.xx.xx:53\\,xx.xx.xx:53}\u0026#39; The above command takes care of the cert-manager installation including the necessary CRDs, but it will also adjust the DNS servers Cert-Manager will use to verify the ownership of my domain.\nDNS01 - Wildcard certificate In this post I will go with wildcard certificate creation. I find it easier to use instead of having a separate cert for everthing I do, as long as they are within the same subdomain. So if I have my services in *.example.com they can use the same certificate. But if I have services in *.int.example.com I can not use the same certificate as LetsEncrypt certificates dont support that. Then you need to create a separate wildcard cert for each subdomain. But Cert-manager will handle that for you very easy.\nThe offiicial Cert-Manager supported DNS01 providers are:\nACMEDNS Akamai AzureDNS CloudFlare Google Route53 DigitalOcean RFC2136 There is also an option to use Webhooks. I did try that as my previous DNS registrar were not on the DNS01 supported list. I did not succeed with using the webhook approach. It could be an issue with the specific webhooks I used or even with my registrar so I decided to migrate over to CloudFlare which is on the supported list, \u0026quot;out of the box\u0026quot;.\nIssuer - CloudFlare and LetsEncrypt The first we need to do is to create a secret for Cert-Manager to use when \u0026quot;interacting\u0026quot; with CloudFlare. I went with API Token. So head over to your CloudFlare control panel and create a token for Cert Manager like this: Here is the permissions: Now use the tokens to create your secret:\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: cloudflare-api-token-secret 5 namespace: cert-manager 6type: Opaque 7stringData: 8 api-token: Apply it kubect apply -f name.of.yaml\nNow create your issuer. LetsEncrypt have two repos, one called staging and one production. Start out with staging until everything works so you dont hit the LetsEncrypt limit. In regards to this I created two issuers, one for staging and one for production. When everything was working and I have verified the certificates etc I deployed the certs using the prod-issuer.\nIssuer-staging:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-staging 5spec: 6 acme: 7 # ACME Server 8 # prod : https://acme-v02.api.letsencrypt.org/directory 9 # staging : https://acme-staging-v02.api.letsencrypt.org/directory 10 server: https://acme-staging-v02.api.letsencrypt.org/directory 11 # ACME Email address 12 email: xxx.xxx@xxx.xxx 13 privateKeySecretRef: 14 name: letsencrypt-key-staging # staging or production 15 solvers: 16 - dns01: 17 cloudflare: 18 apiTokenSecretRef: 19 name: cloudflare-api-token-secret ## created and applied above 20 key: api-token Issuer-production:\n1apiVersion: cert-manager.io/v1 2kind: ClusterIssuer 3metadata: 4 name: letsencrypt-prod 5 namespace: cert-manager 6spec: 7 acme: 8 # ACME Server 9 # prod : https://acme-v02.api.letsencrypt.org/directory 10 # staging : https://acme-staging-v02.api.letsencrypt.org/directory 11 server: https://acme-v02.api.letsencrypt.org/directory 12 # ACME Email address 13 email: xxx.xxx@xxx.xxx 14 privateKeySecretRef: 15 name: letsencrypt-key-prod # staging or production 16 solvers: 17 - dns01: 18 cloudflare: 19 apiTokenSecretRef: 20 name: cloudflare-api-token-secret # created and applied above 21 key: api-token Request certificate Now that the groundwork for cert-manager has been setup, its time to \u0026quot;print\u0026quot; some certificates. Prepare your yamls for both the staging key and production key.\nWildcard-staging:\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: name-tls-test 5 namespace: namespace-you-want-the-cert-in 6spec: 7 secretName: name-tls-staging 8 issuerRef: 9 name: letsencrypt-staging 10 kind: ClusterIssuer 11 duration: 2160h # 90d 12 renewBefore: 720h # 30d before SSL will expire, renew it 13 dnsNames: 14 - \u0026#34;*.example.com\u0026#34; Wildcard-production:\n1apiVersion: cert-manager.io/v1 2kind: Certificate 3metadata: 4 name: name-tls-production 5 namespace: namespace-you-want-the-cert-in 6spec: 7 secretName: name-tls-prod 8 issuerRef: 9 name: letsencrypt-prod 10 kind: ClusterIssuer 11 duration: 2160h # 90d 12 renewBefore: 720h # 30d before SSL will expire, renew it 13 dnsNames: 14 - \u0026#34;*.example.com\u0026#34; Apply the staging request first. Check your certificate status with this command:\n1$ kubectl get certificate -n namespace-you-wrote 2NAME READY SECRET AGE 3name-tls-staging True name-tls-staging 8d Please note that it can take a couple of minutes before the certificate is ready. This applies for production also.\nIf everything went well, delete your staging certificate and apply your production certificate with the production yaml. Thats it. Now Cert-Manager will take care of updating your certificate for your, sit back and enjoy your applications with your always up to date certificates.\nTroubleshooting tips, commands If something should fail there is a couple of commands you can use to figure out whats going on.\n1$ kubectl get issuer 2$ kubectl get clusterissuer 3$ kubectl describe issuer 4$ kubectl describe clusterissuer 5$ kubectl describe certificaterequest 6$ kubectl describe order 7$ kubectl get challenges 8$ kubectl describe challenges For more detailed explanation go here\n","link":"https://blog.andreasm.io/2022/10/11/cert-manager-and-letsencrypt/","section":"post","tags":["cert-manager","certificate","kubernetes"],"title":"Cert Manager and Letsencrypt"},{"body":"","link":"https://blog.andreasm.io/tags/cert-manager/","section":"tags","tags":null,"title":"cert-manager"},{"body":"","link":"https://blog.andreasm.io/tags/certificate/","section":"tags","tags":null,"title":"certificate"},{"body":"","link":"https://blog.andreasm.io/categories/certificate/","section":"categories","tags":null,"title":"Certificate"},{"body":"","link":"https://blog.andreasm.io/tags/network-policies/","section":"tags","tags":null,"title":"network-policies"},{"body":"What is the NSX Antrea integration Assumptions: I assume that the reader is aware of what Antrea is, and what a CNI is and also what NSX is. If not head over here to read more on Antrea and here to read more on NSX.\nFor many years VMware NSX has help many customer secure their workload by using the NSX Distributed Firewall. As NSX has evolved over the years the different platform it supports has also broadened, from virtual machines, bare metal server, cloud workload and kubernetes pods. NSX has had support for security policies in Kubernetes for a long time also with the CNI NCP Read about NCP here recently (almost a year ago since I wrote this article, so not so recent in the world of IT) it also got support for using the Antrea CNI. What does that mean then. Well, it mean we can now \u0026quot;add\u0026quot; our Antrea enabled clusters to NSX. With that Antrea will report their inventory, such as nodes, pods, services, ip addresses, k8s labels into the NSX manager. Another important feature is that we can also create and publish security policies from the NSX manager to the Antrea enabled clusters. Antrea is supported in almost all kinds of Kubernetes platforms, our own Tanzu solutions, upstream k8s, ARM, public cloud etc so it is very flexible. And with the rich information NSX gets from Antrea we can create more clever security policies by using the native kubernetes labels to form security group membership based on these labels. Will get into more of that later. Now lets move on to the components involved.\nAntrea NSX Adapter To understand a bit more how this works, we need to go through a couple of components that is in involved to get this integration in place.\nsdfsdfsdf\nsdf\nsdf\nsdf\nsdf\n","link":"https://blog.andreasm.io/2022/10/11/nsx-antrea-integration/","section":"post","tags":["security","network-policies","antrea","nsx"],"title":"NSX Antrea Integration"},{"body":"","link":"https://blog.andreasm.io/tags/security/","section":"tags","tags":null,"title":"security"},{"body":"This week I was fortunate to get hold of a VMC on AWS environment and wanted to test out the possibility of managing my K8s security policies from my on-prem NSX manager by utilizing the integration of Antrea in NSX. I haven't covered that specific integration part in a blog yet, but in short: by using Antrea as your CNI and you are running NSX-T 3.2 you can manage all your K8s policies from the NSX manager GUI. Thats a big thing. Manage your k8s policies from the same place where you manage all your other critical security policies. Your K8s clusters does not have to be in the same datacenter as your NSX manager. You can utilize VMC on AWS as your scale-out, prod/test/dev platform and still manage your K8s security policies centrally from the same NSX manager.\nIn this post I will go through how this is done and how it works.\nVMC on AWS VMC on AWS comes with NSX, but it is not yet on the version that has the NSX-T integration. So what I wanted to do was to use the VMC NSX manager to cover all the vm-level microsegmentation and let my on-prem NSX manager handle the Antrea security policies. To illustrate want I want to achieve:\nVMC on AWS to on-prem connectivity VMC on AWS supports a variety of connectivity options to your on-prem environment. I have gone with IPSec VPN. Where I configure IPsec on the VMC NSX manager to negotiate with my on-prem firewall to terminate the VPN connection. In VMC I have two networks: Management and Workload. I configured both subnets in my IPsec config as I wanted the flexibility to reach both subnets from my on-prem environment. To get the integration working I had to make sure that the subnet on my on-prem NSX manager resided on also was configured. So the IPsec configurations were done accordingly to support that: Two subnets from VMC and one from on-prem (where my NSX managers resides).\nIPsec config from my VMC NSX manager\nIPsec config from my on-prem firewall\nWhen IPsec tunnel was up I logged on to the VMC NSX manager and configured the \u0026quot;North/South\u0026quot; security policies allowing my Workload segment to any. I created a NSX Security Group (\u0026quot;VMs\u0026quot;) with membership criteria in place to grab my Workload Segment VMs (workload). This was just to make it convenient for myself during the test. We can of course (and should) be more granular in making these policies. But we also have the NSX Distributed Firewall which I will come to later.\nNorth/South policies\nNow I had the necessary connectivity and security policies in place for me to log on to the VMC vCenter from my on-prem management jumpbox and deploy my k8s worker nodes.\nVMC on AWS K8s worker nodes In VMC vCenter I deployed three Ubuntu worker nodes, and configured them to be one master worker and two worker nodes by following my previous blog post covering these steps:\nhttp://yikes.guzware.net/2020/10/08/ako-with-antrea-on-native-k8s-cluster/#Deploy_Kubernetes_on_Ubuntu_2004\nThree freshly deployed VMs in VMC to form my k8s cluster\n1NAME STATUS ROLES AGE VERSION 2vmc-k8s-master-01 Ready control-plane,master 2d22h v1.21.8 3vmc-k8s-worker-01 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 4vmc-k8s-worker-02 Ready \u0026lt;none\u0026gt; 2d22h v1.21.8 After the cluster was up I needed to install Antrea as my CNI.\nDownload the Antrea release from here: https://customerconnect.vmware.com/downloads/info/slug/networking_security/vmware_antrea/1_0\nAfter it has been downloaded, unpack it and upload the image to your k8s master and worker nodes by issuing the docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz\nThen apply it by using the manifest antrea-advanced-v1.2.3+vmware.3.yml found under the /antrea-advanced-1.2.3+vmware.3.19009828/manifests folder. Like this: kubectl apply -f antrea-advanced-v1.2.3+vmware.3.yml and Antrea should spin right up and you have a fully working K8s cluster:\n1NAME READY STATUS RESTARTS AGE 2antrea-agent-2pdcr 2/2 Running 0 2d22h 3antrea-agent-6glpz 2/2 Running 0 2d22h 4antrea-agent-8zzc4 2/2 Running 0 2d22h 5antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d20h 6coredns-558bd4d5db-2j7jf 1/1 Running 0 2d22h 7coredns-558bd4d5db-kd2db 1/1 Running 0 2d22h 8etcd-vmc-k8s-master-01 1/1 Running 0 2d22h 9kube-apiserver-vmc-k8s-master-01 1/1 Running 0 2d22h 10kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 2d22h 11kube-proxy-rvzs6 1/1 Running 0 2d22h 12kube-proxy-tnkxv 1/1 Running 0 2d22h 13kube-proxy-xv77f 1/1 Running 0 2d22h 14kube-scheduler-vmc-k8s-master-01 1/1 Running 0 2d22h Antrea NSX integration Next up is to configure the Antrea NSX integration. This is done by following this guide:\nhttps://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-DFD8033B-22E2-4D7A-BD58-F68814ECDEB1.html\nIts very well described and easy to follow. So instead of me rewriting it here I just point to it. But in general it makes up of a couple of steps needed.\n1. Download the necessary Antrea Interworking parts, which is included in the Antrea-Advanced zip above\n2. Create a certificate to use for the Principal ID User in your on-prem NSX manager.\n3. Import image to your master and workers (interworking-debian-0.2.0.tar)\n4. Edit the bootstrap-config.yaml (https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.2/administration/GUID-1AC65601-8B35-442D-8613-D3C49F37D1CC.html)\n5. Apply the bootstrap-config-yaml and you should end up with this result in your k8s cluster and in your on-prem NSX manager:\n1vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 17 2d22h My VMC k8s cluster: vmc-ant-cluster (in addition to my other on-prem k8s-cluster)\nInventory view from my on-prem NSX manager\nOne can immediately see useful information in the on-prem NSX manager about the \u0026quot;remote\u0026quot; VMC K8s cluster such as Nodes, Pods and Services in the Inventory view. If I click on the respective numbers I can dive into more useful information. By clicking on \u0026quot;Pods\u0026quot;\nPod state, ips, nodes they are residing on etc\nEven in this view I can click on the labels links to get the lables NSX gets from kubernetes and Antrea:\nBy clicking on Services I get all the services running in my VMC k8s cluster\nAnd the service labels:\nAll this information is very useful, as they can be used to create the security groups in NSX and use those groups in security policies.\nClicking on the Nodes I also get very useful information:\nVMC on AWS NSX distributed firewall As I mentioned earlier VMC on AWS also comes with NSX and one should utilize this to segment/create security polices on your worker nodes there. I have just created some simple rules allowing the \u0026quot;basic\u0026quot; needs for my workers, and then created some specific rules for what they are allowed to where all unspecified traffic is blocked by a default block rule.\nBare in mind that this is a demo environment and not representing any production environment as such, but some rules are in place to showcase that I am utilizing the NSX distributed firewall in VMC to microsegment my workload there.\nThe \u0026quot;basic\u0026quot; needs rules are the following: Under \u0026quot;Infrastructure\u0026quot; I am allowing my master and worker nodes to \u0026quot;consume\u0026quot; NTP and DNS. In this environment I do not have any local DNS and NTP servers as they are all public. DNS I am using Google's public DNS servers 8.8.8.8 and 8.8.4.4 and NTP the workers are using \u0026quot;time.ubuntu.com\u0026quot;. I have created a security group consisting of the known DNS servers ip, as I know what they are. But the NTP server's IP I do not know so I have created a security group with members only consisting of RFC1918 subnets and created a negated policy indicating that they are only allowed to reach NTP servers if they not reside on any RFC1918 subnet.\nNTP and DNS allow rules under Infrastructure\nUnder \u0026quot;Environment\u0026quot; I have created a Jump to Application policy that matches my K8s master/worker nodes\nJump to Application\nUnder \u0026quot;Application\u0026quot; I have a rule that is allowing internet access (could be done on the North/South Gateway Firewall section also) by indication that HTTP/HTTPS is allowed as long as it is not any RFC1918 subnet.\nAllow \u0026quot;internet access\u0026quot;\nFurther under Application I am specifying a bit more granular rules for what the k8s cluster is allowed in/out. Again, this is just some simple rules restricting the k8s cluster to not allow any-any by utilizing the NSX DFW already in VMC on AWS. One can and should be more granular, but its to give you an idea.\nIn the K8s-Backbone security policy section below I am allowing HTTP in to the k8s cluster as I am planning to run an k8s application there that uses HTTP where I allow a specific IP subnet/range as source and my loadbalancer IP range as the destination.\nThen I allow SSH to the master/workers for management purposes. Then I am creating some specific rules allowing the necessary ports needed for the Antrea control-plane to communicate with my on-prem NSX manager, which are: TCP 443, 1234 and 1235. Then I create an \u0026quot;Intra\u0026quot; rule allowing the master/workers to talk freely between each other. This should and can also be much more tightened down. When those rules are done processed they will hit a default block rule.\nVMC k8s cluster policy\nDefault drop\nAntrea policies from on-prem NSX manager Now when the \u0026quot;backbone\u0026quot; is ready configured and deployed its time to spin up some applications in my \u0026quot;VMC K8s cluster\u0026quot; and apply some Antrea security policies. Its now time for the magic to begin ;-)\nIn my on-prem environment I already have a couple of Antrea enabled K8s clusters running. On them a couple of demo applications are already running and protected by Antrea security policies created from my on-prem NSX manager. I like to use an application called Yelb (which I have used in previous blog posts here). This application consist of 4 pods. All pods doing their separate thing for the application to work. I have a frontend pod which is hosting the web-page for the application, I have an application pod, db pod and a cache pod. The necessary connectivity between looks like this:\nYelb pods connectivity\nTo make security policy creation easy I make use of all the information I get from Antrea and Kubernetes in form of \u0026quot;Labels\u0026quot;. These labels are translated into tags in NSX. Which makes it very easy to use, and for \u0026quot;non\u0026quot; developers to use as \u0026quot;human-readable\u0026quot; elements instead of IP adresses, pods unique names etc. In this example I want to microsegment the pods that makes up the application \u0026quot;Yelb\u0026quot;.\nCreating NSX Security Groups for Antrea Security Policy Before I create the actual Antrea Security Policies I will create a couple of security groups based on the tags I use in K8s for the application Yelb. The Yelb manifest looks like this:\n1apiVersion: v1 2kind: Service 3metadata: 4 name: redis-server 5 labels: 6 app: redis-server 7 tier: cache 8 namespace: yelb 9spec: 10 type: ClusterIP 11 ports: 12 - port: 6379 13 selector: 14 app: redis-server 15 tier: cache 16--- 17apiVersion: v1 18kind: Service 19metadata: 20 name: yelb-db 21 labels: 22 app: yelb-db 23 tier: backenddb 24 namespace: yelb 25spec: 26 type: ClusterIP 27 ports: 28 - port: 5432 29 selector: 30 app: yelb-db 31 tier: backenddb 32--- 33apiVersion: v1 34kind: Service 35metadata: 36 name: yelb-appserver 37 labels: 38 app: yelb-appserver 39 tier: middletier 40 namespace: yelb 41spec: 42 type: ClusterIP 43 ports: 44 - port: 4567 45 selector: 46 app: yelb-appserver 47 tier: middletier 48--- 49apiVersion: v1 50kind: Service 51metadata: 52 name: yelb-ui 53 labels: 54 app: yelb-ui 55 tier: frontend 56 namespace: yelb 57spec: 58 type: LoadBalancer 59 ports: 60 - port: 80 61 protocol: TCP 62 targetPort: 80 63 selector: 64 app: yelb-ui 65 tier: frontend 66--- 67apiVersion: v1 68kind: ReplicationController 69metadata: 70 name: yelb-ui 71 namespace: yelb 72spec: 73 replicas: 1 74 template: 75 metadata: 76 labels: 77 app: yelb-ui 78 tier: frontend 79 spec: 80 containers: 81 - name: yelb-ui 82 image: mreferre/yelb-ui:0.3 83 ports: 84 - containerPort: 80 85--- 86apiVersion: apps/v1 87kind: Deployment 88metadata: 89 name: redis-server 90 namespace: yelb 91spec: 92 selector: 93 matchLabels: 94 app: redis-server 95 replicas: 1 96 template: 97 metadata: 98 labels: 99 app: redis-server 100 tier: cache 101 spec: 102 containers: 103 - name: redis-server 104 image: redis:4.0.2 105 ports: 106 - containerPort: 6379 107--- 108apiVersion: apps/v1 109kind: Deployment 110metadata: 111 name: yelb-db 112 namespace: yelb 113spec: 114 selector: 115 matchLabels: 116 app: yelb-db 117 replicas: 1 118 template: 119 metadata: 120 labels: 121 app: yelb-db 122 tier: backenddb 123 spec: 124 containers: 125 - name: yelb-db 126 image: mreferre/yelb-db:0.3 127 ports: 128 - containerPort: 5432 129--- 130apiVersion: apps/v1 131kind: Deployment 132metadata: 133 name: yelb-appserver 134 namespace: yelb 135spec: 136 selector: 137 matchLabels: 138 app: yelb-appserver 139 replicas: 1 140 template: 141 metadata: 142 labels: 143 app: yelb-appserver 144 tier: middletier 145 spec: 146 containers: 147 - name: yelb-appserver 148 image: mreferre/yelb-appserver:0.3 149 ports: 150 - containerPort: 4567 As we can see there is a couple of labels that distinguish the different components in the application which I can map to the application topology above. I only want to allow the frontend to talk to the \u0026quot;app-server\u0026quot;, and the app-server to the \u0026quot;db-server\u0026quot; and \u0026quot;cache-server\u0026quot;. And only on the needed ports. All else should be dropped. On my on-prem NSX manager I have created these groups for the already running on-prem Yelb application. I have created four 5 groups for the Yelb application in total. One group for the frontend (\u0026quot;ui-server\u0026quot;), one for the middletier (app server), one for the backend-db (\u0026quot;db-server\u0026quot;), one for the cache-tier (\u0026quot;cache server\u0026quot;) and one last for all the pods in this application:\nSecurity groups filtering out the Yelb pods\nThe membership criteria inside those groups are made up like this, where I am using the labels in my Yelb manifest (these labels are autopopulated so you dont have to guess). Tag equals label frontend and scope equals label dis:k8s:tier:\nGroup definition for the frontend\nThe same goes for the other groups just using their respective labels. The members should then look like this:\nOnly the frontend pod\nThen I have created a security group that selects all pods in my namespace Yelb by using the label Yelb like this:\nWhich then selects all my pods in the namespace Yelb:\nNow I have my security groups and can go on and create my security policies.\nAntrea security policies from NSX manager Head over to Security, Distributed Firewall section in the on-prem NSX manager to start creating security policies based on your security groups. These are the rules I have created for my application Yelb:\nAntrea Security Policies from the NSX manager\nFirst rule allows traffic from my Avi SE's that are being used to create the service loadbalancer for my application Yelb to the Yelb frontend on HTTP only. Notice that the source part here is in the \u0026quot;Applied to field\u0026quot; (goes for all rules in this example). Thee second rule allows traffic from the frontend to the middletier (\u0026quot;app-server\u0026quot;) on port 4567 only. The third rule allows traffic from middletier to backend-db (\u0026quot;db-server\u0026quot; on port 5432 only. The fourth rule allows traffic from middletier to cache (redis cache) on port 6379 only. All rules according to the topology maps above. Then the last rule is where I am using the namespace selection to select all pods in the namespace Yelb to drop all else not specified above.\nTo verify this I can use the Traceflow feature in Antrea from the NSX manager like this:\n(Head over to Plan \u0026amp; Troubleshoot, Traffic Analysis, Traceflow in your NSX manager)\nChoose Antrea Traceflow, choose the Antrea cluster where your application resides, then select TCP under Protocol type, type in Destination Port (4567) and choose where your pods are from the source and destination. In the screenshot above I want to verify that the needed ports are allowed between Frontend and middletier (application pod).\nClick trace:\nWell that worked, now if I change to port to something else like 4568, am I then still allowed to do that?\nNo, I am not. That is because I have my drop rule in place remember:\nI could go on and test all pod to pod connectivity (I have), but you can trust me their are doing their job. Just to save some screenshots. So that is it, I have microsegmented my Yelb application. But what if I want to scale out this application to my VMC environment. I want to achieve the same thing there. Why not, all our groundwork has already been done so lets head out and spin up the same applicaion on our VMC K8s cluster. Whats going to happen in my on-prem NSX manager. This is cool!\nAntrea security policies in my VMC k8s cluster managed by my on-prem NSX manager Before I deploy my Yelb application in my VMC K8s cluster I want to refresh the memory by showing what my NSX manager knows about the VMC K8s cluster inventory. Lets take a look again. Head over to Inventory in my on-prem NSX manager and take a look at my VMC-ANT-CLUSTER:\n3 nodes you say, and 18 pods you say... Are any of them my Yelb pods?\nNo Yelb pods here...\nNo, there are no yelb pods here. Lets make that a reality. Nothing reported in k8s either:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h\nSpin up the Yelb application in my VMC k8s cluster by using the same manifest:\nkubectl apply -f yelb-lb.yaml\nThe result in my k8s cluster:\nandreasm@vmc-k8s-master-01:~/pods$ kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system antrea-agent-2pdcr 2/2 Running 0 3d kube-system antrea-agent-6glpz 2/2 Running 0 3d kube-system antrea-agent-8zzc4 2/2 Running 0 3d kube-system antrea-controller-7bdcb9d657-ntwxm 1/1 Running 0 2d22h kube-system coredns-558bd4d5db-2j7jf 1/1 Running 0 3d kube-system coredns-558bd4d5db-kd2db 1/1 Running 0 3d kube-system etcd-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-apiserver-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-controller-manager-vmc-k8s-master-01 1/1 Running 0 3d kube-system kube-proxy-rvzs6 1/1 Running 0 3d kube-system kube-proxy-tnkxv 1/1 Running 0 3d kube-system kube-proxy-xv77f 1/1 Running 0 3d kube-system kube-scheduler-vmc-k8s-master-01 1/1 Running 0 3d metallb-system controller-7dcc8764f4-6n49s 1/1 Running 0 2d23h metallb-system speaker-58s5v 1/1 Running 0 2d23h metallb-system speaker-7tnhr 1/1 Running 0 2d23h metallb-system speaker-lcq4n 1/1 Running 0 2d23h vmware-system-antrea interworking-7889dc5494-clk97 4/4 Running 18 2d23h yelb redis-server-74556bbcb7-fk85b 1/1 Running 0 4s yelb yelb-appserver-6b6dbbddc9-9nkd4 1/1 Running 0 4s yelb yelb-db-5444d69cd8-dcfcp 1/1 Running 0 4s yelb yelb-ui-f74hn 1/1 Running 0 4s\nWhat does my on-prem NSX manager reports?\nHmm 22 pods\nAnd a lot of yelb pods\nInstantly my NSX manager shows them in my inventory that they are up.\nNow, are they being protected by any Antrea security policies? Lets us do the same test as above by using Antrea Traceflow from the on-prem NSX manager with same ports as above (frontend to app 4567 and 4568).\nTraceflow from my on-prem NSX manager in the VMC k8s cluster\nNotice the selection I have done, everything is the VMC k8s cluster.\nNeed port is allowed\nThat is allowed, what about 4568 (which is not needed):\nAlso allowed\nThat is also allowed. I cant have that. How can I make use of my already created policy for this application as easy as possible instead of creating all the rules all over?\nWhell, lets test that. Head over to Security, Distributed firewall in my on-prem NSX manager.\nNotice the Applied to field here:\nWhat happens if I click on it?\nIt only shows me my local Antrea k8s cluster. That is also visible if I list the members in the groups being used in the rules:\nOne pod from my local k8s cluster\nWhat if I add the VMC Antrea cluster?\nCick on the pencil and select your remote VMC K8s cluster:\nApply and Publish:\nNow lets have a look inside our groups being used by our security policy:\nMore pods\nThe Yelb namespace group:\nInstantly my security groups are being updated with more members!!!\nRemember how I created the membership criteria of the groups? Labels from my manifest, and labels for the namespace? Antrea is cluster aware, and dont have to specify a specific namespace to select labels from one specific namespace, it can select from all namespaces as long as the label matches. This is really cool.\nNow what about my security policies in my VMC k8s cluster? Is it enforcing anything?\nLets check, doing a traceflow again. Now only on a disallowed port 4568:\nResult:\nDropped\nThe Security policy is in place, enforcing what it is told to do. The only thing I did in my local NSX manager was to add the VMC Antrea cluster in my Security Applied to section\n","link":"https://blog.andreasm.io/2022/03/13/managing-your-antrea-k8s-clusters-running-in-vmc-from-your-on-prem-nsx-manager/","section":"post","tags":["antrea","nsx","vmconaws","security"],"title":"Managing your Antrea K8s clusters running in VMC from your on-prem NSX Manager"},{"body":"","link":"https://blog.andreasm.io/categories/security/","section":"categories","tags":null,"title":"Security"},{"body":"","link":"https://blog.andreasm.io/tags/vmconaws/","section":"tags","tags":null,"title":"vmconaws"},{"body":"","link":"https://blog.andreasm.io/categories/vmware-cloud/","section":"categories","tags":null,"title":"VMware-Cloud"},{"body":"","link":"https://blog.andreasm.io/tags/nsx-application-platform/","section":"tags","tags":null,"title":"nsx-application-platform"},{"body":"VMware NSX 3.2 is out and packed with new features. One of them is the NSX Application Platform which runs on Kubernetes to provide the NSX ATP (Advanced Threat Protection) functionality such as NSX Intelligence (covered in a previous post), NSX Network Detection and Response (NDR) and NSX Malware. This post will go through how to spin up a K8s cluster for this specific scenario covering the pre-reqs from start to finish. After that the features itself will be covered in separate posts. Through this post NSX Application Platform will be abbreviated into NAPP.\nGetting started To get started with NAPP its important that one has read the prerequisites needed to be in place on the K8s cluster that is hosting NAPP. In short NAPP is currently validated to run on upstream K8s version 1.7 all the way up to version 1.21, VMware Tanzu (TKC) versions 1.17.17 to 1.21.2. In addtion to a K8s cluster itself NAPP also needs a Registry supporting images/helm charts. In this walkthrough Harbor will be used. I will also go with an upstream K8s cluster version 1.21.8 running on Ubuntu nodes.\nSources being used to cover this post is mainly from VMware's official documentation. So I list them here as an easy way to reference as they are providing the necessary and important information on how, requirements and all the steps detailed to get NAPP up and running. The purpose of this post is to just go through the steps as a more step by step guide. If more information is needed, head over to our official documentation. Below are the links for the software/tools I have used in this post:\nDeploying and Managing the VMware NSX Application Platform\nHarbor registry\nVMware vSphere Container Storage Plugin\nMetalLB¬†LetsEncrypt\nPrerequisites/Context First some context. In this post all K8s nodes are running as virtual machines on VMware vSphere. NSX-T is responsible for the underlaying network connectivity for the nodes and the persistent storage volumes in my K8s cluster is VMFS exposed through the vSphere Container Storage Plugin (CSI). The NSX manager cluster consists of 3 NSX managers and a cluster VIP address.\nThe vSphere environment consists of two ESXi hosts with shared storage from a FC SAN and vCenter managing the ESXi hosts.\nA quick summary of what is needed, tools/software and what I have used in my setup:\nA upstream kubernetes cluster running any of the supported versions stated in the official documentation. I am using 1.21.8 A working CNI, I am using Antrea. A registry supporting images/helm charts using a signed trusted certificate (no support for self-signed certificates). I am using Harbor and LetsEncrypt certificates. A load balancer to expose the NAPP endpoint with a static ip. I am using MetalLB Persistent storage in K8s I am using the vSphere Container Storage Plugin (if you are running the nodes on vSphere). NSX 3.2 of course Required resources for the NAPP form factor you want to go with. The required resources for the K8s cluster supporting NAPP is outlined below (from the official docs page): This post will cover the Advanced form factor where I went with the following node configuration:\n1 master worker/control plane node with 4 vCPUs, 8GB RAM and 200GB local disk (ephemeral storage). For the worker nodes: 3 worker nodes with 16vCPUs, 64GB RAM and 200GB (ephemeral storage) for the persistent storage (the 1TB disk requirement) I went with vSphere CSI to expose a VMFS datastore from vSphere for the persistent volume requirement.\nThe next chapters will go trough the installation of Harbor (registry) and the configuration done there, then the K8s configuration specifically the CSI and MetalLB part as I am following the generic K8s installation covered earlier here: Deploy Kubernetes on Ubuntu 20.04 .\nLets get to it.\nHarbor (registry requirement) Harbor can be run as a pod in Kubernetes or a pod in Docker. I went with the Docker approach and spun up a VM for this sole purpose.\nThe VM is Ubuntu 20.4 with 4vCPU, 8GB RAM and 200GB disk.\nAfter Ubuntu is installed I installed the necessary dependencies to deploy Harbor in Docker by following the Harbor docs here: Harbor Getting Started\nI find it better to just link the steps below instead of copy/paste too much as the steps are very well documented and could also change over time.\nDocker Engine (latest Stable): docker.com/ubuntu Docker Compose (latest Stable): docker.com/linux Prepare the signed cert (if its not already done) NB! The certificate needs to contain the full chain otherwise the Docker client will not accept it. Download the Harbor installer: Harbor installer (I went with 2.4.1) Extract the online installer: tar -zxvf harbor-online-installer-v2.4.1.tgz Edit the harbor.yaml: Go to the folder harbor (result of the extract above) cp the harbor.yml.tmpl to harbor.yml and use your favourite editor and change the following (snippet from the harbor.yml file): Run the installer: sudo ./install.sh --with-chartmuseum The --with-chartmuseum flag is important (The installer is in the same folder as above and if it is not executable make it executable with chmod +x install.sh check/validate whether you are able to log in to your Harbor registry with the following command: sudo docker login FQDNofHarbor --username admin --password (password defined in harbor.yml). It should not complain about the certificate if the certificate is valid. Log in to the Harbor UI by opening a browser and enter your FQDN of your harbor with the use of admin/password. Create a project: I made a public project\nDownload the NAPP images from your my.vmware.com page: VMware-NSX-Application-Platform-3.2.0.0.0.19067744.tgz upload it to your Harbor VM or to another endpoint where you have a Docker client. \u0026quot;Untar\u0026quot; the tgz file Find and edit the upload_artifacts_to_private_harbor.sh by changing the following: DOCKER_REPO=harborFQDN/napp (after the / is the project name you created) DOCKER_USERNAME=admin DOCKER_PASSWORD=Password-Defined-In-Harbor.yml\nSave and run it sudo ./upload_artifacts_to_private_harbor.sh (same here make it executable with chmod +x if its not executable. This takes a long time, so sit back and enjoy or go do something useful like taking a 3km run in about 15 minutes. The end result shoul look something like this in the Harbor GUI: Thats it for Harbor, next up the K8s cluster\nThe K8s cluster where NAPP is deployed As stated in the official documentation for NAPP, K8s needs to be a specific version, it can be upstream K8s, VMware Tanzu (TKC) or other K8s managed platforms such as OpenShift. But the currently validated platforms are at the moment the above two mentioned platforms: upstream K8s and TKC.\nAs I wrote initially I will go with upstream K8s for this. To get this going I prepared 4 VMs where I dedicate one master worker/control-plane node with the above given specifications and 3 worker nodes with the above given specifications. I follow my previous guide for preparing the Ubuntu os and installing K8s here: K8s on Ubuntu so I will not cover this here but just continue from this with the specifics I did to get the CSI driver up, Antrea CNI and MetalLB. First out is the Antrea CNI.\nAssuming the K8s cluster is partially up, due to no CNI is installed. I download the downstream version of Antrea (one can also use the upstream version from the Antrea github repository) from my.vmware.com. One of the reason I want to use the downstream version from my.vmware.com version is that I want to integrate it to my NSX management plane (more on that in an separate post covering NSX-T with Antrea integration). Download the VMware Container Networking with Antrea (Advanced) from your my.vmware.page to your master worker. Unzip the zip file. Copy the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz to all your worker nodes (with scp for example). Found under the folder antrea-advanced-1.2.3+vmware.3.19009828/images (result of the extract previously). Load the antrea-advanced-debian-v1.2.3_vmware.3.tar.gz image on all nodes, including the master worker, with the command sudo docker load -i antrea-advanced-debian-v1.2.3_vmware.3.tar.gz Apply the antrea-advanced-v1.2.3+vmware.3.yml found under the folder antrea-advanced-1.2.3+vmware.3.19009828/manifests from your master worker. A second or two later you should have a fully working Antrea CNI in your K8s cluster. Notice that your CoreDNS pods decided to go into a running state. Thats it for the Antrea CNI. Next up is MetalLB When the CNI is up, its time for MetalLB. Installation of MetalLB is easy and well explained here: Install MetalLB. Next is the CSI for persistent volume. For step by step config of vSphere Container Storage Plugin head over the the following link (Getting Started with VMware vSphere Container Storage Plug-in section) and follow the instructions there which are very well described. That is, if you are running the VMs on vSphere and want to utilize VMFS as the underlying storage for your persistent volumes. Works great and is fairly easy to deploy. I might come back later and write up a short summary on this one. When you are done with the step above and have your Storage Class defined its over to NSX for deployment of NAPP - Yes! Deploy NAPP - NSX Application Platform Its finally time to head over to the NSX manager and start deployment of NAPP\nTo get this show started, I again must refer to the prerequisites page for NAPP. I will paste below and make some comments:\nFirst requirement: NSX version 3.2 (First release with NAPP). Pr now 3.2 is the one and only NSX-T release that supports NAPP. License ---------- Certificate: The first statement with CA-signed certificates is ok to follow. But the second one could be something that needs to be checked. This is valid if you are using NSX-T Self-Signed certificates. Image that you started out with one NSX manager, enabled the VIP cluster address, then it may well be that this cluster IP gets the certificate of your first NSX manager. So its important to verify that all three NSX managers are using their own unique certificate and the VIP uses its own unique certificate. If not, one must update the certificates accordingly. In my environment I had unique certificates on all NSX manager nodes, but my VIP was using NSX manager 1's certificate. So I had to update the certificate on the VIP. I generated a new certificate from the NSX manager here: And followed the instructions here to replace the VIP certificate: Replace Certificates The currently validated platforms NAPP is supported to run on, been through that earlier. Harbor is covered previously with a dedicated section. Harbor is not a strict requirement though. One can BYO registry if it supports image/helm charts When using upstream K8s, the config does not have a default token expiry. If using TKC one must generate a long-lived token so NAPP wont log out from the K8s cluster. Described in the docs Service Name FQDN is a dns record that is mapped to the IP the the endpoint service gets when deployed. Thats were I use MetalLB for this purpose. Just to initiate a type LoadBalancer. If there is a firewall between your NSX manager and the NAPP K8s cluster, one must do firewall openings accordingly. Time sync is important here also. The K8s cluster must be synced to the NSX Manager. Going through the deployment of NAPP from the NSX manager GUI Log in to the NSX manager GUI. Head over to System and find the new section on the left side called: NSX Application Platform\nFrom there the first thing thats needed to populate is the urls to your Harbor registry (or other BYO registry). The urls goes like this:\nHelm Repository: https://harbor.guzware.net/chartrepo/napp -\u0026gt; FQDN for the Harbor instance, then chartrepo and then the name of the projecy you created in Harbor Docker Registry: harbor.guzware.net/napp/clustering without HTTPS, almost the same url just swap places on napp (project in Harbor) and clustering Click save url and it should validate ok and present you with this and the option to continue in the bottom right corner: Next is the Form factor and kubeconfig:\nThe first thing is to upload your K8s kubeconfig file. Select upload and it will validate. Should you get a warning the the K8s version is newer than the kubectl client onboard the NSX manager upload a newer client from my.vmware.com The Cluster Type is only Standard for now. Storage Class is what you defined in K8s with the CSI (Persistent Volumes) Service Name (FQDN) registered in DNS Form Factor - Here you will have three choices: Standard, Advanced and Evaluation: I have gone with the Advanced form factor. The result should look like this: Now you should be able to click next to do the Precheck Platform for validation:\nFinally Review \u0026amp; Update\nInstallation starts\nAnd after some eager waiting the end result should look like this:\nA brief summary from the K8s cluster:\nA bunch of pods - nice!\nThats it - next time I will continue with the features NAPP brings to the table: NSX Intelligence, Network Detection and Response and NSX Malware Prevention\n","link":"https://blog.andreasm.io/2022/01/18/vmware-nsx-application-platform/","section":"post","tags":["nsx","nsx-application-platform","security"],"title":"VMware NSX Application Platform"},{"body":"","link":"https://blog.andreasm.io/tags/ids/","section":"tags","tags":null,"title":"ids"},{"body":"","link":"https://blog.andreasm.io/tags/ips/","section":"tags","tags":null,"title":"ips"},{"body":"This page will explain my lab environment, which is used in all the examples, tutorials in this blog.\nLab overview/connectivity - physical, logical and hybrid It is nice to have an overview of how the underlying hardware looks like and when reading my different articles. So I decided to create some diagrams to illustrate this. Which hopefully will help understanding my blog posts further. First out is the physical components (which is relevant for the posts in this blog).\nPhysical hardware My lab consist of two ESXi hosts, one ToR switch (enterprise dc switch with many capabilities) and a fibrechannel SAN (storage).\nLogical overview To make possible all the things I want to do in my lab I am running most of the networking and other features virtually on top of my physical hardware. This includes NSX-T, virtual routers (VM based) in additition to the router functionality in NSX-T and VM based \u0026quot;perimeter\u0026quot; firewall which is based on PfSense. Below is an logical overview of the network topology in my lab.\n","link":"https://blog.andreasm.io/2021/10/19/my-lab/","section":"post","tags":null,"title":"My LAB"},{"body":"","link":"https://blog.andreasm.io/categories/netowkring/","section":"categories","tags":null,"title":"Netowkring"},{"body":"This post will go through the IDS/IPS built-in feature of the NSX distributed firewall.\nAbbreviations used in this article:\nIDS = Intrusion Detection System IPS = Intrusion Prevention System Introduction to VMware NSX distributed IDS \u0026amp; IPS Before we dive into how to configure and use the distributed IDS and IPS feature in NSX let me just go through the basics where I compare the traditional approach with IDS/IPS and the NSX distributed IDS/IPS. This article is a continuation on the article Microsegmentation with VMware NSX\u0026quot; where I talk about east/west and north/south traffic pattern and being in context with the workload its supposed to protect. Where being in context is a key thing, especially when it comes to security policies and IDS/IPS. Know what you are protecting, make the inspection as relevant as possible, inspection done optimal (reduce false positives, maintain performance) and at the right place.\nThe traditional way of using IDS/IPS In a more traditional infrastructure we have the perimeter firewall that is responsible for the \u0026quot;environment\u0026quot; policies, enforcing policies between the environments and allowing/blocking different types of the services from each environment to communicate. In such an scenario it is often also the same perimeter firewall that is enabled with IDS/IPS. In a datacenter full of virtualized workload this leads to hairpinning the traffic to a centralized appliance for inspection with the consequence of reducing performance, a lot of unnecessary traffic is sent out to the physical infrastructure to reach the perimeter firewall and sent back again. The appliance is not in context of the workload its analyzing traffic from/to so its hard to be very specific enough when it comes to the right signatures etc. The picture below illustrates this:\nIDS/IPS with a centralized appliance\nNSX Distributed IDS and IPS To overcome the challenges of hairpinning traffic in an virtualized environment, we need to have the firewall, IDS and IPS enforced where the workload actually resides. This saves unnecessary traffic being sent out on the physical infrastructure if its not meant to go out and it also gives the network logics (firewall/IDS/IPS) to be part of the dataplane where the actual workload its supposed to protect resides and can have much more insight (being in context of) in whats going on. Things as knowing its a Ubuntu 20.04 and MySQL server you are protecting, makes it much easier to create the firewall policies but also much more pinpointed/granular IDS/IPS policies. This leads to very specific IDS/IPS rules, no false positives, better performance. This is where NSX Distributed IDS and IPS comes into play. Both the NSX Distributed Firewall and IDS/IPS runs on the same host as the virtual workload you are protecting. Its not necessary to redirect traffic, no need to change anything in the infrastructure, its as simple as just enabling the feature and create policies. Those policies can be created with an application centric perspective, as we have the ability to know the workload we are protecting as the below illustration:\nIDS/IPS polices with only workload relevant signatures\nIDS/IPS available on each hypervisor host\nHow to use IDS \u0026amp; IPS in VMware NSX-T To get started with IDPS in NSX is very easy, its already installed on your transport nodes when you have them enabled with NSX. In the following sections I will go through the different parts in the NSX gui that involves the IDPS part and finish up with an example of how to create policies.\nEnable IDPS, settings and signature updates When one log in to the NSX manager GUI one will see it is divided into different categories such as Networking, Security, and Inventory. IDPS is certainly a security feature of NSX so we will head over there.\nAfter clicking on the Security tab, it will take us to the Security Overview page:\nNSX Security Overview\nAs one can see this gives us a great summarized view over the different security parts in NSX, the IDPS, URL Analysis, the DFW, Anomalies. To see more details in the specific area click on the respective feature on the left side menu. In our case, this is the Distributed IDS/IPS menu.\nWhen inside the Distributed IDS/IPS section, head over to the settings page:\nSettings page of IDPS\nOn this page we can manage the signatures (versions), see the status on the signatures version, whether there is an update on the signature database, update and or adjust whether updates are done automatically. If we want to view the complete list of available signatures click on \u0026quot;View and Manage global signature set\u0026quot;. It should present us a list of all signatures:\nGlobal signature set\nHere we can search for a specific signature, or signatures based on the filter you choose in the top right corner. Say I want to search for signatures relevant to MySQL, I type in \u0026quot;mysql\u0026quot;:\nMysql filter\nBut I can also search for a specific CVE ID (one that we have recently been alerted on maybe):\nCVE-2017-12636\nOr a filter based on CVSS score, in this example 7.5:\nCVSS 7.5\nWe can also adjust the Global default action on specific signatures from Alert, Drop and Reject:\nBy hovering over the blue (!) we will be presented with an explanation of how this works:\nInstead of overriding the global setting for a set of signatures here, we will do this in the next section \u0026quot;IDPS Profiles\u0026quot;.\nFurther down on the same page is where we enable or disable the IDPS feature. It can be enabled on a vSphere cluster (a set of hosts managed by a vCenter) or standalone ESXi hosts. And its just as simple as clicking the enable button on the right side. It should turn green when enabled.\nNow that IDPS is enabled lets head over to Profiles.\nIDPS profiles The profiles section is where we create our application specific signatures we want to use in our IDPS policies (later). We want to adjust and narrow down the total amount of signatures to be used when we create our idps policies for our workload. If I want to create an IDPS policy for a specific application I should create a profile that matches this to reduce false positives, and maintain an optimal inspection with IDS/IPS as an added security feature on top of the Distributed Firewall. In my demo I am interested in only vulnerabilities affecting product \u0026quot;Linux\u0026quot;.\nLets start out by clicking \u0026quot;Add Profile\u0026quot; and create the profile.\nNew profile\nGive the profile a name and start adjusting the signatures we want to use, we start by deciding the Severity Category (Critical, High, Medium and Low). For more information on these categories look here: https://docs.vmware.com/en/VMware-NSX-T-Data-Center/3.1/administration/GUID-4343E565-7AC2-40C2-8B12-5FC14893A607.html\nAfter the severity category has been decided we can go ahead and further adjust the specifics we are looking for, please also make note of the IDS Signatures Included number as we proceed in our selection.\nBefore any selections done:\nDefault\nI will go ahead with Severity Critical \u0026amp; High\nThen I can proceed with a selection based Attack Types, Attack Targets, CVSS and Products Affected. I will just post a screenshot from each four options:\nSearch is possible\nSearch is possible\nI will adjust my profile with only Products Affected (this is done to justify the demo of IDS):\nI am satisfied with my selection from now. Let look at the profile now:\nNow I am down to 217 signatures in this specific profile. I also have an option now to override the default action when/if IDPS detects anything and how to respond. Click on the \u0026quot;Manage signatures for this profile\u0026quot; and we should be presented with the signatures relevant only for this profile (after the selection is done):\nThere is an important note to take here. If we only have Alert on the signature, and we want to create an IDPS policy with Detect \u0026amp; Prevent one must have the signature action to either Drop or Reject also. Its not sufficient to just create a policy with Detect \u0026amp; Prevent. That is brilliant if we have signatures in the same profile we don't want to be dropped, but we only want to be notified. Then we can have one rule with Detect \u0026amp; Prevent where some traffic is being dropped (Prevent) while the other is just notified (Detect). So if there is one CVE that you really should drop here you have the option to select just the few of those.\nLets apply the profile to a policy/rule in IDPS so it can act upon it.\nIDPS Policy Under rules in the Distributed IDPS section in NSX:\nClick + Add Policy, this creates a new section:\nName it and then click on the three dots on the left side to create a rule:\nShould now look like this:\nNow we need to fill in source, destination, our IDS Profile created earlier, where to apply the policy and action (mode). In the example below I have already created a couple of security groups (security groups explained in the NSX Distributed firewall article) so I just need to add them to the respective source/destination:\nIDS rule applied to groups used in source and destination\nClick publish and your rule is in effect immediately.\nAlso notice that I did not specify a Service in my rule, here you can be more specific by add the service also, if you know its HTTP for example.\nNow let us check if it detects something....\nDistributed IDS/IPS Events To view detected events, and drill down into the occured events, we must head over to the Events section in our Distributed IDS/IPS section.\nHere we can get a an overview with a timeline on when and where the events have occured.\nIf we look at the overview we can quickly notice if there has been any event in the last 24 hours, 48 hours, last 7 days or 14 days. Lets go through the Events page. Below is a screenshot showing last 24 hours.\nOverview\nTo change the timeline from default 24 hours, take a look at the top right corner and click on the arrow to get the drop down menu.\nNow look at the timeline view using Last 24 hours I can see that there has been 1 event represented by a an orange dot.\nThat tells me very quickly that there has atleast been detected an event. The color code tells me that the event is of severity High.\nIf there are many events in the timeline view, I can choose to filter out the severity I am interested in by unchecking the others.\nLegend color codes\nBy hovering over the orange dot I can get more information on the event:\nI can see what kind of event, and how many attempts of the same kind. If you look closer on the timeline view there are several dots represented. Those represent the other attempts of the same kind. The colored dots will only represent unique occurences within the given timeline. Its also possible to adjust your timeline further if you want to inspect events happening at a certain time within the 24 hours timeline by adjusting the blue sliders:\nAdjusted timeline view\nThen if I adjust it to say just before 18:10 and just after (where there is a dot) the orange dot will appear again as this event suddenly will be unique for this specific time. The bigger timeline view will be updated accordingly. Now I want to know more of this specific event. Look further down, it will be a list (if several unique events has occured represented, again within the timeline give above). The detailed list below will also update according to the adjusted timeline.\nIf one take a look at the below event it says under occurence \u0026quot;Single Attempt\u0026quot;, but I know there are multiple attempts, as I saw before I adjusted the timeline. I \u0026quot;reset\u0026quot; the timeline view back to the full 24 hours view it will be updated to multiple attempts.\nSingle attempt at the given timeline\nMultiple attempts over a longer timeline\nNow if one click on the arrow on the left side more information will be revealed.\nIn this view I can see the source (the attacker, where it is initiated from) and the destination (the target, victim of the \u0026quot;attack\u0026quot;). Intrusion activity, detected and prevented. The number of VMs affected. If one click on the number below VMs affected we will also see a list of VM(s) affected with names:\nIf I now go back to my profile defined earlier, I want to change the signature ID 2023995 to drop and also update my policy to Detect \u0026amp; Prevent. Lets see how this affects the detailed view.\nUpdated the profile\nUpdated the policy\nPrevented events\nWith the profile on this specific signature ID sat do drop and the policy sat to Detect \u0026amp; Prevent it also drops the specific attempt. Meaning I can have a good night sleep, or can I....?\nI should probably do something with the source also. But that should be easy now that we know what the source is.\n","link":"https://blog.andreasm.io/2021/10/19/vmware-nsx-ids-ips/","section":"post","tags":["nsx","ids","ips","security"],"title":"VMware NSX IDS \u0026 IPS"},{"body":"","link":"https://blog.andreasm.io/tags/automation/","section":"tags","tags":null,"title":"automation"},{"body":"","link":"https://blog.andreasm.io/tags/home-assistant/","section":"tags","tags":null,"title":"home-assistant"},{"body":"","link":"https://blog.andreasm.io/categories/home-automation/","section":"categories","tags":null,"title":"Home-Automation"},{"body":"When I finish up the other posts I have started on there will be content coming here also\n","link":"https://blog.andreasm.io/2021/07/14/the-home-automation-category/","section":"post","tags":["home-assistant","zwave","automation"],"title":"The Home Automation category"},{"body":"","link":"https://blog.andreasm.io/tags/zwave/","section":"tags","tags":null,"title":"zwave"},{"body":"NSX ALB has a very useful feature built-in, to function as DNS server for your domains defined in your NSX-ALB environment. Meaning that all host-records will be automatically resolved by fqdn as soon as the service is created.\nIf you have followed my other post about how to configure the AKO (Avi Kubernetes Operator) http://yikes.guzware.net/2020/10/08/ako-with-antrea-on-native-k8s-cluster/ you are familiar with creating DNS profiles in NSX-ALB. The first step in configuring NSX-ALB as DNS provider is to configure one or more domain names in NSX-ALB.\nLog in to the NSX-ALB controller GUI: -\u0026gt; Templates -\u0026gt; IPAM/DNS Profiles\nCreate a profile (if you dont already have one) give it a name and add one or more domain names:\nAfter you have configured a DNS profile head over to -\u0026gt; Administration -\u0026gt; Settings -\u0026gt; DNS Service in the controller GUI to create the DNS Virtual Service:\nFrom here one can click \u0026quot;Add Virtual Service\u0026quot; and configure the DNS VS. Go to the empty drop-down list (if you don't already have DNS VS configured) and click Create Virtual Service. Choose your cloud and VRF context.\nOne can also create a DNS VS directly from the Application menu, but by going this way some fields are automatically decided for the use of DNS Service.\nGive the service a name, and adjust accordingly. I have done some adjustment to the service in my environment such as Service port where I add 53 twice and choose Override TCP/UDP on the last one to get DNS on UDP port 53 also. I have also added my backend DNS servers as a pool to this VS to have them do lookup against those if the record is not found locally (not obligatory). Application-Domain-Name should have the same domain name as defined in your DNS Profile attached to your cloud.\nLeave Policies and Analytics as is. Under Advanced you choose your SE pool where your DNS VS should live. As a best practice the DNS SE should not be shared with other VS'es. So create a dedicated pool for the DNS-VS and if resources are scarce you can defined the SE group to only contain one SE (no redundancy for DNS VS though).\nIn my environment I have also created a Conditional forwarder on my backend DNS servers to look for DNS records in my domains defined in the N-ALB environment. Using NSX-ALB DNS provider service is a brilliant feature as I don't have to manually register any applications/services created in N-ALB or from K8s through AKO as this is all handled by the DNS service in N-ALB. My K8s applications can be spun up/down, without having to care about their dns records as this is all handled automatically by the NSX-ALB.\nDemo:\nTake an application created in NSX-ALB\nPing the dns name\nThat's it. Now NSX-ALB handles all your DNS records for you. If you want your backend DNS servers to forward the request to NSX-ALB head over to your DNS servers and either add a Conditional forwarder for your domains or add a Delegated zone as a sub-domain and point to your DNS-VS VIP.\n","link":"https://blog.andreasm.io/2021/07/12/configure-nsx-advanced-load-balancer-nsx-alb-as-dns-provider/","section":"post","tags":["avi","ako","loadbalancing","dns-service"],"title":"Configure NSX Advanced Load Balancer (NSX-ALB) as DNS provider"},{"body":"","link":"https://blog.andreasm.io/tags/dns-service/","section":"tags","tags":null,"title":"dns-service"},{"body":"Use NFS for your PVC needs If you are running vShere with Tanzu, TKG on vSphere or are using vSphere as your hypervisor for your worker-nodes you have the option to use the vSphere CSI plugin here. In Tanzu this is automatically configured and enabled. But if you are not so privileged to have vSphere as your foundation for your environment one have to look at other options. Thats where NFS comes in. To use NFS for your persistent volumes is quite easy to enable in your environment, but there are some pre-reqs that needs to be placed on your workers (including control plane nodes). I will go through the installation steps below.\nPre-reqs A NFS server available, already configured with shares exported. This could be running on any Linux machine in your environment that has sufficient storage to cover your storage needs for your PVCs. Or any other platform that can export NFS shares.\nInstall and configure This post is based on Ubuntu 20.04 as operating system for all the workers.\nThe first package that needs to be in place is the nfs-common package in Ubuntu. This is installed with the below command, and on all your workers (control-plane and workers):\n1sudo apt install nfs-common -y Now that nfs-common is installed on all workers we are ready deploy the NFS subdir external provisioner link. The below commands is done from your controlplane nodes, if not stated otherwise. I prefer to use Helm. If you dont have Helm installed head over here for how to install Helm. With Helm in place execute the following command to add the NFS Subdir External Provisioner chart:\n1helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/ Then we need to install the NFS provisioner like this:\n1helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ 2 --set nfs.server=x.x.x.x \\ 3 --set nfs.path=/exported/path Its quite self-explanatory but I will quickly to through it. --set nfs.server=x.x.x.x \\ needs to be updated with the IP address to your NFS server. --set nfs.path=/exported/path needs to be updated to reflect the path your NFS server exports.\nThats it actually, you know have a storageclass available in your cluster using NFS. The default values for the storageclass deployed without editing the NFS subdir external provisioner helm values looks like this:\n1$kubectl get storageclasses.storage.k8s.io 2NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE 3nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 51d Values, additional storageclasses If you want to change the default values get the values file, edit it before you deploy the NFS provisioner or get the file, edit it and update your deployment with helm upgrade To grab the values file run this command:\n1helm show values nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \u0026gt; nfs-prov-values.yaml If you want to add additional storageclasses, say with accessmode to RWX, you deploy NFS provisioner with a value file that has the settings you want. Remember to change the class name.\n1 name: XXXXX 2 3 # Allow volume to be expanded dynamically 4 allowVolumeExpansion: true 5 6 # Method used to reclaim an obsoleted volume 7 reclaimPolicy: Delete 8 9 # When set to false your PVs will not be archived by the provisioner upon deletion of the PVC. 10 archiveOnDelete: true 11 12 # If it exists and has \u0026#39;delete\u0026#39; value, delete the directory. If it exists and has \u0026#39;retain\u0026#39; value, save the directory. 13 # Overrides archiveOnDelete. 14 # Ignored if value not set. 15 onDelete: 16 17 # Specifies a template for creating a directory path via PVC metadata\u0026#39;s such as labels, annotations, name or namespace. 18 # Ignored if value not set. 19 pathPattern: 20 21 # Set access mode - ReadWriteOnce, ReadOnlyMany or ReadWriteMany 22 accessModes: XXXXXXX Above is a snippet from the values file.\nDeploying pods with special privileges If you need to deploy pods with special privileges, often mysql containers, you need to prepare your NFS server and filesystem permission for that. Otherwise they will not be able to write correctly to their PV when they are deployed. The example below is a mysql container that needs to create its own permission on the filesystem it writes to:\n1 spec: 2 securityContext: 3 runAsUser: 999 4 fsGroup: 999 So what I did to solve this is the following: I changed my NFS export to look like this on my NFS server: /path/to/share -alldirs -maproot=\u0026quot;root\u0026quot;:\u0026quot;wheel\u0026quot; Then I need to update the permissions on the NFS server filesystem with these commands and permissions:\n1$chown nobody:nogroup /shared/folder 2$chmod 777 /shared/folder ","link":"https://blog.andreasm.io/2021/07/12/kubernetes-persistent-volumes-with-nfs/","section":"post","tags":["persistent-storage","kubernetes","nfs","pvc"],"title":"Kubernetes Persistent Volumes with NFS"},{"body":"","link":"https://blog.andreasm.io/tags/nfs/","section":"tags","tags":null,"title":"nfs"},{"body":"","link":"https://blog.andreasm.io/tags/persistent-storage/","section":"tags","tags":null,"title":"persistent-storage"},{"body":"","link":"https://blog.andreasm.io/tags/pvc/","section":"tags","tags":null,"title":"pvc"},{"body":"","link":"https://blog.andreasm.io/categories/storage/","section":"categories","tags":null,"title":"Storage"},{"body":"Abbreviations used in this article:\nNSX Advanced Load Balancer = NSX-ALB K8s = Kubernetes (8 letters between the K and s in Kubernetes) SSL = Secure Sockets Layer AKO = Avi Kubernetes Operator (AVI now a VMware product called NSX Advanced Load Balancer) In one of my previous posts I wrote about how to install and configure AKO (Avi Kubernetes Operator) to use as Service type LoadBalancer.\nThis post will try to cover the basics of how to use NSX Advanced LoadBalancer by using AKO to handle our Ingress requests (ingress-controller).\nFor more information on Ingress in Kubernetes\nAn API object that manages external access to the services in a cluster, typically HTTP.\nIngress may provide load balancing, SSL termination and name-based virtual hosting.\nIngress Load Balancer Kubernetes Definition Within Kubernetes or K8s, a collection of routing rules that control how Kubernetes cluster services are accessed by external users is called ingress. Managing ingress in Kubernetes can take one of several approaches.\nAn application can be exposed to external users via a Kubernetes ingress resource; a Kubernetes NodePort service which exposes the application on a port across each node; or using an ingress load balancer for Kubernetes that points to a service in your cluster.\nAn external load balancer routes external traffic to a Kubernetes service in your cluster and is associated with a specific IP address. Its precise implementation is controlled by which service types the cloud provider supports. Kubernetes deployments on bare metal may require custom load balancer implementations.\nHowever, properly supported ingress load balancing for Kubernetes is the simplest, more secure way to route traffic. link\nGetting AKO ready While this post assumes AKO is already in place and working in your k8s clusters I will get straight to the parts that involve Ingress. If not head over here to read the official docs how to install Avi Kubernetes Operator (AKO). To verify that you AKO is ready to handle Ingress request, type in this and notice the output:\n1$ kubectl get ingressclasses.networking.k8s.io 2NAME CONTROLLER PARAMETERS AGE 3avi-lb ako.vmware.com/avi-lb \u0026lt;none\u0026gt; 50d Default secret for TLS Ingress As AKO expects all ingresses with TLS termination to have a key and certificate specified, there is a couple of ways this can be done. We can go with a \u0026quot;pr service\u0026quot;, meaning a dedicated set of keys and certs pr service or a default/common set of keys and certificates that AKO can use if nothing else is specified. To apply the common approach, one common key and certificate for one or more applications we need to add a secret for the AKO. Prepare your router-default.yaml definition file like this (the official docs wants you to put in your cert as is, that does not work so you need to base64 encode both keys and certs and paste in below):\n1apiVersion: v1 2kind: Secret 3metadata: 4 name: router-certs-default 5 namespace: avi-system 6type: kubernetes.io/tls 7data: 8 tls.key: \u0026#34;base64 encoded\u0026#34; 9 tls.crt: \u0026#34;base64 encoded\u0026#34; 10 alt.key: \u0026#34;base64 encoded\u0026#34; 11 alt.crt: \u0026#34;base64 encoded\u0026#34; 12 To base64 encode your keys and certs this can be done like this:\nIf you have the keys and certs in a file, from whatever linux terminal type in:\n1cat cert.pem | base64 -w 0 2cat key.pem | base64 -w 0 Then paste into the above yaml accordingly (tls.key:key, tls.crt:crt) If you have both ECDSA and RSA certs use the alt.key and alt.crt to apply both. As soon as everything is pasted, apply the yaml file kubectl apply -f router-defaults.yaml\nApply your ingress service To create an ingress service you need to define this in yaml. An example below:\n1apiVersion: networking.k8s.io/v1 2kind: Ingress 3metadata: 4 name: \u0026#34;NameOfIngress-Service\u0026#34; 5 namespace: \u0026#34;Namespaceofwhereyour-service-resides\u0026#34; 6 labels: 7 app: \u0026#34;ifyouwant\u0026#34; 8 annotations: 9 ako.vmware.com/enable-tls: \u0026#34;true\u0026#34; #\u0026#34;Indicates to Avi that you want to use TLS\u0026#34; 10spec: 11 ingressClassName: avi-lb #\u0026#34;The default class name for AVI\u0026#34; 12 rules: 13 - host: \u0026#34;FQDN\u0026#34; 14 http: 15 paths: 16 - pathType: Prefix 17 path: / 18 backend: 19 service: 20 name: \u0026#34;which-service-to-point-to\u0026#34; 21 port: 22 number: 80 Hostrules and HTTPrules As I mentioned earlier, you can also define specific rules pr server such as certificates. Here we can use Hostrules and HTTPrules to further adjust granular settings pr service. One Hostrule example below:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: name-of-your-rule 5 namespace: namespace-of-your-service 6spec: 7 virtualhost: 8 fqdn: must-match-hostname-above # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: \u0026#34;name-of-certificate\u0026#34; # This must be already defined in your AVI controller 14 type: ref 15 alternateCertificate: 16 name: \u0026#34;name-of-alternate-cert\u0026#34; # This must be already defined in your AVI controller 17 type: ref 18 sslProfile: System-Standard-PFS 19 termination: edge To get all features available head over to the official docs site here\nHostrule example from the official docs:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HostRule 3metadata: 4 name: my-host-rule 5 namespace: red 6spec: 7 virtualhost: 8 fqdn: foo.region1.com # mandatory 9 fqdnType: Exact 10 enableVirtualHost: true 11 tls: # optional 12 sslKeyCertificate: 13 name: avi-ssl-key-cert 14 type: ref 15 alternateCertificate: 16 name: avi-ssl-key-cert2 17 type: ref 18 sslProfile: avi-ssl-profile 19 termination: edge 20 gslb: 21 fqdn: foo.com 22 includeAliases: false 23 httpPolicy: 24 policySets: 25 - avi-secure-policy-ref 26 overwrite: false 27 datascripts: 28 - avi-datascript-redirect-app1 29 wafPolicy: avi-waf-policy 30 applicationProfile: avi-app-ref 31 analyticsProfile: avi-analytics-ref 32 errorPageProfile: avi-errorpage-ref 33 analyticsPolicy: # optional 34 fullClientLogs: 35 enabled: true 36 throttle: HIGH 37 logAllHeaders: true 38 tcpSettings: 39 listeners: 40 - port: 8081 41 - port: 6443 42 enableSSL: true 43 loadBalancerIP: 10.10.10.1 44 aliases: # optional 45 - bar.com 46 - baz.com Httprule example from the official docs:\n1apiVersion: ako.vmware.com/v1alpha1 2kind: HTTPRule 3metadata: 4 name: my-http-rule 5 namespace: purple-l7 6spec: 7 fqdn: foo.avi.internal 8 paths: 9 - target: /foo 10 healthMonitors: 11 - my-health-monitor-1 12 - my-health-monitor-2 13 loadBalancerPolicy: 14 algorithm: LB_ALGORITHM_CONSISTENT_HASH 15 hash: LB_ALGORITHM_CONSISTENT_HASH_SOURCE_IP_ADDRESS 16 tls: ## This is a re-encrypt to pool 17 type: reencrypt # Mandatory [re-encrypt] 18 sslProfile: avi-ssl-profile 19 destinationCA: |- 20 -----BEGIN CERTIFICATE----- 21 [...] 22 -----END CERTIFICATE----- ","link":"https://blog.andreasm.io/2021/07/11/k8s-ingress-with-nsx-advanced-load-balancer/","section":"post","tags":["avi","ako","ingress","kubernetes"],"title":"K8s Ingress with NSX Advanced Load Balancer"},{"body":" This is an introduction post to Antrea, what it is and which features it has.\nFor more details head over to:\nhttps://antrea.io/ and https://github.com/antrea-io/antrea\nFirst of, Antrea is a CNI. CNI stands for Container Network Interface. As the world moves into Kubernetes more and more, we need a good CNI to support everything from network to security within Kubernetes. Thats where Antrea comes into play.\nAntrea has a rich set of features such as:\nKubernetes-native: Antrea follows best practices to extend the Kubernetes APIs and provide familiar abstractions to users, while also leveraging Kubernetes libraries in its own implementation. Powered by Open vSwitch: Antrea relies on Open vSwitch to implement all networking functions, including Kubernetes Service load-balancing, and to enable hardware offloading in order to support the most demanding workloads. Run everywhere: Run Antrea in private clouds, public clouds and on bare metal, and select the appropriate traffic mode (with or without overlay) based on your infrastructure and use case. Windows Node support: Thanks to the portability of Open vSwitch, Antrea can use the same data plane implementation on both Linux and Windows Kubernetes Nodes. Comprehensive policy model: Antrea provides a comprehensive network policy model, which builds upon Kubernetes Network Policies with new features such as policy tiering, rule priorities and cluster-level policies. Troubleshooting and monitoring tools: Antrea comes with CLI and UI tools which provide visibility and diagnostics capabilities (packet tracing, policy analysis, flow inspection). It exposes Prometheus metrics and supports exporting network flow information which can be visualized in Kibana dashboards. Encryption: Encryption of inter-Node Pod traffic with IPsec tunnels when using an overlay Pod network. Easy deployment: Antrea is deployed by applying a single YAML manifest file. As this blog page evolves, it will cover in more technical posts how to use and configure Antrea with examples. As how this webpage is both handled by Antrea network and security features (yes, this wordpress page is hosted on a native K8s cluster with Antrea as CNI)\n","link":"https://blog.andreasm.io/2021/07/10/antrea-kubernetes-cni/","section":"post","tags":["antrea","kubernetes"],"title":"Antrea - Kubernetes CNI"},{"body":"This post will go through the Antrea-native policy resources and how to use them to secure your Kubernetes environment combined with K8s network policies.\nAbbreviations used in this article:\nContainer Network Interface = CNI Antrea Cluster Network Policies = ACNP Antrea Network Policies = ANP Kubernetes Network Policies = K8s policies or KNP When it comes to securing your K8s infrastructure it can be done in several layers in the infrastructure as a whole. This post will focus on the possibilities within the K8s cluster with features in the Antrea CNI. I will go through the Antrea-native policies (Antrea and K8s policies), with examples of when, how and where to use them. As Antrea-native policy resources can be used together with K8s network policies I will show that also.\nThis post will not cover the additional needs of security in your datacenter before reaching your K8s environment. I will cover this in a later post where I go through the use of NSX Distributed Firewall protecting your k8s clusters together with the security policies in Antrea.\nAntrea-native policy resources - short introduction Antrea comes with a comprehensive policy model. We have the Antrea Cluster Network Policies and Antrea Network Policies. The difference being between those two is that the ACNP applies to all objects on the cluster, where ANP is namespaced meaning its applies to objects within the namespace defined in the policy.\nAntrea policies are tiered, meaning the rules will be following an order of precedence. This makes it very useful to divide the rules into the right categories, having different resources in the organization responsible for the security rules. Sec-ops will have their rules in the beginning setting the \u0026quot;ground\u0026quot; before the application owners can set their rules and finally some block all rules that rules out all that is left. Antrea-native policy resources is working together with K8s network policies where the latter is placed in the Application tier below the ANP and ACNP policies. Antrea comes with a set of default tiers as of installation of Antrea, but there is also possible to add custom tiers. Read more here. Here are the default tiers:\n1Emergency -\u0026gt; Tier name \u0026#34;emergency\u0026#34; with priority \u0026#34;50\u0026#34; 2SecurityOps -\u0026gt; Tier name \u0026#34;securityops\u0026#34; with priority \u0026#34;100\u0026#34; 3NetworkOps -\u0026gt; Tier name \u0026#34;networkops\u0026#34; with priority \u0026#34;150\u0026#34; 4Platform -\u0026gt; Tier name \u0026#34;platform\u0026#34; with priority \u0026#34;200\u0026#34; 5Application -\u0026gt; Tier name \u0026#34;application\u0026#34; with priority \u0026#34;250\u0026#34; 6Baseline -\u0026gt; Tier name \u0026#34;baseline\u0026#34; with priority \u0026#34;253\u0026#34; Take a look at the diagram below and imagine your first rules is placed at the first left tier and more rules in the different tiers all the way to the right:\nMaking use of the Antrea-native policy resources In this section I will describe a demo environment with some namespaces and applications and then go through one way of using Antrea-native policy resources together with K8s network policies. In some of my Antrea-native policies I will use namespace selection based on the actual name of the namespace, but in others I will use selection based on labels. The first example below will make use of labels as selection criteria. I will explain why below. To read more on namespace selection: https://github.com/antrea-io/antrea/blob/main/docs/antrea-network-policy.md#select-namespace-by-name\nMeet the demo environment I will only use one K8s cluster in this example which is based on one master worker and two worker nodes.\nI will create three \u0026quot;environments\u0026quot; by using namespaces and label them according to which \u0026quot;environment\u0026quot; they belong to. The three namespaces will be \u0026quot;test-app\u0026quot;, \u0026quot;dev-app\u0026quot; and \u0026quot;prod-app\u0026quot;. I will then add a label on each namespace with the label env=test, env=dev and env=prod accordingly. Within each namespace I will spin up two pods (an Ubuntu 16.04 and Ubuntu 20.04 pod). What I would like to achieve is that each namespace represents their own environment to simulate scenarios where we do have prod, dev and test environments, where none of the environments are allowed to talk to each other. And by using labels, I can create several namespaces and place them into the correct environment by just \u0026quot;tagging\u0026quot; them with the correct labels (e.g env=dev).\nNow in the next section I will go through how I can isolate, and control those environments with Antrea-native policy resources.\nAntrea Cluster Network Policies (ACNP) The first thing I would like to do is to create some kind of basic separation between those environments/namespaces so they cant communicate with each other, and when that is done I can continue to create more granular application policies within each namespace or environment.\nThe first issue I meet is how to create as few rules as possible to just isolate what I know (the tree namespaces which are labeled with three different labels to create my \u0026quot;environments\u0026quot;) without having to worry about additional namespaces being created and those getting access to the \u0026quot;environments\u0026quot;. In this example I have already created three namespaces named \u0026quot;dev-app\u0026quot;, \u0026quot;prod-app\u0026quot; and \u0026quot;test-app\u0026quot;. I \u0026quot;tag\u0026quot; them in Kubernetes with their corresponding \u0026quot;env\u0026quot; labels: \u0026quot;dev\u0026quot;, \u0026quot;prod\u0026quot; and \u0026quot;test\u0026quot;. The reason I choose that approach is that I then can create several namespaces and choose which environment they belong to instead of doing the selection directly on the name of the namespace. I need to create an Antrea Cluster Network Policy as a \u0026quot;default\u0026quot; rule for each of my known environments so I can at a minimum guarantee that within each namespace or environment \u0026quot;intra-traffic\u0026quot; is allowed (traffic within the namespace or namespaces labeled with the same environment label). Meaning that when I do have a complete Antrea-native policy \u0026quot;framework\u0026quot; in place (with a blocking rule at the end taking care of all that is not specified) I can create new namespaces, but if they are not labeled correctly they will not be allowed to talk to any of my environments. This policy is applied at the SecurityOps tier:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: isolate-dev-env 5spec: 6 priority: 5 7 tier: SecurityOps 8 appliedTo: 9 - namespaceSelector: 10 matchLabels: 11 env: dev 12 ingress: 13 - action: Drop 14 from: 15 - namespaceSelector: 16 matchExpressions: 17 - key: env 18 operator: NotIn 19 values: 20 - dev 21 - action: Allow 22 from: 23 - namespaceSelector: 24 matchLabels: 25 env: dev 26 egress: 27 - action: Allow 28 to: 29 - namespaceSelector: 30 matchLabels: 31 env: dev What I am also doing with this ACNP is saying that if you are member of a namespace with the label \u0026quot;env=dev\u0026quot; you are allowed to ingress the namespace Dev, but not if you are not (\u0026quot;operator: NotIn\u0026quot; in the ingress namespaceSelector).\nAlso note that I am allowing specifically an Action allow to the dev environment within the same policy, the reason being is that when I apply my block-all-else rule later on it will block intra traffic within the same environment if it is not specifically specified that it is allowed in this rule.\nNow I just have to recreate this policy for my other two namespaces.\nAlso note that in the egress part I am only allowing traffic to namespace with the lavel \u0026quot;env=dev\u0026quot;. That does not mean right now that I will only allow traffic to anything else, because I don't have any block rules in my cluster yet. Antrea-native policy resources works a bit different than K8s network policies which only supports creating allow policies. In Antrea one can specify both DROP and ALLOW on both INGRESS and EGRESS. I left this with purpose, because I later in will go ahead create a block all rule. Now lets demonstrate this rule:\nBefore applying ACNP namespace isolation rule:\n1kubectl get pod -n dev-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-16-04-7f876959c6-p5nxp 1/1 Running 0 9d 10.162.1.57 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-6fb66c64cb-9qg2p 1/1 Running 0 9d 10.162.1.56 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 1kubectl get pod -n prod-app -o wide 2NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 3ubuntu-16-04-7f876959c6-sfdvf 1/1 Running 0 9d 10.162.1.64 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 4ubuntu-20-04-6fb66c64cb-z528m 1/1 Running 0 9d 10.162.1.65 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Above I list out the pods with IP addresses in my two namespaces \u0026quot;dev-app\u0026quot; and \u0026quot;prod-app\u0026quot;\nNow I enter bash of the Ubuntu20.04 pod in \u0026quot;dev-app\u0026quot; namespace and do a ping to the second pod in the same namespace and then ping another pod in the namespace Prod:\n1kubectl exec -it -n dev-app ubuntu-20-04-6fb66c64cb-9qg2p bash 1root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.57 2PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 364 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=0.896 ms 464 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.520 ms 564 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.248 ms 6 7root@ubuntu-20-04-6fb66c64cb-9qg2p:/# ping 10.162.1.64 8PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. 964 bytes from 10.162.1.64: icmp_seq=1 ttl=64 time=1.03 ms 1064 bytes from 10.162.1.64: icmp_seq=2 ttl=64 time=0.584 ms 1164 bytes from 10.162.1.64: icmp_seq=3 ttl=64 time=0.213 ms I have also written about Octant in one of my posts, in Octant there is an Antrea plugin which gives us some graphical features such as traceflow, which is also a powerful tool to showcase/troubleshoot security policies. Below is a screenshot from Octant before the rule is applied:\nAs you can see, this is allowed. Now I apply my \u0026quot;isolation\u0026quot; ACNP rules \u0026quot;prod\u0026quot;, \u0026quot;dev\u0026quot; \u0026amp; \u0026quot;test\u0026quot;. Also note; to list out the applied ACNP policies the command \u0026quot;kubetcl get acnp\u0026quot; can be used, without looking in a specific namespace as ACNP is clusterwide.\n1kubectl apply -f isolate.environment.prod.negated.yaml 2clusternetworkpolicy.crd.antrea.io/isolate-prod-env created 3kubectl apply -f isolate.environment.dev.negated.yaml 4clusternetworkpolicy.crd.antrea.io/isolate-dev-env created 5kubectl get acnp 6NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 7isolate-dev-env SecurityOps 5 1 1 19s 8isolate-prod-env SecurityOps 6 1 1 25s After they are applied I will try to do same as above:\n1ping 10.162.1.57 2PING 10.162.1.57 (10.162.1.57) 56(84) bytes of data. 364 bytes from 10.162.1.57: icmp_seq=1 ttl=64 time=3.28 ms 464 bytes from 10.162.1.57: icmp_seq=2 ttl=64 time=0.473 ms 564 bytes from 10.162.1.57: icmp_seq=3 ttl=64 time=0.190 ms 664 bytes from 10.162.1.57: icmp_seq=4 ttl=64 time=0.204 ms 7 8ping 10.162.1.64 9PING 10.162.1.64 (10.162.1.64) 56(84) bytes of data. Pinging within the same namespace works perfect, but to one of the other namespaces (here the Prod namespace) is not allowed. Works as intended.\nDoing the same traceflow with Octant again:\nSo to recap, this is how it looks like now:\nNow that I have created myself some isolated environments, I also need to allow some basic needs from the environments/namespaces to things such as DNS. So I will go ahead and create such a rule. Also have in mind that I haven't yet applied the last block-all-rule (so they can still reach those services as of now). I will make that rule applied when all the necessary rules are in place beforehand. In a greenfield environment those \u0026quot;baseline\u0026quot; rules would probably be applied as the first thing before the k8s cluster is taken into use.\nGoing down one tier to NetworkOps I will apply this Antrea Policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: allow-core-dns 5spec: 6 priority: 10 7 tier: NetworkOps 8 appliedTo: 9 - namespaceSelector: {} 10 egress: 11 - action: Allow 12 to: 13 - namespaceSelector: 14 matchLabels: 15 kubernetes.io/metadata.name: kube-system 16 ports: 17 - protocol: TCP 18 port: 53 19 - protocol: UDP 20 port: 53 This policy is probably for some rather \u0026quot;wide\u0026quot; as it just does a \u0026quot;wildcard\u0026quot; selection of all namespaces available and gives them access to the backend kube-system (where the coredns pods are located) on protocol TCP and UDP port 53. But again, this post is just to showcase Antrea policies and how they can be used and to give some insights in general.\nDNS allowed showed with Octant:\nOctant Traceflow\nFor now I am finished with the Cluster Policies and will head over to the Network Policies. This is the ACNP policies applied so far:\n1kubectl get acnp 2NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 3allow-core-dns NetworkOps 10 3 3 19h 4isolate-dev-env SecurityOps 5 1 1 22h 5isolate-prod-env SecurityOps 6 1 1 22h 6isolate-test-env SecurityOps 7 1 1 19h Antrea Network Policies (ANP) Antrea Network Policies are namespaced. So one of the use cases for ANP could be to create rules specific for the services running in the namespace. It could be allowing ingress on certain selections (e.g label/frontend) which runs the application I want to expose or which makes sense for clients to talk to which is the frontend part of the application. Everything else is backend services which not necessary to expose to clients, but on the other hand it could be that those services needs access to other backend services or services in other namespaces. So with ANP one can create ingress/egress policies by using the different selection options defining what is allowed in and out of the namespace.\nBefore I continue I have now applied my ACNP block-rule in the \u0026quot;Baseline\u0026quot; tier which just blocks all else to make sense of the examples used here in this section. Below is the policy (Note that I have excluded some namespaces in this rule) :\n1apiVersion: crd.antrea.io/v1alpha1 2kind: ClusterNetworkPolicy 3metadata: 4 name: block-all-whitelist 5spec: 6 priority: 1000 7 tier: baseline 8 appliedTo: 9 - namespaceSelector: 10 matchExpressions: 11 - key: ns 12 operator: NotIn 13 values: 14 - kube-system 15 - monitoring 16 ingress: 17 - action: Drop 18 from: 19 - namespaceSelector: {} 20 - ipBlock: 21 cidr: 0.0.0.0/0 22 egress: 23 - action: Drop 24 to: 25 - namespaceSelector: {} 26 - ipBlock: 27 cidr: 0.0.0.0/0 Antrea Network Policies Egress rule Now that I have applied my \u0026quot;whitelist\u0026quot; rule I must by now have all my necessary rules in place, otherwise things will stop working (such as access to DNS). I will now apply a policy which is \u0026quot;needed\u0026quot; by the \u0026quot;Prod\u0026quot; environment, which is access to SSH on a remote server. So the policy below is allowing Egress on TCP port 22 to this specific remote SSH server. Lets us apply this policy and test how this works out:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: NetworkPolicy 3metadata: 4 name: allow-prod-env-ssh 5 namespace: prod-app 6spec: 7 priority: 8 8 tier: application 9 appliedTo: 10 - podSelector: {} 11 egress: 12 - action: Allow 13 to: 14 - namespaceSelector: 15 matchLabels: 16 kubernetes.io/metadata.name: prod-app 17 - ipBlock: 18 cidr: 10.100.5.10/32 19 ports: 20 - protocol: TCP 21 port: 22 Just some sanity check before applying the above policy, I am still able to reach all pods within the same namespace due to my \u0026quot;isolation ACNP rules\u0026quot; even though I have my block all rule applied.\nBut I am not allowed to reach anything outside except what is stated in my DNS rule. If I try to reach my remote SSH server from my \u0026quot;Prod\u0026quot; namespace I am not allowed. To illustrate this I have entered \u0026quot;remoted\u0026quot; myself into bash on one of my pods in the Prod namespace and trying to ssh the remote server 10.100.5.10, below is the current result:\n1root@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 2ssh: connect to host 10.100.5.10 port 22: Connection timed out Ok, fine. What does my traceflow say about this also:\nNope cant do it also says Traceflow. Great, everything works out as planned. Now I must apply my policy to allow this.\n1root@ubuntu-20-04-6fb66c64cb-z528m:/# ssh andreasm@10.100.5.10 2andreasm@10.100.5.10\u0026#39;s password: Now I can finally reach my remote SSH server. To confirm again, lets check with Octant:\nThank you very much, that was very kind of you.\nTo summarize so far what we have done. We have applied the ACNP rules/policies to create environment/namespace isolation\n1NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 2allow-core-dns NetworkOps 10 3 3 21h 3block-all-whitelist baseline 20 2 2 25m 4isolate-dev-env SecurityOps 5 1 1 24h 5isolate-prod-env SecurityOps 6 1 1 31m 6isolate-test-env SecurityOps 7 1 1 21h And we have applied a rule to allow some basic \u0026quot;needs\u0026quot; such as DNS with the rule allow-core-dns. And the \u0026quot;block-all-whitelist\u0026quot; policy as a \u0026quot;catch all rule\u0026quot; to block everything not specified in the tiers.\nAnd then we have applied a more application/namespace specific policy with Antrea Network Policy to allow Prod to egress 10.100.5.10 on port 22/TCP. But I have not specified any ingress rules allow access to any services in the namespace Prod coming from outside the namespace. So it is a very lonely/isolated environment for the moment. This is how it looks like now:\nIn the next example I will create an ingress rule to another application that needs to be accessed from the outside.\nAntrea Network Policies Ingress rule To make this section a bit more \u0026quot;understanding\u0026quot; I will use another application as example to easier illustrate the purpose. The example I will be using is a demo application I have been using for several years - Yelb link\nThis application contains of four pods and looks like this:\nYelb diagram\nI already have the application up and running in my environment. But as this application is a bit more complex and contains a frontend which is useless if not exposed or reachable I am exposing this frontend with NSX Advanced Load Balancer. This makes it very easy for me to define the ingress rule as it means I only have to allow the load balancers IPs in my egress rule and not all potential IPs. The load balancers IP's is something I know. Some explanation around the load balancer IP's in my environment is that they are spun up on demand and just pick an IP from a pre-defined IP pool, so instead of pinning the ingress rule to the current IP they have I am bit wide and allow the IP range that is defined. Remember that this is a demo environment and does not represent a production environment. Lets take a look at the policy:\n1apiVersion: crd.antrea.io/v1alpha1 2kind: NetworkPolicy 3metadata: 4 name: allow-yelb-frontend 5 namespace: yelb 6spec: 7 priority: 5 8 tier: application 9 appliedTo: 10 - podSelector: 11 matchLabels: 12 app: yelb-ui 13 ingress: 14 - action: Allow 15 from: 16 - ipBlock: 17 cidr: 10.161.0.0/24 18 ports: 19 - protocol: TCP 20 port: 80 21 endPort: 80 22 name: AllowInYelbFrontend 23 enableLogging: false 24 egress: 25 - action: Allow 26 to: 27 - ipBlock: 28 cidr: 10.161.0.0/24 29 name: AllowOutYelbFrontend 30 enableLogging: false The CIDR in the rule above is the range my load balancers is \u0026quot;living\u0026quot; in. So instead to narrow it down too much in this demo I just allow the range 10.161.0.0/24 meaning I dont have to worry too much if they are getting new IP's within this range making my application inaccessible. When I apply this rule it will be placed in the tier \u0026quot;Application\u0026quot; (See one of the first diagrams in the beginning of this post) with a priority of 5. The basic policies for this application is already in place such as DNS and intra-communication (allowed to talk within the same namespace/environment which in this example is yelb-app/test).\nNow lets see how it is before applying the rule from the perspective of the NSX Advanced Load Balancer which is being asked to expose the frontend of the application:\nFrom NSX Advanced Load Balancer GUI / application showing pool is down\nAs one can see from the above screenshot, the Service Engines (the actual load balancers) are up and running but the application yelb-ui is down because the pool is unreachable. The pool here is the actual pod containing the frontend part of the Yelb app. So I need to apply the Antrea Network Policy to allow the Service Engines to talk to my pod. If I try to access the frontend via the load balancer VIP its also inaccessible:\nLets just apply the rule:\n1kubectl apply -f yelb.frontend.allow.yaml 2networkpolicy.crd.antrea.io/allow-yelb-frontend created 3kubectl get anp -n yelb 4NAME TIER PRIORITY DESIRED NODES CURRENT NODES AGE 5allow-yelb-frontend application 5 1 1 11s And now check the NSX Advanced Load Balancer status page and try to access the application through the VIP:\nNSX ALB is showing green\nWell that looks promising, green is a wonderful colour in IT.\nAnd the application is available:\nYelb UI frontend\nThe rule above only gives the NSX Advanced Load Balancers access to the frontend pod on port 80 of the application Yelb. All the other pods are protected by the \u0026quot;environment\u0026quot; block rule and the default block rule. There is one catch though, we dont have any rules protecting traffic between the pods. Lets say the frontend pod (which is exposed to the outside world) is compromised, there is no rule stopping any traffic coming from this pod to the others within the same namespace/and or environment. That is something we should apply.\nMicrosegmenting the application What we shall do now is to make sure that the pods that make up the application Yelb is only allowed to talk to each other on the necessary ports and nothing else. Meaning we create a policy that does a selection of the pods and apply specific rules for each pod/service within the application, if one refer to the diagram above over the Yelb application one can also see that there is no need for the fronted pod to be allowed to talk to the redis or db pod at all so that should be completely blocked.\nI will go ahead and apply a rule that does all the selection for me, and only allow what is needed for the application to work. The policy I will make use of here is K8s Native Network Policy kubernetes.io\nHere is the rule:\n1apiVersion: networking.k8s.io/v1 2kind: NetworkPolicy 3metadata: 4 name: yelb-cache 5 namespace: yelb 6spec: 7 podSelector: 8 matchLabels: 9 tier: cache 10 ingress: 11 - from: 12 - podSelector: 13 matchLabels: 14 tier: middletier 15 - namespaceSelector: 16 matchLabels: 17 tier: middletier 18 ports: 19 - protocol: TCP 20 port: 6379 21 policyTypes: 22 - Ingress 23--- 24apiVersion: networking.k8s.io/v1 25kind: NetworkPolicy 26metadata: 27 name: yelb-backend 28 namespace: yelb 29spec: 30 podSelector: 31 matchLabels: 32 tier: backenddb 33 ingress: 34 - from: 35 - podSelector: 36 matchLabels: 37 tier: middletier 38 - namespaceSelector: 39 matchLabels: 40 tier: middletier 41 ports: 42 - protocol: TCP 43 port: 5432 44 policyTypes: 45 - Ingress 46--- 47apiVersion: networking.k8s.io/v1 48kind: NetworkPolicy 49metadata: 50 name: yelb-middletier 51 namespace: yelb 52spec: 53 podSelector: 54 matchLabels: 55 tier: middletier 56 ingress: 57 - from: 58 - podSelector: {} 59 - namespaceSelector: 60 matchLabels: 61 tier: frontend 62 ports: 63 - protocol: TCP 64 port: 4567 65 policyTypes: 66 - Ingress 67--- 68apiVersion: networking.k8s.io/v1 69kind: NetworkPolicy 70metadata: 71 name: yelb-frontend 72 namespace: yelb 73spec: 74 podSelector: 75 matchLabels: 76 tier: frontend 77 ingress: 78 - from: 79 ports: 80 - protocol: TCP 81 port: 80 82 egress: 83 - to: 84 ports: 85 - protocol: TCP 86 port: 30567 87 policyTypes: 88 - Ingress 89 - Egress As I have already illustrated above I will not go through showing that the pods can talk to each other on all kinds of port, as they can because they do not have any restriction within the same namespace/environment. What I will go through though is how the above policy affects my application.\nThe rule applied:\n1kubectl apply -f k8snp_yelb_policy.yaml 2networkpolicy.networking.k8s.io/yelb-cache created 3networkpolicy.networking.k8s.io/yelb-backend created 4networkpolicy.networking.k8s.io/yelb-middletier created 5networkpolicy.networking.k8s.io/yelb-frontend created 6 7kubectl get networkpolicies.networking.k8s.io -n yelb 8NAME POD-SELECTOR AGE 9yelb-backend tier=backenddb 80s 10yelb-cache tier=cache 80s 11yelb-frontend tier=frontend 80s 12yelb-middletier tier=middletier 80s So to illustrate I will paste a diagram with the rules applied, and go ahead an see if I am allowed and not allowed to reach pods on ports not specified.\nYelb diagram with policies\nThe first thing I will try is to see if the frontend pod can reach the appserver on the specified port 4567:\nOctant Antrea Traceflow\nAnd the result is in:\nNow, what if I just change the port to something else, say DNS 53... Will it succeed?\n","link":"https://blog.andreasm.io/2021/07/10/antrea-network-policies/","section":"post","tags":["network-policies","antrea","security"],"title":"Antrea Network Policies"},{"body":"","link":"https://blog.andreasm.io/tags/informational/","section":"tags","tags":null,"title":"informational"},{"body":"This post will go through one way of securing your workloads with VMware NSX. It will cover the different tools and features built into NSX to achieve a robust and automated way of securing your workload. It will go through the use of Security Groups, how they can be utilized, and how to create security policies in the distributed firewall section of NSX-T with the use of the security groups.\nIntroduction to NSX Distributed Firewall If we take a look inside a modern datacenter we will discover very soon that there is not so much bare metal anymore (physical server with one operating system and often many services to utilize the resources), most workload today is virtualized. From a network perspective the traffic pattern has shifted from being very much north/south to very much east/west. A typical traffic distribution today between north/south and east/west is a 10% (+/-) north/south and 90%(+/-) east/west. When the traffic pattern consisted of a high amount north/south it made sense to have our perimeter firewall regulate and enforce firewall rules in and out of the DC and between server workload. Due to server virtualization a major part of the DC the workload consist of many virtual machine instances with very specific services and \u0026quot;intra\u0026quot; communication (east/west) is a large part. It is operationally a tough task to manage a perimeter firewall to be the \u0026quot;policy enforcer\u0026quot; between workload in the east/west \u0026quot;zone\u0026quot;. It is also very hard for a discrete appliance to be part of the context (it is outside of the dataplane/context of the workload it is trying to protect).. Will delve into this in more detail later in the article. Will also illustrate east/west and north/south traffic pattern.\n","link":"https://blog.andreasm.io/2021/07/10/microsegmentation-with-vmware-nsx/","section":"post","tags":["informational"],"title":"Microsegmentation with VMware NSX"},{"body":"When starting out a microsegmentation journey with VMware NSX it will be very important to have a tool that gives you all the visibility and insight you need. This is crucial if you dont know your applications requirements in detail and to make the right decisions in defining your NSX security policies.\nNSX Intelligence is your tool for that. This post is just to show a couple of screenshots of how it looks, and the next post will go more into detail how it works and how to use it.\n","link":"https://blog.andreasm.io/2021/07/10/nsx-intelligence-quick-overview/","section":"post","tags":["informational"],"title":"NSX Intelligence - quick overview"},{"body":"","link":"https://blog.andreasm.io/categories/nsx-t/","section":"categories","tags":null,"title":"nsx-t"},{"body":"","link":"https://blog.andreasm.io/categories/vmare-nsx-intelligence/","section":"categories","tags":null,"title":"vmare nsx intelligence"},{"body":"","link":"https://blog.andreasm.io/categories/vmware-nsx/","section":"categories","tags":null,"title":"vmware nsx"},{"body":"This post will cover the steps to bring up a Kubernetes cluster in Ubuntu, then how to install and configure Antrea as CNI and how to install and configure NSX Advanced Load Balancer to use as a servicetype Load Balancer in the k8s environment with the use of Avi Kubernetes Operator.\nAbbreviations used in this post:\nNSX Advanced Load Balancer = NSX ALB Avi Kubernetes Operator = AKO Kubernetes = k8s Container Network Interface = CNI Load Balancer = lb Introduction to this post When working with pods in a k8s cluster there is often the use of nodePort, clusterIP and LoadBalancer. In a three tiered application very often only one of the tier is necessary to expose to the \u0026quot;outside\u0026quot; (outside of the k8s cluster) world so clients can reach the application. There are several ways to do this, and one of them is using the service Load Balancer in k8s. The point of this post is to make use of NSX ALB to be the load balancer when you call for the service load balancer in your application. There are some steps needed to be done to get this up and running, as will be covered in this post, and when done you can enjoy the beauty of automatically provisioned lb's by just stating in your yaml that you want a lb for the specific service/pod/application and NSX ALB will take care of all the configuration needed to make your application available through a VIP. One of the steps involved in making this happen is deploying the AKO in your k8s cluster. AKO runs as a pod and is very easy to deploy and configure.\nDiagram over topology Deploy Kubernetes on Ubuntu 20.04 Prepare the Worker and Master nodes. For this part the initial process was taken from here with modifications from my side.\nThis will be a small 3-node cluster with 1 Master and 2 Worker Nodes.\nInstall 3 Ubuntu VMs, or one and clone from that.\n1sudo hostnamectl set-hostname \u0026lt;hostname\u0026gt; By using Ubuntu Server as image make sure openssh is installed.\nDisable Swap\n1sudo swapoff -a Edit fstab and comment out the swap entry.\nVerify with the command:\n1free -h Install packages on the master and worker nodes: On all nodes: 1sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl Add Kubernetes repository:\nAdd key to Apt:\n1sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add - Add repo:\n1cat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list 2deb https://apt.kubernetes.io/ kubernetes-xenial main 3EOF Install Kubeadm, Kubelet and Kubectl\n1sudo apt-get update 2 3sudo apt-get install -y kubelet kubeadm kubectl Note, this will install default Kubernetes release in apt repo from your distro, if you want to install a specific version, follow this:\nOverview of differente Kubernets versions and dependices:\n_https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-arm64/Packages\n_Lets say we want to install Kubeadmin 1.18.9:\n1sudo apt-get install kubelet=1.18.9-00 kubectl kubeadm=1.18.9-00 Check the below link if you can install a later version of kubectl than kubeadmin\nhttps://kubernetes.io/docs/setup/release/version-skew-policy/\nThe latter apt-get install command install kubelet version 1.18.9 and kubeadm 1.18.9 but kubectl will be the default release in apt (1.19.x for me).\nAfter the the three packages have been installed, set them on hold:\n1sudo apt-mark hold kubelet kubeadm kubectl 2 3kubelet set on hold. 4kubeadm set on hold. 5kubectl set on hold. Install Docker container runtime:\n1sudo apt-get install docker.io -y On the master-node init the Kubernetes master worker: 1sudo kubeadm init --pod-network-cidr=10.162.0.0/16 --apiserver-cert-extra-sans apihost.corp.local Change the CIDR accordingly to match your defined pod-network. It comes down to if you want to do a routable or nat‚Äôed toplogy. And by using the ‚Äìapiserver-cert-extra variable it will generate the certs to also reflect the dns-name, making it easier to expose this with a name instead of just the ip of the master worker.\nOn the worker-nodes, join the control plane of the master worker: 1sudo kubeadm join apihost.corp.local:6443 --token \u0026lt;toke\u0026gt; --discovery-token-ca-cert-hash \u0026lt;hash\u0026gt; The token and hash is presented to you after you have init‚Äôd the master-node in the previous step above.\nAfter you have joined all you worker nodes to the control-plane you have a working Kubernetes cluster, but without any pod-networking plugin (CNI), this will be explained a bit later and is also the reason for creating this guide.\nTo make it easier to access your cluster copy the kube config to your $HOME/.kube folder:\n1mkdir -p $HOME/.kube 2 3sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 4sudo chown $(id -u):$(id -g) $HOME/.kube/config You can now to a kubectl get pods --all-namespaces and notice that the coredns pods have status pending. That is because there is not pod networking up and running yet.\nInstall Antrea as the CNI **To read more about Antrea, go here:\n**https://github.com/vmware-tanzu/antrea\nTo get started with Antrea is very easy.\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/getting-started.md\nI will post a dedicated post for Antrea, covering Antrea Policies and other features in Antrea.\nOn the master worker:\nDecide first which version/release of Antrea you want (latest as of this writing is v1.1.1), this could depend on certain dependencies with other solutions you are going to be using. In my case I am using VMware Advanced LoadBalancer AKO v1.4.2 (Previously AVI Networks) as Service LB for my pods/apps. See this for compatibility guide: https://avinetworks.com/docs/ako/1.4/ako-compatibility-guide/\nTag the release you want to be installed of Antrea:\n_TAG=v1.1.1\n_Again, this is all done on the master-worker\nThen apply:\nkubectl apply -f [https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml](https://github.com/vmware-tanzu/antrea/releases/download/$TAG/antrea.yml)\nThis will automatically create the needed interfaces on all the workers. After a short time check your pods:\n1Kubectl get pods --all-namespaces 2kube-system antrea-agent-bzdkx 2/2 Running 0 23h 3kube-system antrea-agent-lvdxk 2/2 Running 0 23h 4kube-system antrea-agent-zqp6d 2/2 Running 0 23h 5kube-system antrea-controller-55946849c-hczkw 1/1 Running 0 23h 6kube-system antrea-octant-f59b76dd9-6gj82 1/1 Running 0 10h 7kube-system coredns-66bff467f8-6qz2q 1/1 Running 0 23h 8kube-system coredns-66bff467f8-cd6dw 1/1 Running 0 23h Coredns is now running and antrea controller/agents have been installed.\nIf you look at your worker nodes, they have also been configured with some extra interfaces:\n4: ovs-system:\n5: genev_sys_6081:\n6: antrea-gw0:\n**You now have a working container network interface, what now?\n**Wouldn‚Äôt it be cool to utilize a service load balancer to easily scale and expose your frontends?\nInstall AVI Kubernetes Operator Here comes AKO (Avi Kubernetes Operator), VMware Advanced LoadBalancer\nTo read more about AKO visit: https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes and https://avinetworks.com/docs/ako/1.4/ako-release-notes/\nTo be able to install AKO there are some prereqs that needs to be done on both your k8s cluster and NSX-ALB controller.\nPrepare NSX-ALB for AKO Lets start with the pre-reqs on the NSX-ALB controller side by logging into the controller GUI.\nDisclaimer, I have followed some of the official documentation here: https://avinetworks.com/docs/ako/1.2/ako-installation/ but I couldn‚Äôt follow everything there as it did not work in my environment.\nThis guide assumes NSX-ALB has been configured with a default-cloud with vCenter and the networks for your VIP subnet, node-network already has been defined as vDS portgroups in vCenter. In additon to basic knowledge of NSX-ALB.\nIn the NSX-ALB Controller\nCreate a dedicated SE group for the K8s cluster you want to use with AVI, define it the way you would like:\nCreate DNS and IPAM profiles\nIn IPAM define this:\nName, cloud and the VIP network for the SEs frontend facing IP. The network that should be reachable outside the node-network.\nGo to infrastructure and select your newly created IPAM/DNS profiles in your default-cloud:\nWhile you are editing the default-cloud also make sure you have configured these settings:\nDHCP is for the management IP interface on the SEs. The two other options ‚ÄúPrefer Static‚Äù \u0026amp; Use Static‚Äù I have to use otherwise it will not work, the AKO pod refuses to start, crashloopbackoff. And I am not doing BGP on the Avi side, only from the T0 of NSX. One can use the default global routing context as it can be shared, no need to define a custom routing context.\nUnder network select the management network and your SE group created for AKO SEs. And ofcourse enable DHCP:\nNow we are done on the NSX-ALB controller side. Lets jump back to our K8s Master worker to install AKO.\nThe initial steps here I have taken from:\nhttps://www.vrealize.it/2020/09/15/installing-antrea-container-networking-and-avi-kubernets-operator-ako-for-ingress/\nInstall AKO: AKO is installed with helm, so we need to install helm on our master worker:\nsudo snap install helm --classic\nOr whatever preferred way to install packages in Ubuntu.\n**Create a namespace for the AKO pod:\n**\n1 kubectl create ns avi-system **Add AKO incubator repository\n**\n1 helm repo add ako https://projects.registry.vmware.com/chartrepo/ako **Search for available charts and find the version number:\n**\n1helm search repo 2 3 NAME CHART VERSION\tAPP VERSION\tDESCRIPTION 4 ako/ako 1.4.2 1.4.2 A helm chart for Avi Kubernetes Operator 5 Download AKO values.yaml\n1 helm show values ako/ako --version 1.4.2 \u0026gt; values.yaml This needs to be edited according to your needs. See the comments in the value.yaml and you will se whats needed to be updated.\nI will publish my value.yaml file here and comment what I edited.\nI will remove the default comments and replace them with my own, just refer to de default comments by looking at the original yaml file here:\nhttps://raw.githubusercontent.com/avinetworks/avi-helm-charts/master/charts/stable/ako/values.yaml\n1# Default values for ako. 2# This is a YAML-formatted file. 3# Declare variables to be passed into your templates. 4 5replicaCount: 1 6 7image: 8 repository: avinetworks/ako 9 pullPolicy: IfNotPresent 10 11 12### This section outlines the generic AKO settings 13AKOSettings: 14 logLevel: \u0026#34;INFO\u0026#34; #enum: INFO|DEBUG|WARN|ERROR 15 fullSyncFrequency: \u0026#34;1800\u0026#34; # This frequency controls how often AKO polls the Avi controller to update itself with cloud configurations. 16 apiServerPort: 8080 # Specify the port for the API server, default is set as 8080 // EmptyAllowed: false 17 deleteConfig: \u0026#34;false\u0026#34; # Has to be set to true in configmap if user wants to delete AKO created objects from AVI 18 disableStaticRouteSync: \u0026#34;false\u0026#34; # If the POD networks are reachable from the Avi SE, set this knob to true. 19 clusterName: \u0026#34;GuzK8s\u0026#34; # A unique identifier for the kubernetes cluster, that helps distinguish the objects for this cluster in the avi controller. // MUST-EDIT 20 cniPlugin: \u0026#34;\u0026#34; # Set the string if your CNI is calico or openshift. enum: calico|canal|flannel|openshift 21 22### This section outlines the network settings for virtualservices. 23NetworkSettings: 24 ## This list of network and cidrs are used in pool placement network for vcenter cloud. 25 ## Node Network details are not needed when in nodeport mode / static routes are disabled / non vcenter clouds. 26 # nodeNetworkList: [] 27 nodeNetworkList: 28 - networkName: \u0026#34;Native-K8s-cluster\u0026#34; 29 cidrs: 30 - 192.168.0.0/24 # NODE network 31 subnetIP: \u0026#34;10.150.4.0\u0026#34; # Subnet IP of the vip network 32 subnetPrefix: \u0026#34;255.255.255.0\u0026#34; # Subnet Prefix of the vip network 33 networkName: \u0026#34;NativeK8sVIP\u0026#34; # Network Name of the vip network 34 35### This section outlines all the knobs used to control Layer 7 loadbalancing settings in AKO. 36L7Settings: 37 defaultIngController: \u0026#34;true\u0026#34; 38 l7ShardingScheme: \u0026#34;hostname\u0026#34; 39 serviceType: \u0026#34;ClusterIP\u0026#34; #enum NodePort|ClusterIP 40 shardVSSize: \u0026#34;SMALL\u0026#34; # Use this to control the layer 7 VS numbers. This applies to both secure/insecure VSes but does not apply for passthrough. ENUMs: LARGE, MEDIUM, SMALL 41 passthroughShardSize: \u0026#34;SMALL\u0026#34; # Control the passthrough virtualservice numbers using this ENUM. ENUMs: LARGE, MEDIUM, SMALL 42 43### This section outlines all the knobs used to control Layer 4 loadbalancing settings in AKO. 44L4Settings: 45 defaultDomain: \u0026#34;\u0026#34; # If multiple sub-domains are configured in the cloud, use this knob to set the default sub-domain to use for L4 VSes. 46 47### This section outlines settings on the Avi controller that affects AKO\u0026#39;s functionality. 48ControllerSettings: 49 serviceEngineGroupName: \u0026#34;k8s2se\u0026#34; # Name of the ServiceEngine Group. 50 controllerVersion: \u0026#34;20.1.1\u0026#34; # The controller API version 51 cloudName: \u0026#34;Default-Cloud\u0026#34; # The configured cloud name on the Avi controller. 52 controllerIP: \u0026#34;172.18.5.50\u0026#34; 53 54nodePortSelector: # Only applicable if serviceType is NodePort 55 key: \u0026#34;\u0026#34; 56 value: \u0026#34;\u0026#34; 57 58resources: 59 limits: 60 cpu: 250m 61 memory: 300Mi 62 requests: 63 cpu: 100m 64 memory: 75Mi 65 66podSecurityContext: {} 67 # fsGroup: 2000 68 69 70avicredentials: 71 username: \u0026#34;admin\u0026#34; 72 password: \u0026#34;PASSWORD\u0026#34; 73 74 75service: 76 type: ClusterIP 77 port: 80 78 79 80persistentVolumeClaim: \u0026#34;\u0026#34; 81mountPath: \u0026#34;/log\u0026#34; 82logFile: \u0026#34;avi.log\u0026#34; Remember that YAML‚Äôs are indentation sensitive.. so watch your spaces üòâ\n**Deploy the AKO controller:\n**helm install ako/ako --generate-name --version 1.4.2 -f avi-values.yaml --namespace=avi-system\nOne can verify that it has been installed with the following command:\nhelm list --namespace=avi-system\nand\nKubectl get pods --namespace=avi-system\nNAME¬†READY¬†STATUS¬†RESTARTS¬†AGE\nako-0¬†1/1¬†Running¬†0¬†15h\nOne can also check the logs of the pod:\nkubectl logs --namespace=avi-system ako-0\nIf you experience a lot of restarts, go through your config again, I struggled a lot to get it running in my lab the first time after I figured out there were some configs I had to to. I suspected some issues with the pod itself, but the problem was on the NSX-ALB controller side and values.yaml parameters.\nNow to test out the automatic creation of SE and ingress for your frontend install an application and change the service type to use loadBalancer. Everything is automagically created for you. Monitor progress in the NSX-ALB Controller and vCenter.\nNSX-ALB deploys the SE OVAs, as soon as they are up and running they will be automatically configured and you can access your application through the NSX-ALB VIP IP. You can of ofcourse scale the amount of SEs and so on from within the Avi controller.\nThe ips on the right side is the pods, and my frontend/public facing IP is:\nWhich can also be received by using:\nkubectl get service --namespace=app\nNAME¬†TYPE¬†CLUSTER-IP¬†EXTERNAL-IP¬†PORT(S)¬†AGE\nfrontend-external¬†LoadBalancer¬†10.96.131.185¬†10.150.4.2¬†80:31874/TCP¬†15h\nNow we have all the power and features from NSX Advanced LoadBalancer, with full visibility and logging. What about monitoring and features of Antra, the CNI plugin?\nInstall Octant - opensource dashboard for K8s https://github.com/vmware-tanzu/octant\nOctant with Antrea Plugin as a POD\nhttps://github.com/vmware-tanzu/antrea/blob/master/docs/octant-plugin-installation.md\nUpgrading the components used in this blog Upgrade Antrea Say you are running Antrea version v0.9.1 and want to upgrade to v0.10.1. Do a rolling upgrade like this:\nFirst out is the Antrea-Controller!\nkubectl set image deployments/antrea-controller antrea-controller=antrea/antrea-ubuntu:v0.10.1 --namespace=kube-system\nCheck status on upgrade:\nkubectl rollout status deployments/antrea-controller --namespace=kube-system\nThis upgrades the Antrea Controller, next up is the Antrea Agents.\nUpgrading the Antrea-agents:\nkubectl set image daemonset/antrea-agent antrea-agent=antrea/antrea-ubuntu:v0.10.1 --namespace=kube-system\nAs you are doing the rolling upgrades, one can monitor the process by following the pods or use Octant and get real-time updates when the agents have been upgraded.\nDoing changes in the antrea yaml file, if changes is not updated, do a rollut restart:\nkubectl rollout restart daemonset --namespace=kube-system antrea-agent\n","link":"https://blog.andreasm.io/2020/10/08/nsx-advanced-loadbalancer-with-antrea-on-native-k8s/","section":"post","tags":["kubernetes","avi","ako","antrea"],"title":"NSX Advanced LoadBalancer with Antrea on Native K8s"},{"body":"To be able to deploy an Edge node or nodes in your lab or other environment where you only have 2 physical nic you must be able to deploy it on a N-VDS switch as you have already migrated all your kernels etc to this one N-VDS switch.\nBut trying to do this from the NSX-T 2.4 manager GUI you will only have the option to deploy it to VSS or VDS portgroups, the N-VDS portgroups are not visible at all.\nSo, I followed this blog by Amit Aneja which explains how this works.\nSo after I read this blog post I sat out to try this. I had to write down the api-script he used by hand because I could not find it when I searched for a example I could use. By using PostMan I filled out this:\n1{ \u0026#34;resource\\_type\u0026#34;: \u0026#34;EdgeNode\u0026#34;, 2 3\u0026#34;display\\_name\u0026#34;: \u0026#34;YourEdgevCenterInventoryName\u0026#34;, \u0026#34;tags\u0026#34;: \\[\\], \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34;¬†(Your edge MGMT IP adress) \\], \u0026#34;deployment\\_config\u0026#34;: { \u0026#34;vm\\_deployment\\_config\u0026#34;: { \u0026#34;placement\\_type\u0026#34;: \u0026#34;VsphereDeploymentConfig\u0026#34;, \u0026#34;vc\\_id\u0026#34;: \u0026#34;YourvCenterIDFromNSXTManager\u0026#34;, \u0026#34;management\\_network\\_id\u0026#34;: \u0026#34;YourLSPortGroupIDFromNSXTManager\u0026#34;, \u0026#34;default\\_gateway\\_addresses\u0026#34;: \\[ \u0026#34;X.X.X.X\u0026#34; \\], \u0026#34;compute\\_id\u0026#34;: \u0026#34;YourClusterIDFromNSXTManager\u0026#34;, \u0026#34;allow\\_ssh\\_root\\_login\u0026#34;: true, \u0026#34;enable\\_ssh\u0026#34;: true, \u0026#34;hostname\u0026#34;: \u0026#34;yourEdge\\_FQDNName\u0026#34;, \u0026#34;storage\\_id\u0026#34;: \u0026#34;YourDataStoreIDfromNSXTManager\u0026#34;, \u0026#34;management\\_port\\_subnets\u0026#34;: \\[ { \u0026#34;ip\\_addresses\u0026#34;: \\[ \u0026#34;YourEdgeIPMGMT\\_AddressAgain\u0026#34; \\], \u0026#34;prefix\\_length\u0026#34;: 24 } \\], 4 5\u0026#34;data\\_network\\_ids\u0026#34;: \\[ \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_OverLayVLAN(NotTheHostOverlayVLAN)\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34;, \u0026#34;This\\_IS\\_the\\_ID\\_of\\_Your\\_LS\\_PortGroup\\_for\\_the\\_Edge\\_VLANUplink1\u0026#34; \\] }, \u0026#34;form\\_factor\u0026#34;: \u0026#34;SMALL\u0026#34;, 6 7\u0026#34;node\\_user\\_settings\u0026#34;: { \u0026#34;cli\\_username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;root\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34;, \u0026#34;cli\\_password\u0026#34;:\u0026#34;YourPasswordHere\u0026#34; 8 9} } 10 11} Then POST it to your NSX-T manager from Postman and after a short blink, the Edge is deployed, and you have to add it as a transport node in the NSX-T manager. Here it is important that you do this right at once, because (as I found out) this is a one-time config GUI where the first time you will be able to choose the right fp-eth nics. If you try to edit the edge deployment a second time it switches back to only showing the VDS/VSS portgroups. Then you have to redeploy.\nExample screenshots:\nRemember that the Uplink VLANs will belong to their own N-VDS (which you have already defined in their respective Transport Zone) which will not be created on the host, but the Edges. The first N-VDS are already in place on the Hosts. Its only the last two NICs which will be on their own Edge N-VDS switches.\nI am not saying this is a best practice or the right way to do this, but it works in my lab environment so I can fully test out the latest NSX-T 2.4 and continue playing with PKS (when we get CNI plugins for NSX-T 2.4... are we there yet? are we there yet, are we there yet..... its hard to wait ;-) )\n","link":"https://blog.andreasm.io/2019/03/09/deploy-nsx-t-2.4-edge-nodes-on-a-n-vds-logical-switch/","section":"post","tags":["nsx"],"title":"Deploy NSX-T 2.4 Edge Nodes on a N-VDS Logical Switch"},{"body":"","link":"https://blog.andreasm.io/categories/troubleshooting/","section":"categories","tags":null,"title":"Troubleshooting"},{"body":"If you have missing objects in the firewall section before upgrading from NSX-T 2.1 to 2.2 you will experience a General Error in the GUI, on the Dashboard, and in the Firewall section of the GUI. You will even get general error when doing API calls to list the DFW sections¬†https://NSXMGRIP/api/v1/firewall/sections:¬†{ \u0026quot;module\\_name\u0026quot; : \u0026quot;common-services\u0026quot;, \u0026quot;error\\_message\u0026quot; : \u0026quot;General error has occurred.\u0026quot;, \u0026quot;details\u0026quot; : \u0026quot;java.lang.NullPointerException\u0026quot;, \u0026quot;error\\_code\u0026quot; : \u0026quot;100\u0026quot; }\nIf you have upgraded the fix is straight forward. Go to the following KB and dowload the attached jar file.\nUpload this jar file to the NSX-T manager by logging in with root and do a scp command from where you downloaded it. ex: \u0026quot;scp your\\_username@remotehost:nsx-firewall-1.0.jar /tmp\u0026quot;\nThen replace the existing file with the one from the kb article placed here: /opt/vmware/proton-tomcat/webapps/nsxapi/WEB-INF/lib#\nReboot\n","link":"https://blog.andreasm.io/2018/09/05/general-error-nsx-t-manager-firewall-section/","section":"post","tags":["distributed-firewall","nsx","troubleshooting"],"title":"\"General Error\" NSX-T Manager Firewall Section"},{"body":"","link":"https://blog.andreasm.io/tags/distributed-firewall/","section":"tags","tags":null,"title":"distributed-firewall"},{"body":"","link":"https://blog.andreasm.io/tags/troubleshooting/","section":"tags","tags":null,"title":"troubleshooting"},{"body":"","link":"https://blog.andreasm.io/categories/virtualization/","section":"categories","tags":null,"title":"Virtualization"},{"body":"","link":"https://blog.andreasm.io/tags/vsphere/","section":"tags","tags":null,"title":"vsphere"},{"body":"After upgrading vCenter (VCSA) to 6.0 U3 and upgrading the vSphere Replication Appliance to 6.1.2 the plugin in vCenter stops working. Follow this KB\nAnd this KB to enable SSH (for those who are unfamiliar with how to enable SSH in a *nix environment):\n","link":"https://blog.andreasm.io/2017/04/10/vsphere-replication-6.1.2-vcenter-plugin-fails-after-upgrade-vcenter-6.0-u3/","section":"post","tags":["troubleshooting","vsphere","vsphere-replication"],"title":"vSphere Replication 6.1.2 vCenter plugin fails after upgrade (vCenter 6.0 U3)"},{"body":"","link":"https://blog.andreasm.io/tags/vsphere-replication/","section":"tags","tags":null,"title":"vsphere-replication"},{"body":"Lead Solution Engineer @VMware. Been working as an IT consultant since 2004. Where my key areas were delivering professional services on VMware solutions specializing in VMware vSphere, VMware NSX, VSAN, DR-solutions and Hybrid-Cloud. I think I have touched all products coming from VMware since 2003. Some of my certifications are VCIX-DCV, VCIX-NV.\nAreas of interest: Virtulization, Neworking, Security, Linux, OpenSource solutions, BSD, DiY projects, home-automation etc..\nI created this blog page for many years ago where one of the purposes was to use it as my \u0026quot;knowledge\u0026quot; base for different stuff I deploy in my lab. It primarily focuses around my line of work (solutions such as VMware NSX and Tanzu), but will also add other topics out of work scope (different DiY projects, home automation).\n* Wikis, howtos, tips/trick and troubleshooting issues mainly within my own field of competence, as a contribution back to the community for all the help they have given me over the years. Which has been, and still is, very appreciated. Sharing is actually caring.\nAll views expressed on this site is solely my own personal views, not my employer or partners.\n","link":"https://blog.andreasm.io/about/","section":"","tags":null,"title":"About"},{"body":"Lead Solution Engineer @VMware. Been working as an IT consultant since 2004. Where my key areas were delivering professional services on VMware solutions specializing in VMware vSphere, VMware NSX, VSAN, DR-solutions and Hybrid-Cloud. I think I have touched all products coming from VMware since 2003. Some of my certifications are VCIX-DCV, VCIX-NV.\nAreas of interest: Virtulization, Neworking, Security, Linux, OpenSource solutions, BSD, DiY projects, home-automation etc..\nI created this blog page for many years ago where one of the purposes was to use it as my \u0026quot;knowledge\u0026quot; base for different stuff I deploy in my lab. It primarily focuses around my line of work (solutions such as VMware NSX and Tanzu), but will also add other topics out of work scope (different DiY projects, home automation).\n* Wikis, howtos, tips/trick and troubleshooting issues mainly within my own field of competence, as a contribution back to the community for all the help they have given me over the years. Which has been, and still is, very appreciated. Sharing is actually caring.\nAll views expressed on this site is solely my own personal views, not my employer or partners.\n","link":"https://blog.andreasm.io/page/2016-07-14-about/","section":"page","tags":null,"title":"About"},{"body":"","link":"https://blog.andreasm.io/page/","section":"page","tags":null,"title":"Pages"},{"body":"","link":"https://blog.andreasm.io/search/","section":"","tags":null,"title":"Search"},{"body":"","link":"https://blog.andreasm.io/series/","section":"series","tags":null,"title":"Series"}]